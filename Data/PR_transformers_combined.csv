pr_number,created_at,merged_at,author,number_of_comments,number_of_review_comments,number_of_commits,lines_of_code_changed,number_of_files_changed,number_of_reviewers,number_of_approvals,labels,time_to_first_response,number_of_assignees,review_duration,number_of_changes_requested,number_of_build_runs,number_of_build_failures,number_of_linked_issues,time_since_last_commit,test_coverage,number_of_reviews_requested,number_of_revisions,number_of_milestones,dependency_changes,comment_authors,issue_comments_text,reviews_text,issue_text,review_comments
35416,2024-12-25T16:49:57Z,2024-12-26T13:58:53Z,qgallouedec,2,0,1,2,1,1,1,[],1600.0,0,76138.0,0,0,0,0,22238.725866,,2,1,0,False,"['qgallouedec', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35416). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. ~Not related, but relate to the same lines: https://github.com/huggingface/transformers/pull/35207#issuecomment-2562016482 @yzhangcs~ Ignore: point raised wrongNice thanks for fixing it so quickly ! I'll merge this as others are out for a while and this is safe to merge ! ",Nice thanks for fixing it so quickly ! I'll merge this as others are out for a while and this is safe to merge ! ,"# What does this PR do?

Fix 

```
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
```

reported here: https://huggingface.co/answerdotai/ModernBERT-base/discussions/9#6769e0390519344317755b3b

Initially introduced in #35207

The solution does solve the issue, tested locally.
 
## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@muellerzr @SunMarc @ArthurZucker who reviewed #35207",
35207,2024-12-11T10:40:10Z,2024-12-23T15:16:38Z,qgallouedec,8,0,3,6,1,2,2,[],1601.0,0,1298596.0,0,0,0,0,31567.998331,,1,3,0,False,"['yzhangcs', 'qgallouedec', 'HuggingFaceDocBuilderDev', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35207). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Do the slow tests pass under this?

They pass locally. I get errors but I think they aren't related:

```
============================== short test summary info ==============================
FAILED tests/trainer/test_trainer.py::TrainerIntegrationTest::test_auto_batch_size_finder - AttributeError: 'DownloadConfig' object has no attribute 'use_auth_token'
FAILED tests/trainer/test_trainer.py::TrainerIntegrationTest::test_auto_batch_size_with_deepspeed - ModuleNotFoundError: No module named 'mpi4py'
FAILED tests/trainer/test_trainer.py::TrainerIntegrationTest::test_use_liger_kernel_patching - AssertionError: False is not true
FAILED tests/trainer/test_trainer_seq2seq.py::Seq2seqTrainerTester::test_finetune_bert2bert - TypeError: BertModel.forward() got an unexpected keyword argument 'num_items_in_batch'
======== 4 failed, 250 passed, 82 skipped, 89 warnings in 203.93s (0:03:23) ========= @ArthurZucker @muellerzr @SunMarc I'll let you merge when you think it's good. Merging! 🤗  @qgallouedec @SunMarc @ArthurZucker @muellerzr
Hi, I've identified that there still exists a potential issue with the loss reporting in the Trainer class. 
The problem appears to stem from changes introduced after version 4.45.2, specifically in this line:
https://github.com/huggingface/transformers/blob/24c91f095fec4d90fa6901ef17146b4f4c21d0a3/src/transformers/trainer.py#L3703
The current implementation only divides the loss scalar when `num_items_in_batch` is `None`. However, I believe we should divide the loss scalar regardless of the `num_items_in_batch value`, as was the case in `transformers<=4.45.2`.
This change has led to an unexpected doubling of the logged loss when `gradient_accumulated_steps > 1`. 
You can observe the differences by comparing versions:
https://github.com/huggingface/transformers/compare/v4.45.2...v4.46.0.

 @qgallouedec Hello, not sure If I understand correctly, does this commit mean `gradient_accumulated_steps` will never take effect, as `num_items_in_batch` always be calculated? @qgallouedec Just wondering have you observed consistant hehaviors in

https://github.com/huggingface/trl/issues/2456

when switching from transformers v4.45.2 to v4.46
I think this two are quite different regarding loss with gradient accumulation. I think you're wrong @yzhangcs:


if I apply your modification:

```diff
- if num_items_in_batch is None:
-     loss = loss / self.args.gradient_accumulation_steps
+ loss = loss / self.args.gradient_accumulation_steps
```

and run the following script with

- `gradient_accumulation_steps = 2` and   `per_device_train_batch_size = 3`
- `gradient_accumulation_steps = 1` and   `per_device_train_batch_size = 6`

```python
import torch
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import Dataset

num_batch = 32
gradient_accumulation_steps = 2  # or 1
per_device_train_batch_size = 3  # or 6
seq_len = 5

eff_batch_size = per_device_train_batch_size * gradient_accumulation_steps
dataset_len = num_batch * eff_batch_size

data = torch.arange(0, dataset_len * seq_len)
data = data.reshape(dataset_len, seq_len)
data = data.tolist()

model = AutoModelForCausalLM.from_pretrained(""Qwen/Qwen2.5-1.5B"").to(""cuda"")
dataset = Dataset.from_dict({""input_ids"": data, ""labels"": data})

args = TrainingArguments(
    output_dir=f""out_bs_{batch_size}_grad_{grad_accum_steps}_before"",
    per_device_train_batch_size= per_device_train_batch_size,
    gradient_accumulation_steps= gradient_accumulation_steps,
    logging_steps=2,
)

trainer = Trainer(model=model, args=args, train_dataset=dataset)

trainer.train()
```

You get the following results:

#### Without the modification (current main)
<img width=""1028"" alt=""Screenshot 2024-12-26 at 12 22 24"" src=""https://github.com/user-attachments/assets/3fdd7f7d-ab81-40ac-bb8f-f1557980793a"" />


#### With the modif

<img width=""1028"" alt=""Screenshot 2024-12-26 at 12 22 32"" src=""https://github.com/user-attachments/assets/3e0c6395-7c10-4dba-81f3-7f0752a35375"" />



After the suggested modification, altering the gradient accumulation steps alter the results. While before the modification, everything overlap nicely.

This one feels more like the right solution. Do the slow tests pass under this? (There is one in there for grad accum) Yes because in reality this is exactly the right solution: since we are no longer relying on accelerate to div the loss, we need to do so before `backward()` if we don't know our num items in the batch LGTM ! ","This one feels more like the right solution. Do the slow tests pass under this? (There is one in there for grad accum) Yes because in reality this is exactly the right solution: since we are no longer relying on accelerate to div the loss, we need to do so before `backward()` if we don't know our num items in the batch LGTM ! ","# What does this PR do?


Fixes https://github.com/huggingface/trl/issues/2456

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?
",
31325,2024-06-07T21:54:30Z,2024-08-01T12:42:08Z,n17s,15,20,8,241,4,3,2,[],211103.0,0,17436152.0,0,0,0,0,10352.385828,,0,8,0,False,"['amyeroberts', 'HuggingFaceDocBuilderDev', 'gante', 'n17s', 'ArthurZucker', 'ghadiaravi13']","cc @ArthurZucker @gante  I have incorporated the comment about removing the two legacy implementations and added a couple of tests that ensure the cache works the same way as the DynamicCache and its peak memory usage is lower. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_31325). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Is something holding this back? The failing tests are all marked as flaky. > Is something holding this back? The failing tests are all marked as flaky.

@n17s just the limited bandwidth on our end -- my apologies for the delay, the PR looks great! Rebasing should fix the CI issues, we've fixed them on `main` 🤗   > just the limited bandwidth on our end

@gante Oh sorry, I was not aware. I removed flash attention but this unmasked a bug related to synchronization of my non-blocking moving of tensors (never tested without it before😁). I have added explicit synchronization right before a tensor is needed on the device or is safe to be evicted. This makes the tests pass while still keeping all the moving non-blocking. 

Some tests are still failing but they seem unrelated and/or flaky. Since the tests have moved, I have rebased to the latest.  Gentle ping @ArthurZucker  I was off for a week! Back now, will review!  > Looks great, small Q about using cuda streams, we should also add this to the doc / show and example as I guess this works with compile no?

Not sure. How to test it? > You can just compile the model's forward and do this:

So this does not work with offloaded cache. The error message is 
```
AttributeError: 'NoneType' object has no attribute 'synchronize'
```
Stemming from
```python
    self.prefetch_stream.synchronize()
```

Based on my understanding of the discussion on [this pytorch issue](https://github.com/pytorch/pytorch/issues/92804) what is happening is that CUDA streams are not preserved by the current implementation of torch.compile. Since this seems to be a limitation with how torch.compile currently handles things, and may eventually be fixed according to the discussion, I'd say there's not much that can be done on our side.

In light of the recent discussion on the issue that started this PR (#30704), a static offloaded cache can handle the torch.compile case much better than this class. > What is missing here for me is mostly raise error to protect usage (on GPU) mostly, resolve merge conflicts and should be good to go

Merge conflicts have been resolved and I added a check in the constructor to protect usage. Fun fact, the class works even if inference happens on the CPU, as long as there's a GPU on the machine. (It's agreen on my side to merge otherwise!) > One thing missing is a bit of doc / doc with an exampe of how to easily activate this

Added some docs this cache is very helpful (for low-memory long-context inference)
thanks for adding it!@n17s The core of the PR LGTM, thank you for proposing and implementing this cool feature 💛 

To finalize this PR:
1. Let's add an integration test (see [this file](https://github.com/huggingface/transformers/blob/main/tests/test_cache_utils.py) for examples). In particular, I would test that a) using this cache yields the same results as the original `DynamicCache`; b) if we're on a GPU, we should see a lower memory consumption than with `DynamicCache`
2. If you have benchmarks (and a benchmark script), please share it in this PR for future reference! LGTM, thank you for iterating 🤗  Looks great, small Q about using cuda streams, we should also add this to the doc / show and example as I guess this works with compile no?  You can just compile the model's forward and do this: 
```
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
os.environ[""TOKENIZERS_PARALLELISM""] = ""false"" # silence warnings when compiling

device = ""cuda""
ckpt = ""model-to-test""

model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16)
model.to(device)

tokenizer = AutoTokenizer.from_pretrained(ckpt)

prompt = ""Why dogs are so cute?""
inputs = tokenizer(prompt, return_tensors=""pt"").to(""cuda"")

# Specify the max length (including both the prompt and the response)
# When calling `generate` with `cache_implementation=""static"" later, this is also used to create a `StaticCache` object
# with sequence length = `max_length`. The longer the more you will re-use it
model.generation_config.max_length = 128

# without `torch.compile`: each call takes ~ 5.0 seconds (on A100 80G + torch 2.3)
outputs = model.generate(**inputs, do_sample=False)
response = tokenizer.batch_decode(outputs)[0]
print(response)

# `torch.compile(model, ...)` is not recommended as you compile callbacks
# and full generate. We recommend compiling only the forward for now. 
# ""reduce-overhead"" will use cudagraphs. 
model.forward = torch.compile(model.forward, mode=""reduce-overhead"", fullgraph=True)
model.generation_config.cache_implementation = ""static""

# with `torch.compile` (on A100 80G + torch 2.3)
# 1st call: ~ 90 seconds
outputs = model.generate(**inputs, do_sample=False)
response = tokenizer.batch_decode(outputs)[0]
# 2nd call: ~ 60 seconds
outputs = model.generate(**inputs, do_sample=False)
response = tokenizer.batch_decode(outputs)[0]
# 3nd call: ~ 1.5 seconds
outputs = model.generate(**inputs, do_sample=False)
response = tokenizer.batch_decode(outputs)[0]
print(response)
```
What is missing here for me is mostly raise error to protect usage (on GPU) mostly, resolve merge conflicts and should be good to go Very nice! One thing missing is a bit of doc / doc with an exampe of how to easily activate this : ex with high beam search ! either in https://huggingface.co/docs/transformers/v4.43.2/en/generation_strategies or in https://huggingface.co/docs/transformers/en/perf_infer_gpu_one !  Very nice! Nice thanks for iterating!","@n17s The core of the PR LGTM, thank you for proposing and implementing this cool feature 💛 

To finalize this PR:
1. Let's add an integration test (see [this file](https://github.com/huggingface/transformers/blob/main/tests/test_cache_utils.py) for examples). In particular, I would test that a) using this cache yields the same results as the original `DynamicCache`; b) if we're on a GPU, we should see a lower memory consumption than with `DynamicCache`
2. If you have benchmarks (and a benchmark script), please share it in this PR for future reference! LGTM, thank you for iterating 🤗  Looks great, small Q about using cuda streams, we should also add this to the doc / show and example as I guess this works with compile no?  You can just compile the model's forward and do this: 
```
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
os.environ[""TOKENIZERS_PARALLELISM""] = ""false"" # silence warnings when compiling

device = ""cuda""
ckpt = ""model-to-test""

model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16)
model.to(device)

tokenizer = AutoTokenizer.from_pretrained(ckpt)

prompt = ""Why dogs are so cute?""
inputs = tokenizer(prompt, return_tensors=""pt"").to(""cuda"")

# Specify the max length (including both the prompt and the response)
# When calling `generate` with `cache_implementation=""static"" later, this is also used to create a `StaticCache` object
# with sequence length = `max_length`. The longer the more you will re-use it
model.generation_config.max_length = 128

# without `torch.compile`: each call takes ~ 5.0 seconds (on A100 80G + torch 2.3)
outputs = model.generate(**inputs, do_sample=False)
response = tokenizer.batch_decode(outputs)[0]
print(response)

# `torch.compile(model, ...)` is not recommended as you compile callbacks
# and full generate. We recommend compiling only the forward for now. 
# ""reduce-overhead"" will use cudagraphs. 
model.forward = torch.compile(model.forward, mode=""reduce-overhead"", fullgraph=True)
model.generation_config.cache_implementation = ""static""

# with `torch.compile` (on A100 80G + torch 2.3)
# 1st call: ~ 90 seconds
outputs = model.generate(**inputs, do_sample=False)
response = tokenizer.batch_decode(outputs)[0]
# 2nd call: ~ 60 seconds
outputs = model.generate(**inputs, do_sample=False)
response = tokenizer.batch_decode(outputs)[0]
# 3nd call: ~ 1.5 seconds
outputs = model.generate(**inputs, do_sample=False)
response = tokenizer.batch_decode(outputs)[0]
print(response)
```
What is missing here for me is mostly raise error to protect usage (on GPU) mostly, resolve merge conflicts and should be good to go Very nice! One thing missing is a bit of doc / doc with an exampe of how to easily activate this : ex with high beam search ! either in https://huggingface.co/docs/transformers/v4.43.2/en/generation_strategies or in https://huggingface.co/docs/transformers/en/perf_infer_gpu_one !  Very nice! Nice thanks for iterating!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #30704 

This PR introduces OffloadedCache. This is a KV cache implementation that reduces GPU memory usage in exchange for more CPU memory usage and a small increase in generation time. During the forward passes in `generate`, it only keeps two layers of KV cache on the device: the current layer and the next layer. All other layers are on the CPU and are prefetched/evicted as necessary.

It can be used by passing `cache_implementation=""offloaded""` in the `GenerationConfig` like this:
```python
        gen_config = GenerationConfig(
            cache_implementation=""offloaded"", 
            # other generation options such as 
            num_beams=4,
            num_beam_groups=2,
            num_return_sequences=4,
            diversity_penalty=1.0,
            max_new_tokens=50,
            early_stopping=True,
        )

        outputs = model.generate(
            inputs[""input_ids""],
            generation_config=gen_config
        )
```

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Issue #30704
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR. @ArthurZucker @gante 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->","I would overload these two functions with a `NotImplementedError` exception -- they are exclusively used for backwards compatibility, and we would like to get rid of them in the future 🤗  I have removed these implementations but did it the way it is recommended in the Python docs and left a comment in the code with a pointer to the docs. ```suggestion
            model_name, device_map=""auto"", torch_dtype=torch.float16
```
FA2 places additional requirements on the test. Given that it is not relevant for the test itself, we should remove it 🤗  ```suggestion
            model_name, device_map=""auto"", torch_dtype=torch.float16
```
(same comment as above) I have removed flash attention from the tests. Fixed. does that only work when using compile or in general ?  at some point we should warn if generating on CPU that GPU is required + if we use cuda stream this requires a hardware that supports cuda streams why don't we update the mapping that gives us the cahce class here?  CUDA streams should work in general. torch.compile docs don't mention any known issues related to streams. Hmm, the point of this class is you want to generate on the GPU but you have more CPU RAM than GPU RAM. If you are generating on the CPU you don't need this class at all. There is no such mapping? The only mappings I see in the code are
```python
 NEED_SETUP_CACHE_CLASSES_MAPPING = {""static"": StaticCache, ""sliding_window"": SlidingWindowCache, ""hybrid"": HybridCache}
 QUANT_BACKEND_CLASSES_MAPPING = {""quanto"": QuantoQuantizedCache, ""HQQ"": HQQQuantizedCache}
```
`OffloadedCache` does not need setup and is not a quantized cache. Yeah I was thinking about the `NEED_SETUP_CACHE_CLASSES_MAPPING`, but alrgith Added a check to protect this. let's rather raise en error here!  would make sense to explain how the streams work, basically one stream for prefetching, and one stream that computes can't the prefetch stream be used for that as well?  It should not! Eviction needs to be queued after all other operations on these tensors happening on the default stream. If we call it from prefetch stream, it is possible that the prefetch stream moves it out (and the memory allocator puts something else in that memory) before all read operations from the default stream are done. I will add a small comment in the code about this.  Fixed Done"
35348,2024-12-19T20:28:11Z,2024-12-24T12:21:59Z,NielsRogge,3,4,23,2270,17,4,1,"['New model', 'Vision']",1909.0,0,434244.0,0,0,0,0,169440.841354,,0,23,0,False,"['NielsRogge', 'HuggingFaceDocBuilderDev', 'xenova']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35348). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks for review, revert the drop path update cause it wasn't need. Looks like we can merge :)

One thing left: All model checkpoints are on the hub, would need to be transferred to the facebook org: https://huggingface.co/models?other=dinov2_with_registers (and then replace `nielsr` by `facebook` everywhere)

 @NielsRogge Would it be possible to adapt the `interpolate_pos_encoding` function to use the same approach as Dinov2 (but just with options for antialiasing):

https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/src/transformers/models/dinov2/modeling_dinov2.py#L78-L114

This current version has issues with tracing of the graph (see https://github.com/huggingface/transformers/pull/33226).Nice! The drop path update might be a bit breaking, you can import the function in modular IMO Yep feel free to merge in the meant time 🤗 thanks for updating !","Nice! The drop path update might be a bit breaking, you can import the function in modular IMO Yep feel free to merge in the meant time 🤗 thanks for updating !","# What does this PR do?

This PR adds DINOv2 with registers, this time using the new modular tool.

Fixes #27379

To do:

- [x] make @bernardzach a co-author","this is the old __init__ !  ```suggestion
    def drop_path(self, input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:
``` There is a mismatch between the models on the hub and the model_type specified here. See https://huggingface.co/facebook/dinov2-with-registers-small/blob/main/config.json#L18, it is `dinov2_with_registers`.

This causes an issue when loading the models and re-saving them (e.g., for finetuning or conversions)
 cc @ydshieh weird this wasn't caught by the tests, is this something we can add a test for?"
34815,2024-11-19T17:13:23Z,2024-12-24T18:32:45Z,vasqu,7,3,9,22,2,4,2,[],238927.0,0,3028811.0,0,0,0,0,178562.604504,,0,9,0,False,"['vasqu', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'MekkCyber']","Thanks for the fix @vasqu, LGTM ! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34815). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker 
See my comments above: 
> I tried looking into integrating the env validation function before even creating the quantizers but that would require a complete restructure, especially since the env is checked after creation to get additional info like device map etc. So I kept it as simple as I could and wrapped the unsafe stuff.

The problem is some quantizers need additional information that is not available at creation; another option I could see it to add a post_init to create those unsafe parts :) gentle ping @ArthurZucker  Not sure if it was fixed by recent quantizer compressed changes, let's resolve conflicts and good to merge Forgot about this PR myself :sweat_smile: should be good to merge now! Thx for the merges here, ci sometimes do be flaky tho will merge it once the ci is green @vasqu 🤗 Thanks for the fixes !  Looks good but I think we should always call `validate_environment` before even initializing the quantizer (so probably update super (`HfQuantizer`) to call it at init time no?  Thanks for explaining",Thanks for the fixes !  Looks good but I think we should always call `validate_environment` before even initializing the quantizer (so probably update super (`HfQuantizer`) to call it at init time no?  Thanks for explaining,"# What does this PR do?
Fixes some unsafe imports for GPTQ and CompressedTensors as well as an unsafe metadata check for GPTQ, i.e. metadata can't be checked if the package doesn't exist...

I tried looking into integrating the env validation function before even creating the quantizers but that would require a complete restructure, especially since the env is checked after creation to get additional info like device map etc. So I kept it as simple as I could and wrapped the unsafe stuff.

Minimal reproducible script to trigger gptq errors (when optimum is not installed):
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig


model_id = ""facebook/opt-125m""
tokenizer = AutoTokenizer.from_pretrained(model_id)
dataset = [""auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.""]
gptq_config = GPTQConfig(bits=4, dataset=dataset, tokenizer=tokenizer)


quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""auto"", quantization_config=gptq_config)
```


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #34765


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@SunMarc @MekkCyber ","Changed the order here so we can guarantee that the metadata check doesn't raise an error itself. Also separated the package checks but that's more of a preference of mine to differentiate, lmk if I should change this back to checking both at once. I think it's fine to have the checks separated, wdyt @SunMarc ? yes ! "
33141,2024-08-27T11:15:40Z,2024-09-30T12:47:18Z,mobicham,26,30,29,274,8,2,1,[],277.0,0,10292402.0,0,0,0,0,194035.632711,,0,29,0,False,"['rohit-gupta', 'SunMarc', 'blap', 'HuggingFaceDocBuilderDev', 'mobicham']","1/3
https://github.com/huggingface/transformers/pull/33141/commits/5cb7d81547908dea660f525be5f77d9065b6edeb 
Removed the `check_old_param` hack. 
The problem however is that `HQQLinear.state_dict` is huge, which makes loading extremely slow. So I added `run_expected_keys_check` which skips those checks for `HQQLinear` params. I am not sure if it's a clean way. If you just init a dummy `HQQLinear` you wouldn't get all the `state_dict` params anyway :thinking:  so if you disable that check it will complain that the parameters is not in the expected keys, let me know if there's a better way of doing this 2/3: Multi-gpu loading
Loading on multi-gpu looks like it's working fine. There's an issue with the BitBlas backend I just reported <a href=""https://github.com/microsoft/BitBLAS/issues/154"">here</a>
Forcing the input to use the same device was done on the <a href=""https://github.com/mobiusml/hqq/commit/1e620a91c3453c7655925cf9c39d1beed14d518e"">hqq</a> lib side.

3/3: state_dict on the same safetensor chunk
I run tests with different models and it's working fine (<a href=""https://gist.github.com/mobicham/5c469f6c9e8244fb31fef99e1d5ccb13""> gist</a>):
```Python
model_id  = 'meta-llama/Meta-Llama-3-8B-Instruct' #OK
model_id  = 'meta-llama/Meta-Llama-3-70B' #OK 
model_id = ""facebook/opt-125m"" #OK
model_id = ""meta-llama/Llama-2-13b-chat-hf"" #OK
model_id = ""microsoft/Phi-3-mini-128k-instruct"" #OK
model_id = ""google/gemma-2-9b-it"" #OK
model_id = ""google/gemma-2-2b"" #OK
```

 so I think for the moment we can leave it until someone reports some issue,  I can't reproduce the problem anyway.


Next steps:
-  Revisit the comments above (@mobicham )
- Change/disable settings for hqqConfig because now saving/loading doesn't support quant scales/zeros as well as meta-data offloading. Need to deprecate it as well on the hqq lib side and a new pip version 2.0.0 (@mobicham ) @SunMarc 
- Reverted back to `if isinstance(module, (torch.nn.Linear, HQQLinear)):` but we still need that  `run_expected_keys_check` otherwise it breaks
- Updated the default `HqqConfig` default params since `quant_scale`, `quant_zero`, and `offload_meta` are now deprecated. Also done on the hqq-lib side. I also updated the tests, the doc and made a new hqq lib pip release `0.2.0` Regarding this: https://github.com/huggingface/transformers/pull/33141#discussion_r1734388659
The issue is that to remove that additional check, we need to have all the HQQLinear dict keys for each layer in the list of expected keys. There are 19 keys  per HQQLinear module. For a small model like `LLama3-8B`, that means `32*7*19=4256` checks per parameter which is extremely slow The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33141). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. There are TODOs to be done before merging:
- Check if adding a bias on architectures that don't support the bias by default breaks the hqq model loading.
- Trying to get rid of `run_expected_keys_check` by updating the `expected_keys`. This will require some modification on the hqq lib side as well to return the list of all the valid keys of the state dict. Then bump up min hqq lib version in transformers. > There are TODOs to be done before merging:
> 
> * Check if adding a bias on architectures that don't support the bias by default breaks the hqq model loading.

✅  Checked, all good!

> * Trying to get rid of `run_expected_keys_check` by updating the `expected_keys`. This will require some modification on the hqq lib side as well to return the list of all the valid keys of the state dict. Then bump up min hqq lib version in transformers.

❌  Removed the `run_expected_keys_check` hack by extending `expected_keys` in the case where `HQQLinear` state dict is loaded but `Linear` is present instead. 
https://github.com/huggingface/transformers/pull/33141/commits/7e019b3619a2ba6972e409ce39b009210c467252
There's an issue with the bias on some architectures, I am investigating

Update: the second issue is resolved now ✅ You can test with this gist: https://gist.github.com/mobicham/701dd564c52590203ee09631425ad797 Nice could you just update a bit the description of the PR ?  @ArthurZucker just a friendly reminder to review this PR when you have a moment. Let me know if you need any clarifications or if there’s anything I can help with. Thank you very much :pray: 
 Waiting for this! Just for curiosity, what miss to merge? > Just for curiosity, what miss to merge?

Waiting for @mobicham to check the latest review and give me to heads-up to merge ! This should be done soon ! Also it looks like that there are some conflits to fix Thanks for iterating @mobicham! Merging! @mobicham minor documentation issue, but the transformers documentation page for quantization has a giant features matrix which still says serialization of HQQ models is not supported


https://huggingface.co/docs/transformers/main/quantization/overview Would you like to open a PR to fix this @rohit-gupta ?  @rohit-gupta thanks for flagging ! now model.save_pretrained(save_path) give this:



```

Traceback (most recent call last):
  File ""C:\Users\Admin\Desktop\Python\0.LLMs\hqq\hqq1b.py"", line 35, in <module>
    model.save_pretrained(save_path)
  File ""C:\Users\Admin\Desktop\Python\0.LLMs\hqq\venv\Lib\site-packages\transformers\modeling_utils.py"", line 2932, in save_pretrained
    state_dict_split = split_torch_state_dict_into_shards(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Admin\Desktop\Python\0.LLMs\hqq\venv\Lib\site-packages\huggingface_hub\serialization\_torch.py"", line 330, in split_torch_state_dict_into_shards
    return split_state_dict_into_shards_factory(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Admin\Desktop\Python\0.LLMs\hqq\venv\Lib\site-packages\huggingface_hub\serialization\_base.py"", line 108, in split_state_dict_into_shards_factory
    storage_id = get_storage_id(tensor)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Admin\Desktop\Python\0.LLMs\hqq\venv\Lib\site-packages\huggingface_hub\serialization\_torch.py"", line 382, in get_torch_storage_id
    if tensor.device.type == ""meta"":
       ^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'device'


```


 @blap is this related to the latest transformer changes? Otherwise, which hqq version causes this? > @blap is this related to the latest transformer changes? Otherwise, which hqq version causes this?

I think so. I didn't had this problem in the release of hqq in transformers.
hqq version: 0.2.3
transformers version: 4.47.0.dev0 > > @blap is this related to the latest transformer changes? Otherwise, which hqq version causes this?
> 
> I think so. I didn't had this problem in the release of hqq in transformers. hqq version: 0.2.3 transformers version: 4.47.0.dev0

@SunMarc do you know what was changed by any  chance? Transformers version 4.48.0.dev0 still has this problem... Any one from the HF team can track down this problem please? What changed ? Nothing on the hqq lib side changed much. @SunMarc ? Can you share your script @blap ? I'll have a look asap !  










> Can you share your script @blap ? I'll have a look asap !




```

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig

model_id      = ""mllmTeam/PhoneLM-1.5B""
repo          = ""PhoneLM-1.5B""
nbits         = 4
group_size    = None
axis          = 0
save_path     = repo+""-nbits""+str(nbits)+""-GS""+str(group_size)+""-Axis""+str(axis)+""-HQQ2""
cache_dir     = repo+""-cache""
device        = ""cpu""
compute_dtype = torch.float16

#Quantize
quant_config  = HqqConfig(nbits=nbits, group_size=group_size, axis=axis, quant_scale=False, quant_zero=False)

#Load the model
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    torch_dtype=compute_dtype, 
    cache_dir=cache_dir,
    device_map=device, 
    quantization_config=quant_config,
    low_cpu_mem_usage=True,
    trust_remote_code=True
)

#Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)

# Save
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

```

Error:




```

Traceback (most recent call last):
  File ""C:\Users\Admin\Desktop\Python\0.LLMs\hqq\hqq1b.py"", line 32, in <module>
    model.save_pretrained(save_path)
  File ""C:\Users\Admin\Desktop\Python\0.LLMs\hqq\venv\Lib\site-packages\transformers\modeling_utils.py"", line 2971, in save_pretrained
    state_dict_split = split_torch_state_dict_into_shards(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Admin\Desktop\Python\0.LLMs\hqq\venv\Lib\site-packages\huggingface_hub\serialization\_torch.py"", line 369, in split_torch_state_dict_into_shards
    return split_state_dict_into_shards_factory(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Admin\Desktop\Python\0.LLMs\hqq\venv\Lib\site-packages\huggingface_hub\serialization\_base.py"", line 108, in split_state_dict_into_shards_factory
    storage_id = get_storage_id(tensor)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Admin\Desktop\Python\0.LLMs\hqq\venv\Lib\site-packages\huggingface_hub\serialization\_torch.py"", line 746, in get_torch_storage_id
    if tensor.device.type == ""meta"":
       ^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'device'


```





Nice ! Let's fix the issue regarding the torchao backend and we can merge this. I left a few comments Added a couple of comments !  Left a suggestion about axis Thanks for iterating ! Excited to see HQQ models on the hub 🔥 Nice update ! left a few comments ",Nice ! Let's fix the issue regarding the torchao backend and we can merge this. I left a few comments Added a couple of comments !  Left a suggestion about axis Thanks for iterating ! Excited to see HQQ models on the hub 🔥 Nice update ! left a few comments ,"Follow-up to https://github.com/huggingface/transformers/pull/32379
The goal of this PR is to add full support to save/load HQQ-quantized models directly in transformers. So far, serialization was done on the hqq-lib side via the `.pt` format which is not safe and doesn't work with very large models (>100B params) since the model is not sharded.

What was done during this PR:
- Make sure saving/loading HQQ-quantized models works properly.
- Make sure multi-gpu support works with the hqq backends (required some updates the hqq lib side)
- Make sure adding biases in architectures that do not have a bias by default works (biases are used in some HQQ-calibrated models)
- Added `update_expected_keys()` call in the quantizer. This allows loading quantized models that were initialized with `torch.nn.Linear` instead

Full gist to try it out: https://gist.github.com/mobicham/701dd564c52590203ee09631425ad797
","In my [PR](https://github.com/huggingface/transformers/pull/32379/files#:~:text=if%20isinstance(module%2C%20HQQLinear)%3A,), I have two cases: 
- if module is HQQLinear, we do a return 
- if module is a Linear, we replace it by hqqlinear + load_state dict 

Here, we do the same. This will make the loading way slower since we load the layer multiple times and it is probably what you are experiencing.  with the comment below, this might not be needed Reverting back to that version :+1:  Unfortunately we still need that check otherwise it breaks 
```Python
File ~/py310_env2/lib/python3.10/site-packages/accelerate/utils/modeling.py:358, in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp1
6_statistics, tied_params_map)
    355     return
    357 if old_value.device == torch.device(""meta"") and device not in [""meta"", torch.device(""meta"")] and value is None:
--> 358     raise ValueError(f""{tensor_name} is on the meta device, we need a `value` to put in on {device}."")
    360 if value is not None:
    361     if old_value.shape != value.shape:

ValueError: weight is on the meta device, we need a `value` to put in on cuda:0
``` Could you explain what happens here ? I don't understand why we are getting this error if we don't set that Maybe sure to push the commit ! To me, it's still not reverted.  it would be nice to keep the multi-gpu test  Are these deprecated in the latest version of hqq ? Would it make sense to enforce the user to download the latest version of hqq ?  Breaking change here. Is this the recommended value now ?  Yeah, I saw that transformers installs the latest version of hqq in the docker file Yes, I have been planning to do it for a long time, because only `axis=1` supports the fast backends like `torchao` and `bitblas`, so the new default value is `axis=1` instead of `axis=0` Oh I think I removed it by mistake, let me take a look at it It was not removed actually, it's still there: https://github.com/huggingface/transformers/blob/cbe219fe3ab5694f1fd2ace6597cb74bd8324d9a/tests/quantization/hqq/test_hqq.py#L121-L141  it;s here, when I reverted back, it broke the test, there was some issue with the values not matching. Let me try again
https://github.com/huggingface/transformers/blob/cbe219fe3ab5694f1fd2ace6597cb74bd8324d9a/src/transformers/quantizers/quantizer_hqq.py#L154-L178 Apologies for the confusion, it actually works with the reverted changes: https://github.com/huggingface/transformers/pull/33141/commits/2bb974c5e4faae8e110fa3bb42f22b0a1e5f7736 Then, we need to update `is_hqq_available` to force the user to install the latest hqq. Also, we can potentially let the user know that these args are deprecated if they set it in the latest version of hqq.  oh thx for checking !  Well, I will leave it to you but I think it will be great to let the user know that axis have been set to 1 since it supports fast backends. How I would do is to set axis = None and in the post_init, if axis is None, we set it to 1 + add a `logger.info` Like this https://github.com/huggingface/transformers/pull/33141/commits/9f7c23577b2f13423250dcb766dec0d421fe71ea ? `axis` is required and it should be either 0 or 1 actually. It's just that before it was 0 and now it's 1.
What should the `logger.info` say? I think the diff makes it seem like `axis` was removed, here you can see that `axis=1` is by default now. https://github.com/huggingface/transformers/blob/ff982b3b2c8e9f4580ad0a32b90c40b3dcae5985/src/transformers/utils/quantization_config.py#L186-L217 Answer here: https://github.com/huggingface/transformers/pull/33141/#issuecomment-2314984385 ```suggestion
        axis: Optional[int] = None,
```

Then add in the init : 
```py
if axis is None: 
   axis=1
   logger.info(""Setting axis=1 as faster backends such as torchao or bitblas are only compatible with it."")
``` ```suggestion
        axis (`Optional[int]`, *optional*, defaults to `None`):
``` see new review  yes could you also update the `validate_environment` function where this check is used to remind the user that he needs to upgrade their version ?  >  Also, we can potentially let the user know that these args are deprecated if they set it in the latest version of hqq.

if we see these in the kwargs, we need to warn them that they are deprecated.  https://github.com/huggingface/transformers/pull/33141/commits/383e0283a00ac6f2f1763d1a92bc0acba6332aa8 https://github.com/huggingface/transformers/pull/33141/commits/383e0283a00ac6f2f1763d1a92bc0acba6332aa8 `validate_environment` message mentioning at least `0.2.0`: https://github.com/huggingface/transformers/pull/33141/commits/4682a725c52ab2b2bb00e021b6e145d1f6d0abb6

Deprecated keys in kwargs message:
https://github.com/huggingface/transformers/pull/33141/commits/813ed627b3079f53e0423f0f24a3be78a30c169c"
35334,2024-12-19T03:08:43Z,2024-12-24T11:36:00Z,jiqing-feng,4,0,2,4,1,2,2,[],40001.0,0,462437.0,0,0,0,0,203617.877916,,1,2,0,False,"['jiqing-feng', 'HuggingFaceDocBuilderDev', 'Rocketknight1', 'MekkCyber']",cc @SunMarc @MekkCyber  Thanks for the PR @jiqing-feng LGTM 🔥 ! Can you update the branch please  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35334). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Updated to the main branch. Now it's ready to merge. Thanks!SGTM ! ,SGTM ! ,"The IPEX awq linear is inherited from GEMM awq linear, see [here](https://github.com/casper-hansen/AutoAWQ/blob/main/awq/modules/linear/gemm_ipex.py#L14). So it's safe to convert GEMM to IPEX when no Cuda is found. It will let users run AWQ model on a non-cuda device without modifying the quantization config to change the version.",
35366,2024-12-20T14:14:09Z,2024-12-24T09:53:57Z,ydshieh,2,0,2,253,1,1,1,[],1633.0,0,329990.0,0,0,0,0,209739.55819,,1,2,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35366). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. OK, I will just remove itThanks! Better to just remove it or rename the extension for it to not be run no? Or do the # actually skip everything inside? If so, LGTM!","Thanks! Better to just remove it or rename the extension for it to not be run no? Or do the # actually skip everything inside? If so, LGTM!","# What does this PR do?

Let's rework it for more security later.",
31512,2024-06-20T07:15:47Z,2024-12-23T15:36:16Z,statelesshz,15,5,3,7,1,5,3,['Quantization'],11342.0,0,16145554.0,0,0,0,0,230479.028847,,0,3,0,False,"['amyeroberts', 'Titus-von-Koeller', 'SunMarc', 'SlightwindSec', 'HuggingFaceDocBuilderDev', 'statelesshz', 'github-actions[bot]', 'younesbelkada']","cc @younesbelkada @SunMarc  Thanks @statelesshz ! 
Let us know also with @Titus-von-Koeller once you have a WIP branch on bitsandbytes as well to add that in the multi-backend ongoing effort Hey all!

I'm only aware of [a backend stub that was added on the multi-backend-refactor branch by @statelesshz](https://github.com/TimDettmers/bitsandbytes/pull/1223/files), no implementation logic. There wasn't much context given on the PR at the time, but since the implementation was yet to come, for me that was ok as is; as an incremental step.

@statelesshz Could you please elaborate a bit more what you mean by the following statement? Where does your implementation live right now?
> internally we first added NPU support based on bitsandbytes-0.42.0

Generally, there've been quite a lot of changes in 0.43.0 and even 0.43.1 (the setup logic), so it would be really advisable to already take those changes into account.

Regarding the timeline: we'll go into public alpha/beta testing of the Intel/AMD backends, finalize the refactoring (integrating with the Torch dispatcher through the torch.library API) in the next weeks and do a full release soon after. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.

Please note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_31512). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.

Please note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored. @younesbelkada @Titus-von-Koeller  Good day! I have submitted a PR to add nf4 quant/dequant support to Ascend NPU. The corresponding end-to-end test results are as follows:
https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1422#issuecomment-2490880479
PTAL 🤗  @SunMarc I have deleted the changes of ""Bnb4BitHfQuantizer"". But CI has an error when executing check_repository_consistency. The specific error information is as follows:
```
Traceback (most recent call last):
  File ""/root/transformers/utils/check_copies.py"", line 1110, in <module>
    check_copies(args.fix_and_overwrite, args.file)
  File ""/root/transformers/utils/check_copies.py"", line 860, in check_copies
    raise Exception(
Exception: Found the following copy inconsistencies:
- src/transformers/quantizers/quantizer_bnb_4bit.py: copy does not match quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer.update_device_map at line 271
Run `make fix-copies` or `python utils/check_copies.py --fix_and_overwrite` to fix them.
```
I tried to fix this problem locally based on the error message. After executing `python utils/check_copies.py --fix_and_overwrite`, it seemed that all the changes in this PR were rolled back. The specific result is shown in the figure below.
![image](https://github.com/user-attachments/assets/7cc3c5ab-0105-49ba-904c-aec6746c9d21)

Any guidance would be appreciated. Hey @statelesshz , this happens because this method have the following comment above: 
`# Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer.update_device_map`. So you just need to delete that line ! 
 > Hey @statelesshz , this happens because this method have the following comment above: `# Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer.update_device_map`. So you just need to delete that line !

Hey @SunMarc, thanks for your suggestion, it worked. CI reported that there was one test case failing, I think it's not caused by this PR. The failing test case can pass in my local environment:
![image](https://github.com/user-attachments/assets/40319e50-9f31-408e-9991-b5d1e65a0127)

cc @SunMarc @younesbelkada  > The failing test case can pass in my local environment:

Indeed this is not related. Can you try to rebase the PR ? I've also pinged Arthur for a final check before merging @SunMarc done! 🤗  Hey @ArthurZucker, could you help take a look at this PR, thanks :-) Hey @ArthurZucker , this PR addresses a key issue I'm also facing, and I'm hoping it can be merged soon. Thanks!+1 on @matthewdouglas comments ! LGTM otherwise LGTM !  Yep looks good to me ! 🤗 sorry for the delayed review!",+1 on @matthewdouglas comments ! LGTM otherwise LGTM !  Yep looks good to me ! 🤗 sorry for the delayed review!,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR can make bnb nf4 QLoRA out of the box on Ascend NPUs. 🤗 

Related PR: https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1422


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","We'll need a version check of bnb in the future for this check Nit: Let's move this to line 33 below with the other `is_xxx_available` imports. Same suggestion as in the 4bit quantizer. Apart from 8bit not being implemented on the bnb side yet, is there a reason this would not also be needed in the `Bnb8BitHfQuantizer` as well? @matthewdouglas Thanks for your review 🤗. The change to `Bnb8BitHfQuantizer` was just to make it easier for us to verify that support for bnb 8bit was correct. Apparently it was unnecessary and I have removed it."
35068,2024-12-03T23:13:24Z,2024-12-23T12:05:00Z,matthewdouglas,2,3,1,15,1,3,2,['Quantization'],1682.0,0,1702425.0,0,0,0,0,273751.666078,,0,1,0,False,"['BenjaminBossan', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35068). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. LGTM, thanks. The failing test `tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py::XLMRobertaXLModelTest::test_assisted_decoding_matches_greedy_search_1_same` appears to be unrelated.LGTM ! Left a question A welcome change! THanks 🤗 ",LGTM ! Left a question A welcome change! THanks 🤗 ,"# What does this PR do?

Simplifies the dequantization for bitsandbytes int8 weights. There is a similar PR open in [huggingface/peft#2245](https://github.com/huggingface/peft/pull/2245/files).

In the upcoming bitsandbytes release we will have an added API for for int8 dequantization. For backwards compatibility, a simplified (but functionally equivalent) dequantization operation is performed.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@SunMarc @BenjaminBossan 


","This line equivalent to what we had before ?  Yes. It is also the same way the selected 8bit weights are dequantized for fp16 compute in the LLM.int8() implementation: https://github.com/bitsandbytes-foundation/bitsandbytes/blob/7dca70049566b5b1c55cbd67e1cb191729a98152/bitsandbytes/autograd/_functions.py#L380

The method that this replaces was multiplying the quantized weights by a quantized identity matrix (int8 * int8 -> int32):
$W_{INT32} = I_{INT8}*W_{INT8} $

The dequantization feature `mm_dequant` was intended for dequantizing int32 matmul results, and as such performs $W_{FP16} = (W_{INT32} * (SC_{im} \bigotimes SCB)) / (127 * 127)$.

![image](https://github.com/user-attachments/assets/ac6dfd4c-8f9e-473d-a9cb-b2b714d07351)

In this new version, I'm avoiding the cost of creating the identity matrix, quantizing it, and performing the matmul realizing int32 values. Mathematically this is equivalent, and more importantly it's simpler and introduces less error.
 Nice ! "
35184,2024-12-10T16:05:53Z,2024-12-23T18:12:45Z,yonigozlan,1,0,4,67,3,1,1,[],1714.0,0,1130813.0,0,0,0,0,266214.771716,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35184). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice thanks 🤗 ,Nice thanks 🤗 ,"# What does this PR do?

Add a test in `ImageProcessingTestMixin`, which is only ran on fast image processors. The test compiles a fast image processor, run a processing pass and compares the result with an eager image processor.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

## Who can review?



<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35046,2024-12-02T20:22:13Z,2024-12-23T16:01:00Z,MekkCyber,1,0,3,5,3,1,1,[],1701.0,0,1798729.0,0,0,0,0,274120.007701,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35046). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?
Adding missing `logger.info` statements in some quantizers to inform users which dtype is set during quantization.

## Who can review ?
@SunMarc ",
35363,2024-12-20T13:32:04Z,2024-12-23T15:59:02Z,mfarre,4,1,8,149,2,3,2,[],2174.0,0,268019.0,0,0,0,0,274239.751026,,0,8,0,False,"['andimarafioti', 'HuggingFaceDocBuilderDev', 'mfarre', 'yonigozlan']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35363). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Let's wait for @yonigozlan to be sure! thanks @andimarafioti 
thanks @yonigozlan if you give me your LGTM I will merge the changes: I followed your suggestions adding some tests and adding your code proposal.

> Yes that looks much better! Just suggested a check to avoid hallucinations like this:
> 
> ```python
> messages = [
>     {
>         ""role"": ""user"",
>         ""content"": [
>             {""type"": ""image""},
>             {""type"": ""text"", ""text"": ""What do we see in this image?""},
>         ]
>     }
> ]
> prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
> inputs = processor(text=prompt, return_tensors=""pt"")
> inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
> 
> 
> # Generate
> generated_ids = model.generate(**inputs, max_new_tokens=50)
> generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)
> 
> print(generated_texts)
> 
> ['User:What do we see in this image?\nAssistant: The image depicts a scene from a historical or fictional setting, likely from the medieval period, given the attire and architecture. The central focus is on a large, ornate gate, which appears to be the entrance to a castle or a fortified structure.']
> ```
> 
> also in `Idefics3ProcessorTest` could we add a test for text only inference and one to check that an error is raised if we have an image in the conversation, but no images are passed to the processor?

 Looks great thanks @mfarre for fixing this! LGTM for me, but let's maybe wait for @ArthurZucker 's review before merging :)LGTM!
<img src=""https://media0.giphy.com/media/8fen5LSZcHQ5O/giphy.gif""/> Yes that looks much better! Just suggested a check to avoid hallucinations like this:
```python
messages = [
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image""},
            {""type"": ""text"", ""text"": ""What do we see in this image?""},
        ]
    }
]
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, return_tensors=""pt"")
inputs = {k: v.to(DEVICE) for k, v in inputs.items()}


# Generate
generated_ids = model.generate(**inputs, max_new_tokens=50)
generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)

print(generated_texts)

['User:What do we see in this image?\nAssistant: The image depicts a scene from a historical or fictional setting, likely from the medieval period, given the attire and architecture. The central focus is on a large, ornate gate, which appears to be the entrance to a castle or a fortified structure.']
```

also in `Idefics3ProcessorTest` could we add a test for text only inference and one to check that an error is raised if we have an image in the conversation, but no images are passed to the processor? Thanks for fixing 🤗 ","LGTM!
<img src=""https://media0.giphy.com/media/8fen5LSZcHQ5O/giphy.gif""/> Yes that looks much better! Just suggested a check to avoid hallucinations like this:
```python
messages = [
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image""},
            {""type"": ""text"", ""text"": ""What do we see in this image?""},
        ]
    }
]
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, return_tensors=""pt"")
inputs = {k: v.to(DEVICE) for k, v in inputs.items()}


# Generate
generated_ids = model.generate(**inputs, max_new_tokens=50)
generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)

print(generated_texts)

['User:What do we see in this image?\nAssistant: The image depicts a scene from a historical or fictional setting, likely from the medieval period, given the attire and architecture. The central focus is on a large, ornate gate, which appears to be the entrance to a castle or a fortified structure.']
```

also in `Idefics3ProcessorTest` could we add a test for text only inference and one to check that an error is raised if we have an image in the conversation, but no images are passed to the processor? Thanks for fixing 🤗 ","# What does this PR do?

Fixing Idefics3 processor to work with batches that do not include images


## Who can review?
@andimarafioti 

","```suggestion
        elif text is not None:
            if any(n_images_in_text):
                raise ValueError(
                    f""Found {sum(n_images_in_text)} {self.image_token.content} tokens in the text but no images were passed.""
                )
            text_inputs = self.tokenizer(text=text, **output_kwargs[""text_kwargs""])
```

I'd add this otherwise we won't have an error if there are image tokens in the conversation, but no images passed"
35115,2024-12-06T08:43:04Z,2024-12-10T07:40:40Z,xspirus,7,0,1,4,1,2,2,[],7601.0,0,1493615.0,0,0,0,0,275585.012893,,0,1,0,False,"['SunMarc', 'chiragjn', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'xspirus']","If you are planning to do a patch, it would be nice to include this @ArthurZucker  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35115). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I am facing a related issue where the `gather` itself crashes when `num_items_in_batch` ends up `None`. There is some mismatch in computing total steps, my dataloader is getting exhausted beforehand.
I'll check why, but maybe this needs to be handled as well

```
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/axolotl/train.py"", line 192, in train
[rank1]:     trainer.train(resume_from_checkpoint=resume_from_checkpoint)
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/transformers/trainer.py"", line 2164, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""<string>"", line 302, in _fixed_inner_training_loop
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/transformers/trainer.py"", line 5149, in get_batch_samples
[rank1]:     num_items_in_batch = self.accelerator.gather(num_items_in_batch).sum().item()
[rank1]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/accelerator.py"", line 2458, in gather
[rank1]:     return gather(tensor)
[rank1]:            ^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 376, in wrapper
[rank1]:     return function(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 437, in gather
[rank1]:     return _gpu_gather(tensor)
[rank1]:            ^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 356, in _gpu_gather
[rank1]:     return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 129, in recursively_apply
[rank1]:     raise TypeError(
[rank1]: TypeError: Unsupported types (<class 'NoneType'>) passed to `_gpu_gather_one`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.
``` This is probably because of https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L5151-L5154 where an exception has occurred. A check for None would probably fix the issue. I confirmed in my case `batch_samples` len is 0 which means `next(epoch_iterator)` is raising a StopIteration which is a bit worrying and points to miscalculation somewhere in total steps

While we should handle None I would also like to get to the root cause and see if everything makes sense cc @muellerzr there should be a simple fix! 🤗  Opened a new issue https://github.com/huggingface/transformers/issues/35387 with reproduction detailsLGTM ! Thanks for the details and opening this PR @xspirus !  Got it, will include in the patch then!","LGTM ! Thanks for the details and opening this PR @xspirus !  Got it, will include in the patch then!","In method `Trainer#get_batch_samples`, the return values should be a list of batch samples and an integer indicating the number of items that exist in the batch. However, this was not actually a case and what was returned instead of an integer, was a tensor with one element. In the multi-GPU setup, this tensor is placed in a different device than the loss tensor, causing the loss function to raise a `RuntimeError`.

The problem arises from
https://github.com/huggingface/transformers/blob/5d7739f15a6e50de416977fe2cc9cb516d67edda/src/transformers/trainer.py#L5139-L5144, where the outer `sum` operates over a list of tensors which means that the final result is also a tensor. To counter this issue, a new check (after the accelerator gathering) has been added in order to convert a potential tensor to an integer before returning the `num_items_in_batch`.

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #35086 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ArthurZucker @muellerzr @SunMarc 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34997,2024-11-28T14:12:00Z,2024-12-23T15:54:49Z,BlackSamorez,21,30,45,1249,16,3,2,[],478.0,0,2166169.0,0,0,0,0,274495.050318,,0,45,0,False,"['BlackSamorez', 'SunMarc', 'HuggingFaceDocBuilderDev', 'MekkCyber', 'Rocketknight1']","cc @SunMarc @MekkCyber cc @MekkCyber  Failed tests look like a problem on the runner's end Hey @BlackSamorez, Thanks for adding this quantization method so quickly ! I added some very small nits  @SunMarc @MekkCyber thanks for your feedback!
I think I addressed all of your concerns. Monday ping just in case :)
@SunMarc @MekkCyber Hey @BlackSamorez, thanks for iterating and integrating this method 🤗 ! LGTM ! Can you add a documentation in `docs/source/en/quantization/higgs.md` to explain a bit how Higgs works, why it only works for models > 8B for now, and how to load a quantized model ? Also i left a small question  Added documentation Added usage example in docs.
Also, I had to slightly rework the`HiggsConfig` to add more quantization parameters. It turned out that the previous hard- coded setup didn't work for `gemma` models. Now it should be fine. Thank you @BlackSamorez ! Looks great to me ! Just out of curiosity what was the reason why hard-coding the `group_size` don't work with `gemma`, it looks the same  > Thank you @BlackSamorez ! Looks great to me ! Just out of curiosity what was the reason why hard-coding the `group_size` don't work with `gemma`, it looks the same

It wasn't `groups_size`, it was `hadamard_size`. We have to pad matrices to a multiple of it. It was 1024, but gemma internal dimensions are not divisible by 1024. Padding occurred, and pre-compiled kernels stopped working for new shapes.
Lowering `hadamard_size` to 512 by default fixed the issue.  @SunMarc Hi! Is there a chance you could take a look at it this week? We wanted to release it before NIPS. > @SunMarc Hi! Is there a chance you could take a look at it this week? We wanted to release it before NIPS.

I'll do that now. When is the deadline ? After my review, I still need this to be reviewed by a core maintainer  > When is the deadline ?

There isn't a hard deadline, but it would be very nice to have this merged this week or early next week. Pre-quantized a bunch of models (including `Llama-3.3-70B-Instruct`) and added a link to the collection to docs. Gentle ping @ArthurZucker  Can you fix the CI with `make style` for the quality and potentially rebase the PR @BlackSamorez  ?  Things to recheck later: 
 - `make style` doesn't actually fix `python utils/custom_init_isort.py --check_only` errors. I had to run `python utils/custom_init_isort.py` manually.
 - `pillow` (`PIL`) is needed for `make quality` but isn't included in `pip install -e .[quality]` The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34997). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Quantization tests are nightly, right? That's right ! Thanks for integrating this new quantization method so fast! I left some comments and don't forget to also update the documentation so that the users knows how to use it !  Thanks for your work ! Just a few nits. ",Thanks for integrating this new quantization method so fast! I left some comments and don't forget to also update the documentation so that the users knows how to use it !  Thanks for your work ! Just a few nits. ,"### HIGGS 0-Shot Quantization

HIGGS is a new 0-shot quantization algorithm that combines Hadamard preprocessing with MSE-Optimal quantization grids to achieve lower quantization error and SOTA performance. You can find more information in the [paper](https://arxiv.org/abs/2411.17525).

Runtime support for HIGGS is implemented through [FLUTE](https://arxiv.org/abs/2407.10960), and its [library](https://github.com/HanGuo97/flute?tab=readme-ov-file).

This PR **adds support for HIGGS+FLUTE into `transformers` allowing for low-error 0-shot quantization and fast LLM inference**.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","not necessary to put it here. The check on device_map when we initialize the quantizer would be enough.  Checks in `validate_environment`  should be enough 
```suggestion
``` to remove
```suggestion
``` let's check if cuda is available in `validate_environment` instead  could you add a comment on what we are doing here ?  let's do the version check in `is_flute_available` and you shouldn't pin the version like that. We should allow a minimum version.  Can you use instead `modules_to_not_convert` just like the other quantization method ?  What are the accepted values ?  Could you explain a bit more what this is used for ?  Can you add a description of what this is used for ? The user shouldn't have to worry about that ?  add in post_init checks for bits and p  to remove or uncomment  can we use a smaller model like a tiny llama ? This will be better for our CI thanks !  to remove or perform some tests with this device_map. I think we shouldn't allow users to pass this kind of device_map and some check should be added in `validate_environment`. Check for example the awq integration code Added comments to this and possible repacking happening afterwards  The docker image will be deployed on an instance with cuda 11.8 but on the `flute` github I noticed you need to specify `https://flute-ai.github.io/whl/cu118` in that case just a small nit, I think we should specify that it enables both loading and quantization of models because there are other quantizers that only enable loading  Just a small nit to help users
```suggestion
        raise NotImplementedError(f""Device capability {target_device_cc} not supported for FLUTE (yet?) to verify your device capability check out https://developer.nvidia.com/cuda-gpus"")
``` Just for my understanding why do we need the `num_sms_packed` ? Just a small question, is the `group_size` a constant ? Do you mean that serialization is not implemented yet ? so we can't save a quantized model and load it ? I think you meant `nb_higgs_linear` 😉  sorry if i'm mistaken, I don't believe we use this function anywhere Codes packing is `sms` dependent. We need to remember what was the `sms` of the machine on which the codes were packed on (`num_sms_packed`) to be able to check if we need to repack or not. Moreover, we need `num_sms_packed` to do the repacking itself.  Yes, there are a few hard-coded constants right now, including the group size. I think I will do a small refactoring to spell them out more explicitly.  No, serialization is fully functional. This message got copied with bnb code I borrowed and I forgot to remove it.
By the way, bnb implemented serialization quite some time ago as well. Removed Done Updated the docstring to better reflect what those are Done"
35394,2024-12-23T10:09:10Z,2024-12-23T15:27:46Z,MekkCyber,1,0,2,2,1,1,1,[],1599.0,0,19117.0,0,0,0,0,276117.711878,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35394). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM ! ,LGTM ! ,"# What does this PR do?
Fix VPTQ test 

## Who can review ? 
@SunMarc ",
35278,2024-12-14T19:16:14Z,2024-12-23T15:22:05Z,alvarobartt,1,0,1,6,1,2,2,[],1599.0,0,763553.0,0,0,0,0,276459.026571,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35278). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Indeed, thanks for the documentation update! Thanks!","Indeed, thanks for the documentation update! Thanks!","# What does this PR do?

This PR updates the typing for `tokenizer` in the `PaliGemmaProcessor` to be `GemmaTokenizerFast` instead of `LlamaTokenizerFast`.

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review?

Not sure who to tag, maybe @molbap as he contributed to the model AFAIK, feel free to ping anyone else if needed (sorry if that's the case)",
34991,2024-11-28T11:41:48Z,2024-12-23T14:51:31Z,MekkCyber,1,0,6,12,1,2,2,[],1617.0,0,2171385.0,0,0,0,0,278293.302148,,1,6,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34991). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks ! ,Thanks ! ,"# What does this PR do?
Deprecate `_is_quantized_training_enabled` in `modeling_utils.py` as it's no longer used, and it's replaced by `hf_quantizer.is_trainable`

## Who can review ?
@SunMarc ",
34578,2024-11-02T09:40:28Z,2024-12-23T12:54:57Z,tibor-reiss,5,15,7,110,2,4,1,"['Vision', 'Multimodal', 'Processing']",1475974.0,0,4418070.0,0,0,0,0,285289.240658,,1,7,0,False,"['tibor-reiss', 'HuggingFaceDocBuilderDev', 'yonigozlan', 'ArthurZucker']","cc @yonigozlan can you take a look!  @yonigozlan Thanks for the review, I have implemented your suggestions. Could you check again please? Friendly reminder @yonigozlan  Pinging @ArthurZucker for a core maintainer review The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34578). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.I appreciate this PR since I am working on SAM2. It seems nice :) Thanks a lot for working on this!
Looks great to me overall, only thing is to use the existing methods to handle positional args for backward compatibility.

On a separate note, and I know this precedes this PR, but it looks to me that there is only an image processor in this processor? Why was all this logic created as a processor at all and not just added to an image processor?

If anyone has any explanation for that, I’d really appreciate it. :) Looks good to me, thanks for iterating!
Last step is to add the `ProcessorTesterMixin` to `SamProcessorTest` in `test_processor_sam.py`, and make sure all the tests pass. Since the processor really only process images, there will probably be quite a few tests to override.
And also you'' have to rebase on main! Left two small comments and after these modifications LGTM! Thanks for working on this  Thanks @yonigozlan for the reviews! 🤗 ","I appreciate this PR since I am working on SAM2. It seems nice :) Thanks a lot for working on this!
Looks great to me overall, only thing is to use the existing methods to handle positional args for backward compatibility.

On a separate note, and I know this precedes this PR, but it looks to me that there is only an image processor in this processor? Why was all this logic created as a processor at all and not just added to an image processor?

If anyone has any explanation for that, I’d really appreciate it. :) Looks good to me, thanks for iterating!
Last step is to add the `ProcessorTesterMixin` to `SamProcessorTest` in `test_processor_sam.py`, and make sure all the tests pass. Since the processor really only process images, there will probably be quite a few tests to override.
And also you'' have to rebase on main! Left two small comments and after these modifications LGTM! Thanks for working on this  Thanks @yonigozlan for the reviews! 🤗 ","Adds uniformized processors for SAM following https://github.com/huggingface/transformers/issues/31911.

@qubvel @molbap","Can we also define the input time in `audio` and `video`? I think it this `segmentation_maps` is not required for the inference of SAM. Is there reason for the existance of this *args? This is needed for backwards compatibility - even if it is not used. E.g. take the following call: `processor(images, None, input_points)`. If I would remove it from `_add_args_for_backward_compatibility`, such calls would break.
If I understood correctly, #31911 is about uniform kwargs, and then at a later step the API would be cleaned up further. In SAM's case, `_add_args_for_backward_compatibility` will be removed, and then `segmentation_maps` can be removed as well. Yes, see comment above. Purely for bc and should be removed at a later stage. I don't understand what you mean here, could you please be more specific?

The signature of `__call__` is given, it should not be extended as far as I understand. However, it can be added as part of `VideosKwargs` and/or `AudioKwargs`. Can you point me where this attribute/variable is defined? *time -> *type, I think it is okay since this does not requires audio and video We actually have an existing way to handle such positional args, you can take a look at [udop processor](https://github.com/huggingface/transformers/blob/main/src/transformers/models/udop/processing_udop.py) for example :) These should be added to the `optional_call_args` attribute (see [udop processor](https://github.com/huggingface/transformers/blob/6c3f168b36882f0beebaa9121eafa1928ba29633/src/transformers/models/udop/processing_udop.py#L81)) This won't be needed as we now have `prepare_and_validate_optional_call_args` ```suggestion
                # The following is to capture `segmentation_maps`, `input_points`, `input_labels` and `input_boxes` arguments that may be passed as a positional argument.
        # See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details,
        # or this conversation for more context: https://github.com/huggingface/transformers/pull/32544#discussion_r1720208116
        # This behavior is only needed for backward compatibility and will be removed in future versions.
        #
        *args,
``` ```suggestion
            **self.prepare_and_validate_optional_call_args(*args),
``` I'd say you only need `ProcessorTesterMixin` for `SamProcessorTest`, so you can remove this here and the skipped tests as well same here Makes sense. However, I had to then add back the `prepare_image_inputs` methods. With this, I updated the comment."
35318,2024-12-18T14:02:19Z,2024-12-23T12:45:56Z,taha-yassine,0,0,1,3,1,1,1,[],,0,427417.0,0,0,0,0,285831.920265,,0,1,0,False,[],Thanks for updating! 🤗 ,Thanks for updating! 🤗 ,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

https://github.com/huggingface/transformers/pull/31629 introduced the ability to use `position_ids` in FlashAttention-2, but not all models were updated to support it. This PR is a straightforward extension to GPT-NeoX models.

I was looking into including GPT-2 too, but from a quick glance at the code it seems a little bit trickier.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ArthurZucker

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35011,2024-11-29T05:07:55Z,2024-12-20T11:42:40Z,Tavish9,2,0,1,2,1,2,2,"['DeepSpeed', 'bug', 'Vision']",1828412.0,0,2100489.0,0,0,0,0,286424.246043,,1,1,0,False,"['Tavish9', 'qubvel']","Hi @muellerzr, just checking in to see if this PR has gotten lost in your inbox or if it’s just having a little vacation! 😄 It’s been a couple of weeks, and we’re all eagerly waiting to see it merged. 

Also, @qubvel  why not merge without code review since the change is minimal? 🤗

Thanks a lot! 🙏 Ok, I'm going to merge this, the change is minimal and it should not affect anything elseLooks good to me, thanks!

cc @muellerzr  LGTM","Looks good to me, thanks!

cc @muellerzr  LGTM","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR addresses an issue with the initialization of tensor shapes in the `ZoeDepth` model when using the DeepSpeed framework with ZeRO-3 optimization.

Under DeepSpeed ZeRO-3, model parameters are partitioned, and the torch.Tensor class’s initialization behavior is overridden (details can be found in [here](https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/zero/partition_parameters.py#L593)). This override alters the behavior of tensor initialization, causing the error of the following line:

```python
self.register_buffer(""k_minus_1"", torch.Tensor([self.k - 1]).view(1, -1, 1, 1), persistent=False)
print(self.k_minus_1.shape)  # torch.Size([1, 63, 1, 1]) wrong!
```

To resolve this, the initialization has been updated to use torch.tensor instead of torch.Tensor, which preserves the intended shape under DeepSpeed ZeRO-3.
```python
self.register_buffer(""k_minus_1"", torch.tensor([self.k - 1]).view(1, -1, 1, 1), persistent=False)
print(self.k_minus_1.shape)  # torch.Size([1, 1, 1, 1]) correct!
```

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35187,2024-12-10T19:45:11Z,2024-12-23T12:10:00Z,winglian,4,0,3,66,33,1,1,[],64903.0,0,1095890.0,0,0,0,0,287988.250012,,0,3,0,False,"['winglian', 'Rocketknight1', 'ArthurZucker']","Hi @winglian, this looks like a good change, but this code is inherited by many other models using `# Copied from`. You'll need to run `make fix-copies` to propagate the change to them before the CI will be green! thanks @Rocketknight1 TIL! changes made! cc @ArthurZucker for review - failing test seems unrelated! @winglian took the liberty to push the required changes, as I supposed you probably were on holidays! 🎄 Perfect! thanks for fixing! Let's just rebase and we should merge",Perfect! thanks for fixing! Let's just rebase and we should merge,"# What does this PR do?

This doesn't really have a performance improvement, but at least keeps torch.compile from having a graph break here.

<img width=""1031"" alt=""Screenshot 2024-12-10 at 2 44 06 PM"" src=""https://github.com/user-attachments/assets/ca5198dd-0f80-4ff6-9704-a6cb587f8ca5"">



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ArthurZucker 
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35235,2024-12-12T13:39:35Z,2024-12-18T15:53:39Z,ArthurZucker,11,30,99,15413,107,4,1,[],349309.0,0,944963.0,0,0,0,0,288053.012836,,0,99,0,False,"['SimJeg', 'poedator', 'HuggingFaceDocBuilderDev', 'Cyrilvallez', 'ArthurZucker', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35235). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Confirmed slow tests with Llama, everything is similar to main! run-slow: vit

(just a check unrelated to this PR) Thanks a lot for the feedback @vasqu! Please have a look at the fixes in https://github.com/huggingface/transformers/pull/35342! @ArthurZucker @Cyrilvallez do you plan to refactor `modeling_phi3.py` using the `ALL_ATTENTION_FUNCTIONS` you recently introduced ? I see Phi 3 is in the list shared by @ArthurZucker at the beginning of this PR. It would be very helpful for our [kvpress](https://github.com/NVIDIA/kvpress) package. We plan to update it to be compatible with the future v4.48, any idea of when it will be released ? (december ? january ?) Release will happen on 🎅🏻 🎁 !
Yeah for sure. Unless you submit a PR first 👀 not sure it will be in this release as we are all going on holidays but in january's release it will be included
 I'm going to holidays too so I'll wait for January ^^ Happy holidays! Happy holidays! I noticed that there is still torch.reshape used in quite a few places, for instance in `modeling_gpt2.py`  Won't it be an obstacle to compiling these models? Why not replacing it with einops and using `einops._torch_specific.allow_ops_in_compiled_graph()`  ? Hey! Indeed for gpt2 I used a 'reshape' instead of the usual 'view' because I hit an edge case that wasn't compatible with viewing at some point (but I will recheck that it still appears with latest developments, might have been an artifact during the debugging process). Whenever possible (most of the time), 'reshape' is actually equivalent to 'view' so no worries there anyway 😉 Einops has never been used in our modeling codes, and we avoid using it at all costs! `Llama` uses `reshape` and pretty sur it's compile compatible! 🤗 super small nit Cannot approve as it's mine hehe but APPROVED 🟢  Very impressive work, kudos to you both! Great work guys! I think there might be value in keeping some comments, e.g. why call contiguous on sdpa, and clarifying the fa usage on recasting to half (which originates from PEFT and/or rope).","super small nit Cannot approve as it's mine hehe but APPROVED 🟢  Very impressive work, kudos to you both! Great work guys! I think there might be value in keeping some comments, e.g. why call contiguous on sdpa, and clarifying the fa usage on recasting to half (which originates from PEFT and/or rope).","# What does this PR do?
Todo in this PR:

- [ ] Cohere
- [ ] Chameleon
- [ ] DBRX
- [x] Gemma
- [x] Gemma2
- [x] GLM (modular donc rien à faire je crois)
- [ ] gpt_neoX et GPT2
- [x] Granite
- [ ] Jamba
- [ ] JetMoe
- [ ] Mimi
- [x] Mistral
- [x] Mixtral
- [ ] Mllama
- [ ] Moshi
- [ ] Nemotron
- [ ] OPT
- [x] Phi
- [ ] Ph3
- [ ] PhiMoe
- [x] Qwen2
- [ ] qwen2Moe
- [ ] qwen2VL
- [ ] SableML
- [x] StartCoder2 -> Modular normalement oK
- [ ] Idefics1,2,3
- [x] Olmo
- [x] Olmo2
- [ ] Siglip
- [ ] Whisper
","This should help with kwargs as well for GPT2, if we include it we have to probably re-define sdpa and flash (for the layer-wise scaling)!  It might be what's failing your tests! why not put the scaling as a forced argument instead?  for modular gemma let's just copy paste the previous example!@ Long term we want to get rid of this, but let's keep it in the back of our heads! The init has to use the correct layers for consistency with our philosophy I think (explicit use of layers that are different as python would require it!) super super nice ```suggestion
``` Can we add a comment for GPT2 saying that we did not add support for the special options so the sdpa, flex and flash are the standards and don't support the special options (that eager does) needs to explicitly use `GraniteAttention`!  ```suggestion
        hidden_states = residual + hidden_states * self.residual_multiplier # main diff with Llama
``` ```suggestion
        # MAIN DIFF WITH LLAMA
        inputs_embeds = inputs_embeds * self.embedding_multiplier
``` ```suggestion
        logits = logits / self.config.logits_scaling # MAIN DIFF WITH LLAMA
``` ```suggestion
            sliding_window=getattr(self.config, ""sliding_window"", None), # diff with llama 
```
if I am not mistaken it's the only one same comment about explicit use of classes that are changed!  Ahh lol the only diff being i:
```python
        output_router_logits = (
            output_router_logits if output_router_logits is not None else self.config.output_router_logits
        )
``` ```suggestion
``` it's now the same as llama btw
 I suppose this is the main diff!  it's now the. same as llama now let's just add a comment for main diff with llama! this one's breaking no? To revert! ```suggestion
```
 init can be the same as llama as well I think we can standardize more! ```suggestion
        attn_output = nn.functional.dropout(attn_output, p=self.residual_dropout, training=self.training) # main diff with llama
``` I dont'think we should be needing this! same as llama now! More standards! perfect that might be an issue, we can ping @michaelbenayoun maybe!"
34191,2024-10-16T06:57:54Z,2024-10-17T20:34:40Z,ArthurZucker,21,18,58,5997,41,7,1,[],85612.0,0,5890928.0,0,0,0,0,290989.082502,,0,58,0,False,"['JianbangZ', 'Hannibal046', 'danielhanchen', 'aliencaocao', 'bauwenst', 'muellerzr', 'paulcx', 'cphillippi-stripe', 'GirinMan', 'ArthurZucker', 'thusinh1969', 'taehyunzzz', 'iridescentee']","Continuing discussion from https://github.com/huggingface/transformers/pull/34198 - @muellerzr is correct if we truncate and partition all sequences to block sizes of 128 for Wikitext, the GA fix and without the GA fix should nearly be equivalent. I can also repro this:
![image](https://github.com/user-attachments/assets/8fde0302-6059-4d28-aae0-d96ef0f78a34)

But unfortunately if we don't pad it and use varying sequence lengths, the loss for `bsz = 1, ga = 32` actually seems to go haywire:
![image](https://github.com/user-attachments/assets/7dda264d-97df-4770-803d-05d5db14a056)

If you plot the training loss for `bsz = 1, ga = 32` with and without the fix (blue and yellow), and also `bsz = 32, ga = 1` (red) under a log scale, we get:
![image](https://github.com/user-attachments/assets/ad71b3d4-b0c5-4ffa-bce2-0277beb0b725)

And if we plot them on the left and right axes:
![image](https://github.com/user-attachments/assets/ab4f7588-4fec-4254-aeaf-ab6e1595660c)

All experiments here: https://docs.google.com/spreadsheets/d/1RUiVuFNfnl9eBAa3JhvkKb0hm20m4NqnUO-OWDPpNos/edit?usp=sharing - also the `bsz = 32` varying sequence lengths unfortunately OOMed on a A100 40GB, so only 50% of the training went through.

TLDR: The fix works as expected, making GA and full batch training equivalent. The effect on truncated or merged sequence lengths is less pronounced, but varying sequence lengths makes the issue become very obvious Can also confirm, and confirm the fix in #34198 solves it

![W B Chart 10_17_2024, 5_45_37 AM](https://github.com/user-attachments/assets/7d97354c-44e8-462c-9f00-b369b844a927)



![W B Chart 10_17_2024, 5_45_37 AM(1)](https://github.com/user-attachments/assets/104e0727-55cc-4c06-8dc4-5fe39e0e74e1)
 Quick question: Is Distributed Data Parallel (DDP) training affected by this issue in the same way? @Hannibal046 If you used gradient accumulation along with DDP, yes. Consider the following scenario with two samples of different lengths (wo gradient acc):

1. Sample A: 10 tokens
2. Sample B: 5 tokens

- Case 1: Single GPU (batch_size=2)

```python
loss = average_loss(10 tokens + 5 tokens)
gradient_1 = backprop(loss)  # Based on average of 15 tokens
```
- Case 2: Distributed Training (2 GPUs, local_batch_size=1)
```python
# GPU 1
loss_1 = average_loss(10 tokens)
gradient_1 = backprop(loss_1)

# GPU 2
loss_2 = average_loss(5 tokens)
gradient_2 = backprop(loss_2)

final_gradient = 0.5 * gradient_1 + 0.5 * gradient_2
```
so `gradient_1 != final_gradient`.
 The issue fixed in this PR is about gradient accumulation, not DDP.
What you explained about is orthogonal with gradient accumulation, and may be intended behavior. After using this fix, the following problem occurred:
the grad norm and loss (custom loss from trl cpo trainer) increased dramatically

![image](https://github.com/user-attachments/assets/64251d7a-10f3-4193-91eb-529360b9930f)
green：llama34b-deepspeed-zero3-8gpus-fullparams-bs4-grad8-wo_fix
orange: llama34b-deepspeed-zero3-8gpus-fullparams-bs4-grad8-wo_fix
 > After using this fix, the following problem occurred: the grad norm and loss (custom loss from trl cpo trainer) increased dramatically
> 
> ![image](https://private-user-images.githubusercontent.com/738834/378013278-64251d7a-10f3-4193-91eb-529360b9930f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkzMDE2NDUsIm5iZiI6MTcyOTMwMTM0NSwicGF0aCI6Ii83Mzg4MzQvMzc4MDEzMjc4LTY0MjUxZDdhLTEwZjMtNDE5My05MWViLTUyOTM2MGI5OTMwZi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMDE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTAxOVQwMTI5MDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04ZWVhYjkwYWQ2NzY2NjZkN2IzYjRiY2EyMzM5MWU5NDk0ZDI1ZjRkYzllYWMzM2E3OGUyMjNmNmIzODM4NjcwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.wZ_6eX1qKsC0rQhCCXzGyTaJJTdaZ5H7LVgktlQPlNM) green：llama34b-deepspeed-zero3-8gpus-fullparams-bs4-grad8-wo_fix orange: llama34b-deepspeed-zero3-8gpus-fullparams-bs4-grad8-wo_fix

Can confirm the same issue I think the fix isnt complete, another fix is needed to make it normal. See https://github.com/hiyouga/LLaMA-Factory/issues/5747#issuecomment-2425488226 > I think the fix isnt complete, another fix is needed to make it normal. See [hiyouga/LLaMA-Factory#5747 (comment)](https://github.com/hiyouga/LLaMA-Factory/issues/5747#issuecomment-2425488226)

Hi @ArthurZucker , would you mind check the further fix? Just so we can note this on the right issue please put comments here: https://github.com/huggingface/transformers/pull/34198

This PR only modified the loss functions, that one is actually what made modifications in the trainer

Note though: I would likely not expect old LR's to behave similar. I'd expect old LR's to behave like as though GA wasn't used.  As [hiyouga/LLaMA-Factory#5747 (comment)](https://github.com/hiyouga/LLaMA-Factory/issues/5747#issuecomment-2425488226) suggested, maybe a few changes will make this fix work again? > > After using this fix, the following problem occurred: the grad norm and loss (custom loss from trl cpo trainer) increased dramatically
> > ![image](https://private-user-images.githubusercontent.com/738834/378013278-64251d7a-10f3-4193-91eb-529360b9930f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkzMDE2NDUsIm5iZiI6MTcyOTMwMTM0NSwicGF0aCI6Ii83Mzg4MzQvMzc4MDEzMjc4LTY0MjUxZDdhLTEwZjMtNDE5My05MWViLTUyOTM2MGI5OTMwZi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMDE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTAxOVQwMTI5MDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04ZWVhYjkwYWQ2NzY2NjZkN2IzYjRiY2EyMzM5MWU5NDk0ZDI1ZjRkYzllYWMzM2E3OGUyMjNmNmIzODM4NjcwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.wZ_6eX1qKsC0rQhCCXzGyTaJJTdaZ5H7LVgktlQPlNM) green：llama34b-deepspeed-zero3-8gpus-fullparams-bs4-grad8-wo_fix orange: llama34b-deepspeed-zero3-8gpus-fullparams-bs4-grad8-wo_fix
> 
> Can confirm the same issue
same. encountered when fine-tuning using qwen2.5.  Fix is being worked on here: https://github.com/huggingface/transformers/pull/34283 Oh my.... Hello all, I could not follow the whole context of the PR here, but I am guessing a fix is underway regarding the loss becoming huge due to [this code](https://github.com/huggingface/transformers/blob/1d063793318b20654ebb850f48f43e0a247ab7bb/src/transformers/trainer.py#L3605). The code scales the loss by gradient accumulation step in the latest trainer (not in previous version). The snippet is included in a single training iteration step, for which I think scaling the loss by grad_acc_step is not appropriate, if the loss is not de-scaled by grad_acc_step inside the accelerator.backward call in the following line.

Does the latest accelerator backward de-scale by grad_acc_step (or is being fixed to do so)? If so, maybe the output loss of training_step() should be de-scaled one more time by grad_acc_step?
For now, is it correct to scale down the learning rate by the grad_acc_step for the original functionality? Yes accelerate de-scales the loss when calling `backward()` which is why we do so. (Accelerate has always done this) @muellerzr Thank you for the reply. I have just one more question to clear things up.

You've mentioned that accelerate had always internally de-scaled the loss by gradient accumulation steps.
However, the previous trainer.py implementations do not scale (scale up) the loss by grad_acc_steps before accelerator.backward. 
If accelerator.backward had been internally de-scaling the loss by grad_acc_steps, does that mean the previous trainer had been training a model on 1/grad_acc_steps * sum(loss) instead of sum(loss) when grad_acc_steps > 1?
 FYI: As also noted by @cphillippi-stripe, the `@property` `PreTrainedModel.loss_function` that was added by this PR breaks all software packages that have models which (1) inherit from `PreTrainedModel` while (2) setting a `loss_function` beforehand. 

I maintain [such a package](https://github.com/bauwenst/ArchIt/blob/4018ead262e4a6551bb6f1ff21bf6f52af6fce70/src/archit/instantiation/abstracts.py#L234-L236), and indeed, training is now broken and my IDE complains that `loss_function` cannot be set. 

![image](https://github.com/user-attachments/assets/94b1a179-e24f-4862-8177-944c104fcc0d)

The default behaviour of setting the loss to *causal LM* loss when no `loss_type` is defined in the config seems **highly** unwarranted to me. It is assumed now that either (1) models define this brand new `loss_type` field in the config and otherwise (2) the loss type can be inferred from the `ForXYZ` suffix:

https://github.com/huggingface/transformers/blob/63766abe362b930522dd073c9173499ba5fde02a/src/transformers/modeling_utils.py#L5110-L5120

Yet, this fails to take into account the following cases where `transformers` is used as a dependency:
1. Model classes defined for *tasks that don't have a known auto class* in `transformers`. For example: if your package defines a `ModelForDependencyParsing`, the above regex search will not find its loss and default to causal loss, which is wrong.
 
```python
  File "".../ArchIt/src/archit/instantiation/tasks.py"", line 328, in computeLoss
    return self.loss_function(arc_scores, arc_labels) + self.loss_function(rel_scores, rel_labels)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: ForCausalLMLoss() missing 1 required positional argument: 'vocab_size'
```

2. Model classes that have a known task, but it's not in the name (e.g. `ModelForSentenceClassifying`). Unless the config is from after October 2024, the config does not define its loss type.

3. Loss is not inherent to a `PreTrainedModel`. There are plenty of papers that start from the model weights of one model and then change its loss function for continued pretraining (take [this one](https://aclanthology.org/2024.acl-long.136/) as a recent example).

TL;DR: Seems like a design flaw to make `loss_function` a `@property` and seems like a mistake to overwrite it to `ForCausalLMLoss` for all models that don't fit the very narrow range of tasks supported by `transformers`. > FYI: As also noted by @cphillippi-stripe, the `@property` `PreTrainedModel.loss_function` that was added by this PR breaks all software packages that have models which (1) inherit from `PreTrainedModel` while (2) setting a `loss_function` beforehand.
> 
> I maintain [such a package](https://github.com/bauwenst/ArchIt/blob/4018ead262e4a6551bb6f1ff21bf6f52af6fce70/src/archit/instantiation/abstracts.py#L234-L236), and indeed, training is now broken and my IDE complains that `loss_function` cannot be set.
> 
> ![image](https://private-user-images.githubusercontent.com/145220868/395201187-94b1a179-e24f-4862-8177-944c104fcc0d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzQwMzk3MzMsIm5iZiI6MTczNDAzOTQzMywicGF0aCI6Ii8xNDUyMjA4NjgvMzk1MjAxMTg3LTk0YjFhMTc5LWUyNGYtNDg2Mi04MTc3LTk0NGMxMDRmY2MwZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjEyJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIxMlQyMTM3MTNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lMjRmZWU1OWNkMDJiN2Q0ZGI3YmI5YTc5MTFiMzIxYzkzNmYwNTZhYzE3NzYyNTZlMDhkNzUyNjViMzE3ZDAwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.tDkfmLNbPhn8zFCYx3vh2KIaykrCzBXoBQi59duoHsA)
> 
> The default behaviour of setting the loss to _causal LM_ loss when no `loss_type` is defined in the config seems **highly** unwarranted to me. It is assumed now that either models define this brand new `loss_type` field in the config and that otherwise the loss type can be inferred from the `ForXYZ` suffix:
> 
> https://github.com/huggingface/transformers/blob/63766abe362b930522dd073c9173499ba5fde02a/src/transformers/modeling_utils.py#L5110-L5120
> 
> Yet, this fails to take into account the following cases where `transformers` is used as a dependency:
> 
> 1. Model classes defined for _tasks that don't have a known auto class_ in `transformers`. For example: if your package defines a `ModelForDependencyParsing`, the above regex search will not find its loss and default to causal loss, which is wrong.
> 
> ```python
>   File "".../ArchIt/src/archit/instantiation/tasks.py"", line 328, in computeLoss
>     return self.loss_function(arc_scores, arc_labels) + self.loss_function(rel_scores, rel_labels)
>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> TypeError: ForCausalLMLoss() missing 1 required positional argument: 'vocab_size'
> ```
> 
> 2. Model classes that have a known task, but it's not in the name (e.g. `ModelForSentenceClassifying`). Unless the config is from pre-October 2024, the config does not define its loss type.
> 3. Loss is not inherent to a `PreTrainedModel`. There are plenty of papers that start from the model weights of one model and then change its loss function for continued pretraining (take [this one](https://aclanthology.org/2024.acl-long.136/) as a recent example).
> 
> TL;DR: Seems like a design flaw to make `loss_function` a `@property` and seems like a mistake to overwrite it to `ForCausalLMLoss` for all models that don't fit the very narrow range of tasks supported by `transformers`.

Not a fix but a temporary workaround for me was this patch after loading a mdoel:
```
model = AutoModelForSequenceClassification.from_pretrained(...)

old_loss_function = model.loss_function
@lru_cache
def get_loss_function(self):
    def fixed_loss(labels, pooled_logits, config, **kwargs):
        labels = labels.to(pooled_logits.device)
        return old_loss_function(labels, pooled_logits, config, **kwargs)
    return fixed_loss
    
model.__class__.loss_function = property(get_loss_function)
```

Also, @ArthurZucker, I believe this fix confirms removing the `labels = labels.to(pooled_logits.device)` line breaks the multi-gpu setup. I would be very surprised if only Mistral is affected here given similar lines were removed from other models. Initially, I tried patching this at the `PreTrainedModel.loss_function` level, but for whatever reason the loaded models don't seem to pick up the change, and I'm not sure why. Feedback taken, will open a PR to get to something better. It's hard to take everything into account, and support important to get your feedbacks! 🤗 
Will merge the fix to loss on multi-GPU (I don't think we test all tasks for training and that is a flaw in our test environnement for sure! ) cc @ydshieh as for a TODO 😉 

Thanks for coming forward with this fix so quickly. There is probably not much I can help with, but I took a look and added some comments. Very good! I'll ask Daniel if he's down to review, it would be very useful to have his opinion. Thanks","Thanks for coming forward with this fix so quickly. There is probably not much I can help with, but I took a look and added some comments. Very good! I'll ask Daniel if he's down to review, it would be very useful to have his opinion. Thanks","# What does this PR do?
First draft

End goal is to make it easy for anyone to:
- change the loss for his model 
- contribute a new loss for a model (like vision model, ENCODEC etc)
- allow passing arbitrary kwargs, interfacing 

TODO:
- [ ] Fix deformable detr loss computation","```suggestion
    if num_items is not None:
        loss = nn.functional.cross_entropy(shift_logits, shift_labels, ignore_index=-100, reduction=""sum"")
        loss = loss / num_items
    else:
        loss = nn.functional.cross_entropy(shift_logits, shift_labels, ignore_index=-100)
``` I think users could be really confused when they read this message. They don't know what and where LOSS_MAPPING is and they don't know what value they should add there. Similar issue with potential confusion. Just wondering aloud: Instead of matching based on class name, could we do a mapping from class to loss, and then do something like:

```python
for key, val in LOSS_MAPPING.items():
    if isinstance(self, key):
        loss = val
        break
else:  # no break
    # raise error
```

I assume the matching exists for custom classes that are out there in the wild. If name is a more reliable predictor than inheritance or if I'm misunderstanding, please disregard my comment. Will look into improving, but this looks super slow This would only be run once because of the LRU cache, right? @muellerzr as you wrote the snippet! Yep will update this  Also we get the classname from the class itself and want to have good defaults instead of matching against full name! i believe currently it is not ignoring the losses and potentially the functional API might be faster? And due to the ""sum"" the ignore_index value being 0 will not make a difference in the above change
```
loss.masked_fill_(shift_labels == -100, 0)
``` ```suggestion
    if num_items is not None:
        loss = nn.functional.cross_entropy(shift_logits, shift_labels, ignore_index=-100, reduction=""sum"")
        loss = loss / num_items
``` aligned with the suggestion here! ```suggestion
from .loss_deformable_detr import DeformableDetrImageLoss
from .loss_for_object_detection import (
    _set_aux_loss,
    generalized_box_iou,
    linear_sum_assignment,
)
``` ```suggestion
    num_items_in_batch = kwargs.pop(""num_items_in_batch"", None)

    if num_items_in_batch is not None:
        loss = nn.functional.cross_entropy(shift_logits, shift_labels, ignore_index=-100, reduction=""sum"")
        loss = loss / num_items_in_batch
``` ain't there a missing space in
```python
if num_items_in_total_batchis not None:
```
? thanks! also do we want to check if `num_items_in_total_batch` is not zero? Everthing that uses `cross_entropy` will need this fix. So let's change this up some. 

I think we should define our own `CrossEntropyLoss` that we call, that does the following:

```python
def CrossEntropyLoss(logits, labels, num_items_in_batch=None, **kwargs):
    reduction = ""sum"" if num_items_in_batch is not None else ""mean""
    loss = nn.functional.cross_entropy(logits, labels, reduction=reduction, **kwargs)
    if reduction == ""sum"":
        loss /= num_items_in_batch
    return loss
````    

We can then call it everywhere we need to Hi, @ArthurZucker,

I'm seeing issues w/ multi-gpu sequence classification training after this change (using Mistral). I believe it is due to the removal of this line (which I'm having a very hard time monkeypatching via `loss_function` for some reason). I also see this line very often in quite a few different models here. Were multi-gpu setups tested here for training in sequence classification mode? I'm really curious how this passed if so, because I don't see any lines like this in the new `ForSequenceClassificationLoss` function that `self.loss_function` now seems to resolve to.

I'm seeing tracebacks like this (edited for privacy):
```
...
215       File "".../training_workflow.py"", line 426, in train
216     trainer.train()
217       File "".../transformers/transformers/transformers/trainer.py"", line 2123, in train
218     return inner_training_loop(
219            ^^^^^^^^^^^^^^^^^^^^
220       File "".../transformers/transformers/transformers/trainer.py"", line 2481, in _inner_training_loop
221     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
222                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
223       File "".../transformers/transformers/transformers/trainer.py"", line 3579, in training_step
224     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
225            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
226       File "".../transformers/transformers/transformers/trainer.py"", line 3633, in compute_loss
227     outputs = model(**inputs)
228               ^^^^^^^^^^^^^^^
229       File "".../torch/torch/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl
230     return self._call_impl(*args, **kwargs)
231            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
232       File "".../torch/torch/torch/nn/modules/module.py"", line 1541, in _call_impl
233     return forward_call(*args, **kwargs)
234            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
235       File "".../accelerate/accelerate/accelerate/utils/operations.py"", line 823, in forward
236     return model_forward(*args, **kwargs)
237            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
238       File "".../accelerate/accelerate/accelerate/utils/operations.py"", line 811, in __call__
239     return convert_to_fp32(self.model_forward(*args, **kwargs))
240                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
241       File "".../torch/torch/torch/amp/autocast_mode.py"", line 16, in decorate_autocast
242     return func(*args, **kwargs)
243            ^^^^^^^^^^^^^^^^^^^^^
244       File "".../peft/peft/peft/peft_model.py"", line 1446, in forward
245     return self.base_model(
246            ^^^^^^^^^^^^^^^^
247       File "".../torch/torch/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl
248     return self._call_impl(*args, **kwargs)
249            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
250       File "".../torch/torch/torch/nn/modules/module.py"", line 1541, in _call_impl
251     return forward_call(*args, **kwargs)
252            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
253       File "".../peft/peft/peft/tuners/tuners_utils.py"", line 197, in forward
254     return self.model.forward(*args, **kwargs)
255            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
256       File "".../accelerate/accelerate/accelerate/hooks.py"", line 170, in new_forward
257     output = module._old_forward(*args, **kwargs)
258              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
259       File "".../transformers/transformers/transformers/models/mistral/modeling_mistral.py"", line 1200, in forward
260     loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)
261            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
262       File "".../transformers/transformers/transformers/loss/loss_utils.py"", line 67, in ForSequenceClassificationLoss
263     loss = fixed_cross_entropy(pooled_logits.view(-1, num_labels), labels.view(-1), **kwargs)
264            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
265       File "".../transformers/transformers/transformers/loss/loss_utils.py"", line 26, in fixed_cross_entropy
266     loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)
267            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
268       File "".../torch/torch/torch/nn/functional.py"", line 3086, in cross_entropy
269     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
270            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
271 
272 Message:
273 
274     RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)
```

For now, if you think you have a good way I can just patch this, I'd appreciate it, but replacing `PretrainedModel.loss_function` doesn't seem to do the trick..."
35390,2024-12-22T13:24:20Z,2024-12-22T19:00:07Z,Cyrilvallez,6,0,3,15,3,1,1,[],1133.0,0,26955.0,0,0,0,0,342976.394459,,0,3,0,False,"['vasqu', 'HuggingFaceDocBuilderDev', 'Cyrilvallez']","cc @ArthurZucker, this should be included in the coming release! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35390). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @Cyrilvallez not sure if `view` is gonna cause issues as non-contiguous tensors will result in an error (which really is the only reason to use reshape instead), i.e. calling contiguous before is safer imo. We were using 'view' without 'contiguous' before the refactors, so this should be fine! Sounds good! Was just overly cautious then :) No worries, thanks for double-checking!Okay thanks",Okay thanks,"# What does this PR do?

In GPT2 we have to pass `is_causal` explicitly for SDPA, but it causes double occurence for FA2 as highlighted in https://github.com/huggingface/transformers/issues/35380. This fixes it.
Also reverts the `reshape` to simple `view` for simplicity (even if they are technically equivalent, as reshape calls view if possible)


",
35050,2024-12-03T00:22:25Z,2024-12-20T11:40:38Z,jla524,0,0,2,28,2,1,1,[],,0,1631868.0,0,0,0,0,426579.930123,,0,2,0,False,[],Sounds good! thanks for adding TP support!,Sounds good! thanks for adding TP support!,"# What does this PR do?

Addresses #34789 (issue)

## Who can review?

@ArthurZucker
",
31817,2024-07-06T13:40:00Z,2024-10-28T15:02:22Z,seanswyi,11,11,20,198,6,5,4,['trainer'],182970.0,0,14509601.0,0,0,0,0,460992.7999,,0,20,0,False,"['amyeroberts', 'umarbutler', 'SunMarc', 'HuggingFaceDocBuilderDev', 'seanswyi']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_31817). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Gentle ping @muellerzr @SunMarc  > While this may seem hacky to you, I think this perfectly expands our given system. Any chance you could add a few tests to this? (Over in `test_trainer.py`). Very nice work!

Thanks! And sure, I think I'll have some time to work on it over the weekend. @muellerzr (cc. @SunMarc)

Hello. I've implemented a new test case inside of `tests/trainer/test_trainer.py` as suggested. Patches were used to control the output of the `_evaluate` method.

The first test case is when a value for `metric_for_best_model` is explicitly provided, and the second is when it's not provided and therefore defaults to the loss. Each case should have 2 and 3 checkpoints saved, respectively, as per the `side_effect`s.

I've changed the DefaultFlowCallback object so that if `save_strategy` is either `""no""` or `""best""` then a checkpoint will not be saved at the end of training, as is the default behavior; it didn't really make sense to me that we'd want to keep only the best checkpoint but the last checkpoint is also being saved.

Please let me know if there are any problems or additional changes that I should make!

---

After inspecting the tests it seems like I wasn't supposed to manually alter the training arguments. The most recent two commits undid those. @muellerzr @SunMarc No rush, but just a gentle nudge/reminder! Thanks. @muellerzr Hi. Just wondering if this PR is still relevant or not. Thanks. @ArthurZucker @SunMarc @muellerzr Sorry to keep pinging you guys on this, but I'm noticing that the tests seem to be possibly stuck after the merge. Could anybody check this when they have the chance? Thanks.

https://github.com/huggingface/transformers/runs/32165247051 Indeed, thanks for noticing @seanswyi but don't worry, I check the most recent [CI](https://github.com/huggingface/transformers/actions/runs/11657144637/job/32454510816) and the test is passing. Can you cancel the run @ArthurZucker ? Or it will just cancel by itself after a few days ?  > Indeed, thanks for noticing @seanswyi but don't worry, I check the most recent [CI](https://github.com/huggingface/transformers/actions/runs/11657144637/job/32454510816) and the test is passing. Can you cancel the run @ArthurZucker ? Or it will just cancel by itself after a few days ?

Ah, got it. Thanks for following up! If `load_best_model_at_end = True`, the strategy cannot be set to `best`. This solves https://github.com/huggingface/transformers/issues/35070, cheers! 🥳
While this may seem hacky to you, I think this perfectly expands our given system. Any chance you could add a few tests to this? (Over in `test_trainer.py`). Very nice work! Thanks for the reminder @seanswyi ! I've added a suggestion. Please also fix the conflits and we should be good to merge !  Looks good, just a small nit! Thanks for iterating !  Great work, thanks for iterating!","While this may seem hacky to you, I think this perfectly expands our given system. Any chance you could add a few tests to this? (Over in `test_trainer.py`). Very nice work! Thanks for the reminder @seanswyi ! I've added a suggestion. Please also fix the conflits and we should be good to merge !  Looks good, just a small nit! Thanks for iterating !  Great work, thanks for iterating!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Addresses https://github.com/huggingface/transformers/issues/31626.

Adds a new option called `""best""` for `TrainingArguments.save_strategy` which saves the model checkpoint each time a new best performance is achieved.

### Details

1. The previous `_save_checkpoint` method was in charge of not only saving the model checkpoint but also determining the best metric and best checkpoint. The logic for determining a new best metric was separated out into the `_determine_best_metric` method.
2. `_determine_best_metric` is called after every evaluation inside of `_maybe_log_save_evaluate`. The return value `new_best_metric` is used to determine whether or not a new best metric has been achieved, and if the save strategy is `""best""` then the TrainerControl's `should_save` flag is switched on.
    * Contrary to what I initially thought, `best_metric` does not seem to be tracked by default. Rather, it's only tracked when `args.metric_for_best_model` is provided. I believe that a best metric of some sort should always be tracked, and therefore if a value is not provided then the validation loss is used to determine a new best.
3. A new object called `SaveStrategy` was created in `trainer_utils` that adds a new attribute called `BEST` to the previous `IntervalStrategy`.

I'm not sure if I like the rather ""hack-y"" way that I implemented this by manually switching the TrainerControl's `should_save` flag rather than delegating it to the callback handler like the other flags are dealt with. The problem is that the flags are normally updated before calling `_maybe_log_save_evaluate` inside of the inner training loop, which means there's no way for us to determine whether or not a new best metric has been achieved with the current logic. I'm not sure if I'm making sense, but I'm open to any other suggestions.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@muellerzr @SunMarc 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc 

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","This can be refactored:

```suggestion
            self.control.should_save = new_best_metric
``` Similarly here:

```suggestion
            operator = np.greater if self.args.greater_is_better else np.less
``` I prefer that we don't set a hardcoded metric `eval_loss` because it will be hard to find the issue. Instead, we should raise an error if the user do not set `metric_for_best_model` when using `SaveStrategy.BEST` or `load_best_model_at_end`.Hence, the last if block should only happen when inside`if self.args.metric_for_best_model is not None:`. 
LMK if this sounds good to you !  Thanks for the suggestion, that sounds good to me. Just to make sure I understood correctly:

1. Throw an error if the user selects `SaveStrategy.BEST` or `load_best_model_at_end` but did not set a `metric_for_best_model`.
2. Remove the else block with the hard-coding of `metric_value`, hence only setting `new_best_metric = True` when `self.args.metric_for_best_model is not None`.

Let me know if there are any problems with my understanding. Otherwise, I'll go ahead and align my branch with `main` and implement the changes! I changed the previous code of `if self.state.best_metric is None` to explicitly setting upper and lower bounds for the values for comparisons, along with separating it out from the value comparison (i.e., `operator`).

When the `metric_for_best_model` was set to a loss value, having the `self.state.best_metric` be `None` wasn't properly comparing the values.

Let me know if there'd be a better way. ```suggestion
            is_new_best_metric = self._determine_best_metric(metrics=metrics, trial=trial)
```
 Thanks for the suggestion; I also prefer this since it's more descriptive of the variable. 👍 that's right. Sorry for the delay !  @seanswyi Q: does this change mean that this logic:

https://huggingface.co/docs/transformers/main/main_classes/trainer#transformers.Seq2SeqTrainingArguments.metric_for_best_model

> ... Will default to ""loss"" if unspecified and load_best_model_at_end=True (to use the evaluation loss).

is broken now? I haven't tested it yet, but it seems like it is. The docs probably need an update. @shcheklein Yeah, I don't think that there's any case where `metric_for_best_model` defaults to loss."
34764,2024-11-17T22:27:24Z,2024-12-21T08:51:09Z,bastrob,5,13,8,638,4,3,2,"['Vision', 'run-slow']",82167.0,0,2888625.0,0,0,0,0,472725.012153,,0,8,0,False,"['HuggingFaceDocBuilderDev', 'bastrob']","Yes, Im agree it makes sense to enable any image resolution, i just wanted to clarify this point. I will work on it, thanks for your feedback ! Hi @qubvel, I pushed a version to handle image size at any resolution, what do you think about it ? If it sounds good to you @qubvel, Im ready to push the last commit :) Hello @qubvel, `box_predictor` now takes `interpolate_pos_encoding` as extra parameter to enable the dynamic computation of `box_bias` inside.
Plus, I've added `# copied from` statement. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34764). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Hi @bastrob! Thanks for opening PR!

It would be great to enable any image resolution, but If you will not find a way to manage `height != width` images we can limit it to a square input image size. We just have to make sure the proper error is raised. Hi @bastrob, thanks for digging into it and adding tests! Overall looks great 🚀  just added some nits regarding variable naming.

Could you please also push an empty commit with the message`[run-slow] owlvit, owlv2` to trigger slow tests (this commit should be the last one, so I can approve the CI run). Thanks! Hi @bastrob, thanks for the update! Please, see the comments below 🤗  Thanks, looks good to me, just a few notes re the style! Also tested locally and predictions look good 👍 

can you please also push empty commit with the message `[run-slow] owlvit, owlv2` to trigger slow tests for these models in CI? (it should be **the last commit at the moment** I approve the run of CI) Thanks for handling this non-standard case! Looks great to me! Super good that things are copied from one another! And thanks for also adding the tests. I am usually not a fan of adding codepaths, so if there is a way to avoid checking if interpolate pos encoding at 4 differents places, to just do it when we actually do the interpolation, it would be a lot better! 
We can merge whatsoever 🤗 ","Hi @bastrob! Thanks for opening PR!

It would be great to enable any image resolution, but If you will not find a way to manage `height != width` images we can limit it to a square input image size. We just have to make sure the proper error is raised. Hi @bastrob, thanks for digging into it and adding tests! Overall looks great 🚀  just added some nits regarding variable naming.

Could you please also push an empty commit with the message`[run-slow] owlvit, owlv2` to trigger slow tests (this commit should be the last one, so I can approve the CI run). Thanks! Hi @bastrob, thanks for the update! Please, see the comments below 🤗  Thanks, looks good to me, just a few notes re the style! Also tested locally and predictions look good 👍 

can you please also push empty commit with the message `[run-slow] owlvit, owlv2` to trigger slow tests for these models in CI? (it should be **the last commit at the moment** I approve the run of CI) Thanks for handling this non-standard case! Looks great to me! Super good that things are copied from one another! And thanks for also adding the tests. I am usually not a fan of adding codepaths, so if there is a way to avoid checking if interpolate pos encoding at 4 differents places, to just do it when we actually do the interpolation, it would be a lot better! 
We can merge whatsoever 🤗 ","# What does this PR do?

Towards https://github.com/huggingface/transformers/issues/30579
Hey, this is a draft, Im wondering how can we manage the variables impacted by the dynamic input change in the OwlViTForObjectDetection class (self.sqrt_num_patches, self.box_bias) ?
Is there a better way to handle this ?

The interpolate_pos_encoding allows new input size respecting height==width strictly  ? 
In that case I should ensure this.
If not, I think  sqrt_num_patches needs to be decomposed too (_h, _w), some examples where it might throws exc:
https://github.com/bastrob/transformers/blob/30f3c2d56729974ec0d1d9e2fc4fd633ab697eb2/src/transformers/models/owlvit/modeling_owlvit.py#L1355
https://github.com/bastrob/transformers/blob/30f3c2d56729974ec0d1d9e2fc4fd633ab697eb2/src/transformers/models/owlvit/modeling_owlvit.py#L1459
https://github.com/bastrob/transformers/blob/30f3c2d56729974ec0d1d9e2fc4fd633ab697eb2/src/transformers/models/owlvit/modeling_owlvit.py#L1719

Fixes: #34622

## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
@amyeroberts
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
","I assume we can always use this code path without if/else Same here Let's rename it everywhere for clarity`self.sqrt_patch_dim_h` -> `self.num_patches_height` and please make it two separate lines
```python
self.num_patches_height = ...
self.num_patches_width = 
```
 ```suggestion
    def normalize_grid_corner_coordinates(num_patches_height: int, num_patches_width: int) -> torch.Tensor:
``` should it be patch_dim_h for y_coordinates and patch_dim_w for x_coordinates? Whoops nice catch ! I was a bit tunnel vision.. Anyway thanks for noticing it  btw, if this method is identical to the method of another model, such as ViT, we should add `Copied from` statement We should either compute it dynamically or take it from instance attributes. Modifing instance attributes outside of the init method might lead to bugs. Instead, we should probably pass extra arguments to the `box_predictor` method and compute `box_bias` dynamically inside.  Thanks! 
```suggestion
        if interpolate_pos_encoding:
            _, num_patches_height, num_patches_width, _ = feature_map.shape
            box_bias = self.compute_box_bias(num_patches_height, num_patches_width)
        else:
            box_bias = self.box_bias

        box_bias = box_bias.to(feature_map.device)
``` 
```suggestion
        if interpolate_pos_encoding:
            _, _, height, width = pixel_values.shape
            num_patches_height = height // self.config.vision_config.patch_size
            num_patches_width = width // self.config.vision_config.patch_size
        else:
            num_patches_height = self.num_patches_height
            num_patches_width = self.num_patches_width
``` same here We need to add it to `OWLV2_OBJECT_DETECTION_INPUTS_DOCSTRING` and `OWLV2_IMAGE_GUIDED_OBJECT_DETECTION_INPUTS_DOCSTRING` as well"
35322,2024-12-18T16:42:49Z,2024-12-20T17:16:02Z,joelpaulkoch,0,4,2,17,1,3,2,"['Documentation', 'Modular']",,0,175302.0,0,0,0,0,528324.194125,,0,2,0,False,[],Thanks for adding the commands! We can merge after @ArthurZucker has double-checked your comment 🙂  Nice thanks ! 🤗 let's add @stevhliu's suggestions and we should be good to go! ,Thanks for adding the commands! We can merge after @ArthurZucker has double-checked your comment 🙂  Nice thanks ! 🤗 let's add @stevhliu's suggestions and we should be good to go! ,"# What does this PR do?

Added some more details to the modular transformers documentation that could have helped me.

- Adds hints to general contribution guides 
- Lists which utils scripts are available to generate single-files from modular files and check their content

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@stevhliu ","I'm not sure if this is accurate since it fails in my case (#35320).
I'm assuming this is the check the TODO is referring to, please double check. Nicer to put the command in a copyable code cell :)

```suggestion
To generate a single file from the modular file, run the following command.

```bash
python utils/modular_model_converter.py --files-to-parse src/transformers/models/<your_model>/modular_<your_model>.py
```
``` cc @ArthurZucker 

```suggestion
Run the command below to ensure the generated content matches `modular_<your_model>.py`

```bash
python utils/check_modular_conversion.py --files src/transformers/models/<your_model>/modular_<your_model>.py
```
``` Yes, thanks, this is better 👍 "
35310,2024-12-17T20:46:19Z,2024-12-20T17:22:44Z,stevhliu,1,0,1,18,1,1,1,[],1704.0,0,246989.0,0,0,0,0,528427.770939,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35310). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,Forgot to add the contents of the json file to `register_pipeline`! 😅 cc @tcapelle,
35368,2024-12-20T14:54:58Z,2024-12-20T17:17:29Z,Uvi-12,1,4,6,14,1,1,1,[],6554.0,0,8551.0,0,0,0,0,528747.227479,,0,6,0,False,['Uvi-12'],"I have updated the documentation as per your review, please let me know if any other adjustments are required.Thanks for your improvements! Thanks, LGTM!","Thanks for your improvements! Thanks, LGTM!","Fixes #35367

This PR makes several improvements to the audio_classification.md documentation for better clarity and readability.

@stevhliu","```suggestion
Audio classification - just like with text - assigns a class label as output from the input data. The only difference is instead of text inputs, you have raw audio waveforms. Some practical applications of audio classification include identifying speaker intent, language classification, and even animal species by their sounds.
``` I don't think it's necessary to change the first sentence, but good with `This'll --> This will` I don't think this change is necessary either; `can` suggests it's _potentially_ helpful, but in this case I think it is definitely helpful. These additions also don't seem necessary (duplicate maybe?)"
35263,2024-12-13T16:26:40Z,2024-12-13T17:43:45Z,Uvi-12,4,1,3,4,1,1,2,[],2079.0,0,605074.0,0,0,0,0,531523.301529,,0,3,0,False,"['stevhliu', 'HuggingFaceDocBuilderDev', 'Uvi-12']","Hi @stevhliu, While reviewing the documentation, I noticed a few areas where the clarity and quality could be enhanced. These aren't necessarily typos or errors, but rather improvements that could make the content easier to understand. Would it be okay for me to proceed with making these improvements? Hi, feel free to open an issue describing your proposed improvements The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35263). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Hi, feel free to open an issue describing your proposed improvements

Thank you, I will do as you said. For this PR I have made the changes as per your review. If everything looks good please proceed with merging the PR.Thanks for improving!",Thanks for improving!,"Fixes #35261 

- Fixed possessive form: ""it's"" corrected to ""its"" in reference to the MInDS-14 dataset.
- Corrected unit capitalization: ""khz"" updated to ""kHz"" for proper scientific notation.
- Added a comma for clarity: A missing comma was added to improve grammatical correctness in the sentence about the output_dir parameter.","```suggestion
The MInDS-14 dataset has a sampling rate of 8kHz (you can find this information in its [dataset card](https://huggingface.co/datasets/PolyAI/minds14)), which means you'll need to resample the dataset to 16kHz to use the pretrained Wav2Vec2 model:
```"
35268,2024-12-13T20:02:26Z,2024-12-16T17:50:11Z,Uvi-12,0,0,1,28,1,1,1,[],,0,592125.0,0,0,0,0,531526.916317,,0,1,0,False,[],Thanks for improving!,Thanks for improving!,"Fixes #35267 

This PR makes several improvements to the asr.md documentation for better clarity and readability.

@stevhliu

",
35305,2024-12-17T08:39:33Z,2024-12-17T17:45:51Z,Uvi-12,1,0,1,12,1,1,1,[],23604.0,0,287496.0,0,0,0,0,531529.424427,,0,1,0,False,['Rocketknight1'],Linking the issue as well: https://github.com/huggingface/transformers/issues/35304Thanks!,Thanks!,"This PR resolves the documentation issue in audio_classification.md where the term ""finetune"" was incorrectly used. The correct form, ""fine-tune"" (with a hyphen), has been applied throughout the document.

@stevhliu",
34995,2024-11-28T12:46:30Z,2024-12-20T15:03:26Z,ydshieh,3,2,10,125,5,2,1,[],1693.0,0,1909019.0,0,0,0,0,536791.184738,,0,10,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34995). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. also fixed

> tests/models/persimmon/test_modeling_persimmon.py::PersimmonModelTest::test_generate_with_static_cache > tests/models/persimmon/test_modeling_persimmon.py::PersimmonModelTest::test_generate_with_static_cache

200 runs: 8 failed (before) v.s 0 failed (PR)Cool, I like that we have some utils that can be re-used!","Cool, I like that we have some utils that can be re-used!","# What does this PR do?

#34464 make some generate tests less flaky. However, with 

> tests/models/paligemma/test_modeling_paligemma.py::PaliGemmaForConditionalGenerationModelTest::test_generate_with_static_cache

it fails 0.6% of the time.

This PR takes the same idea in #34558 to make `test_generate_with_static_cache` even much less flaky. Running 2000 times and all pass.

It also make this process easier to applied to other tests in the future whenever necessary","> tests/models/seamless_m4t_v2test_modeling_seamless_m4t_v2.py::SeamlessM4Tv2GenerationTest::test_speech_generation

previously failed 50% of the time. Now running 300 times and all pass.
 Run 500 times

> tests/models/musicgen_melody/test_modeling_musicgen_melody.py::MusicgenMelodyDecoderTest::test_eager_matches_sdpa_inference_1_bfloat16

2 failed (before) v.s. 0 failed with PR

> tests/models/musicgen_melody/test_modeling_musicgen_melody.py::MusicgenMelodyTest::test_eager_matches_sdpa_inference_1_bfloat16

3 failed (before) v.s. 0 failed with PR

"
35154,2024-12-08T18:16:11Z,2024-12-20T08:27:47Z,vasqu,18,30,16,444,2,4,3,"['run-slow', 'State space models']",6153.0,0,1026107.0,0,0,0,0,535922.521368,,0,16,0,False,"['vasqu', 'molbap', 'HanGuo97', 'saeed6944']","Integration tests will probably need an update but I don't have a GPU for the 7B atm.

Edit: If you could update these integration tests, then gladly :D especially since I'm on vacay very soon Hey @vasqu thanks! Taking a look in a min also I added the slow label - feel free to launch a commit with message ""[run-slow] mamba2"" so we can trigger the slow CI! that way we make sure multi-gpu is indeed fixed @molbap Yup, added an empty commit - will get to the comments/review a bit later :saluting_face: 

(I could expect some failures on the integration tests, not sure let's see) Attempt 2 at multi gpu, at least a different error :p  Things that remain:
- [ ] Mulit-GPU *sigh*
- [ ] Supersede slow path fix with same tests included?
- [ ] Compile compatibility (future)
- [ ] Refactor some stuff (future)

Otherwise, ready to go @molbap 

Edit: Hub seems to have some unrelated issues  Could you trigger the slow runs @molbap ? I added the slow path fix here and closed the old PR. The only thing that didn't work before was the multi GPU caching so let's see how it goes this time. 👍 I'm on vacay for a while, so feel free to update / correct some stuff if you want to. Will be back after xmas, so I guess early merry xmas :) Got it, will update if needed! Enjoy the holidays @vasqu ! Thanks for the effort @vasqu! Just wanted to quick check in to understand the status of current HF implementation of Mamba-2.

### `cuda_kernels_forward`
- The current implementation correct, since most of the proposed changes are style changes.

### `torch_forward`
- The current implementation is incorrect, and the PR fixes two things.
1. Reduction dimension.
2. `dt` clamping.

- There are some updates to masking, but not sure when/whether these are used.

Are these statements correct? Thanks in advance! Hey @HanGuo97, you understood most of the changes imo. I guess only things I'd add on would be the caching issues specifically:
- cuda path wrongly initiates the conv cache
- torch path had some issues with dimensions when caching / inferring (I think you mean this as well)
- overall cleanup of how inference is prepared as there was a mixup in regards to non-cached inference
- multi-gpu device management caused issues (when interacting with the cache)

The masks were there before but a bit of a refactor tbh. They're important to ensure that batched inference works correctly - that's something that is incorrect in the original mamba(2) implemenation (except you use varlen implementations, i.e. those that use cu_seq_lens).

Lmk if something's unclear!  Thank you for the clarifications! 

Is the current PR mostly complete, or can we expect more significant fixes? I’m asking because I’m refactoring the implementation for other reasons and prefer to base it on a more stable version.

On a separate note, do you have any suggestions for testing the implementation’s correctness? I was considering comparing it with the original Mamba-2 implementation, but I’m unsure if I might overlook something. No problem! I don't think that there will be any more significant changes. So, this should be a good ground to work on.

The original implementation is well made as is. I only have my gripes when using batches. Depending on your refactor it might easier to look at earlier versions without cu_seq as it adds a bit of complexity. Otherwise, kind of self-promo but I've also written a mamba2 version back before it came to transformers - maybe that can help ( https://github.com/vasqu/mamba2-torch but look into the open PR if you want correct batched inference). Got it, thanks again for the explanations (and enjoy the holidays)! Any reason to withhold this PR? Seems like Bamba (#34982) has been merged and silently merged this PR's fixes (https://github.com/huggingface/transformers/pull/34982/commits/856cb3a41f9131f9f1c35fbe250dfdd2f38a8368) without a (commit) reference / dubious credit (i.e. adopted this ""refactor"" but fixed bugs themself?).

Failing tests are unrelated (internal hub failures).

@molbap @ArthurZucker  Hi @vasqu, this should be merged soon! In terms of precedence I'll add an `From PR ... by @vasqu` in comment so future users understand where the fixes come from, not a worry
ping @ArthurZucker  for merge!  Hey 👋 I don't need direct credit, I just think that the list given in the docstring is misleading: 
https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src/transformers/models/bamba/modular_bamba.py#L214-L218
The changes are mainly because of the cache + dropping some attributes.Just some comments for clarification Some more comments for the cache Hey @vasqu thanks a bunch! left a couple questions/comments but looks good .. Seems to run on multi-GPU! Thanks a bunch, @ArthurZucker alright for me once all tests green (hub tests unrelated) Thanks a lot @vasqu 😉 
Your last comment is completely aligned with our philosophy: if indeed bamba is now the same, we shall add bamba with modular, isolating the differences if there are any left! cc @molbap on this! Merry Christmas as well! ","Just some comments for clarification Some more comments for the cache Hey @vasqu thanks a bunch! left a couple questions/comments but looks good .. Seems to run on multi-GPU! Thanks a bunch, @ArthurZucker alright for me once all tests green (hub tests unrelated) Thanks a lot @vasqu 😉 
Your last comment is completely aligned with our philosophy: if indeed bamba is now the same, we shall add bamba with modular, isolating the differences if there are any left! cc @molbap on this! Merry Christmas as well! ","# What does this PR do?
Kind of a follow-up to #34901 as there are some issues in the current code:
- Caching
- Dt clamping
- Following Mamba1 standards a bit closer
- Multi-Gpu fix when caching
- Slow path fix (supersedes #34901)

Fixes #33567
Fixes #34817

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@molbap @ArthurZucker ","Added the other imports as well. More of a preference ~ It's used multiple times, so why not make it a function :) Follows Mamba1 more closely and makes it tensors only instead of a dict. Added those comments to see where we are in the block This was missing We already make sure that we have conv1d based on entering this path No cut needed Just to keep naming consistent Squeezing messes up dimensions completely. As it was before, we got seq_len + batch_size as next input  This clamp was wrong as we need the limits, not max/min - max/min are only for the init. This caused some bigger diffs and explains why dt_min alone didn't have much of an impact (as it's close to 0). Should fix the multi-gpu issues Minimal slow path fix but #34901 makes it a bit more efficient. Not necessary as we do not cache on this path Added a mask, maybe for some other tests as well. The new test. `if hidden_states.ndim < 3 or attention_mask is None:` this line changed but the essence remains Tests work now This is unnecessary as generate will add to the ids and masks itself. Hence, no need to manually update it ourselves. We do not need to detect that we are decoding - generate will append which is enough Will update the description if that's the case Not even sure if something else messed it up. Atp, it works so i let it be Wasn't really necessary as we only need `conv_kernel` for cache pos as explained below. Same as in Mamba1.

Edit: Explanation [around L1053-1056 ish](https://github.com/huggingface/transformers/blob/16ca5d8bb98043ace9329ac1eed64be5980f29fd/src/transformers/models/mamba2/modeling_mamba2.py#L1053-L1056) sure! Let's make the name more specific though like `apply_mask_to_padding_states` currently will break torch compile, FWIW I'm not sure about this - seems improvable yes, but the squeeze is a no-op unless `seq_len == 1`, so in caching situation indeed. So we're ending up with a [batch_size, H] tensor instead of a [batch_size, seq_len, H] tensor. Then, we're splitting this one on the last dimension, so it should be fine ah, that's a good catch!  alright, the idea is to have the move be a no-op if we're on the same GPU, else move it to the correct one? 
I'd be pro-moving explicitly each tensor to its device of choice before, not inline

```suggestion
            cache_device = cache_params.ssm_states.device
            dA = dA.to(device=cache_device)
            dBx = dBx.to(device=cache_device)
            cache_params.update_ssm_state(
                layer_idx=self.layer_idx,
                new_ssm_state=cache_params.ssm_states[self.layer_idx] * dA + dBx
```
and possibly do that even earlier, as soon as the tensors are introduced! wonder if there's a cleaner way than using dirdctly the `conv_kernel` here (but it's fine as is) Alright, is it intended that it is only tuned out for the first element of the batch? nice, now it's aligned with cuda kernel forward in naming. TBH the whole split is the same for cuda and torch so could be factored out?"
35344,2024-12-19T15:27:09Z,2024-12-19T16:22:37Z,warner-benjamin,3,6,4,108,2,3,2,[],2322.0,0,84380.0,0,0,0,0,537391.679144,,0,4,0,False,"['stefan-it', 'HuggingFaceDocBuilderDev', 'sileod']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35344). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi, thanks, is there a plan to support multiplechoice ? Have you already road-tested the token classification implementation? E.g. using the ModernBERT Large model only yields to ~91% F-Score on CoNLL-2003 (Test set, I tried some batch size and learning rate configurations), in two different frameworks (Transformers and Flair). I am not sure if ModernBERT needs really special hyper-parameters or if there are fixes needed in the architecture :thinking: 

I tested it with b5a557e5fe2d015bd36214a95878370eaed51571.The rotary embedding solution is a temporary fix - I'll introduce a modular-approved one soon, but we have a release to hit.

I didn't realise the ForTokenClassification model also requires the dense -> act -> norm -> drop, I thought it was only drop like with BERT. My apologies, thanks for the quick fix. Thanks 🤗","The rotary embedding solution is a temporary fix - I'll introduce a modular-approved one soon, but we have a release to hit.

I didn't realise the ForTokenClassification model also requires the dense -> act -> norm -> drop, I thought it was only drop like with BERT. My apologies, thanks for the quick fix. Thanks 🤗","This PR resolves two issues with ModernBERT so it will be working for release in just under an hour.

First, ModernBERT has a final head layer `ModernBertPredictionHead` with a pretrained Linear layer and LayerNorm . In downstream heads, these two layers are loaded to `ModernBertPoolingHead`. I readded `ModernBertPoolingHead` to `ModernBertForTokenClassification` so TokenClassification models will load the pretrained layer weights.

Second, #35235 appears to have broken the SDPA/Eager paths by changing how the RoPE layer ModernBERT inherited from works. In the interest of expediency, I unmoduarized it so we can have a working ModernBERT implementation on all paths.

cc @ArthurZucker @tomaarsen ","that is even more code pathes. At this point are are the hidden states passed to the model correct? Like sequence classification passes.
Depending on the task, the hidden states should be approprioatly passed to the layer if it is always false, why do we have an argument for it?  Because it's true for the SequenceClassification head The pooling layer needs to pools for heads which need it, like `ModernBertForSequenceClassification`, and not pool for heads which don't, like `ModernBertForTokenClassification`.

I can swap `ModernBertPoolingHead` out for the `ModernBertPredictionHead` in the `ModernBertForTokenClassification`, if you want.

I thought this way would be less confusing and easier to follow when/if future ForTask heads get added Resolved in f2be82f2b777e901add429c4892bb0cdfba5a1f5 Resolved in f2be82f2b777e901add429c4892bb0cdfba5a1f5"
35241,2024-12-12T15:32:31Z,2024-12-20T14:40:55Z,ydshieh,1,0,1,6,1,1,1,[],1744.0,0,688106.0,0,0,0,0,538145.170235,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35241). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Agreed!,Agreed!,"# What does this PR do?

Just to move forward with a [huntr report](https://huntr.com/bounties/adc0155f-4af7-455a-a7c1-33ad799c1d24). Use `weights_only=True` for `torch.load` in `src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py`.

### Remark

Although `TransfoXL` is deprecated, I am trying to check if this PR breaks something by running

>  tokenizer = TransfoXLTokenizer.from_pretrained(""transfo-xl-wt103"")

but that is failing even on `main`. I  don't do anything trying to fix it though.",
35364,2024-12-20T13:47:29Z,2024-12-20T14:10:44Z,ArthurZucker,2,0,6,11,1,0,0,[],2378.0,0,2515.0,0,0,0,0,538838.452201,,1,6,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']","Nice.

I have

> glob.glob(""tests/**/test_**.py"", recursive=True)

a few lines below, but it's just a nit if you find helpful The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35364). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.",,"# What does this PR do?
There is no need to check the diff when you want to test all. As computing the diff might sometimes take a looong time. ",
35243,2024-12-12T16:19:20Z,2024-12-20T14:04:36Z,nhamanasu,11,8,5,4,2,4,3,[],4488.0,0,683116.0,0,0,0,0,540327.246007,,0,5,0,False,"['Rocketknight1', 'ydshieh', 'nhamanasu']","- The failure of `tests_generate` will be resolved in this PR: https://github.com/huggingface/transformers/pull/35246
- The failure of `tests_torch` will be resolved in this PR: https://github.com/huggingface/transformers/pull/35249 cc @ydshieh @muellerzr @SunMarc  will let at least one of @muellerzr or @SunMarc to approve too. I replaced ""./test"", ""./examples"", ""./generation"", and ""./regression"" with tmp_dir.

The only part left we need to find is the causes of producing ""./None"" dir, I think. Maybe I did something unnecessary...

New failure `FAILED tests/trainer/test_trainer.py::TrainerIntegrationWithHubTester::test_push_to_hub - AssertionError: '__DUMMY_TRANSFORMERS_USER__/tmpnlv7o37x' != '__DUMMY_TRANSFORMERS_USER__/tmp_mcwovml'` may be related to the part I replaced. 

Testing push_to_hub requires the checkpoint dir which is not temporary I guess?

I'm ready to revert the last commit and just leave .gitignore intact.

I want your review @SunMarc . Hi, the last commit of changing won't work as the tmp_dir is created within the scope of 

>         with tempfile.TemporaryDirectory() as tmp_dir:

outside the `with` block, the directory is removed automatically and the sequential code blocks won't be able to access it.  It's my trivial mistake. Thank you for pointing it out.
I'll fix them. I guess it's good to just revert that commit , keep this PR as doing what it intends to do. For the temp. directory, we can address them in another PR (if eventually we really wish ) Thank you for your comments! I reverted the last commit, and gonna make another PR to fix tmp_dir issue. 

→ I split the PR as @ydshieh suggested: https://github.com/huggingface/transformers/pull/35266 @ydshieh cc. @SunMarc 
Thank you for your last commit to fix .gitignore!

I wonder what kind of step is left to finally merge this PR. 
If there's something l still have to do, I'd be happy if you could let me know 🙇  @ArthurZucker We need an approval from you :-) 🙏 LGTM (don't know the reason why it was pinned to 1.2.0 previously)

CircleCI jobs don't install `codecarbon` so no need to build and check before merge. We can see next dailiy CI next week.

Thanks LGTM ! Left a comment thanks for the update! 🤗 ","LGTM (don't know the reason why it was pinned to 1.2.0 previously)

CircleCI jobs don't install `codecarbon` so no need to build and check before merge. We can see next dailiy CI next week.

Thanks LGTM ! Left a comment thanks for the update! 🤗 ","# What does this PR do?

- Related to https://github.com/mlco2/codecarbon/pull/391, I found we need to update codecarbon to run test_trainer.py properly in Python 3.11 or above. In current main branch, ""codecarbon==1.2.0"" was set, so I rewrote this to ""codecarbon>=2.8.1"". It properly worked.
- test_trainer.py generates some output directories which could be staged through git. I added them in .gitignore


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

- trainer: @muellerzr and @SunMarc","Do you know which test is causing this. ? We try to put everything that are saved in temp folder Thank you for your comment! Let me see...

I've searched the lines maybe related to:

### "".test""
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L1193
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L1454
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L1476
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L1508
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L1516
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L1579
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L1623
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L1682

### ""./regression"":
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L555
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L738
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L990
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L1009


### ""./examples""
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L3188
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L3206
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L3240
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L3255
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L3282


### ""./None""
I couldn't find any directly specified part, but I've seen ""./None"" folder is made every time I run `test_trainer`. I suspect that there are some lines which don't pass output_dir to TrainingArguments, or the variables passed to TrainingArguments are occasionally None. I'm not sure all of these are the main cause, but when I ran `test_trainer.py`, it surely made the folders I wrote in .gitignore. I found 117 lines which use `with tempfile.TemporaryDirectory() as tmp_dir:` and pass tmp_dir to TrainingArguments, so most of the test experiments properly utilizes temp dir, and the lines I picked above seem to be irregular.  I found another non-tmp-dir specified in `test_trainer.py`

### ""./generation""
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L786
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L801
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L883
https://github.com/huggingface/transformers/blob/bc6ae0d55e11e46eaed4da71b6bc5087d38cec70/tests/trainer/test_trainer.py#L898 let's  also revert this one in this PR :-) Thanks, I reverted it! ```suggestion
.ruff_cache

```"
35358,2024-12-20T11:13:42Z,2024-12-20T13:36:31Z,ydshieh,1,0,3,265,53,1,1,[],1598.0,0,8571.0,0,0,0,0,542011.618124,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35358). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice! 🤗 ,Nice! 🤗 ,"# What does this PR do?

Bye",
35291,2024-12-16T08:52:51Z,2024-12-20T13:37:05Z,jiwoong-choi,1,2,2,4,1,2,2,['torch export'],18616.0,0,362654.0,0,0,0,0,541979.644768,,0,2,0,False,['Rocketknight1'],"This seems legit to me, but cc @arthurzucker for core maintainer reviewHi @jiwoong-choi, thanks for the update! I faced a similar issue with vision models and can confirm that this should fix `torch.export`. Alternatively, we could use a non-inplace operation for `masked_fill`, but your solution seems better because it does not change the original behavior. Thanks for fixing","Hi @jiwoong-choi, thanks for the update! I faced a similar issue with vision models and can confirm that this should fix `torch.export`. Alternatively, we could use a non-inplace operation for `masked_fill`, but your solution seems better because it does not change the original behavior. Thanks for fixing","# What does this PR do?

Fix the `torch.export` failure caused by `AttentionMaskConverter._make_causal_mask`.

Recent changes in torch dynamo prevent mutations on tensors converted with aten::_to_copy. To address this, we can clone such tensor before performing in-place operation `masked_fill_` only when the code is being compiled by torch dynamo. (relevant issue on PyTorch: https://github.com/pytorch/pytorch/issues/127571)

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

PyTorch: @gante @Rocketknight1 
","We have the `is_dynamo_compiling` function:

```python
from transformers.utils import is_dynamo_compiling
``` I agree with @qubvel, tho I don't mind modifying to have an inplace operation! "
35342,2024-12-19T13:16:26Z,2024-12-20T11:09:34Z,Cyrilvallez,2,4,5,42,4,3,1,[],1656.0,0,78791.0,0,0,0,0,550828.711113,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35342). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. LGTMCool thanks for updating, thanks @vasqu for the review Just as a heads up, maybe it would be nice to doc what attn type supports what feature, e.g. sdpa and fa cannot output attn weights or use head mask while flex attn cannot use dropout.

Which brings me to the point to ask if these things have been considered (flex attn dropout not supported for example).","Cool thanks for updating, thanks @vasqu for the review Just as a heads up, maybe it would be nice to doc what attn type supports what feature, e.g. sdpa and fa cannot output attn weights or use head mask while flex attn cannot use dropout.

Which brings me to the point to ask if these things have been considered (flex attn dropout not supported for example).","# What does this PR do?

Improve the attention interfaces with comments/dtype fix
","we could also just always apply contiguous!  Are we deprecating these checks completely?

There were 2 reasons for similar type of downcasts in the past
- RoPE (I think this is a non-issue atp)
- PEFT (silently upcasting)

Peft is the reason why the target dtype has been introduced and should be passed (see https://github.com/huggingface/transformers/blob/0ade1caa356dce6b70ef8293addeb0898f177206/src/transformers/modeling_flash_attention_utils.py#L186) 
but if it's not an issue anymore it's fine. Agree, calling contiguous doesn't affect performance much either way so the comment clarification suffices imo Re-added them!"
35355,2024-12-20T07:27:09Z,2024-12-20T09:18:16Z,ydshieh,1,0,2,23,1,1,1,[],1645.0,0,6668.0,0,0,0,0,557509.686025,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35355). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Let 's just update the PR description to explain the motivation behind this! ,Let 's just update the PR description to explain the motivation behind this! ,"# What does this PR do?

We have seen the usage are almost doubled in the past 2 month. We are investigating why this happens, but we also try to reduce the usage bu downgrade the runners we need to use.",
34931,2024-11-25T22:20:52Z,2024-12-20T11:08:12Z,CISC,14,0,20,158,7,1,1,[],959.0,0,2119693.0,0,0,0,0,550862.117299,,0,20,0,False,"['CISC', 'Rocketknight1', 'ArthurZucker']","~~Hmmm, looks like I'm missing something important regarding documentation, not sure what though?~~
> Exception: The following objects are in the public init so should be documented:
> - AsyncTextIteratorStreamer

Was missing autodoc entry. @gante @ArthurZucker Ready for review.

The `check_code_quality` workflow failure makes no sense to me, not sure how to fix? @ArthurZucker @gante ping @ArthurZucker ping with sugar on top?

Seems @gante is on holiday, anyone else who can review? Gentle ping @gante, or maybe @zucchini-nlp if I haven't totally overloaded her with pings at this point! @Rocketknight1 Thanks! BTW, know what's up with `check_code_quality` and/or how to fix it? @CISC that's usually a code style thing. Try `pip install transformers[quality]` followed by `make fixup` @Rocketknight1 It's not, it's complaining about the import order in `src/transformers/__init__.py`, which I did not change... :( @CISC `make fixup` will also resolve issues like that! In this case, the `__init__.py` has actually been modified in this PR to allow the class to be imported (check `Files changed` - it's in there!). Our code styling tools just want to correctly sort/order inputs into the standard format. @Rocketknight1 Sure, I changed the file, but I didn't change the sort order, it should already be sorted! :P Sorry @CISC reviewing! Do you want us to fix the CIs for you? I can push directlyu > Do you want us to fix the CIs for you? I can push directlyu

Yes please, thank you. :) thanks @CISC 🤗 Looks marvelous sorry for being so late. I actually wanted to wait because we added https://github.com/huggingface/tokenizers/pull/1678 a decode stream api in `tokenizers` that fixes some issues with prefix space, but the changes are for another PR and for the base Text streamer! 🤗 cc @gante ","Looks marvelous sorry for being so late. I actually wanted to wait because we added https://github.com/huggingface/tokenizers/pull/1678 a decode stream api in `tokenizers` that fixes some issues with prefix space, but the changes are for another PR and for the base Text streamer! 🤗 cc @gante ","# Asynchronous text generation streaming

Added a new `AsyncTextIteratorStreamer` class that allows you to have fully asynchronous text generation streaming in your application. Works identically to `TextIteratorStreamer` except that you iterate the stream as follows:
```python
generated_text = """"
async for new_text in streamer:
    generated_text += new_text
```

~~Requires Python 3.11+ due to use of `asyncio.timeout` to handle timeout.~~ Falls back to `asyncio.wait_for` if `asyncio.timeout` is not available.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?

~~Any hints on how best to implement an async test would be appreciated. :)~~ pytest-asyncio seems to be the way to go...

## Who can review?

@gante @ArthurZucker",
33932,2024-10-03T22:16:35Z,2024-10-24T09:02:54Z,Abhishek-TAMU,23,10,38,212,9,1,2,[],63609.0,0,6691661.0,0,0,0,0,558352.532663,,0,38,0,False,"['ma787639046', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'Abhishek-TAMU']","This way we can potentially add more kwargs without changing the forward! 
 Thanks for the review and welcoming this PR.
The changes suggested by you have been made @ArthurZucker  @ArthurZucker Made the necessary changes. 
Feel free to suggest changes if required any. Thanks! @ArthurZucker Would you mind facilitating in moving ahead with the related PR in TRL which supports this PR: https://github.com/huggingface/trl/pull/2158 ? Okay! Overall looks good. 
1. we need to protect the import of Unpack : 
2. let's just add an example in the documentation of how to use this! a small python snippet! The CI's should mostly go green with this! 
Then you will have `make fix-copies` that is gonna propagate the changes! 
 Once we merge I'll ping TRL team to make sure they don't miss it!  @ArthurZucker Changes have been pushed to fix CI tests.  For documentation does below sample script works which only performs tuning of Llama model using `SFTTrainer` with `torch_compile` flag on and using DataCollator with padding free ? @ArthurZucker 


```
#################### IMPORTS ###################

import math
import datasets, dataclasses
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments
)
from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM

#################### MODEL LOADING USING FLASH ATTENTION ###################

model = AutoModelForCausalLM.from_pretrained(""meta-llama/Llama-3.2-1B"", attn_implementation=""flash_attention_2"")
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

#################### DATA PREPROCESSING (PADDING_FREE) ###################

response_template = ""\n### Label:""
response_template_ids = tokenizer.encode(
    response_template, add_special_tokens=False
)[2:]
data_collator = DataCollatorForCompletionOnlyLM(
    response_template_ids,
    tokenizer=tokenizer,
    ignore_index= -100,
    padding_free=True
)
format_dataset = lambda example: {  
    ""output"": example[""output""] + tokenizer.eos_token
}
data_files = {""train"": ""path/to/dataset""}
json_dataset = datasets.load_dataset(""json"", data_files=data_files)
formatted_train_dataset = json_dataset[""train""].map(format_dataset)

################# Trainer CONFIG ############################

train_args = TrainingArguments(
    num_train_epochs=5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=0.00001,
    weight_decay=0,
    warmup_ratio=0.03,
    lr_scheduler_type=""cosine"",
    logging_steps=1,
    include_tokens_per_second=True,
    save_strategy=""epoch"",
    output_dir=""tmp"",
    torch_compile=True,
    torch_compile_backend=""inductor"",
    torch_compile_mode=""default""
)
transformer_train_arg_fields = [x.name for x in dataclasses.fields(SFTConfig)]
transformer_kwargs = {
    k: v
    for k, v in train_args.to_dict().items()
    if k in transformer_train_arg_fields
}
training_args = SFTConfig(**transformer_kwargs)

####################### TUNING #####################

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=formatted_train_dataset,
    data_collator=data_collator,
    dataset_text_field=""output"",
    args=training_args,
)
trainer.train()
``` Yep looks perfect! Especially if we can have % improvement ! 
 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33932). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Made changes as suggested. Feel free to have a look at overall changes to confirm @ArthurZucker . Also which section can I add the above documentation script ? Probably under this section: https://huggingface.co/docs/transformers/llm_optims#flashattention-2  If you have issue with just make fix copies I can take it over if you want! Sure, That would be helpful. Thank you! There seems to be some mismatch in `src/transformers/models/glm/modeling_glm.py`.  On it! I am just waiting on #34283 to be merged! The new helper will be this:
<img width=""441"" alt=""image"" src=""https://github.com/user-attachments/assets/d7061ff6-535e-4558-b454-039869d2c345"">

(with loss kwargs!) Thanks @ArthurZucker for the code change to accomodate LossKwargs.  Thanks @Abhishek-TAMU for your contribution! 🚀  > Once we merge I'll ping TRL team to make sure they don't miss it

Hi @ArthurZucker, do you mind facilitating this ? PR: https://github.com/huggingface/trl/pull/2158 Hi @Abhishek-TAMU @ArthurZucker , very nice PR for adding FlashAttentionKwargs. 
In [modeling_llama.py#L959](https://github.com/Abhishek-TAMU/transformers/blob/cb08b6371101dae92867d52b1a3b24405779d46f/src/transformers/models/llama/modeling_llama.py#L959), I noticed that `**flash_attn_kwargs` is added to the inputs of `decoder_layer` when gradient checkpointing is not used. Could you please also add `flash_attn_kwargs` when using gradient checkpointing in the above [if branch at line 938](https://github.com/Abhishek-TAMU/transformers/blob/cb08b6371101dae92867d52b1a3b24405779d46f/src/transformers/models/llama/modeling_llama.py#L938)? If checkpointing function does not accept kwargs, can we make all FlashAttentionKwargs as optional input fields of `LlamaDecoderLayer.forward`? kwargs are by default optional! 
Checkpoint indeed does not support kwargs, at least the way we have formulated it. Will be fixed by #34987A very nice PR and very much welcome! 
Let's add general kwargs, #31446 has some commits with that 
![image](https://github.com/user-attachments/assets/95f38652-6163-4c66-b161-ab8103eebc24) Nice! Thinking that we can call then `flash_attn_kwargs` to not depend on versioning ! 🚀 last few nits and we can add these flash_attn_kwargs to all models afterwards! Great contribution LGTM letting you resolve the conflicts and we can merge!
 Okay! This LGTM!
You just need to run the `make fix-copies` to make sure CIs go green 🚀 ","A very nice PR and very much welcome! 
Let's add general kwargs, #31446 has some commits with that 
![image](https://github.com/user-attachments/assets/95f38652-6163-4c66-b161-ab8103eebc24) Nice! Thinking that we can call then `flash_attn_kwargs` to not depend on versioning ! 🚀 last few nits and we can add these flash_attn_kwargs to all models afterwards! Great contribution LGTM letting you resolve the conflicts and we can merge!
 Okay! This LGTM!
You just need to run the `make fix-copies` to make sure CIs go green 🚀 ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR removes the function call `prepare_fa2_from_position_ids` in `flash_attention_forward` as it causes graph break when `torch_compile` flag is turned on in Training [arguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments.torch_compile) to use in SFTTrainer to perform padding free tuning of Llama model. This is because code in `prepare_fa2_from_position_ids` incur a cpu-gpu sync that is unavoidable. 
Hence `cu_seq_lens_q`, `cu_seq_lens_k`, `max_length_k`, `max_length_q` is now taken from the batch in `DataCollatorForCompletionOnlyLM` with this [PR](https://github.com/huggingface/trl/pull/2158) to avoid call to `prepare_fa2_from_position_ids` in `flash_attention_forward`. 

CC: @ani300 @ArthurZucker

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Actually something we had planned 😅 cc @gante on generate unpadding the input!  @Cyrilvallez as well if you want to have fun IMO can be quite impactfull!  these are are FlashAttention specific. IMO it would make sense to just add them as `fa2_kwargs` for example. We can use something like this: https://github.com/huggingface/transformers/blob/ce81eb8d14ae39e183a8956cce3f7e2e46ecaa8f/src/transformers/processing_utils.py#L81 

 ```suggestion
        **kwargs: Unpack[Fa2Kwargs],
``` ```suggestion
        **fa2_kwargs: Unpack[Fa2Kwargs],
``` ```suggestion
from typing import List, Optional, Tuple, Union
``` ```suggestion
from ...processing_utils import (
    FlashAttentionKwargs, Unpack
)    
``` We should keep this in case cu_seq_lens_q/k and etc are not passsed to compute them!  ```suggestion
```
I think we should do this in the `_flash_attention_forward` wrapper this we have 0 modeling changes, and all models will benefit easily from this! nice catch this was breaking compile on my side as well"
34224,2024-10-17T14:59:21Z,2024-12-20T08:22:05Z,nikosanto13,10,0,2,48,10,2,2,[],431095.0,0,5505764.0,0,0,0,0,560884.200322,,1,2,0,False,"['nikosanto13', 'LysandreJik', 'ylacombe']","Maybe also cc @xenova for ONNX :) @LysandreJik nobody responded so far, should I cc someone else? The best suited to review is @ylacombe I believe; let's give Yoach a few days to answer, thanks for your PR @nikosanto13! ok nice, thanks for the help @LysandreJik  thanks for mentioning @ylacombe - I'll rebase and add an empty commit for the slow tests.

For your comment about this being a costly operation, I admit I pretty much adopted the same patch (with the previous pr), without giving it much consideration. My initial thought is that this shouldn't be much of a concern (given that the masks are boolean for the majority of cases).

Let me conduct a mini benchmark and I'll return with actual numbers. > Let me conduct a mini benchmark and I'll return with actual numbers.

Great, looking forward to it @ylacombe I conducted a mini benchmark using WavLM (`microsoft/wavlm-base`).

System info: A10 GPU (24GB VRAM)

Details on the benchmark: I run WavLM's forward pass for both train and eval modes. In the code below, you can see the details - a batch (batch_size = 16) with 10-sec audios is generated and fed through the model.

I created the following script:
```python

from contextlib import nullcontext

import numpy as np
import torch
from torch.autograd import profiler

from transformers import AutoFeatureExtractor, WavLMModel


# taken from torch.autograd.profiler
def _format_time(time_us):
    """"""Define how to format time in FunctionEvent.""""""
    US_IN_SECOND = 1000.0 * 1000.0
    US_IN_MS = 1000.0
    if time_us >= US_IN_SECOND:
        return f""{time_us / US_IN_SECOND:.3f}s""
    if time_us >= US_IN_MS:
        return f""{time_us / US_IN_MS:.3f}ms""
    return f""{time_us:.3f}us""

# taken from torch.autograd.profiler
def _format_memory(nbytes):
    """"""Return a formatted memory size string.""""""
    KB = 1024
    MB = 1024 * KB
    GB = 1024 * MB
    if abs(nbytes) >= GB:
        return f""{nbytes * 1.0 / GB:.2f} Gb""
    elif abs(nbytes) >= MB:
        return f""{nbytes * 1.0 / MB:.2f} Mb""
    elif abs(nbytes) >= KB:
        return f""{nbytes * 1.0 / KB:.2f} Kb""
    else:
        return str(nbytes) + "" b""


def run_forward(model, feat_extractor, train=False):

    if train:
        model.train()
    else:
        model.eval()

    ctx = torch.no_grad() if not train else nullcontext()

    input_values = torch.randn(16, 10*feat.sampling_rate).to(device)
    attention_mask = torch.ones_like(input_values, dtype=torch.bool)

    with ctx, profiler.profile(use_cuda='cuda', profile_memory=True) as prof:
        output = model(input_values, attention_mask=attention_mask)

    events_list = prof.key_averages()
    mask_event = [elem for elem in events_list if elem.key == 'MASK_HIDDEN_STATES'][0]

    print(mask_event)
    return mask_event.device_time, mask_event.device_memory_usage


if __name__ == ""__main__"":

    device = 'cuda'
    model = WavLMModel.from_pretrained(""microsoft/wavlm-base"").to(device)
    feat = AutoFeatureExtractor.from_pretrained(""microsoft/wavlm-base"")

    times, mems = [], []
    for i in range(10):
        time, mem = run_forward(model, feat)
        # skipping first iteration due to additional overhead
        if i > 0:
            times.append(time)
            mems.append(mem)

    avg_time, std_time = np.mean(times), np.std(times)
    avg_mem, std_mem = np.mean(mems), np.std(mems)

    print(f""Average time: {_format_time(avg_time)} ± {_format_time(std_time)}"")
    print(f""Average memory: {_format_memory(avg_mem)} ± {_format_memory(std_mem)}"")
```

**NOTE**: In addition to that, you also have to wrap the masking operation with the `profiler.record_function` context manager. E.g. in modeling_wavlm.py and the `WavLMEncoder` forward, I replaced this

```python
if attention_mask is not None:
       expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])
       hidden_states[~expand_attention_mask] = 0
```

with this:

```python
# don't forget to import torch.autograd.profiler
# from torch.autograd import profiler
if attention_mask is not None:
            with profiler.record_function(""MASK_HIDDEN_STATES""):
                expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])
                hidden_states[~expand_attention_mask] = 0
```


-----

The following measurements were obtained:

| branch/mode |  Time  | CUDA Memory Consumption |
|--------------|--------| ----------------------------- |
| main/train | 278.000us ± 7.333us  |  8.00 Kb ± 0.0 b  |
| onnx-export-bugfix-where-nodes/train | 435.222us ± 12.417us | 11.70 Mb ± 0.0 b |
| main/eval   |  145.444us ± 1.066us  |  0.0 b ± 0.0 b  |
| onnx-export-bugfix-where-nodes/eval |  251.333us ± 3.127us |  5.85 Mb ± 0.0 b | 



 Hey @nikosanto13, thanks for conducting the benchmark! I'm not sure to understand what you benchmarked though. Did you compare the speed of this new operation as compared to how it was done previously? Hey @ylacombe - well, based on your message I figured I should benchmark both speed and memory consumption.

As you can see in the attached code snippet I'm comparing those two measurements for a forward pass of the WavLM model, before (main) and after the changes (my branch).

Do you think of a more appropriate way to benchmark this? I'd be happy to extend this. Thanks @nikosanto13, it's much clearer now. Looks like it's not a costly operation. This PR looks great to me.

cc @ArthurZucker  or @Rocketknight1 for a core maintainer review!LGTM @nikosanto13 , thanks for opening the PR!

My only worry is that it seems like a costly operation, are we sure this is the best / most elegant way of expanding the mask ?

BTW, you should make sure tests pass. Could you rebase on main, and check that the whole CI is green ?

We also have to make sure this doesn't break our slow tests, let's do an empty commit with the message name: `[run-slow] data2vec, hubert, sew, sew_d, unispeech, unispeech_sat, wav2vec2, wav2vec2, wav2vec2_bert, wav2vec2_conformer, wavlm` Sounds good 🤗 let's merge!","LGTM @nikosanto13 , thanks for opening the PR!

My only worry is that it seems like a costly operation, are we sure this is the best / most elegant way of expanding the mask ?

BTW, you should make sure tests pass. Could you rebase on main, and check that the whole CI is green ?

We also have to make sure this doesn't break our slow tests, let's do an empty commit with the message name: `[run-slow] data2vec, hubert, sew, sew_d, unispeech, unispeech_sat, wav2vec2, wav2vec2, wav2vec2_bert, wav2vec2_conformer, wavlm` Sounds good 🤗 let's merge!","# What does this PR do?

This old issue https://github.com/huggingface/transformers/issues/10004 described an error in the onnx export of wav2vec2-base-960h.
The issue was partially fixed for some of the speech foundation models in this PR: https://github.com/huggingface/transformers/pull/16004.

This PR:
* applies the same change of expanding the attention mask in all modeling scripts that this occurs (e.g. I came across this when trying to export `wavlm-base-plus` (modeling_wavlm.py) to onnx),
* applies the same change of expanding the downsampled padding mask in the <Wav2Vec2, Hubert, ...>ForSequenceClassification modules' forward implementation, because the onnx export of these fails similarly, due to the same broadcasting-related error

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Tagging @xenova, @ylacombe, @eustlb  (let me know if I should also add someone else)",
34770,2024-11-18T06:44:26Z,2024-12-20T08:45:53Z,wejoncy,13,30,24,650,21,4,2,[],28715.0,0,2772244.0,0,0,0,0,559299.737806,,0,24,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev', 'wejoncy', 'MekkCyber', 'YangWang92', 'ArthurZucker']","cc @MekkCyber  Niiiiice! 🚀  Awesome PR @wejoncy, Thank you for adding this amazing quantization method ! The integration looks very smooth🔥! I Just left some minor comments > Awesome PR @wejoncy, Thank you for adding this amazing quantization method ! The integration looks very smooth🔥! I Just left some minor comments

Hi @MekkCyber   Really appreciate your review on this PR and thanks for your interest on VPTQ quantization integration into transformers.
I have addressed your comments in the new commits thanks again.

Please let me know if there is anything need to be fixed. Thanks for iterating @wejoncy LGTM 🔥! Left only two small nits Thanks for the updates @wejoncy ! Can't wait for the PR to be merged ! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34770). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Thanks the PR ! Excited to see more VTPQ models on the hub. Just a few nits. Thanks also for the nice tests and the documentation

Thank @SunMarc, for your review! We are actively developing new models and algorithms, and we expect to release support for multimodal models (LLaMA 3.2 and Qwen2-VL) next month. friendly ping @ArthurZucker  Hi @ArthurZucker ,
A friendly Ping. Could you please take a look this PR?  I will be actively updating code if any suggestions. 
Thanks. Hi @ArthurZucker,
Just a polite ping to check in—may I ask if there’s anything else we need to modify? Thank you!

Best regards,
Yang Super sorry for the late review, I am a bit under water 😓  > Super sorry for the late review, I am a bit under water 😓

Thanks so much! And thanks to everyone for their help! Special thanks to @xianbaoqian for the support—super grateful! 😊Thanks the PR ! Excited to see more VTPQ models on the hub. Just a few nits. Thanks also for the nice tests and the documentation  Let's go! 🤗 
","Thanks the PR ! Excited to see more VTPQ models on the hub. Just a few nits. Thanks also for the nice tests and the documentation  Let's go! 🤗 
","# What does this PR do?

This PR is intended to add support for Extreme Low-bit Vector Post-Training Quantization(VPTQ) to the transformers library.

VPTQ is a novel Post-Training Quantization method that leverages Vector Quantization to high accuracy on LLMs at an extremely low bit-width (<2-bit). VPTQ can compress 70B, even the 405B model, to 1-2 bits without retraining and maintain high accuracy.. More details here: https://github.com/microsoft/vptq


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@SunMarc @younesbelkada @ArthurZucker 
","Just a small nit it would be nice to link the paper too Just for consistency with other quantization methods the imports are not done inside the `replace_with_xxx_linear` you can move them outside😁. And no need to check if `vptq` and `accelerate` are available at this stage, the `validate_environment` method in the `quantizer` will take care of that Just a question, I noticed you don't need `modules_to_not_convert` so I assume you quantize all layers, even the `emebeddings` and `lm_head`. Does that work even in extreme quantization ? Just a small nit, for consistency and a better performance you can do this import inside `_process_model_before_weight_loading` 🙏 In the overview page, you specified that `vptq` don't work on cpu, was that intended ? I think It would be cleaner to have something like this for vtpq : 
```
def is_vptq_available(min_version: str = VPTQ_MIN_VERSION):
    return _accelerate_available and version.parse(_vptq_version) >= version.parse(min_version)
```
You can follow  the example of `accelerate` in the `src/transformers/utils/import_utils.py` From what I understand here the config.json file will contain a vptq layer config with all the specified parameters for each Linear layer in each model layer, so for a Llama3-70B  with 80 model layers and 7 Linears in each layer that would be 80*7 = 560 layer configs. Is there a way to reduce that ? Can you add a test for `replace_with_vptq_linear` please to test the conversion ? that would be awesome 😃 Small nit ! If inference on cpu works, you can add a test of the cpu case too  Sure.  Good suggestion.
Thanks Well, we used `modules_to_convert` in config json, it has all the layers which should to be quantized and include corresponding parameters.
But I add `modules_to_not_convert` in the function signature to allow user setting which layer should be excluded. Sure. Done Sounds good. Thanks. Yes, It's exactly true. Config file contains all specific parameters and it a bit huge.
We used this to customize different quantization parameters for different linear layers to have the best performance.
But we will try to figure out a way to allow a simplified configuration. Sure.  I have fixed the `CPU` device option availability check.  We will optimize the CPU kernel in the next VPTQ release.
After that I will open another PR to support CPU and add a `CPU` unit test. Just a small nit, now that `is_vptq_available` checks the version too, you can change the message to specify that `vptq` should be installed and it requires the min version you want Just for a bit of clarity you can specify here in a comment that 25 corresponds to 24 `decoder.layers.{layer_idx}.fc1` + `lm_head` in `modules_to_not_convert` Can we, for each `Linear` layer like `q_proj` or `down_proj`, have a list of parameter values, and each parameter corresponds to a layer ? We are working CPU kernels, but let's mark it unavailable for CPU at first. Yes. we have VPTQ_MIN_VERSION=0.0.4 as default.

Good suggestion. I have added the minimal version in the error logs for guiding user clearly. Sounds good. Thanks Hi @MekkCyber  sorry, not quite following your suggestion, could you please explain a little bit more? 
What we did is for each `linear` layer, we give it a paraments dict, such as
```
model.decoder_model.layers.0.self_attn.q_proj: {""v_len"":[-1, 8], .....},
model.decoder_model.layers.0.self_attn.k_proj: {""v_len"":[-1, 8], .....},
.
.
.
model.decoder_model.layers.n.self_attn.q_proj: {""v_len"":[-1, 8], .....},
``` Feel free to add the link to the VPTQ-community page, so the users can test other models. Also could you also redirect users to the VPTQ library or a script if they want to quantize a model.  not used 
```suggestion
``` Could you add the type of each args like the other quantization config ?  why is this needed ?  If you have any graphs/benchmark to share about the performance of this method, feel free to add it also.  Hi @wejoncy, sorry I wasn't clear, I meant smth like this for example for the `up_proj` linears : 
```
model.layers.mlp.up_proj: {
        ""bias"": [null, null, ..., null],
        ""enable_norm"": [true, true, false, ..., true],
        ...
        ""num_centroids"": [
          (-1, 65536), (-1, 65536), ...,
        ],
        ...
      },
```
So that each property contains a list of values, and each value corresponds to a layer, so the length of the lists, is the number of layers, and for the i-th layer, we access the `bias`property for example using : `model.layers.mlp.up_proj[""bias""][i]`"
35347,2024-12-19T19:28:25Z,2024-12-19T22:45:52Z,tomaarsen,1,0,1,4,1,1,1,[],1731.0,0,11847.0,0,0,0,0,595458.535208,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35347). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice, thanks for adding! 🚀 ","Nice, thanks for adding! 🚀 ","# What does this PR do?

Add a link to a Text Classification finetuning script that finetunes a ModernBERT model on GLUE.

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

Documentation: @stevhliu
cc @ohmeow @NohTow @bclavie @warner-benjamin 

- Tom Aarsen
",
35340,2024-12-19T10:33:14Z,2024-12-19T16:05:25Z,ArthurZucker,1,0,3,119,4,1,1,[],1589.0,0,19933.0,0,0,0,0,619484.879796,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35340). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM! Thanks!,LGTM! Thanks!,"# What does this PR do?
Since the FA2 interface changed, it's not useful to test the classes anymore",
35158,2024-12-09T07:18:20Z,2024-12-19T13:03:36Z,warner-benjamin,14,30,91,3570,19,4,0,['New model'],86597.0,0,913628.0,0,0,0,0,601484.69568,,1,91,0,False,"['tomaarsen', 'warner-benjamin', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'staghado']","cc @Cyrilvallez ! 🤗  @ArthurZucker @Cyrilvallez 
ModernBERT requires no `token_type_ids`, but the tokenizers rely on `PreTrainedTokenizerFast`. By default, this produces `token_type_ids`. Is it preferable that we:
1. Use `model_input_names` in all `config.json` of all ModernBERT models. This means that ""fresh tokenizers"" won't work out of the box, but people don't normally make fresh tokenizers.
2. Create a custom `ModernBertTokenizerFast` that is literally just:
```python

class ModernBertTokenizerFast(PreTrainedTokenizerFast):
    model_input_names = [""input_ids"", ""attention_mask""]
```

cc @warner-benjamin @orionw

- Tom Aarsen With all of these changes in place, I was able to confirm that the output to one of the trained models using the original research implementation nearly matches the output of the `transformers` ModernBERT-converted model. The only difference that remains is that the research implementation fuses the `self.mlp(self.mlp_norm(...))` using `@torch.compile(dynamic=True)`. I also only tested with a small input - I still have to test 1) larger inputs, 2) truncation, 3) batches, etc.

Do we allow something like this to get an exact 1-1 match?
```python
    @torch.compile(dynamic=True)
    def compiled_mlp(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return self.mlp(self.mlp_norm(hidden_states))
```

Here's an indication of the difference between with and without:
```
tensor([[[ 0.0078,  0.0078,  0.0039,  ...,  0.0117,  0.0342,  0.0039],
         [ 0.0156,  0.0000,  0.0234,  ...,  0.0000,  0.0156, -0.0273],
         [-0.0015, -0.0010, -0.0020,  ...,  0.0000,  0.0022, -0.0015],
         ...,
         [-0.0195,  0.0000, -0.0029,  ..., -0.0156,  0.0146,  0.0039],
         [ 0.0078, -0.0273, -0.0059,  ..., -0.0215,  0.0103,  0.0088],
         [-0.0020,  0.0005, -0.0005,  ..., -0.0012, -0.0012, -0.0020]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SubBackward0>)
Min: -0.375
Max: 0.140625
Mean: -8.249282836914062e-05
Std: 0.01708984375
```


- Tom Aarsen I have a working first version with FlexAttention but for now there are still numerical differences between all three implementations(fa2, flexattn, sdpa) @staghado does it give normal results with e.g. mask filling? If so, then feel free to push it into this branch (or if you don't have access, push it to a branch on your fork or something, and I can cherry-pick it into this PR).

Does it use unpadded or padded? I had a feeling like the SDPA/Eager attention differences compared to FA2 actually resulted when applying the sin/cos rotation, but for FlexAttention that would be equivalent as the FA2 path if you're using unpadded.

- Tom Aarsen The current FlexAttention PyTorch API requires batched query/key input so can't be used with unpadded inputs without pad/unpad logic(Edit : actually it can)
I tried with both unpadding and padding and both exhibit differences with FA2.
I will push the code sometime tomorrow(still need to clean and do some more tests).
Mask filling performance across all three implementations is very similar but the probabilities are slightly off, here is a small table :
input : The ancient [MASK] stood silently at the edge of the dense forest

| impl          | prediction | prob      |
|----------------|-----------|------------|
| FA2            | tree      | 0.09228515625 |
| FA2            | trees     | 0.07177734375 |
| FA2            | ruins     | 0.04638671875 |
| FlexAttention  | tree      | 0.08349609375 |
| FlexAttention  | trees     | 0.0693359375 |
| FlexAttention  | ruins     | 0.047607421875 |
| SDPA           | tree      | 0.09033203125 |
| SDPA           | trees     | 0.0751953125 |
| SDPA           | ruins     | 0.04833984375 | > Is the prunning necessary or is it to follow previous implementations? 🤗

Not necessary at all. I just added it to 1) match BERT a bit better and 2) allow the BERT tests to pass. I'll remove it fully. @ArthurZucker due to small differences between FA2/SDPA/Flex/Eager, we would love to default to FA2 if `flash_attn` is installed. Would this be possible via some config option, or do we *have* to rely on users to manually specify `attn_implementation=""flash_attention_2""`? These are the remaining test failures:
```
FAILED tests/models/modernbert/test_modeling_modernbert.py::ModernBertModelTest::test_cpu_offload - torch._dynamo.exc.InternalTorchDynamoError: AttributeError: 'dict' object has no attribute 'node'
FAILED tests/models/modernbert/test_modeling_modernbert.py::ModernBertModelTest::test_disk_offload_bin - torch._dynamo.exc.InternalTorchDynamoError: AttributeError: 'dict' object has no attribute 'node'
FAILED tests/models/modernbert/test_modeling_modernbert.py::ModernBertModelTest::test_disk_offload_safetensors - torch._dynamo.exc.InternalTorchDynamoError: AttributeError: 'dict' object has no attribute 'node'
FAILED tests/models/modernbert/test_modeling_modernbert.py::ModernBertModelTest::test_resize_tokens_embeddings - AssertionError: expected size 84==109, stride 32==32 at dim=0
FAILED tests/models/modernbert/test_modeling_modernbert.py::ModernBertModelTest::test_retain_grad_hidden_states_attentions - RuntimeError: This backward function was compiled with non-empty donated buffers which requires create_graph=False and retain_graph=False. Please keep backwar...
```
with these reasons:

* `test_cpu_offload`, `test_disk_offload_bin`, `test_disk_offload_safetensors`: Unsure, something about 1) torch.compile and 2) some weight being on the meta device, but without a value to put it back on CUDA.
* `test_resize_tokens_embeddings`: Caused by torch.compile breaking with resizing
* `test_retain_grad_hidden_states_attentions`: Caused by torch.compile, though this might be fixable with some parameters to the compile call.

They don't strictly all have to be fixed, but we can also disable those tests and give the users warnings when they do something that doesn't work with this architecture (e.g. resizing tokens seems to not work currently?).

cc @warner-benjamin The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35158). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. the model with FA2 and the RoPE kernel are not torch.compile compatible, we can't compile the whole model while using these. > the model with FA2 and the RoPE kernel are not torch.compile compatible, we can't compile the whole model while using these.

FA2 is compatible now, but the FA RoPE kernel isn't yet. I have a in progress fix I need to get merged into the FA repo. We want mean pooling as an option for classification because Local Attention means unlike BERT the CLS token doesn't see all the output in all the attention layers, so mean pooling could outperform CLS on greater than 128 token sequences.

Also, I added the pooling head to TokenClassification because otherwise we are throwing away one pretrained linear layer `ModernBertPoolingHead.dense`. Great addition! Thanks all for your hard work! 🤗 Beyond the obvious (sdpa, eager, flex attention, and documentation), I haven't seen anything outrageous or very unexpected in my first scroll-through.
I recognize that this implementation goes a bit beyond our ""usual"" with unpadding/padding when possible, but I personally don't mind. Beyond this change (and the other obvious upgrades like RoPE), I quite like how this still mirrors the original BERT rather closely.

I'll have to actually start running this to get a better feel, but so far so good.

Also, the SequenceClassification and TokenClassification classes don't exist yet. Good work! 
Is the prunning necessary or is it to follow previous implementations? 🤗 ","Beyond the obvious (sdpa, eager, flex attention, and documentation), I haven't seen anything outrageous or very unexpected in my first scroll-through.
I recognize that this implementation goes a bit beyond our ""usual"" with unpadding/padding when possible, but I personally don't mind. Beyond this change (and the other obvious upgrades like RoPE), I quite like how this still mirrors the original BERT rather closely.

I'll have to actually start running this to get a better feel, but so far so good.

Also, the SequenceClassification and TokenClassification classes don't exist yet. Good work! 
Is the prunning necessary or is it to follow previous implementations? 🤗 ",This PR will add ModernBERT to Transformers.,"I don't think these are necessary, they might just be leftover copy-pastes from Gemma. This does not exist yet - I'd love to see this as it allows for training cross-encoder/reranker models. Also does not exist yet. The plan is to support all the standard encoder task heads. ```suggestion
            (""modernbert"", (None, ""PreTrainedTokenizerFast"" if is_tokenizers_available() else None)),
```
`GPTNeoXTokenizerFast` differs from what was used during training.
We could also use`ModernBertTokenizerFast` depending on my previous comment.  ```suggestion
	return_dict = return_dict if return_dict is not None else self.config.use_return_dict
	
        if batch_size is None and seq_len is None:
```
Required, or the `None` default will always cause the returning of a tuple (which is unexpected). ```suggestion
            hidden_states, _ = self._pad_outputs(hidden_states, indices, batch_size, seq_len)
```
This also returns padded `labels`, which we don't use. ```suggestion
                logits, _ = self._pad_outputs_no_grad(logits, indices, batch_size, seq_len)[0]
            else:
                logits, _ = self._pad_outputs(logits, indices, batch_size, seq_len)[0]
```
This also returns padded `labels`, which we don't use. ```suggestion
        hidden_activation=""gelu"",
```
`gelu` matches the research implementation. There's slight discrepancies otherwise with `gelu_python`. fixed with the latest push fixed The [0] index should have taken care of that, but I matched the formatting of the other one. fixed For the moment, I have it set as `PreTrainedTokenizerFast` pending discussion. You're right - I encountered the actual issue with the previous comment, and I extrapolated the fix to this one, not realising that there was an `[0]`. ```suggestion
from torch import nn
```
a nit, but the `as nn` is a bit unnecessary, let's avoid it. ```suggestion

    @torch.compile(dynamic=True)
    def compiled_mlp(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return self.mlp(self.mlp_norm(hidden_states))

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        cu_seqlens: Optional[torch.Tensor] = None,
        max_seqlen: Optional[int] = None,
    ) -> torch.Tensor:
        """"""Forward pass for a ModernBert layer, including both attention and MLP.

        Args:
            hidden_states: (total_nnz, dim)
            position_ids: (total_nnz,)
            attention_mask: (batch, max_seqlen)
            cu_seqlens: (batch + 1,)
            max_seqlen: int
        """"""
        attn_out = hidden_states + self.attn(
            self.attn_norm(hidden_states),
            position_ids=position_ids,
            cu_seqlens=cu_seqlens,
            max_seqlen=max_seqlen,
            attention_mask=attention_mask,
        )
        return attn_out + self.compiled_mlp(attn_out)
```
Required to match research implementation's performance. With this in place, the difference between the original outputs and ModernBert become:
```
tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<SubBackward0>)
Min: 0.0
Max: 0.0
Mean: 0.0
Std: 0.0
``` I think the default should be ""gelu"" I think we ran most of the evaluations using CLS pooling (although I think we have better results with mean pooling)

(edit: sorry the two comments should refer to modular_modernbert.py, where the values that are auto-generated comes from) The default is ""gelu"" indirectly right now (see the if None, then same as hidden_activation), but I prefer just using ""gelu"" immediately.
Fixed in 142ff11ea201842b609acd23cbf58c3ac70bbfbb In my opinion, this shouldn't be an argument, but instead automatically set based on `config._attn_implementation`. E.g. `False` for ""eager"", `True` for ""flash_attention_2"". I *suspect* that this will have to be unsqueezed or batch-ified somewhere. We don't have the eager attention yet, but it probably requires expanding the attention mask to 4D somewhere around here, e.g.:
```python
        # expand attention_mask
        if self.config._attn_implementation == ""eager"" and attention_mask is not None:
            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)
```

I didn't get the other attentions working yet @warner-benjamin, perhaps you can try and tackle those. `self.loss_function` does not exist, I believe. Let's try and get rid of these. Either via the `config`, or avoid needing them altogether.
I also feel like `pad_logits` should be inferrable via `config.unpad_inputs` and `pad_logits_no_grad` via `config.unpad_no_grad`.

I'm not sure what we should do with the `sparse_prediction` stuff.  Resolved via 38f959bf096bd05e1174ed4495daac004bf115fa, always open to suggestions Added via 5e3d61d7180540003f5753c3f4186b456d160291 Added via 5e3d61d7180540003f5753c3f4186b456d160291 Added this in 5e3d61d7180540003f5753c3f4186b456d160291 all of this should go in the `def _init_weights(self, module):` function of the `ModerBertPreTrainedModel` and we should remove the ModuleTypes enumerations. "
31654,2024-06-27T11:06:08Z,2024-07-11T12:01:47Z,OmarManzoor,5,6,5,57,2,2,1,[],615615.0,0,15136754.0,0,0,0,0,620691.086471,,0,5,0,False,"['OmarManzoor', 'wkml', 'amyeroberts']","> Hi @OmarManzoor,
> 
> Thanks for addressing this! We want to make sure we catch any place where the renaming happens, so any place where `if gamma in key` and `if beta in key` are True (so key can be a longer string that contains beta or gamma). As you've added, this would be in `_load_pretrained_model` but also in [_load_state_dict_into_model](https://github.com/huggingface/transformers/blob/e65502951593a76844e872fee9c56b805598538a/src/transformers/modeling_utils.py#L662)

Hi @amyeroberts 
Thanks for the feedback. Should we remove it during initialization? I added it in post init because during the main init we might not have the parameters declared. >> Given the diff, I'm slightly confused, were there no warnings being triggered before? It seems like they were from the tests and logging messages

I basically removed the warning code that I added in the post init method. Should that be kept? @OmarManzoor Ah, OK. I think the diff was rendering funny on github. Should be OK.  > Looks great - thanks for adding and iterating on this!

Thank you. Why have you added warnings only for the initialization process and not for renaming during loading as well? The model I'm using is timm's convnext (which is even the companion framework to transformers), which would have the parameter gamma. When loading he just tells me that I didn't successfully load the gamma function without telling me why, and I think the user should be informed when renaming the state_dict, otherwise it will cause unnecessary confusion.Hi @OmarManzoor, 

Thanks for addressing this! We want to make sure we catch any place where the renaming happens, so any place where `if gamma in key` and `if beta in key` are True (so key can be a longer string that contains beta or gamma). As you've added, this would be in `_load_pretrained_model` but also in [_load_state_dict_into_model](https://github.com/huggingface/transformers/blob/e65502951593a76844e872fee9c56b805598538a/src/transformers/modeling_utils.py#L662) Hi @OmarManzoor, thanks for iterating on this! 

Given the diff, I'm slightly confused, were there no warnings being triggered before? It seems like they were from the tests and logging messages 
 Looks great - thanks for adding and iterating on this! ","Hi @OmarManzoor, 

Thanks for addressing this! We want to make sure we catch any place where the renaming happens, so any place where `if gamma in key` and `if beta in key` are True (so key can be a longer string that contains beta or gamma). As you've added, this would be in `_load_pretrained_model` but also in [_load_state_dict_into_model](https://github.com/huggingface/transformers/blob/e65502951593a76844e872fee9c56b805598538a/src/transformers/modeling_utils.py#L662) Hi @OmarManzoor, thanks for iterating on this! 

Given the diff, I'm slightly confused, were there no warnings being triggered before? It seems like they were from the tests and logging messages 
 Looks great - thanks for adding and iterating on this! ","# What does this PR do?
This adds a warning message to notify about the renaming of `gamma` and `beta` parameters during initialisation and also during loading.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #29554


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you write any new necessary tests?


## Who can review?

@amyeroberts 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc 

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","I don't this is quite right, this assumes the weight is called ""beta"" in the state dict, but it could be called ""layer.beta"" More importantly, we should check that the parameter is renamed as well Let abstract out this warning a bit, as it's repeated in so many places. 

Let's create a global which we can format where needed 

```
PARAM_RENAME_WARNING = ""A parameter name that contains `{}` will be renamed internally to `{}`. Please use a different name to suppress this warning.""
```

And then in the function: 

```suggestion
            if ""beta"" in key:
                logger.warning(PARAM_RENAME_WARNING.format(""beta"", ""bias""))
                return key.replace(""beta"", ""bias"")
            if ""gamma"" in key:
                logger.warning(PARAM_RENAME_WARNING.format(""gamma"", ""weight""))
                return key.replace(""gamma"", ""weight"")
``` I tried this out and it seems that the parameter is not renamed at all. Basically when we load the model using from_pretrained it seems that the parameter is still present with the name `gamma_param`. It shouldn't rename the value in the model, but will rename the value in the state_dict, I believe. Could you dive into the loading logic and verify what's happening?  I tried updating the tests. Could you kindly have a look?"
32970,2024-08-22T20:02:06Z,2024-09-17T18:36:11Z,Nik-Kras,20,0,5,29,2,3,3,[],46696.0,0,10246778.0,0,0,0,0,640110.115435,,1,5,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'gante', 'ylacombe', 'Nik-Kras', 'Daniel-Kelvich', 'eustlb', 'dengchengxifrank']","@Nik-Kras Thank you for opening this issue 🤗 

Regarding beam search: the core method was untouched, except for bug fixes on edge cases. In other words, it is possible but unlikely, our integration tests should have caught noticeable differences in quality 🤗 We did change the input preparation for `generate` in Whisper, which might explain the changes you're seeing. 

If you can get us a snippet where the outputs are different between an old version and the current version, that could help us pin the potential bug! 🐛 

(passing the return questions in the issue along to @sanchit-gandhi / @kamilakesbi , as this is a Whisper-specific question :) ) > @Nik-Kras Thank you for opening this issue 🤗

Happy to contribute!

> If you can get us a snippet where the outputs are different between an old version and the current version, that could help us pin the potential bug! 🐛

Above, I attached a code snippet to generate 5 hypotheses with beam search and print them along with sequences_scores. 

Here is the output for Transformers 4.25.1
```
Hypothesis 1:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.4627407491207123
Hypothesis 2:  How is Mozilla going to handle and be with this? Thank you and Q.. Score: -0.4789799749851227
Hypothesis 3:  How is Mozilla going to handle and be with this? Thank you, and cute.. Score: -0.48414239287376404
Hypothesis 4:  How is Mozilla going to handle and be with this? Thank you and cute.. Score: -0.4972183108329773
Hypothesis 5:  How is Mozilla going to handle and be with this? Thank you, and Q.. Score: -0.5054414868354797
```

Here is the output for Transformers 4.44.1 (after my fix to return sequences_scores)
```
Hypothesis 1:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495032072067261
Hypothesis 2:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495031476020813
Hypothesis 3:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495032072067261
Hypothesis 4:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495031476020813
Hypothesis 5:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495032072067261
```

I only study this topic, but according to articles that explain Beam Search: 
- https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24
- https://huggingface.co/blog/how-to-generate

For each step, you select top `num_beams` tokens instead of a greedy selection of top-1 tokens. Therefore, you should select `num_beams` of different tokens. And while they could converge for some part of the sequence (if other sequences have too low scores), in the end, they still must be different according to the algorithm. And I am not sure the update in input data could change it.

Tell me if I misunderstood Beam Search, but I've seen @patrickvonplaten explaining it with top-k selection, which would result in different tokens. Couldn't find the source, unfortunately.  @Nik-Kras ah, an important detail: Whisper has a more complex generation algorithm, which goes beyond the traditional beam search (i.e. it has many heuristics on top) 🤗 Have a look at the [original paper](https://arxiv.org/pdf/2212.04356), starting with Table 7 @gante Thanks for sharing! I really appreciate your assistance, as it helps me to learn more!

I was sure that even if Whisper has additional algorithms on top of Beam Search, the results over different versions of Transformers should've been similar, if not the same. 
Do you think that Whisper's deviation from the original Beam Search implementation could be a valid explanation of the behaviour above?

(I am just learning, so might be wrong) @Nik-Kras 

> the results over different versions of Transformers should've been similar

This is a valid argument that we're trying to honor in general! The exceptions are a) bug fixes b) clear performance upgrades (quality or throughput). The changes for Whisper would fall in clear performance upgrades -- see the [original PR](https://github.com/huggingface/transformers/pull/27658) for WER upgrades.

> Do you think that Whisper's deviation from the original Beam Search implementation could be a valid explanation of the behaviour above?

Certainly! For instance, the modification to generate with a fallback suggested in the paper explicitly asks to regenerate the output in certain conditions. It also overwrites a few parameters in some conditions, including the number of beams for beam search. @Nik-Kras  I wonder what exactly the ""scores"", and ""sequences_scores"" mean. I also wonder if I pass output_logits=True to the generate method, where can I check the specific operation process. For example,if my lable is [1,12,3],and I get the logits, how can I use logits to calculate the CrossEntropy Loss. Thanks! I believe you are trying to find the source to help with your CrossEntropy issue; this Pull Request is quite different; it focuses on the Beam Search methodology of generating sequences. Read more about it here:
- https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24
- https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175/15?u=patrickvonplaten
- https://huggingface.co/docs/transformers/generation_strategies#beamsearch-multinomial-sampling

> @Nik-Kras I wonder what exactly the ""scores"", and ""sequences_scores"" mean.

Below is my personal reasoning
Scores are kind of logits used to select the next token for each beam in the beam search. Sequences_scores is a kind of rating of each generated sequence that helps to select the best one. It is commonly calculated by multiplying the probabilities of each selected token in the beam. So, if the generated sequence consists of low-probability words, there is probably something wrong with the sequence, and it shall not be selected. Sequence Score simplifies your selection by comparing one number to the other. @Nik-Kras Thanks for the documents. I can get the logits but as for each timestep in beam search, I wonder which index the beam search algorithm picks (Sometimes not the largest one). I want to calculate the CrossEntropy Loss at each timestep as a ""metric"" and finally get the N-Best results. Or may I change my problems as: how can I get the N-best results with scores? I think it is inappropriate for me to ask questions under this pull request. Would it be better to ask such questions in issues? Thanks！！！ cc @eustlb if you have some bandwidth to take a look at this while @ylacombe is off! Also, how long is your input audio ? @ylacombe The audio is 7 seconds long. It is taken from Common Voice 17 and you can find it by ground truth transcription ""How is Mozilla going to handle ambiguities like queue and cue?"". I did put a link to the audio from HuggingFace, but it seems like it is broken. Trying again [link](https://datasets-server.huggingface.co/cached-assets/NikitaKras/cv_17_test_en_es_fr/--/92f162ad51a63a36d3ed1d4f55d77b2bd9b6b84c/--/default/en/8/audio/audio.mp3?Expires=1725485286&Signature=MEBcgycJ-RRybtkMjMgcXZix6fC5z7Z50OULb7lG3fMah8Q9No3V70fY90-01MKs~HCEuHHqBJi9Ag56-VlU88wjpKvDtn3vdtG5ySk-hymjiXcx~do~fqGGYOjyb10A8BlxdX9s2WgEmRGtEIQGRj0~6Y2zxGBLmZcfHQCGpJZnPkwPRox1oUsN3oKIfWWeJIYf5UIP9evHTo9vahsD-4RroAcern47DDFrXAxjiphc7KD49yRtbvomJdrioO0AHmRlmPjFBD9injgWshJNFyZEkChKvvHbbTAH7Tg4VmcDfILU5I13gdNqSqdr4GUsaJgbprKTndPXmMYY2PEwNw__&Key-Pair-Id=K3EI6M078Z3AC3)

I am a bit distracted by other projects at the moment, but the test should be very easy to write, I will add it soon. 
I am not so confident about [beam_indices](https://github.com/ylacombe/transformers/blob/979f4774f619e43a7c121c49b3c9cdc0b48d687a/src/transformers/generation/utils.py#L279-L280) as I remember there are several bugs / issues around it. It would take some time, so I would leave it for another PR Hey @Nik-Kras, thanks for circling back on this!
Whisper is used by many users, so it'd be great to have the test added soon! 
If you don't have the bandwidth in the next coming days, let us know and we can supersede the PR to add the test and `beam_indices` - while keeping you as co-author of course! @ylacombe I will allocate some time early next week, if that is fine with you @ylacombe All done. It was an easy fix; the most time I spent was setting it up :)

However, I would want to get your attention on a side issue I found - the inconsistency of Whisper's generation over different versions. I explained the comparison above, mentioning specific versions and my observations.
To fix the beam_indicies, I compared the output with version 4.25.1
The type and shape of it is the same, but the data is **significantly different**

I see that `sequences_scores` and `beam_indicies ` are not just computed differently. I strongly believe they are computed wrong. I don't expect Whisper to give 5 identical beams due to the beam search (with identical scores as well). To perform LLM post-correction, I need transcription variants, which is a desired behaviour.

Please have a look at the changes in how the output is generated. I am unsure how to spot a bug there, but I strongly believe there is one. @ylacombe Here are results of an experiment.
Correct transcription: ""How is Mozilla going to handle ambiguities like queue and cue?""
Audio is taken from Common Voice 17 dataset

code
```python
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
import torch
import librosa

# Load the processor and model
processor = AutoProcessor.from_pretrained(""openai/whisper-tiny"")
model = AutoModelForSpeechSeq2Seq.from_pretrained(""openai/whisper-tiny"")

# Load and preprocess the audio file
audio_path = ""audio.mp3""
audio, sr = librosa.load(audio_path, sr=16000)  # Ensure the sample rate is 16kHz

# Preprocess the audio to get the input features
inputs = processor(audio, sampling_rate=16000, return_tensors=""pt"")

# Generate the transcription using Beam Search with the model
beam_outputs = model.generate(
    inputs[""input_features""],
    num_beams=5,  # Number of beams
    num_return_sequences=5,  # Number of hypotheses to return
    early_stopping=True,
    output_scores=True, 
    return_dict_in_generate=True,
)

# Decode the generated transcriptions
hypotheses = [processor.decode(output_ids, skip_special_tokens=True) for output_ids in beam_outputs.sequences]

# Print out the hypotheses
for i, hypothesis in enumerate(hypotheses):
    print(f""Hypothesis {i + 1}: {hypothesis}. Score: {beam_outputs.sequences_scores[i]}"")
```

transformers v4.25.1
```
Hypothesis 1:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.4627407491207123
Hypothesis 2:  How is Mozilla going to handle and be with this? Thank you and Q.. Score: -0.4789799749851227
Hypothesis 3:  How is Mozilla going to handle and be with this? Thank you, and cute.. Score: -0.48414239287376404
Hypothesis 4:  How is Mozilla going to handle and be with this? Thank you and cute.. Score: -0.4972183108329773
Hypothesis 5:  How is Mozilla going to handle and be with this? Thank you, and Q.. Score: -0.5054414868354797
```

transformers v4.44.1 + My Fix
```
Hypothesis 1:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495038032531738
Hypothesis 2:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495040416717529
Hypothesis 3:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495036840438843
Hypothesis 4:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495036244392395
Hypothesis 5:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495033264160156
```

transformers v4.44.1 + My Fix + Your Fix
```
Hypothesis 1:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495038032531738
Hypothesis 2:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495040416717529
Hypothesis 3:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495036840438843
Hypothesis 4:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495036244392395
Hypothesis 5:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495033264160156
```

There is something else. The core algorithm of beam search seems incorrect. The algorithm should NOT select the same tokens for different beams, that is the point of top_k sampling in the beam search.
I guess another issue should be opened [Nik-Kras](https://github.com/Nik-Kras), thanks for taking the time to test it. I have the same results on my side. 

I managed to identified why this is happening. It's been introduced in #30984 and is happening because `_expand_variables_for_generation` [artificially expands the batch size to `num_return_sequences`](https://github.com/huggingface/transformers/blob/516ee6adc2a6ac2f4800790cabaad66a1cb4dcf4/src/transformers/models/whisper/generation_whisper.py#L1076-L1084). When it's then passed to [`GenerationMixin.generate`](https://github.com/huggingface/transformers/blob/516ee6adc2a6ac2f4800790cabaad66a1cb4dcf4/src/transformers/models/whisper/generation_whisper.py#L834-L844) and since `batch_size=5`, the model generates `batch_size*num_beams` beams but only keeps the most probable of them for each element of the batch.

In other words, `num_return_sequences` is not compatible with short-form and long-form generation anymore. Now that I've identified the issue, I'll think about how to best solve it. Happy to get your thoughts here!
Also cc @eustlb

PS: don't forget to do `make fixup` to correct your code quality as indicated [here](https://huggingface.co/docs/transformers/en/contributing#create-a-pull-request). Otherwise, your PR won't be merged


 Thanks for iterating! LGTM !

Would you like to open another issue for the problem you've identified? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32970). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I can still reproduce this bug with the latest transformers. Indeed, is has not been solved yet. It's mainly due to the fact that the best sequences of beam search are ambiguous when it comes to Whisper's generation, as the underlying implementation does multiple (and possibly) independent calls to GenerationMixin's generate (the method that handles beam search). A heuristic to concatenate and sort sequences after those multiple calls should be found, or we should revert the feature has it's not compatible out of the box with Whisper. 
Nevertheless, it is the next point in the bug-fixing roadmap; thanks for your patience!! 🤗(the changes lgtm, but let's get the green light from one of our audio folks as well 🤗 ) Hey @Nik-Kras, thanks for opening this PR!

You get your reasoning right, and we're indeed missing `sequence_scores`. The fix seems great to me!

Do you think:
1. You could also take care of [`beam_indices`](https://github.com/ylacombe/transformers/blob/979f4774f619e43a7c121c49b3c9cdc0b48d687a/src/transformers/generation/utils.py#L279-L280) as indicated in #32373 ?
2. Add a test [here](https://github.com/huggingface/transformers/blob/979f4774f619e43a7c121c49b3c9cdc0b48d687a/tests/models/whisper/test_modeling_whisper.py#L375) to make sure the error doesn't happen again ?

Regarding the change in beam search behaviour, let's open a subsequent issue about it, once your PR is merged! I believe it might be solved by #32336, don't hesitate to try it out and let us know if it solved your issue

cc @eustlb for visibility Thanks for the fix @Nik-Kras! LGTM! You should do `make fixup` to correct your code quality as indicated [here](https://huggingface.co/docs/transformers/en/contributing#create-a-pull-request).

Regarding the change of behaviour, this is indeed really concerning. it might be related to #32246, which I've opened a PR for here: #32336

I'll  try using #32336 on top of your PR to see if it solves your issue. I'll let you know how it goes.

If it doesn't solve the regression, then let's open another issue with a reproducer and the expected results! Thanks for fixing and adding a test! ","(the changes lgtm, but let's get the green light from one of our audio folks as well 🤗 ) Hey @Nik-Kras, thanks for opening this PR!

You get your reasoning right, and we're indeed missing `sequence_scores`. The fix seems great to me!

Do you think:
1. You could also take care of [`beam_indices`](https://github.com/ylacombe/transformers/blob/979f4774f619e43a7c121c49b3c9cdc0b48d687a/src/transformers/generation/utils.py#L279-L280) as indicated in #32373 ?
2. Add a test [here](https://github.com/huggingface/transformers/blob/979f4774f619e43a7c121c49b3c9cdc0b48d687a/tests/models/whisper/test_modeling_whisper.py#L375) to make sure the error doesn't happen again ?

Regarding the change in beam search behaviour, let's open a subsequent issue about it, once your PR is merged! I believe it might be solved by #32336, don't hesitate to try it out and let us know if it solved your issue

cc @eustlb for visibility Thanks for the fix @Nik-Kras! LGTM! You should do `make fixup` to correct your code quality as indicated [here](https://huggingface.co/docs/transformers/en/contributing#create-a-pull-request).

Regarding the change of behaviour, this is indeed really concerning. it might be related to #32246, which I've opened a PR for here: #32336

I'll  try using #32336 on top of your PR to see if it solves your issue. I'll let you know how it goes.

If it doesn't solve the regression, then let's open another issue with a reproducer and the expected results! Thanks for fixing and adding a test! ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

Fixed missing `sequences_scores`  in the Whisper beam search output .
Transformers v4.44.1 still has a bug with missing `sequences_scores` even when these values are explicitly requested. Below is the code snippet that you could use in the current version and get an error due to missing `sequences_scores`, while it will work fine with version 4.25.1

To reproduce the experiment, download any speech audio file, install transformers 4.44.1 and 4.25.1 and check the output.
In case you have problems with 4.25.1 due to `jax` and `jaxlib` versions, remove version checkers Exceptions and change versions in the setup.py to:
```python
""jax>=0.4"",
""jaxlib>=0.4.10"",
```

Audio used for this example: [link](https://datasets-server.huggingface.co/cached-assets/NikitaKras/cv_17_test_en_es_fr/--/92f162ad51a63a36d3ed1d4f55d77b2bd9b6b84c/--/default/en/8/audio/audio.mp3?Expires=1724360457&Signature=YpC9kQFkrhSElVZjOYy4hhHzb7ffmj7wEDJlFNHjNAy0h6uL9n2UB7FC~CJvxWS2WHc1D~bch~p~o2SzlTjVN9JIP9sNbgAIUPrCN1PYVV8u-uja4T9dqg69rtYczjuP2fUAs7ZwOOWEHQgAz4nCkOR23WrH73L~roW9BMXwn69DetzcqNPAmAktn5GXtbFQq36ShVJrOGPKsHQ-~NhSpYD-qw~00et6xjgM5mFJrlp8GK7z7c2F4ko9Kv7kuT1JzHKZEQHDGGnIGuEnXNUUwJsvJbhx4hspGr5EqY0eZPmUJ7ZhXJCyP-hu~jw3~2sqIrY4i6Jox3qw3RhaygYrrg__&Key-Pair-Id=K3EI6M078Z3AC3)

Code to reproduce the issue
```python
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
import torch
import librosa

# Load the processor and model
processor = AutoProcessor.from_pretrained(""openai/whisper-tiny"")
model = AutoModelForSpeechSeq2Seq.from_pretrained(""openai/whisper-tiny"")

# Load and preprocess the audio file
audio_path = ""audio.mp3""
audio, sr = librosa.load(audio_path, sr=16000)  # Ensure the sample rate is 16kHz

# Preprocess the audio to get the input features
inputs = processor(audio, sampling_rate=16000, return_tensors=""pt"")

# Generate the transcription using Beam Search with the model
beam_outputs = model.generate(
    inputs[""input_features""],
    num_beams=5,  # Number of beams
    num_return_sequences=5,  # Number of hypotheses to return
    early_stopping=True,
    output_scores=True, 
    return_dict_in_generate=True,
)

# Decode the generated transcriptions
hypotheses = [processor.decode(output_ids, skip_special_tokens=True) for output_ids in beam_outputs.sequences]

# Print out the hypotheses
for i, hypothesis in enumerate(hypotheses):
    print(f""Hypothesis {i + 1}: {hypothesis}. Score: {beam_outputs.sequences_scores[i]}"")
```

Output before the fix:
```
TypeError: 'NoneType' object is not subscriptable
```

Expected output:
```
Hypothesis 1:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495032072067261
Hypothesis 2:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495031476020813
Hypothesis 3:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495032072067261
Hypothesis 4:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495031476020813
Hypothesis 5:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.5495032072067261
```

Solves [#32373](https://github.com/huggingface/transformers/issues/32373) issue


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

PS: I am not sure that Beam Search behaviour is working as expected. The Beam Search algorithm in Transformers version 4.25.1 looks more natural to me. At least there is more diversity in beams, and they are not just repeated. I assume there is another bug in the Beam Search algorithm in the current version. But I am not confident enough to say that. Could you please review the output of the current version above and the output of the older version below and tell me if that is an expected behaviour @patrickvonplaten, @zucchini-nlp and @gante?

Output of the same code above, but with Transformers version 4.25.1, where hypotheses are more diverse (which is expected due to topk sampling). Also, scores are less similar in comparison to Transformers version 4.44.1.
```
Hypothesis 1:  How is Mozilla going to handle and be with this? Thank you.. Score: -0.4627407491207123
Hypothesis 2:  How is Mozilla going to handle and be with this? Thank you and Q.. Score: -0.4789799749851227
Hypothesis 3:  How is Mozilla going to handle and be with this? Thank you, and cute.. Score: -0.48414239287376404
Hypothesis 4:  How is Mozilla going to handle and be with this? Thank you and cute.. Score: -0.4972183108329773
Hypothesis 5:  How is Mozilla going to handle and be with this? Thank you, and Q.. Score: -0.5054414868354797
```

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35201,2024-12-11T06:20:03Z,2024-12-19T08:53:48Z,probicheaux,2,0,1,10,1,2,2,[],147.0,0,700425.0,0,0,0,0,645387.156105,,0,1,0,False,['probicheaux'],"Tested locally with reproducer from https://github.com/huggingface/transformers/issues/35200 and it fixes the issue Let me know if there's anything else I can do to help get this mergedLGTM - thanks for the fix! cc @ArthurZucker for merge
Issue was already present for PaliGemma1 Don't worry this is ready! Thanks 🤗 ","LGTM - thanks for the fix! cc @ArthurZucker for merge
Issue was already present for PaliGemma1 Don't worry this is ready! Thanks 🤗 ","# What does this PR do?

Solves: https://github.com/huggingface/transformers/issues/35200

Makes sure to add the `<eos>` token to the `suffix` in either case (`<image>` is present in `text` or not). This avoids gotchas when finetuning.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ArthurZucker @molbap ",
34982,2024-11-28T00:35:17Z,2024-12-18T19:18:18Z,fabianlim,5,30,44,4138,19,4,2,"['New model', 'run-slow', 'State space models']",46615.0,1,1845035.0,0,0,0,0,644663.878764,,0,44,0,False,"['fabianlim', 'raghukiran1224', 'Rocketknight1', 'ArthurZucker']","Hi @fabianlim, do you have a paper reference for this model or any details on the trained checkpoints? @Rocketknight1 thanks for reaching out. Yes my colleagues are preparing a paper and a GitHub repo with the (training) code. And checkpoints will be 1.8T, 2T, 2.2T, and an sft model. We will update the PR accordingly. 

cc: @raghukiran1224 The data used is all open, we plan to share any and all details on what the community would want! Open source is the name of the game 😄  Cool! @molbap will be the point of contact at Hugging Face for this PR, so feel free to ping me or him if you have any questions as you're working on it. Kudos! 🚀 Added a few short comments & pointers for the docs! All tests + integration pass and modular looks better, thanks for working on it and congrats on the model! pinging @ArthurZucker so core review starts ;)  Great work, a few nits to adresse and we can merge! ","Added a few short comments & pointers for the docs! All tests + integration pass and modular looks better, thanks for working on it and congrats on the model! pinging @ArthurZucker so core review starts ;)  Great work, a few nits to adresse and we can merge! ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

This PR merges the `BambaModel`, which is a hybrid [mamba2](https://github.com/state-spaces/mamba) architecture with SwiGLU. The checkpoints are  jointly trained by IBM, Princeton, and UIUC.

The implementation is based off  [ai21labs/Jamba-v0.1](https://huggingface.co/ai21labs/Jamba-v0.1) and the mamba2 implementation ported over to HF for the codestral model.

cc: @ani300, @raghukiran1224 

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Contributors list should go here, you can add huggingface usernames and/or github usernames here  I think for the time being we'd be better off adding `bamba` to the same function in `GenerationMixin`, in order to just be aligned, we can mention the TODO somewhere else `raise` rather than `assert` here Reviewed the modular file - a few blocks related to `layer_type` and the double masks, similar to `Jamba`, but it's cumbersome to manipulate especially in cached situations. Other than that it seems OK to me! @molbap oh ok.. so in principle this file should be ok? @ani300 needs to make some changes as it is outdated. @ani300 let me know after you fix `modular` then I will address this addressed in https://github.com/huggingface/transformers/pull/34982/commits/94a13d6b6ab6730dd6bc71f95b4a3ca823ffd93a addressed in https://github.com/huggingface/transformers/commit/94a13d6b6ab6730dd6bc71f95b4a3ca823ffd93a addressed in https://github.com/huggingface/transformers/pull/34982/commits/4cff28c8fa9f8ae0f4db7d1901e3880da15c32e2 this is done now This should be filled in as well before merging ```suggestion
``` updated in https://github.com/huggingface/transformers/pull/34982/commits/44788dce2bc7bea0dfc95b2fdf437cf714be6363 updated in https://github.com/huggingface/transformers/pull/34982/commits/44788dce2bc7bea0dfc95b2fdf437cf714be6363 ```suggestion
# Copyright 2024 IBM and the HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from typing import TYPE_CHECKING

from ...utils import _LazyModule
from ...utils.import_utils import define_import_structure


if TYPE_CHECKING:
    from .configuration_bamba import *
    from .modeling_bamba import *
    from .processing_bamba import *
else:
    import sys

    _file = globals()[""__file__""]
    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)
``` and you need to define `__all__` like:
```python 
__all__ = [
    ""GemmaModel"",
    ""GemmaForCausalLM"",
    ""GemmaForSequenceClassification"",
    ""GemmaForTokenClassification"",
    ""GemmaPreTrainedModel"",
]
``` 
at the end of the modular file! see modular gemma2  let's remove asserts and rather raise errors please we like to have a more explicit dict like this one https://github.com/huggingface/transformers/blob/deac971c469bcbb182c2e52da0b82fb3bf54cccf/src/transformers/models/mllama/convert_mllama_weights_to_hf.py#L41, but it's not blocking merge! really not sure this is worth changing as it's just ads an assert.  Otherwise we should use the notation like https://github.com/huggingface/transformers/blob/deac971c469bcbb182c2e52da0b82fb3bf54cccf/src/transformers/models/glm/modular_glm.py#L71  as a reviewer, but also any dev that is gonna read this code, we need to know what the differences are with Mamba2Mixer. 

Could you add comments here explaining why we have to redefine everything?  This is a lot more similar to : 
```
class JambaAttentionDecoderLayer(nn.Module):
    def __init__(self, config: JambaConfig, layer_idx: int):
        super().__init__()
        num_experts = config.layers_num_experts[layer_idx]
        self.self_attn = JAMBA_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)

        ffn_layer_class = JambaSparseMoeBlock if num_experts > 1 else JambaMLP
        self.feed_forward = ffn_layer_class(config)
        self.input_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.pre_ff_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

```
which should be a better base 😉   forward and init should be the same as LlamaForcausalLM which you should be able to inherit from and just change the prepare input for generation as it is the only difference no? addressed in latest commit done in latest commit leaving for future/followup PR changed to adapt from GLM added a list to the comments done in latest commit done in latest commit"
35323,2024-12-18T17:09:30Z,2024-12-19T08:45:27Z,ydshieh,1,0,2,2,1,1,1,[],1613.0,0,56159.0,0,0,0,0,645887.069273,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35323). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.!,!,"# What does this PR do?

Let it run the first job only if:

 - the PR is still in `open` state
 - the comment body contains `run-slow` (or its variants)",
35321,2024-12-18T15:58:19Z,2024-12-19T08:08:28Z,tonywu71,1,0,4,46,3,1,1,[],4467.0,0,58209.0,0,0,0,0,648109.184845,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35321). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Sounds good! ,Sounds good! ,"# What does this PR do?

This PR fixes:
- a typo in the ColPali quickstart in the model card
- clean the documentation for ColPali.

## Who can review?

- @yonigozlan 
- @ArthurZucker",
27995,2023-12-13T08:08:42Z,2024-01-11T10:25:00Z,jiqing-feng,1,0,4,16,2,2,2,[],2305083.0,0,32118854.0,0,0,0,0,670042.101165,,0,4,0,False,['jiqing-feng'],"Hi @gante @amyeroberts . Would you please help me to review it? Thx!LGTM, thank you for the fix 💛  Thanks, would be nice to have a reproducer to add as a test but good to merge","LGTM, thank you for the fix 💛  Thanks, would be nice to have a reproducer to add as a test but good to merge","Hi @gante . Would you please have a look at this PR. The motivation is that I try to put assistant model and self model in different cuda device, or put assistant model on CPU. This PR should enable assistant model on a different device.

I have tested it on both decoder-only model and encoder-decoder model. Could you please help me to review it? Thanks!
",
28631,2024-01-22T05:14:32Z,2024-03-22T12:02:43Z,jiqing-feng,21,1,6,2,1,1,1,[],10643.0,0,28673307.0,0,0,0,0,670039.188979,,0,6,0,False,"['jiqing-feng', 'HuggingFaceDocBuilderDev', 'yao-matrix', 'amyeroberts']","Hi @amyeroberts . I have tested it on difference CPUs and didn't find any crashes. Could you please help me review it? Thx! Hi @jiqing-feng, I think the correct solution here is to enable the necessary types in the ASR pipeline, rather than removing this cast. > Hi @jiqing-feng, I think the correct solution here is to enable the necessary types in the ASR pipeline, rather than removing this cast.

Sorry for that I didn't make it clear. We can see that the function name is `_ensure_tensor_on_device`, so it should only ensure the tensor is on the correct device, instead of changing tensor types. Of course, we can enable the necessary types in the ASR pipeline, but not in this function. @jiqing-feng Can you share a code snippet showing usage which this PR fixes and a full traceback of the error without this fix?  > @jiqing-feng Can you share a code snippet showing usage which this PR fixes and a full traceback of the error without this fix?

Hi @amyeroberts . I run the following script in CPU:
```python
from transformers import pipeline
from datasets import load_dataset
from datasets import Audio
import torch

minds = load_dataset(""PolyAI/minds14"", name=""de-DE"", split=""train"")
minds = minds.cast_column(""audio"", Audio(sampling_rate=16_000))
example = minds[0]

asr = pipeline(""automatic-speech-recognition"", model=""maxidl/wav2vec2-large-xlsr-german"", torch_dtype=torch.bfloat16)
output = asr(example[""audio""][""array""])
print(output)
```
will get the following error:
```
Traceback (most recent call last):
  File ""/home/jiqingfe/transformers/test.py"", line 11, in <module>
    output = asr(example[""audio""][""array""])                                                                                                                                                                                     File ""/home/jiqingfe/transformers/src/transformers/pipelines/automatic_speech_recognition.py"", line 292, in __call__
    return super().__call__(inputs, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/base.py"", line 1154, in __call__                                                                                                                                   return next(
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/pt_utils.py"", line 124, in __next__
    item = next(self.iterator)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/pt_utils.py"", line 266, in __next__                                                                                                                                processed = self.infer(next(self.iterator), **self.params)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/base.py"", line 1068, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/automatic_speech_recognition.py"", line 524, in _forward                                                                                                            outputs = self.model(**inputs)
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 1967, in forward
    outputs = self.wav2vec2(
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 1552, in forward
    extract_features = self.feature_extractor(input_values)
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 460, in forward
    hidden_states = conv_layer(hidden_states)
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 335, in forward
    hidden_states = self.conv(hidden_states)
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/conv.py"", line 310, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File ""/home/jiqingfe/miniconda3/envs/ipex/lib/python3.10/site-packages/torch/nn/modules/conv.py"", line 306, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
RuntimeError: Input type (torch.FloatTensor) and weight type (CPUBFloat16Type) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
```

But it will output the correct answer if applied my changes:
```
{'text': 'ich möchte gerne geld auf mein konto einzallen'}
``` Hi @jiqing-feng, thanks for providing more information. This isn't something I think we should merge in. Not all CPUs support bf16 and so removing this will cause issues for users who are running on GPU for prediction. See #17637 for the reasons for adding this in. 

As mentioned in #28199, autocasting your model before passing to the pipeline is an alternative to adding autocast to the pipeline. 

The pipelines are not intended to cover all cases, but rather give a simple one-line entry point to predictions. I'd suggest using the [modeling code directly](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC.forward.example), where you'll have more control  > Hi @jiqing-feng, thanks for providing more information. This isn't something I think we should merge in. Not all CPUs support bf16 and so removing this will cause issues for users who are running on GPU for prediction. See #17637 for the reasons for adding this in.
> 
> As mentioned in #28199, autocasting your model before passing to the pipeline is an alternative to adding autocast to the pipeline.
> 
> The pipelines are not intended to cover all cases, but rather give a simple one-line entry point to predictions. I'd suggest using the [modeling code directly](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC.forward.example), where you'll have more control

Hi @amyeroberts . Thanks for your reply. 

For the 1st issue, we know that not all CPUs support bf16 and fp16, but I think we should let it crash while forward so that the users will know that they shouldn't use low-precision data types, and they can cast model and inputs to fp32.

For the 2nd issue, I can't understand why it affects GPU. I ran the code in #17637 , and the device is `cuda:0`, so removing this code won't have any impact.


cc @Narsil 
 @jiqing-feng Apologies - there was a typo on my part. I meant cause issues for users running on CPU for prediction. 

I understand the idea of allowing it to crash, but the reality is that this logic is a breaking change, as it will stop working for users when it previously did work.  > @jiqing-feng Apologies - there was a typo on my part. I meant cause issues for users running on CPU for prediction.
> 
> I understand the idea of allowing it to crash, but the reality is that this logic is a breaking change, as it will stop working for users when it previously did work.

Sorry, I don't understand why it stops working for users. On my test, it won't work for bf16 models if we keep this code, and works if we remove this code. Can you give me an example to show how my changes impact users? Thx!

BTW, I added a warning there to remind users to use fp32 if the device does not support low-precision. > > @jiqing-feng Apologies - there was a typo on my part. I meant cause issues for users running on CPU for prediction.
> > I understand the idea of allowing it to crash, but the reality is that this logic is a breaking change, as it will stop working for users when it previously did work.
> 
> Sorry, I don't understand why it stops working for users. On my test, it won't work for bf16 models if we keep this code, and works if we remove this code. Can you give me an example to show how my changes impact users? Thx!
> 
> BTW, I added a warning there to remind users to use fp32 if the device does not support low-precision.

Hi @amyeroberts . Do you mind having a look at my comment? Thx! Hi @jiqing-feng, 

The reason it will break now it that previously users would rely on this casting to make their pipelines work. Most users use pipelines as a simple API to do a single call and get outputs, and therefore aren't aware of the internals. This will cause a sudden unexpected change in behaviour and is trying to solve a problem which no-one else has reported yet. 

I understand the desire to use bfloat16 for your use case when this dtype is supported on CPU. In this case, I would suggest using the modeling code directly instead of the pipeline. 

If bfloat16 becomes more widely supported on CPU or this becomes a commonly requested feature, then we can reconsider. For now, I don't think this PR should be merged in.  > Hi @jiqing-feng,
> 
> The reason it will break now it that previously users would rely on this casting to make their pipelines work. Most users use pipelines as a simple API to do a single call and get outputs, and therefore aren't aware of the internals. This will cause a sudden unexpected change in behaviour and is trying to solve a problem which no-one else has reported yet.
> 
> I understand the desire to use bfloat16 for your use case when this dtype is supported on CPU. In this case, I would suggest using the modeling code directly instead of the pipeline.
> 
> If bfloat16 becomes more widely supported on CPU or this becomes a commonly requested feature, then we can reconsider. For now, I don't think this PR should be merged in.

@jiqing-feng , I think @amyeroberts 's concern is reasonable. Could you help to check whether your change will break current uses cases across precision. If you can show that your changes don't break anything and bring benefits to some of them, this will facilitate the discussion.  > Hi @jiqing-feng,
> 
> The reason it will break now it that previously users would rely on this casting to make their pipelines work. Most users use pipelines as a simple API to do a single call and get outputs, and therefore aren't aware of the internals. This will cause a sudden unexpected change in behaviour and is trying to solve a problem which no-one else has reported yet.
> 
> I understand the desire to use bfloat16 for your use case when this dtype is supported on CPU. In this case, I would suggest using the modeling code directly instead of the pipeline.
> 
> If bfloat16 becomes more widely supported on CPU or this becomes a commonly requested feature, then we can reconsider. For now, I don't think this PR should be merged in.

Hi @amyeroberts . Thanks for your clarification. I understand your concern, so I tested my changes on CPU which only supports `fp32`, and my changes did not break anything. You can test my changes with any model in any CPU. If you still think it is not acceptable, I can follow your suggestion to cast input type in the ASR pipeline, see [here](https://github.com/huggingface/transformers/pull/28631#issuecomment-1906513738) > > Hi @jiqing-feng,
> > The reason it will break now it that previously users would rely on this casting to make their pipelines work. Most users use pipelines as a simple API to do a single call and get outputs, and therefore aren't aware of the internals. This will cause a sudden unexpected change in behaviour and is trying to solve a problem which no-one else has reported yet.
> > I understand the desire to use bfloat16 for your use case when this dtype is supported on CPU. In this case, I would suggest using the modeling code directly instead of the pipeline.
> > If bfloat16 becomes more widely supported on CPU or this becomes a commonly requested feature, then we can reconsider. For now, I don't think this PR should be merged in.
> 
> Hi @amyeroberts . Thanks for your clarification. I understand your concern, so I tested my changes on CPU which only supports `fp32`, and my changes did not break anything. You can test my changes with any model in any CPU. If you still think it is not acceptable, I can follow your suggestion to cast input type in the ASR pipeline, see [here](https://github.com/huggingface/transformers/pull/28631#issuecomment-1906513738)

Hi @amyeroberts , should I enable casting inputs data type in the ASR pipeline? Would like to get your feedback, thx! > > Hi @jiqing-feng,
> > The reason it will break now it that previously users would rely on this casting to make their pipelines work. Most users use pipelines as a simple API to do a single call and get outputs, and therefore aren't aware of the internals. This will cause a sudden unexpected change in behaviour and is trying to solve a problem which no-one else has reported yet.
> > I understand the desire to use bfloat16 for your use case when this dtype is supported on CPU. In this case, I would suggest using the modeling code directly instead of the pipeline.
> > If bfloat16 becomes more widely supported on CPU or this becomes a commonly requested feature, then we can reconsider. For now, I don't think this PR should be merged in.
> 
> Hi @amyeroberts . Thanks for your clarification. I understand your concern, so I tested my changes on CPU which only supports `fp32`, and my changes did not break anything. You can test my changes with any model in any CPU. If you still think it is not acceptable, I can follow your suggestion to cast input type in the ASR pipeline, see [here](https://github.com/huggingface/transformers/pull/28631#issuecomment-1906513738)

Can you share how you tested the pipelines here? Ideally with a reproducible snippet? > > > Hi @jiqing-feng,
> > > The reason it will break now it that previously users would rely on this casting to make their pipelines work. Most users use pipelines as a simple API to do a single call and get outputs, and therefore aren't aware of the internals. This will cause a sudden unexpected change in behaviour and is trying to solve a problem which no-one else has reported yet.
> > > I understand the desire to use bfloat16 for your use case when this dtype is supported on CPU. In this case, I would suggest using the modeling code directly instead of the pipeline.
> > > If bfloat16 becomes more widely supported on CPU or this becomes a commonly requested feature, then we can reconsider. For now, I don't think this PR should be merged in.
> > 
> > 
> > Hi @amyeroberts . Thanks for your clarification. I understand your concern, so I tested my changes on CPU which only supports `fp32`, and my changes did not break anything. You can test my changes with any model in any CPU. If you still think it is not acceptable, I can follow your suggestion to cast input type in the ASR pipeline, see [here](https://github.com/huggingface/transformers/pull/28631#issuecomment-1906513738)
> 
> Can you share how you tested the pipelines here? Ideally with a reproducible snippet?

@jiqing-feng , I suppose talk on code and results is much better than talk-on-talk, could you pls share your test cases and results so people can discuss concretely.

We can use the format like:
test case 1  | test results before this PR | test results after this PR Hi @amyeroberts . 
Here is my test script, it's derived from [asr_pipeline](https://huggingface.co/learn/audio-course/en/chapter2/asr_pipeline) and only changed the `torch_dtype` in the pipeline.
```python
from transformers import pipeline
from datasets import load_dataset
from datasets import Audio
import torch

minds = load_dataset(""PolyAI/minds14"", name=""de-DE"", split=""train"")
minds = minds.cast_column(""audio"", Audio(sampling_rate=16_000))
example = minds[0]

asr = pipeline(""automatic-speech-recognition"", model=""maxidl/wav2vec2-large-xlsr-german"", torch_dtype=torch.bfloat16)
output = asr(example[""audio""][""array""])
print(output)
```

### Experiments
My experiments configs and results before and after my change are as follows:
1. instance only support fp32, **torch_dtype=torch.float32**
    - before change: output `{'text': 'ich möchte gerne geld auf mein konto einzallen'}`
    - after change: output `{'text': 'ich möchte gerne geld auf mein konto einzallen'}`
2. instance only support fp32, **torch_dtype=torch.bfloat16**
    - before change: output `RuntimeError: Input type (float) and bias type (c10::BFloat16) should be the same`
    - after change: output `{'text': 'ich möchte gerne geld auf mein konto einzallen'}`
3. instance support bf16, **torch_dtype=torch.float32**
    - before change: output `{'text': 'ich möchte gerne geld auf mein konto einzallen'}`
    - after change: output `{'text': 'ich möchte gerne geld auf mein konto einzallen'}`
4. instance support bf16, **torch_dtype=torch.bfloat16**
    - before change: output `RuntimeError: Input type (float) and bias type (c10::BFloat16) should be the same`
    - after change: output `{'text': 'ich möchte gerne geld auf mein konto einzallen'}`


### Appending:
For config4, we can obtain 2x speed-up by adding `torch_dtype=torch.bfloat16` compared to fp32.
The full traceback of `RuntimeError: Input type (float) and bias type (c10::BFloat16) should be the same`:
```
Traceback (most recent call last):
  File ""/home/jiqingfe/test_asr.py"", line 11, in <module>
    output = asr(example[""audio""][""array""])
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/automatic_speech_recognition.py"", line 285, in __call__
    return super().__call__(inputs, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/base.py"", line 1188, in __call__
    return next(
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/pt_utils.py"", line 124, in __next__
    item = next(self.iterator)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/pt_utils.py"", line 266, in __next__
    processed = self.infer(next(self.iterator), **self.params)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/base.py"", line 1102, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/automatic_speech_recognition.py"", line 524, in _forward
    outputs = self.model(**inputs)
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 1969, in forward
    outputs = self.wav2vec2(
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 1554, in forward
    extract_features = self.feature_extractor(input_values)
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 461, in forward
    hidden_states = conv_layer(hidden_states)
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 336, in forward
    hidden_states = self.conv(hidden_states)
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/conv.py"", line 310, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File ""/home/jiqingfe/.conda/envs/cpu/lib/python3.10/site-packages/torch/nn/modules/conv.py"", line 306, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
RuntimeError: Input type (float) and bias type (c10::BFloat16) should be the same
```

The CPU config of the instance only supports fp32:
```
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              96
On-line CPU(s) list: 0-95
Thread(s) per core:  2
Core(s) per socket:  24
Socket(s):           2
NUMA node(s):        2
Vendor ID:           GenuineIntel
CPU family:          6
Model:               85
Model name:          Intel(R) Xeon(R) Gold 6252N CPU @ 2.30GHz
Stepping:            7
CPU MHz:             2300.000
CPU max MHz:         3600.0000
CPU min MHz:         1000.0000
BogoMIPS:            4600.00
Virtualization:      VT-x
L1d cache:           32K
L1i cache:           32K
L2 cache:            1024K
L3 cache:            36608K
NUMA node0 CPU(s):   0-23,48-71
NUMA node1 CPU(s):   24-47,72-95
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities
```

The CPU config of the instance supports bf16 (contains amx_bf16 and avx512_bf16):
```
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              224
On-line CPU(s) list: 0-223
Thread(s) per core:  2
Core(s) per socket:  56
Socket(s):           2
NUMA node(s):        2
Vendor ID:           GenuineIntel
CPU family:          6
Model:               143
Model name:          Intel(R) Xeon(R) Platinum 8480L
Stepping:            7
CPU MHz:             2000.000
CPU max MHz:         3800.0000
CPU min MHz:         800.0000
BogoMIPS:            4000.00
L1d cache:           48K
L1i cache:           32K
L2 cache:            2048K
L3 cache:            107520K
NUMA node0 CPU(s):   0-55,112-167
NUMA node1 CPU(s):   56-111,168-223
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities
``` Hi @amyeroberts .  Pls let know if you need more cases : )  Hi @jiqing-feng - thanks for sharing! I won't have time to look this week, but will at the start of next week  Hi @amyeroberts . Thanks for your review. I think it should be ready to merge : ) The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_28631). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for providing detailed scripts and outputs! ",Thanks for providing detailed scripts and outputs! ,"Hi @amyeroberts . Refer to [28199](https://github.com/huggingface/transformers/pull/28199). Since `Autocast` cannot integrate into the pipeline, I propose that keep the inputs dtype in the pipeline. Otherwise, it will block the low-precision usage in both ASR and text-to-audio. 

BTW, we will be ready for review once we confirm that it works on different CPUs, please keep this PR open. Thanks!","I'd actually just remove this completely 

```suggestion
```"
31585,2024-06-25T06:25:15Z,2024-07-11T21:22:26Z,jiqing-feng,6,4,2,2,1,2,1,[],4204.0,0,15277050.0,0,0,0,0,670054.318425,,0,2,0,False,"['jiqing-feng', 'amyeroberts', 'LysandreJik']","I found this problem came from numpy, in python3.8, numpy will cast int to float:
![image](https://github.com/huggingface/transformers/assets/107918818/bf4bcf4a-8b6c-43d8-944f-8dc52ced1323)

So I suggest that we can use `p_mask.numpy()` instead of `np.array(p_mask)`


 Hi @amyeroberts , could you take a look at this PR? I am waiting for your response, thx! Hey @jiqing-feng! I'm trying to reproduce the issue but failing at doing so with python 3.8.18 and numpy 1.24.4.

```
>>> import torch
>>> import numpy as np
>>> a = torch.tensor([1,2,3], dtype=torch.int64)
>>> a
tensor([1, 2, 3])
>>> np.array(a)
array([1, 2, 3])
>>> import sys
>>> sys.version_info
sys.version_info(major=3, minor=8, micro=18, releaselevel='final', serial=0)
```

What's your torch version? > Hey @jiqing-feng! I'm trying to reproduce the issue but failing at doing so with python 3.8.18 and numpy 1.24.4.
> 
> ```
> >>> import torch
> >>> import numpy as np
> >>> a = torch.tensor([1,2,3], dtype=torch.int64)
> >>> a
> tensor([1, 2, 3])
> >>> np.array(a)
> array([1, 2, 3])
> >>> import sys
> >>> sys.version_info
> sys.version_info(major=3, minor=8, micro=18, releaselevel='final', serial=0)
> ```
> 
> What's your torch version?

torch                       2.3.0+cpu > > Hey @jiqing-feng! I'm trying to reproduce the issue but failing at doing so with python 3.8.18 and numpy 1.24.4.
> > ```
> > >>> import torch
> > >>> import numpy as np
> > >>> a = torch.tensor([1,2,3], dtype=torch.int64)
> > >>> a
> > tensor([1, 2, 3])
> > >>> np.array(a)
> > array([1, 2, 3])
> > >>> import sys
> > >>> sys.version_info
> > sys.version_info(major=3, minor=8, micro=18, releaselevel='final', serial=0)
> > ```
> > 
> > 
> >     
> >       
> >     
> > 
> >       
> >     
> > 
> >     
> >   
> > What's your torch version?
> 
> torch 2.3.0+cpu

I just checked that torch 2.3.1+cpu fixed this issue; you can close this PR if you think there is no need to do this change. BTW, I suppose the change will not break anything, and it's more common. Thx! @jiqing-feng Thanks for investigating across the different pytorch versions. If the fix it only in later versions, then this is a change we'd still want as we officially support torch >= 1.11Thanks for fixing! ",Thanks for fixing! ,"Hi @Narsil  @amyeroberts 

This PR fixed the error for question-answering pipeline, the error could be reproduced by
```python
from transformers import pipeline
pipe = pipeline(""question-answering"", model=""hf-internal-testing/tiny-random-bert"")
question = ""What's my name?""
context = ""My Name is Sasha and I live in Lyon.""
pipe(question, context)
```

Traceback:
```
Traceback (most recent call last):
  File ""test_qa.py"", line 5, in <module>
    pipe(question, context)
  File ""/home/jiqingfe/miniconda3/envs/ccl/lib/python3.8/site-packages/transformers/pipelines/question_answering.py"", line 393, in __call__
    return super().__call__(examples[0], **kwargs)
  File ""/home/jiqingfe/miniconda3/envs/ccl/lib/python3.8/site-packages/transformers/pipelines/base.py"", line 1235, in __call__
    return next(
  File ""/home/jiqingfe/miniconda3/envs/ccl/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py"", line 125, in __next__
    processed = self.infer(item, **self.params)
  File ""/home/jiqingfe/miniconda3/envs/ccl/lib/python3.8/site-packages/transformers/pipelines/question_answering.py"", line 546, in postprocess                                                    starts, ends, scores, min_null_score = select_starts_ends(
  File ""/home/jiqingfe/miniconda3/envs/ccl/lib/python3.8/site-packages/transformers/pipelines/question_answering.py"", line 124, in select_starts_ends
    undesired_tokens = undesired_tokens & attention_mask
TypeError: ufunc 'bitwise_and' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

```","Does this still work if you run the pipeline in jax? 

```py
from transformers import pipeline
pipe = pipeline(""question-answering"", model=""hf-internal-testing/tiny-random-bert"", framework=""flax"")
question = ""What's my name?""
context = ""My Name is Sasha and I live in Lyon.""
``` It will raise a value error.
`ValueError: Pipeline cannot infer suitable model classes from hf-internal-testing/tiny-random-bert`.
 Besides, `temsor.numpy()` has been already used in other pipelines like [ASR](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/automatic_speech_recognition.py#L560) OK, yes, looking into it we seem so assume either tf or pt everywhere in the pipeline, so even though I think this would break things for jax tensors it's not something we need to take account of at the moment. Thanks for testing! "
31401,2024-06-13T07:46:09Z,2024-07-03T08:22:56Z,jiqing-feng,19,4,12,55,2,3,2,['Generation'],77803.0,0,16308995.0,0,0,0,0,670056.63313,,0,12,0,False,"['jiqing-feng', 'gante', 'amyeroberts']","The failed CIs seem not related to my changes Hi @gante . Sorry for not making it clear. Could you run this script:
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


model_id = ""meta-llama/Llama-2-7b-chat-hf""
assistant_model_id = ""Felladrin/Llama-68M-Chat-v1""
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token_id = tokenizer.eos_token_id

model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(""cuda"")
assistant_model = AutoModelForCausalLM.from_pretrained(assistant_model_id, torch_dtype=torch.bfloat16).to(""cpu"")

prompt = ""Assisted decoding is""
inputs = tokenizer(prompt, return_tensors=""pt"").to(model.device)

model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=8, min_new_tokens=8, do_sample=False)
```

It will get the error `Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!`.

Full traceback
```
Traceback (most recent call last):
  File ""/workspace/jiqing/hete_specdecode/test_assisted.py"", line 16, in <module>
    model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=8, min_new_tokens=8, do_sample=False)
  File ""/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/workspace/jiqing/transformers/src/transformers/generation/utils.py"", line 1853, in generate
    result = self._assisted_decoding(
  File ""/workspace/jiqing/transformers/src/transformers/generation/utils.py"", line 3698, in _assisted_decoding
    candidate_input_ids, candidate_logits = candidate_generator.get_candidates(input_ids)
  File ""/workspace/jiqing/transformers/src/transformers/generation/candidate_generator.py"", line 229, in get_candidates
    assistant_output = self.assistant_model.generate(**assistant_generation_kwargs, **self.assistant_kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/workspace/jiqing/transformers/src/transformers/generation/utils.py"", line 1896, in generate
    result = self._sample(
  File ""/workspace/jiqing/transformers/src/transformers/generation/utils.py"", line 2648, in _sample
    next_token_scores = logits_processor(input_ids, next_token_logits)
  File ""/workspace/jiqing/transformers/src/transformers/generation/logits_process.py"", line 98, in __call__
    scores = processor(input_ids, scores)
  File ""/workspace/jiqing/transformers/src/transformers/generation/logits_process.py"", line 157, in __call__
    eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument test_elements in method wrapper_CUDA_isin_Tensor_Tensor)

``` HI @gante . I just found the real issue happens [here](https://github.com/gante/transformers/blob/b7672826cad31e30319487af876e608d8af7d37b/src/transformers/generation/utils.py#L1470-L1471), pls take a review. Thx! I would like to add a test for this. Do you know where I should add this test? Thx! > This makes sense, thank you for digging deeper and iterating @jiqing-feng ! 💛
> 
> Regarding tests: it's a bit tricky to test two devices on our CI AFAIK 🤔 @amyeroberts do you have suggestions on how to test it? [TL;DR @jiqing-feng found that assisted generation fails if the two models are on different devices, because the special tokens are copied from the main model to the assistant model]

I think we can just run the test on a device with GPU; there is almost no limitation for CPU because we can run a very tiny model on CPU just for functionality. > Regarding tests: it's a bit tricky to test two devices on our CI AFAIK 🤔 @amyeroberts do you have suggestions on how to test it? [TL;DR @jiqing-feng found that assisted generation fails if the two models are on different devices, because the special tokens are copied from the main model to the assistant model]

@gante There's certain tests in our suite which require multiple devices e.g. [test_model_parallelization](https://github.com/huggingface/transformers/blob/1f9387d33d40562add4be03176e22ddf9ed0d4ab/tests/test_modeling_common.py#L2911), which we can denote with the `require_torch_multi_accelerator` and `require_torch_multi_gpu` decorators. 

In this case, I'd suggest having two tests, one for the single accelerator case, and another which only runs in the multi device case.  derp, ofc a GPU is enough (which has a CPU paired up), what a brain fart on my end :D

@jiqing-feng could you add two tests like the script in [this comment of yours](https://github.com/huggingface/transformers/pull/31401#issuecomment-2171993325) to [this file](https://github.com/huggingface/transformers/blob/main/tests/generation/test_utils.py)? More precisely:
1. Inside `GenerationIntegrationTests`;
2. Using the `@slow` decorator;
3. One of the tests with the `@require_torch_multi_gpu` decorator with each model in a different gpu, another with `@require_torch_gpu` with the assistant on cpu
4. Let's use one of our tiny test models like `hf-internal-testing/tiny-random-MistralForCausalLM` (as both main model and assistant) Hi @gante . I have added the tests, could you please take a review? Thx!

BTW, the failed CIs seem not related to my changes Hi @amyeroberts. Could you please take a review? The failed CIs are not related to my changes :) @jiqing-feng Regarding the failing tests, could you rebase on main to include upstream changes? This should resolve the failures on CI 

Could you also run and share the output of executing the following in a multi-gpu environment:  

```
RUN_SLOW=1 pytest -k ""test_assisted_decoding_in_different_gpu or test_assisted_decoding_in_different_gpu""
``` @jiqing-feng rebasing the PR should get CI green 🤗  Hi @amyeroberts . I run the 2 tests individually and got passed, see
![image](https://github.com/huggingface/transformers/assets/107918818/146aa3cd-a248-47d0-956d-5a8fe5c4177d)


I also run your command and got the following output
![image](https://github.com/huggingface/transformers/assets/107918818/9cbf44ba-c36d-43ca-b784-73183b57734e)
These failed tests are due to some import error:
![image](https://github.com/huggingface/transformers/assets/107918818/50321890-85bc-452a-8faf-b6b4f652dc42)
 Hi @amyeroberts . Do you need more actions before merging? Please let me know, thx! Hi @amyeroberts @gante . I think this PR should be ready to merge :) @jiqing-feng OK, sorry, I think I messed up with the pytest command. Could you try this instead: 

```
RUN_SLOW=1 pytest tests/generation/test_utils.py::GenerationIntegrationTests::test_assisted_decoding_in_different_gpu
RUN_SLOW=1 pytest tests/generation/test_utils.py::GenerationIntegrationTests::test_assisted_decoding_in_gpu_cpu 
``` > @jiqing-feng OK, sorry, I think I messed up with the pytest command. Could you try this instead:
> 
> ```
> RUN_SLOW=1 pytest tests/generation/test_utils.py::GenerationIntegrationTests::test_assisted_decoding_in_different_gpu
> RUN_SLOW=1 pytest tests/generation/test_utils.py::GenerationIntegrationTests::test_assisted_decoding_in_gpu_cpu 
> ```

All passed

![image](https://github.com/huggingface/transformers/assets/107918818/698db079-2276-4f95-b82c-5ebccb203bf4)
 Hi @amyeroberts . The failed CIs are not relate to my changes, would you please review my changes? Hi @amyeroberts @gante , would you please help to merge this PR? Thx! Hi @jiqing-feng, we had to wait for somethings to be resolved upstream and to wait for a new CI run (which I triggered last night)Hi @jiqing-feng! Thank you for opening this PR 🤗 

To the best of my knowledge, the changes you're suggesting should not be needed. As such, I've asked a few questions below to understand why we need these changes :) This makes sense, thank you for digging deeper and iterating @jiqing-feng ! 💛 

Regarding tests: it's a bit tricky to test two devices on our CI AFAIK 🤔 @amyeroberts do you have suggestions on how to test it? [TL;DR @jiqing-feng found that assisted generation fails if the two models are on different devices, because the special tokens are copied from the main model to the assistant model] Thanks for fixing! ","Hi @jiqing-feng! Thank you for opening this PR 🤗 

To the best of my knowledge, the changes you're suggesting should not be needed. As such, I've asked a few questions below to understand why we need these changes :) This makes sense, thank you for digging deeper and iterating @jiqing-feng ! 💛 

Regarding tests: it's a bit tricky to test two devices on our CI AFAIK 🤔 @amyeroberts do you have suggestions on how to test it? [TL;DR @jiqing-feng found that assisted generation fails if the two models are on different devices, because the special tokens are copied from the main model to the assistant model] Thanks for fixing! ","Hi @gante . This PR is to fix the assisted decoding when the model and assistant model are on different devices.

It can be easily reproduced by:
```python
model = model.to(""cuda"")
model.generate(**inputs, assistant_model=assistant_model.to(""cpu""))
```
","Uhmmm... when the `LogitsProcessor` instances are created in `generate`, they are done through the `_get_logits_processor` function, which has as input `device=inputs_tensor.device`. 

In other words, if the inputs to the assistant model are in the same device as the assistant, this `to` operation should not be needed. We do move the assistant model inputs to the same device (on [this line](https://github.com/gante/transformers/blob/b7672826cad31e30319487af876e608d8af7d37b/src/transformers/generation/candidate_generator.py#L197)). 

Can you elaborate on why this change is needed? 🤗  Similarly to the comment above, these extra `to` operations should not be needed if the model inputs are in the same device as the model -- see [this call](https://github.com/huggingface/transformers/blob/d9daeff2978eda167c7256b2c928941f2248bfbe/src/transformers/generation/utils.py#L1659) Yes, but before we move the `input_tensor` to the same device as `assiatant_model`, the logits processor already initialized [here](https://github.com/gante/transformers/blob/b7672826cad31e30319487af876e608d8af7d37b/src/transformers/generation/utils.py#L1801). The `eos_token_id` also in the target model device, see [here](https://github.com/huggingface/transformers/blob/d9daeff2978eda167c7256b2c928941f2248bfbe/src/transformers/generation/utils.py#L1477). The `eos_token_id` and `pad_token_id` has the same device as the target model, see [here](https://github.com/huggingface/transformers/blob/d9daeff2978eda167c7256b2c928941f2248bfbe/src/transformers/generation/utils.py#L1477)."
31625,2024-06-26T03:57:56Z,2024-09-20T23:43:31Z,jiqing-feng,18,11,18,147,6,4,3,[],36041.0,0,15199486.0,0,0,0,0,670059.0097,,1,18,0,False,"['amyeroberts', 'aliencaocao', 'LysandreJik', 'HuggingFaceDocBuilderDev', 'Rocketknight1', 'jiqing-feng', 'yao-matrix']","Related to https://github.com/huggingface/transformers/pull/31342 but I dont quite get your changes - what exactly does it fix? When I tested all the pipelines in fp16 none of them had issues outputting logits > Related to #31342 but I dont quite get your changes - what exactly does it fix? When I tested all the pipelines in fp16 none of them had issues outputting logits

Hi @aliencaocao , FP16 works fine but BF16 is not acceptable for numpy, you can see:
![image](https://github.com/huggingface/transformers/assets/107918818/f5e3e98e-cddd-4736-9017-fcf78fe130d5)


 Hi @SunMarc. I have fixed all your comments; please review them. BTW, the failed tests are due to the connection error, not related to my changes. Hi @amyeroberts , would you please review this PR? Thx! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_31625). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @amyeroberts . I have passed all tests, do you mind take a review and merge it? Thx! Hi @amyeroberts . This PR should be ready to merge, please take a review, thx! Hi @jiqing-feng, thanks for opening this PR! I'll get to reviewing it soon, but will likely be in a few days. Hi @amyeroberts . Do you mind reviewing this PR? Thx. Hi @amyeroberts . Do you think it could be merged? @amyeroberts , could you help review this PR? Thx. @SunMarc , do you have a suggestion any other people from HF can help review and merge this PR? Seems @amyeroberts has no bandwidth on this these serval months. Thx. Sorry for the delay @yao-matrix!

@Rocketknight1 is managing the pipeline; Matt, would you mind reviewing this PR when you have a second? > Overall this PR seems good to me! However, I prefer `if outputs.dtype in (torch.bfloat16, torch.float16)` rather than `if outputs.dtype == torch.bfloat16`, so we can catch `float16` as well.
> 
> Other than that, I'm happy with it!

Hi @Rocketknight1 , thanks for your review, I have fixed it by your comments. Hi @Rocketknight1 , do you mind helping to re-run the tests? Thx. Hi @jiqing-feng, another PR at #33554 touched the same files. I'm sorry - I didn't realize that it was doing the same thing as this PR! That PR has been merged, so I've merged this PR with the code there to avoid conflicts. @jiqing-feng tests pass now and this looks good - are you okay for us to merge it?

Also cc @LysandreJik for final review, but no rush! > @jiqing-feng tests pass now and this looks good - are you okay for us to merge it?
> 

Yes, please.Thanks for addding this @jiqing-feng ! Left a few comments ! Nice tests =)  Thanks for iterating ! Just a few nits !  Thanks for iterating @jiqing-feng  !  Overall this PR seems good to me! However, I prefer `if outputs.dtype in (torch.bfloat16, torch.float16)` rather than `if outputs.dtype == torch.bfloat16`, so we can catch `float16` as well.

Other than that, I'm happy with it! That works for me! Thank you @jiqing-feng!","Thanks for addding this @jiqing-feng ! Left a few comments ! Nice tests =)  Thanks for iterating ! Just a few nits !  Thanks for iterating @jiqing-feng  !  Overall this PR seems good to me! However, I prefer `if outputs.dtype in (torch.bfloat16, torch.float16)` rather than `if outputs.dtype == torch.bfloat16`, so we can catch `float16` as well.

Other than that, I'm happy with it! That works for me! Thank you @jiqing-feng!","Hi @amyeroberts @Narsil .

As the previous PR #31444 mentioned, it could enable low-precision pipelines by converting the outputs to `float()`. I followed the codes in [here](https://github.com/huggingface/transformers/blame/main/src/transformers/pipelines/text_classification.py#L205-L210). Do you mind taking a review? Thx!
","I saw in the other PR that we are also doing this for float16 even when numpy supports it. Maybe we should only do the `upscasting` for `bfloat16` ? WDYT @amyeroberts  why did you remove the dtype check ?  add dtype check  Add dtype check and maybe split these steps for readability ```suggestion
            # To enable using bf16
``` ```suggestion
                # To enable using bf16
``` Why do we need to change that ? The fp16 support was working in the past for pipeline. Is it because you are using it in your tests ?  Yes, it is for the fp16 tests. sounds good !  This seems like a regression to me! The previous code worked with both `torch.bfloat16` and `torch.float16`. I use torch 2.4, numpy-2.1.1 in python 3.10 and it failed:
![image](https://github.com/user-attachments/assets/e5bf0154-b975-45fd-9608-0a2c4ab85715)
"
34483,2024-10-29T07:25:55Z,2024-10-31T12:55:53Z,jiqing-feng,4,0,3,43,2,1,1,[],479.0,0,4386997.0,0,0,0,0,670069.258936,,0,3,0,False,"['jiqing-feng', 'HuggingFaceDocBuilderDev', 'Rocketknight1']","The issue can be reproduced by:
```python
import torch
from transformers import pipeline

question = ""How many programming languages does BLOOM support?""
context = ""BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.""

question_answerer = pipeline(""question-answering"", model=""bert-large-uncased-whole-word-masking-finetuned-squad"", torch_dtype=torch.bfloat16)
out = question_answerer(question=question, context=context)
``` Hi @Rocketknight1 , could you please review this PR ? Thx!
 Merging without core maintainer review since this is a straightforward pipeline dtype fix that shouldn't have any side effects The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34483). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Yes, good fix, and thank you for the clean reproducer!","Yes, good fix, and thank you for the clean reproducer!","Hi @SunMarc @ArthurZucker 

The same issue as #31625 . Numpy can only process fp32 data type but the input tensor could be bf16/fp16 based on the model's dtype. Please review it, thx!
",
34478,2024-10-29T03:27:58Z,2024-10-31T22:59:23Z,jiqing-feng,3,2,4,48,2,1,1,[],114315.0,0,4401272.0,0,0,0,0,670072.999743,,0,4,0,False,"['jiqing-feng', 'HuggingFaceDocBuilderDev', 'Rocketknight1']","cc @stevhliu  Hi @stevhliu . Thanks for your review. I have fixed all the comments, now it's ready to merge :) The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34478). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for updating! Feel free to mark as ready whenever you're ready :) Thanks! 🤗 ",Thanks for updating! Feel free to mark as ready whenever you're ready :) Thanks! 🤗 ,Update CPU doc,"```suggestion
Mixed precision uses single (fp32) and half-precision (bf16/fp16) data types in a model to accelerate training or inference while still preserving much of the single-precision accuracy. Modern CPUs such as 3rd, 4th, and 5th Gen Intel® Xeon® Scalable processors natively support bf16. 6th Gen Intel® Xeon® Scalable processors natively support bf16 and fp16. You should get more performance out of the box by enabling mixed precision training with bf16 or fp16.
``` Need to also delete the closing `</Tip>` tag"
34713,2024-11-13T06:32:44Z,2024-12-18T14:50:00Z,jiqing-feng,6,0,16,77,2,2,2,['Quantization'],530181.0,1,3094179.0,0,0,0,0,670080.44431,,0,16,0,False,"['jiqing-feng', 'HuggingFaceDocBuilderDev', 'matthewdouglas', 'Titus-von-Koeller']","Thanks for this great work in conjunction with [BNB PR #1418](https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1418), @jiqing-feng 🔥🤗

I'll do my best to provide feedback on this ASAP so we can iterate. That said, I need to balance it with other high-impact topics like quantization improvements, the custom_ops registration refactor (which underpins merging all this into `main` on BNB) and general maintenance (e.g., resolving the currently broken CI integration tests). Still, this remains one of our top priorities before the end of the year, and we’re aiming to make maximum progress on this topic and bring these new functionalities live ASAP.

Thanks so much to you and the Intel team ❤️ for your continued valuable work and support on this! It’s highly appreciated, and I’m looking forward to a final sprint to materialize the fruits of our collaboration. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34713). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Will review this today and tmr. I'm going to take over for reviewing this. Will work on getting access to hw to run this on.
 cc: @SunMarc  Hi @matthewdouglas . Thanks for your testing.
When you said before this PR, does it means you observed the failed issue without this PR and the failed issue disappear with this PR?
Do I need any changes before merging? @jiqing-feng That's correct: I observed that with this PR applied the tests run correctly.. No changes needed. Thanks!I was able to try this on an Intel machine with GPU Max 1100 where everything looks good with this PR applied.

Before this PR (on transformers latest release), we see several failures and a crash:

```
tests/quantization/bnb/test_4bit.py::Pipeline4BitTest::test_pipeline Fatal Python error: Aborted

Thread 0x00007f538b656640 (most recent call first):
  File ""/opt/conda/lib/python3.11/threading.py"", line 331 in wait
  File ""/opt/conda/lib/python3.11/threading.py"", line 629 in wait
  File ""/opt/conda/lib/python3.11/site-packages/tqdm/_monitor.py"", line 60 in run
  File ""/opt/conda/lib/python3.11/threading.py"", line 1045 in _bootstrap_inner
  File ""/opt/conda/lib/python3.11/threading.py"", line 1002 in _bootstrap

Current thread 0x00007f5843c5d740 (most recent call first):
  File ""/opt/conda/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py"", line 68 in build_alibi_tensor
  File ""/opt/conda/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py"", line 577 in build_alibi_tensor
  File ""/opt/conda/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py"", line 671 in forward
  File ""/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1747 in _call_impl
  File ""/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1736 in _wrapped_call_impl
  File ""/opt/conda/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py"", line 973 in forward
  File ""/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py"", line 170 in new_forward
  File ""/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1747 in _call_impl
  File ""/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1736 in _wrapped_call_impl
  File ""/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py"", line 3222 in _sample
  File ""/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py"", line 2231 in generate
  File ""/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py"", line 116 in decorate_context
  File ""/opt/conda/lib/python3.11/site-packages/transformers/pipelines/text_generation.py"", line 370 in _forward
  File ""/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py"", line 1208 in forward
  File ""/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py"", line 1308 in run_single
  File ""/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py"", line 1301 in __call__
  File ""/opt/conda/lib/python3.11/site-packages/transformers/pipelines/text_generation.py"", line 272 in __call__
  File ""/usr/src_host/transformers/tests/quantization/bnb/test_4bit.py"", line 513 in test_pipeline```  Thanks for your PR!","I was able to try this on an Intel machine with GPU Max 1100 where everything looks good with this PR applied.

Before this PR (on transformers latest release), we see several failures and a crash:

```
tests/quantization/bnb/test_4bit.py::Pipeline4BitTest::test_pipeline Fatal Python error: Aborted

Thread 0x00007f538b656640 (most recent call first):
  File ""/opt/conda/lib/python3.11/threading.py"", line 331 in wait
  File ""/opt/conda/lib/python3.11/threading.py"", line 629 in wait
  File ""/opt/conda/lib/python3.11/site-packages/tqdm/_monitor.py"", line 60 in run
  File ""/opt/conda/lib/python3.11/threading.py"", line 1045 in _bootstrap_inner
  File ""/opt/conda/lib/python3.11/threading.py"", line 1002 in _bootstrap

Current thread 0x00007f5843c5d740 (most recent call first):
  File ""/opt/conda/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py"", line 68 in build_alibi_tensor
  File ""/opt/conda/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py"", line 577 in build_alibi_tensor
  File ""/opt/conda/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py"", line 671 in forward
  File ""/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1747 in _call_impl
  File ""/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1736 in _wrapped_call_impl
  File ""/opt/conda/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py"", line 973 in forward
  File ""/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py"", line 170 in new_forward
  File ""/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1747 in _call_impl
  File ""/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1736 in _wrapped_call_impl
  File ""/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py"", line 3222 in _sample
  File ""/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py"", line 2231 in generate
  File ""/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py"", line 116 in decorate_context
  File ""/opt/conda/lib/python3.11/site-packages/transformers/pipelines/text_generation.py"", line 370 in _forward
  File ""/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py"", line 1208 in forward
  File ""/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py"", line 1308 in run_single
  File ""/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py"", line 1301 in __call__
  File ""/opt/conda/lib/python3.11/site-packages/transformers/pipelines/text_generation.py"", line 272 in __call__
  File ""/usr/src_host/transformers/tests/quantization/bnb/test_4bit.py"", line 513 in test_pipeline```  Thanks for your PR!","1. BNB for CPU and XPU path do not support autocast lora finetune for now.
2. XPU do not support gpt2 for now.
3. Add llama tests",
33460,2024-09-13T06:47:05Z,2024-10-04T14:25:11Z,jiqing-feng,5,10,13,158,6,3,2,[],1016234.0,0,8363723.0,0,0,0,0,670075.590487,,0,13,0,False,"['jiqing-feng', 'HuggingFaceDocBuilderDev']","Hi @SunMarc . Do you mind reviewing this PR? It enables AutoAWQ CPU path. Thx! Hi @SunMarc @ArthurZucker . This PR is ready to be reviewed. Thx! Hi @SunMarc . I have fixed all your comments, please review it. Thx! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33460). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @SunMarc . I have fixed the log, now it's ready to merge. Thanks!Thanks for this addition @jiqing-feng ! Left a few comments  Nice ! Thanks for iterating @jiqing-feng  ! Just a small nit  Clean  🧼 thanks 🤗 ",Thanks for this addition @jiqing-feng ! Left a few comments  Nice ! Thanks for iterating @jiqing-feng  ! Just a small nit  Clean  🧼 thanks 🤗 ,This PR enables CPU AWQ model with IPEX version.,"Do we need to hardcode the `pad_token_id` like that ? Can we do that:  `pad_token_id = tokenizer.eos_token_id` ?  The input_ids will be on the cpu by default
```suggestion
input_ids = tokenizer.encode(""How to make a cake"", return_tensors=""pt"")
``` Need to add a logger.info to tell  the user that we didn't fuse the mlp with ipex. Also, for curiosity, why this doesn't work with ipex while for other layers with attention layers, it works ```suggestion
        input_ids = tokenizer.encode(""How to make a cake"", return_tensors=""pt"")
``` Make sure to also to update  the condition `if device_map is None:`  below now that we also support cpu. 

We need to do a backend check for cuda and ipex but also 
- if version == ipex and device_map is not `None` or device_map != ""cpu, then ask the user to make sure to move the model to cpu afterwards 
- elif device_map is None or device_map == ""cpu"" then ask the user to move it to cuda afterwards.  can you use a smaller model to test ?  pad token to change as reviewed in the readme That's because the fused mlp needs awq_ext which only supported in CUDA, see [here](https://github.com/casper-hansen/AutoAWQ/blob/main/awq/modules/fused/mlp.py#L37-L42). We'd like to add CPU support in the future, but for now, let's skip this feature.   To put in the else condition You can also add in the error that the user can try with ipex if they have an intel CPU."
34647,2024-11-08T01:43:35Z,2024-11-19T11:44:44Z,jiqing-feng,10,1,10,5,1,2,3,[],128490.0,0,3543526.0,0,0,0,0,670084.352391,,1,10,0,False,"['jiqing-feng', 'HuggingFaceDocBuilderDev', 'Rocketknight1', 'zucchini-nlp']","Looks good, but do you have an example of code that failed before, that is fixed by this PR? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34647). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Looks good, but do you have an example of code that failed before, that is fixed by this PR?

I run this case in a CPU-only device
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = ""Felladrin/Llama-68M-Chat-v1""

text = [""I am happy because"", ""This is""]
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
input_ids = tokenizer(text, return_tensors=""pt"", padding=True)

quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""auto"", quantization_config=quantization_config)
model.generation_config.cache_implementation = ""static""
model.generate(**input_ids)
```

Error: `IndexError: list index out of range` Hi @Rocketknight1 . The script can reproduce this error easily both on AWQ and BNB. Besides, it's not safe to get index without checking the list length. Hi @Rocketknight1 @ArthurZucker @SunMarc @gante @zucchini-nlp , do you mind reviewing this change? Thanks Hi @Titus-von-Koeller . This change is needed for bitsandbytes cpu path, can you help to review it?

Besides, it's also needed for AWQ cpu path which is already enabled here: #33460  The following script can reproduce the AWQ error:
I run this case on a CPU-only device.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AwqConfig

model_id = ""PrunaAI/JackFram-llama-68m-AWQ-4bit-smashed""

text = [""I am happy because"", ""This is""]
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
input_ids = tokenizer(text, return_tensors=""pt"", padding=True)

# quantization_config = BitsAndBytesConfig(load_in_8bit=True)
quantization_config = AwqConfig(version=""ipex"")

model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""auto"", quantization_config=quantization_config)
model.generation_config.cache_implementation = ""static""
model.generate(**input_ids)

``` Hi @aymeric-roucher  @LysandreJik , do you have time to review this bug fix? Thanks! @jiqing-feng sorry for late reply, the `transformers` team was off last week and someone will review the PR soon. Seems like you tagged all relevant people already Hi @SunMarc , I have applied your changes, thanks!Thanks for the fix ! I've suggested a similar fix that follows more closely what we have in accelerate. LMK if this works for you !  Nice, thanks !  LGTM 🤗 ","Thanks for the fix ! I've suggested a similar fix that follows more closely what we have in accelerate. LMK if this works for you !  Nice, thanks !  LGTM 🤗 ","Hi @Rocketknight1 @ArthurZucker @SunMarc @gante 

We have ""cpu"" in hf_device_map when using bnb model in CPU. The cpu device bnb model should be accepted by transformers because CPU backend has been enabled in BNB. Please take a review, thx!","What about this ? This is taken from the `dispatch_model` function. 
```suggestion
                if set(self.hf_device_map.values()) == {""cpu""} or set(self.hf_device_map.values()) == {""cpu"", ""disk""}:
                    main_device = ""cpu""
                else:
                    main_device = [d for d in self.hf_device_map.values() if d not in [""cpu"", ""disk""]][0]
```"
34712,2024-11-13T05:31:21Z,2024-11-15T14:45:24Z,jiqing-feng,2,0,4,40,2,2,2,[],179195.0,0,3097857.0,0,0,0,0,670087.427617,,0,4,0,False,"['jiqing-feng', 'HuggingFaceDocBuilderDev']","Hi @SunMarc @ArthurZucker . This [PR](https://github.com/casper-hansen/AutoAWQ/pull/631) adds IPEX GPU path for AWQＭodel as Intel GPU (usually called xpu in torch) is already supported in AutoAWQ , please review it, thanks!  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34712). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice ! Thanks for adding this and updating the docs !  Very cool, thank you!","Nice ! Thanks for adding this and updating the docs !  Very cool, thank you!","Enable XPU path in AutoAWQ
",
35224,2024-12-12T07:39:47Z,2024-12-13T08:35:50Z,alexrs-cohere,1,1,1,2517,19,1,1,[],558789.0,0,558791.0,0,0,0,0,695847.742224,,0,1,0,False,['guangy10'],@alexrs-cohere Thanks for adding cohere2 with test for export to ExecuTorch! Nice! 🔥 ,Nice! 🔥 ,"# What does this PR do?
Add `Cohere2` model to transformers.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
@ArthurZucker @Cyrilvallez 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",let's improve this!
35319,2024-12-18T15:07:17Z,2024-12-18T15:38:19Z,eustlb,1,0,1,18,1,1,1,[],1629.0,0,1866.0,0,0,0,0,707524.379911,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35319). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks,Thanks,"# What does this PR do?

Fix mini typo in rendering links to GenerationMixin's generate :) 
",
34495,2024-10-29T17:04:23Z,2024-12-18T17:59:07Z,McPatate,2,8,14,503,8,2,1,['run-benchmark'],1623.0,0,4323286.0,0,0,0,0,699078.730098,,0,14,0,False,"['HuggingFaceDocBuilderDev', 'McPatate']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34495). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I'll update the README and then we can merge!Very nice! Changes looks great, good to go when you have the pannel thing sorted out! or written in the doc how to handle when you only have say 3 metrics!","Very nice! Changes looks great, good to go when you have the pannel thing sorted out! or written in the doc how to handle when you only have say 3 metrics!","Adding `benchmarks_entrypoint.py` file, which will be run from the benchmarks CI.

This python script will list all python files from the `benchmark/` folder and run the included `run_benchmark` function, allowing people to add new benchmarks scripts.

","`ImportError` is native should work as well no?  is there a way to only pass the logger and a class that wrapps around your `continue_metric_collection` . 
This way the only thing people need when adding a new file with run_benchmark is: 
1. Call start() on the class
2. compute some stuff
3. call .record() (for example) which take the dict of inputs, and adds them to the data, commit and close! 

completely up to debate, try to find the simplest way to do it! 🤗  Yes that's probably the best path forward! Will iterate and let you know when I have something nicer I don't get an `ImportError` because the function I use to import the module returns `None` when it can't import a given module afaiu. I can always throw an `ImportError` though perfect thanks lmk if the changes look good to you no worries"
34135,2024-10-13T15:40:59Z,2024-12-18T13:13:21Z,eustlb,7,30,75,389,3,3,5,['run-slow'],68981.0,0,5693542.0,0,0,0,0,716228.00691,,0,75,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker', 'ylacombe', 'eustlb']","Even if we usually return the `decoder_input_ids` and `eos_token` with `generate` in Transformers, and this is why we have a `skip_special_tokens=True` option in the tokenizer, I think it is better here **not to return** context tokens. Indeed, this would imply a lot of ambiguities: since we actually have multiple calls to generate when doing sequential decoding, why would we include those tokens only for the first segment of the concatenated sequence of tokens and the last (for the eos token)? This would let users think that generation was indeed in one shot and that tokens are indeed the concatenated ones. Likewise, it would be even worse for the returned segments: some will have the context tokens, and some won't, depending on if the segment is the first of a new call to generate.


 For these reasons, I think it is better to stick with OpenAI's choice: return only the generated tokens. Moreover, this is the way it is currently implemented in Transformers : long-form generation does not return context tokens. As a consequence, this also comes with the advantage of requiring fewer changes in the current codebase, reducing the potential for mistakes. WDYT @ylacombe? Also pinging @ArthurZucker here since you've worked on Whisper integration.  **Correction:**
After discussion, it has been decided to rather go for solution 2: return only the generated tokens and skip all the special tokens.
Pros: fewer changes, no ambiguities. 
Cons: need to overwrite generic tests of GenerationMixin Sounds good! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34135). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Let's make sure the last failing tests are fixed before asking for @ArthurZucker's review! Thanks a lot @ylacombe for the review. I updated it based on your comments (see the updated PR comment) !   The four failing slow tests are expected! Merging 🤗Thanks for working on this @eustlb, LGTM!

The way you adapted the generation tests is quite elegant IMO, let's hear if the core maintainers agree.

Nice work as well on the benchmark results. 

If we sum them up, we're now aligned with the original Whisper implementation in terms of WER. However, short-form transcription is still considerably slower. This is not an issue in itself, but the fact that the RTFx is the same for long-form transcription seems to indicate that we could bridge the gap as well for short-form transcription. What are your thoughts on this?


Note: Supersedes #33917 Thanks, would have enjoyed a solution without wrappers around generate as IMO it's too magic. Explicit is usually how we go. Tho I don't think this is a blocker, unless you did not try other solution (fine with generate modifications! 👀 ) 😉  one small nit and you can merge 🔥  Hey @eustlb, thanks for iterating! Thanks @eustlb for iterating! LGTM now! THanks for removing the wrapper!","Thanks for working on this @eustlb, LGTM!

The way you adapted the generation tests is quite elegant IMO, let's hear if the core maintainers agree.

Nice work as well on the benchmark results. 

If we sum them up, we're now aligned with the original Whisper implementation in terms of WER. However, short-form transcription is still considerably slower. This is not an issue in itself, but the fact that the RTFx is the same for long-form transcription seems to indicate that we could bridge the gap as well for short-form transcription. What are your thoughts on this?


Note: Supersedes #33917 Thanks, would have enjoyed a solution without wrappers around generate as IMO it's too magic. Explicit is usually how we go. Tho I don't think this is a blocker, unless you did not try other solution (fine with generate modifications! 👀 ) 😉  one small nit and you can merge 🔥  Hey @eustlb, thanks for iterating! Thanks @eustlb for iterating! LGTM now! THanks for removing the wrapper!","# What does this PR do?

This PR finalizes #30984 which enabled short-form (_<=30sec_) and long-form generation using temperature fallback. Indeed, the original OpenAI implementation uses the same [decode_with_fallback](https://github.com/openai/whisper/blob/25639fc17ddc013d56c594bfbf7644f2185fad84/whisper/transcribe.py#L279) for both short-form and long-form audio, while we used to have temperature fallback only for long-form audio.

It aims to solve issues and divergences with the original implementation: 
1. When decoding with timestamps, the Transformers implementation skips the last segment for short-form audio while the OpenAI implementation does not: it will use the sliding window strategy and go to the last generated timestamp (so indeed no differences between inferring short-form and long-form audio). This was not detected in the tests since [test_tiny_timestamp_generation](https://github.com/huggingface/transformers/blob/37ea04013b34b39c01b51aeaacd8d56f2c62a7eb/tests/models/whisper/test_modeling_whisper.py#L2048) and [test_large_timestamp_generation](https://github.com/huggingface/transformers/blob/37ea04013b34b39c01b51aeaacd8d56f2c62a7eb/tests/models/whisper/test_modeling_whisper.py#L2185) are incorrect. 
2. Miscalculation of the `avg_logprobs` that triggered temperature fallback when it should not.
3. When decoding short-form / long-form audio, we returned:
   1. **short-form**`decoder_input_ids + predicted tokens (including the eos token)` 
   2. **long-form** only the `predicted tokens (without eos token)` 
Since short-form and long-form generation are now merged, we need a consistent way of returning outputs (at least for the tokens, we still need to differentiate for past_key_values see here). To be consistent with generate convention and since the tokenizer has the skip_special_tokens argument, I went for option 1.

##  🚨 Important changes 🚨
 ➡️  **Previously:**
	•	Short-form: Returned a `ModelOutput` or `torch.LongTensor`, including decoder input IDs and the EOS token ID.
	•	Long-form: Returned a `Dict` or `torch.LongTensor`, excluding decoder input IDs and the EOS token ID.

➡️  **From now on:**
Short-form and long-form generation are now treated identically, meaning output differentiation based on these modes is no longer applicable.

Decoder input IDs and EOS token IDs are never returned, except in two specific cases: when `return_dict_in_generate=True` and (`return_timestamps=False` or `force_unique_generate_call=True`).

In this case, the output will be a `ModelOutput`, which is the result of the underlying call to GenerationMixin’s generate. Indeed, `return_timestamps=False` ensures no seeking occurs; only a single call to generate is made. Therefore, this output includes both decoder input IDs and the EOS token ID.

## Testing

> [!NOTE]  
> This PR reconciles our implementation with OpenAI's. It should therefore be tested with the updated and verified tests introduced in #34111

## Evaluations

Let’s verify the effectiveness of this fix. We will evaluate both accuracy and inference speed. To do this, we will test on four short-form test sets and two long-form test sets (effectively filtering samples as ≤30 sec and >30 sec). For the short-form sets, we will compare results with the current main branch of Transformers on the first 100 samples. (Indeed, Issue #2, which involves miscalculation of the avg_logprobs, causes the full test set to be too slow.)

⚠️ Moreover, it was also tested with [this PR](https://github.com/huggingface/transformers/pull/34537) merged. 

As explained in [this PR](https://github.com/huggingface/transformers/pull/34111), we need to use a simple Whisper fork to ensure consistency with the same input features.

> [!IMPORTANT]
> **TL;DR:** We achieve perfect 1-to-1 matching in prediction results with greedy decoding for both short-form and long-form samples, validating that our implementation matches OpenAI’s original decoding algorithm. For short-form, this PR is ~5x faster than the current implementation (due to multiple incorrect temperature fallbacks).❗

<details>
  <summary>Results 📊</summary>
  
### → short-form (first 100 samples)

Set 1: [edinburghcstr/ami](https://huggingface.co/datasets/edinburghcstr/ami), config `""ihm""`, split `test[:100]`
Set 2: [distil-whisper/chime4](https://huggingface.co/datasets/distil-whisper/chime4), config `""1-channel""`, split `test[:100]`
Set 3: [google/fleurs](https://huggingface.co/datasets/google/fleurs), config `""en_us""`, split `test[:100]`

Wandb results [here](https://wandb.ai/eustache-lebihan-hf/benchmark-whisper-short)!

|                                       | set 1 - WER | set 1 - RTFx | set 1 - WER | set 1 - RTFx | set 1 - WER | set 1 - RTFx |
| ------------------------------------- | ----------- | ------------ | ----------- | ------------ | ----------- | ------------ |
| **whisper-orig ([script](https://gist.github.com/eustlb/1828634ea898b77e47ebdaa38268cf83/2eb34add5400aa376e596b1cb8e576ea09c52cd2))**      | 17.35       | 6.55         | 4.62        | 12.21        | 3.78        | 14.61        |
| **whisper-orig-fork ([script](https://gist.github.com/eustlb/465cbe9319627089ab127b404012b9f4/44260c3aa0017f0d3776f9259322298d096c7dc0))** | 19.4        | 6.45         | 4.55        | 12           | 3.78        | 14.12        |
| **this PR ([script](https://gist.github.com/eustlb/1fa238d17d4f1f49b19b79c834fb5c7d/c31de96fd7e9d6388219c2cb621063352b2fcea6))**           | 19.4        | 3.49         | 4.55        | 8.05         | 3.78        | 10.59        |
| **currrent main ([script](https://gist.github.com/eustlb/1fa238d17d4f1f49b19b79c834fb5c7d/ed5d5d18c8e97eb8416631506a448ce4e50b8fa1))**     | 25.8        | 0.73         | 5.67        | 1.49         | 7.09        | 1.9          |</details>

### → short-form (full test sets)

Set 1: [edinburghcstr/ami](https://huggingface.co/datasets/edinburghcstr/ami), config `""ihm""`, split `test`
Set 2: [distil-whisper/chime4](https://huggingface.co/datasets/distil-whisper/chime4), config `""1-channel""`, split `test`
Set 3: [google/fleurs](https://huggingface.co/datasets/google/fleurs), config `""en_us""`, split `test`

Wandb results [here](https://wandb.ai/eustache-lebihan-hf/benchmark-whisper-short)!

|                       | **Set 1 - WER** | **Set 1 - RTFx** | **Set 2 - WER** | **Set 2 - RTFx** | **Set 3 - WER** | **Set 3 - RTFx** |
| --------------------- | --------------- | ---------------- | --------------- | ---------------- | --------------- | ---------------- |
| **whisper-orig ([script](https://gist.github.com/eustlb/1828634ea898b77e47ebdaa38268cf83/185f8c8e8e90e087c548232dad43e1010a3c5d31))**      | 16.22           | 7.86             | 10.74           | 11.45            | 4.21            | 13.79            |
| **whisper-orig-fork ([script](https://gist.github.com/eustlb/465cbe9319627089ab127b404012b9f4/b21c03011b7d8bcf9af7f937bfa35439608fb052))** | 16.36           | 7.91             | 11.08           | 11.51            | 4.14            | 13.90            |
| **this PR ([script](https://gist.github.com/eustlb/1fa238d17d4f1f49b19b79c834fb5c7d/819ec3d768df5d2e41b9367b7cc3301330476d02))**           | 16.36           | 4.18             | 11.08           | 7.51             | 4.14            | 10.67            |

### → long-form 

Set 1: [distil-whisper/tedlium-long-form](https://huggingface.co/datasets/distil-whisper/tedlium-long-form), config `""default""`, split `test`
Set 2: [distil-whisper/meanwhile](https://huggingface.co/datasets/distil-whisper/meanwhile), config `""default""`, split `test`

Wandb results [here](https://wandb.ai/eustache-lebihan-hf/benchmark-whisper-long)!

|                                       | **Set 1 - WER** | **Set 1 - RTFx** | **Set 2 - WER** | **Set 2 - RTFx** |
| ------------------------------------- | --------------- | ---------------- | --------------- | ---------------- |
| **whisper-orig ([script](https://gist.github.com/eustlb/1828634ea898b77e47ebdaa38268cf83/4619d515e03708464050aa0f0b205536ea6497db))**      | 172.15          | 4.98             | 264.66          | 4.09             |
| **whisper-orig-fork ([script](https://gist.github.com/eustlb/465cbe9319627089ab127b404012b9f4/8c717feb0c9ea98e0849945c78b62c381ce94069))** | 172.15          | 4.83             | 264.66          | 4.00             |
| **this PR ([script](https://gist.github.com/eustlb/1fa238d17d4f1f49b19b79c834fb5c7d/52cba2a9e8109f022418b9874b5791717abb4284))**           | 172.15          | 4.88             | 264.66          | 3.98             |
| **current main ([script](https://gist.github.com/eustlb/1fa238d17d4f1f49b19b79c834fb5c7d/9a343e35d28ea39d7dac8f39d5faffafbb4e613f))**     | 172.15          | 4.92             | 264.66          | 3.98             |
","If I remember correctly, the kwargs arguments should have priority over the generation config, which is not the case here.

cc @ArthurZucker could you confirm ? Could you explain why we still remove the eos token here ? (nit) no need to say ""adapted from"", it's already implied.
```suggestion
``` (nit) should we get rid of this to improve readability? WDYT ?

```suggestion
```
 Same comment here! yep! 
```suggestion
        num_beams = kwargs.get(""num_beams"", generation_config.get(""num_beams"", 1))
        )
```
rather no? (default to kwargs if wargs is not None good catch I guess! nice explanation! > we need to return decoder input ids to make sure assistant generated tokens are the same as the shape of the logits from the forward path

or something to explain why we need to do this! 🤗 
Also really not a fan of non explicit solutions like this one, tho I am guessing there are not many other things you can do! I don't mind it for testing purposes !  added in [c9b36c3](https://github.com/huggingface/transformers/pull/34135/commits/c9b36c3a5a299f5a5f7fa9c3eadeef0f04b417d5) we don't really need to import this no? we can check the str or `__class__`!  I am guessing we are patching because the assistant model can be another class than `WhisperForCausalLM` right?  There's a case in which we append an empty list to `token_timestamps_list`, aren't we going to raise an error if `i` refers to this empty list? (nit)
```suggestion
                    - eventually if `return_timestamps=True` and the passed input is <= 30 seconds / <= 3000 mel input features, if the last predicted timestamp is not < 30 seconds which will trigger a second call to generate starting to this timestamp.
``` Nice! I think this part would benefit from being clearer: don't hesitate to use bullet points and spaces, and avoid parentheses. I'm not sure to follow. There's two if conditions, how are they linked ? Is it "",and if the last..."" or "", or if the last..."" ? I assume that the length of the list is equal to the number of calls to the internal `generate` method? As discussed offline, I don't think we should have behavior that depends on an internal call over which the user has no influence.

Before, we had those extra tokens if the audio was short (<30s). Now, we add to this an internal condition: there's only one call to the internal `generate`.

IMO, we should have a consistent behaviour: removing these special tokens by default. And add an additional flag to allow this edge case.  No, it is a list of segments, meaning a list whose length depends on the predicted timestamp tokens. The only way to know the number of calls to the internal `generate` method here is to look at the segment's `result`.
Indeed, we can have `[{start_1, end_1, result_1}, {start_2, end_2, result_1}, {start_3, end_3, result_2}]`: the two first segments come from the same call to internal generate (result_1) and the last to the next one (result_2).  fixed in [0ea240f](https://github.com/huggingface/transformers/pull/34135/commits/0ea240fa88d2c7aeb3b8968427fb3f15a8e76e15) 👌 fixed in [0ea240f](https://github.com/huggingface/transformers/pull/34135/commits/0ea240fa88d2c7aeb3b8968427fb3f15a8e76e15) 👌 nice catch! fixed in [0ea240f](https://github.com/huggingface/transformers/pull/34135/commits/0ea240fa88d2c7aeb3b8968427fb3f15a8e76e15) 👌 What if `kwargs[""force_unique_generate_call""]=True` ? Is it accurate? I thought it was the case if force_unique_generate_call=True? (maybe ?)
```suggestion
                - `torch.LongTensor`, excluding the decoder input ids and end of sequence id, in all other cases.
``` I've added a `force_unique_generate_call` arg to generate 👌 with this you are going to iterate at least twice on the current segments. if you have both force unique generate call and return token timestamps  Nice catch! fixed in [a1f4e43](https://github.com/huggingface/transformers/pull/34135/commits/a1f4e4315f21d6b8f903930c1530d5697a85bfe3)"
35222,2024-12-12T06:56:57Z,2024-12-18T09:14:22Z,faaany,7,0,3,6,1,1,1,[],23885.0,0,526645.0,0,0,0,0,730567.707259,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1', 'faaany', 'ydshieh']","cc @ydshieh  LGTM, but is still in draft mode or ready to go? > LGTM, but is still in draft mode or ready to go?

Yes, we are ready to go.  @Rocketknight1 Maybe we can merge once you approved? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35222). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. This seems significant enough that it might need a core maintainer! cc @LysandreJik @ArthurZucker  I talked to ArthurZucker offline and we agree that I have some level of permissions to merge PRs like this.Thanks",Thanks,"Currently this test is restricted to run on CUDA, but it can also run on other accelerators like XPU. 
",
35211,2024-12-11T15:19:59Z,2024-12-18T12:56:49Z,ydshieh,1,8,7,465,3,2,1,[],1743.0,0,596212.0,0,0,0,0,717219.06094,,0,7,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35211). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Huge work! Let's add this to our internal notion as well ! 🤗 ,Huge work! Let's add this to our internal notion as well ! 🤗 ,"# What does this PR do?

","does it only cancel for the PR?  lgtm this might be kinda tricky   let's comment here that this is where we create the check / the action sticker, that needs to be updated later on I think super nice as well it will ONLY cancel the one triggered by the same PR (or issue). The runs triggered by other PR/issue won't be canceled. 

  - (If it is an issue, it won't run any actual job: we restrict this below)
 ok it's your work. I copied your code 😆 "
35217,2024-12-11T18:28:33Z,2024-12-18T08:54:32Z,SunMarc,1,0,7,29,2,2,2,[],1824.0,0,570361.0,0,0,0,0,731757.000166,,0,7,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35217). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.No problem for me! @SunMarc thanks for this 🤗 LGTM,No problem for me! @SunMarc thanks for this 🤗 LGTM,"# What does this PR do ? 

This PR enables to load a model by passing a state_dict only with `low_mem_cpu_usage=True`. 

Added a test that was not passing before this PR ! 

Required for DDUF (reference: https://github.com/huggingface/diffusers/pull/10037)",
35300,2024-12-16T19:57:22Z,2024-12-17T18:27:23Z,stevhliu,1,0,1,2,1,1,1,[],1650.0,0,81005.0,0,0,0,0,783784.776502,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35300). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM!,LGTM!,"Provides additional clarification that `register_pipeline` registers a pipeline's implementation, task type, and supported backend to a models `config.json` file.",
35294,2024-12-16T13:01:38Z,2024-12-17T17:36:32Z,alexrs-cohere,0,1,2,9,1,1,1,[],,0,102894.0,0,0,0,0,786840.094264,,0,2,0,False,[],Thanks for adding more details 🤗 ,Thanks for adding more details 🤗 ,"# What does this PR do?
Adds more details in the `cohere2` model docs.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker, @stevhliu ","This makes it sound less like its optimized only for code developed by Cohere and C4AI :)

```suggestion
[C4AI Command R7B](https://cohere.com/blog/command-r7b) is an open weights research release of a 7B billion parameter model developed by Cohere and Cohere For AI. It has advanced capabilities optimized for various use cases, including reasoning, summarization, question answering, and code. The model is trained to perform sophisticated tasks including Retrieval Augmented Generation (RAG) and tool use. The model also has powerful agentic capabilities that can use and combine multiple tools over multiple steps to accomplish more difficult tasks. It obtains top performance on enterprise-relevant code use cases. C4AI Command R7B is a multilingual model trained on 23 languages.
```"
35306,2024-12-17T09:01:13Z,2024-12-17T17:34:41Z,zzzzzsa,0,0,1,2,1,1,1,[],,0,30808.0,0,0,0,0,786951.948156,,0,1,0,False,[],"Good catch, thanks!","Good catch, thanks!","# What does this PR do?

Remove unused parameter in batch inference example in llava doc.


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@stevhliu",
35303,2024-12-17T06:04:14Z,2024-12-17T17:33:50Z,jla524,0,0,1,4,2,1,1,[],,0,41376.0,0,0,0,0,787003.71418,,0,1,0,False,[],Thanks 🤗 ,Thanks 🤗 ,"# What does this PR do?

Fixes image preview in multi-GPU inference docs

before:
<img width=""1475"" alt=""image"" src=""https://github.com/user-attachments/assets/d644af47-9584-4b7d-9114-6240eead0d50"" />

after:
<img width=""1475"" alt=""image"" src=""https://github.com/user-attachments/assets/8910c657-a1e8-4b57-a7c3-9167a30fc4f6"" />


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@stevhliu",
35302,2024-12-17T05:44:14Z,2024-12-17T17:32:00Z,jla524,0,0,2,94,9,1,1,[],,0,42466.0,0,0,0,0,787113.993503,,0,2,0,False,[],Thanks for applying the fixes everywhere!,Thanks for applying the fixes everywhere!,"# What does this PR do?

Fix typos in quicktour, a follow up for https://github.com/huggingface/transformers/pull/35272

## Who can review?

@stevhliu",
35264,2024-12-13T17:09:38Z,2024-12-17T16:34:18Z,qubvel,1,0,4,2,1,1,1,"['Vision', 'run-slow', 'Processing']",2059.0,0,343480.0,0,0,0,0,790576.99275,,1,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35264). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks 🤗 ,Thanks 🤗 ,"# What does this PR do?

Limit the number of repetitions in a regular expression pattern to prevent the method from hanging.

Here is a code sample to test previous and updated regex matches, along with a performance test.

The only different example is `# 1.`, which I believe should be parsed. Therefore, it can also be considered as a fix, not just a breaking change.


```python

import re
import time


# Original Regex (Vulnerable to ReDoS)
def original_post_process(generation):
    return re.sub(r""^#+ (?:\.?(?:\d|[ixv])+)*\s*(?:$|\n\s*)"", """", generation, flags=re.M)

# Updated Regex (Avoids ReDoS by simplifying backtracking risks)
def updated_post_process(generation):
    return re.sub(r""^#+ (?:[\d+\.]+|[ixv\.]+)?\s*(?:$|\n\s*)"", """", generation, flags=re.M)


# Test cases to validate equivalence and performance
def test_post_process_equivalence():
    test_cases = [
        
        # Simple headings
        ""# Section"",
        ""# 1.2.3"",
        ""# .1.2.3"",
        ""# .i.v.x"",

        # Standard headings
        ""# Heading 1\n## 1. Subheading\n### 1.1 Sub-subheading\n#### IV. Roman numeral heading\nRegular text starts here."",
        
        # Trailing spaces
        ""#    \n## 1.    \n### Roman numeral heading with spaces    \nRegular text here."",
        
        # Roman numerals
        ""# i\n# iv Roman numeral heading\n# x Section\nText with valid content."",
        
        # Mixed content
        ""# Heading 1\n## Subheading 1\nRegular text.\n### Subheading with text\nSome more regular text."",
        
        # Non-heading patterns
        ""# This is a valid heading\nSome text that shouldn't be removed.\n# Heading with text afterward\nText with valid content."",
        
        # Completely empty or irregular inputs
        """",
        ""   \n   \n   "",
        
        # Inputs with special characters
        ""# # Special heading\nRegular text."",
        
        # Escaped markdown syntax
        ""\\# Escaped heading\n## Valid heading\nText content."",
        
        # Multiline text blocks
        ""# Valid heading\n\nText under heading.\n\n## Another valid heading\n\nMore text here."",
        
        # Random non-heading input
        ""This is just random text with no headings.\nAnother line of text."",

        # Long problematic input for ReDoS
        ""# "" + ""0"" * 25 + "":\n"",  # Long problematic input for ReDoS

        # Large multiline input
        ""# Heading 1\n## 1. Subheading\n### 1.1 Sub-subheading\n#### IV. Roman numeral heading\nRegular text starts here.\n"" * 100,

    ]

    for i, input_str in enumerate(test_cases):
        original_output = original_post_process(input_str)
        updated_output = updated_post_process(input_str)

        if original_output != updated_output:
            print(""\nInput:\n"", input_str)
            print(""\nOriginal:\n"", original_output)
            print(""\nUpdated:\n"", updated_output)
            print(""\n"" * 3)
            # raise ValueError(f""Test {i + 1}: Outputs do not match!"")

        print(f""Test {i + 1}: Outputs match!"")


# Performance comparison
def performance_test():
    long_input = ""# "" + ""0"" * 25 + "":\n""  # Long problematic input for ReDoS

    # Test original method
    start_time = time.time()
    original_post_process(long_input)
    print(f""Original method execution time: {time.time() - start_time:.6f} seconds"")

    # Test updated method
    start_time = time.time()
    updated_post_process(long_input)
    print(f""Updated method execution time: {time.time() - start_time:.6f} seconds"")


if __name__ == ""__main__"":
    # Run tests
    print(""Running equivalence tests..."")
    test_post_process_equivalence()

    print(""\nRunning performance tests..."")
    performance_test()

```

Output:
```
Running equivalence tests...
Test 1: Outputs match!
Test 2: Outputs match!
Test 3: Outputs match!
Test 4: Outputs match!
Test 5: Outputs match!

Input:
 #    
## 1.    
### Roman numeral heading with spaces    
Regular text here.

Original:
 
## 1.    
### Roman numeral heading with spaces    
Regular text here.

Updated:
 

### Roman numeral heading with spaces    
Regular text here.

Test 7: Outputs match!
Test 8: Outputs match!
Test 9: Outputs match!
Test 10: Outputs match!
Test 11: Outputs match!
Test 12: Outputs match!
Test 13: Outputs match!
Test 14: Outputs match!
Test 15: Outputs match!
Test 16: Outputs match!
Test 17: Outputs match!

Running performance tests...
Updated method execution time: 0.000503 seconds
```

Even for  100000 zeros the time now does not exceed a second.
",
35292,2024-12-16T10:14:30Z,2024-12-17T16:27:26Z,ydshieh,1,0,1,7,1,1,1,[],1673.0,0,108778.0,0,0,0,0,790987.777884,,1,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35292). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Yep sounds good we'll have the tag and comment anyways,Yep sounds good we'll have the tag and comment anyways,"# What does this PR do?

As discussed offline: the benchmark workflow is updated and is triggered directly with push to main / pull_request events.
There is no need to be called in  `.github/workflows/push-important-models.yml` (and it's currently failing due to the format mismatching)",
35296,2024-12-16T15:43:15Z,2024-12-17T14:18:42Z,Rocketknight1,2,0,3,23,2,1,1,[],117.0,0,81329.0,0,0,0,0,798712.018684,,1,3,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']","cc @LysandreJik @ArthurZucker for core maintainer review! I checked with `make pre-release` locally and it worked as intended. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35296). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Sounds good, let's just put 🔴 🔴 🔴  in the PR name!","Sounds good, let's just put 🔴 🔴 🔴  in the PR name!","This PR updates `release.py` to delete model conversion scripts. These scripts are generally included with specific model classes to convert checkpoints in non-Transformers formats. Often these scripts have to open insecure file types, because those were the file types the model was released with (e.g. `pickle` or old Torch `.bin` checkpoints). This results in vulnerability scanners flagging us, and can cause compliance issues for users. 

We don't see this as a serious attack vector in practice because users would have to be induced to download a malicious file and call an obscure conversion script on it, but excluding these files from release wheels should help with compliance issues!

Fixes #34840",
34941,2024-11-26T15:00:39Z,2024-12-17T13:44:47Z,OmarManzoor,10,13,10,658,7,4,3,"['Vision', 'SDPA', 'run-slow']",64078.0,0,1810043.0,0,0,0,0,800555.531309,,0,10,0,False,"['qubvel', 'OmarManzoor', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34941). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Here is the [code](https://gist.github.com/qubvel/6c62984bf43e40efbc850fc4fbdf4d92), can you run it with your env to > > check? Also my code might be not the best way to benchmark, would appreciate if you share your code!

Maybe your script might be better than what I am using but nevertheless here is the script I used for inference. 
[inference_image_benchmark](https://gist.github.com/OmarManzoor/fc02a514258d9b8e9e0138cb18e7bcd0)
I basically just modified the script that we have been using for the text models.

 Here are the results using your script

### BeitForImageClassification

|   Image batch size |   Eager (s/iter) | Eager CI, %   |   SDPA (s/iter) | SDPA CI, %   |   SDPA speedup |
|-------------------:|-----------------:|:--------------|----------------:|:-------------|---------------:|
|                  1 |            0.012 | ±0.5%         |           0.011 | ±0.3%        |          1.135 |
|                  4 |            0.013 | ±0.2%         |           0.011 | ±0.2%        |          1.181 |
|                 16 |            0.045 | ±0.1%         |           0.035 | ±0.1%        |          1.3   |
|                 32 |            0.088 | ±0.1%         |           0.067 | ±0.1%        |          1.322 |


## Environment:

```console
Python version: 3.10.14 (main, Jul 23 2024, 15:53:02) [GCC 9.4.0]
Transformers version: 4.47.0.dev0
Torch version: 2.5.1+cu124
GPU: NVIDIA GeForce RTX 2060 SUPER
``` Ok, looks good, can you update documentation benchmarks then? Maybe it's worth adding memory stats to my script and rerunning it > Ok, looks good, can you update documentation benchmarks then? Maybe it's worth adding memory stats to my script and rerunning it

Okay I think we can use your inference benchmarks but for the training part I think we can keep the current benchmarks? Also did you have a look at the script I shared, is there something that is not quite correct in that? @qubvel Do the slow tests still need to be run? @ArthurZucker please review when you have bandwidth! @dvrogozh Thank you for verifying @ArthurZucker Can this be merged? > Hey all! Sorry for the delay, we are in the middle of a huge refactoring in #35235, which is why I wanted to wait a bit, but good work should be rewarded, so let's merge this! Thanks for being patient 🤗

Thank you!Glad to see you in another sdpa-PR @OmarManzoor! Thanks for working on the implementation 👍  Overall looking great, just a few notes: Hey, I'm getting the following values for benchmark

### BeitForImageClassification

|   Image batch size |   Eager (s/iter) | Eager CI, %   |   SDPA (s/iter) | SDPA CI, %   |   SDPA speedup |
|-------------------:|-----------------:|:--------------|----------------:|:-------------|---------------:|
|                  1 |            0.013 | ±0.2%         |           0.011 | ±0.5%        |          1.122 |
|                  4 |            0.013 | ±0.2%         |           0.011 | ±0.1%        |          1.122 |
|                 16 |            0.026 | ±0.1%         |           0.021 | ±0.5%        |          1.232 |
|                 32 |            0.048 | ±0.8%         |           0.039 | ±0.3%        |          1.252 |

with the following env:
```
Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Transformers version: 4.47.0.dev0
Torch version: 2.5.0+cu118
GPU: NVIDIA A10G
```
Here is the [code](https://gist.github.com/qubvel/6c62984bf43e40efbc850fc4fbdf4d92), can you run it with your env to check? Also my code might be not the best way to benchmark, would appreciate if you share your code! Thanks for the ping! Looking great to me, just one comment @OmarManzoor : thank you for rebase and adding thresholds path for xpu. I verified it on my side against upstream pytorch xpu - both added tests work for me. Hey all! Sorry for the delay, we are in the middle of a huge refactoring in #35235, which is why I wanted to wait a bit, but good work should be rewarded, so let's merge this! Thanks for being patient 🤗 ","Glad to see you in another sdpa-PR @OmarManzoor! Thanks for working on the implementation 👍  Overall looking great, just a few notes: Hey, I'm getting the following values for benchmark

### BeitForImageClassification

|   Image batch size |   Eager (s/iter) | Eager CI, %   |   SDPA (s/iter) | SDPA CI, %   |   SDPA speedup |
|-------------------:|-----------------:|:--------------|----------------:|:-------------|---------------:|
|                  1 |            0.013 | ±0.2%         |           0.011 | ±0.5%        |          1.122 |
|                  4 |            0.013 | ±0.2%         |           0.011 | ±0.1%        |          1.122 |
|                 16 |            0.026 | ±0.1%         |           0.021 | ±0.5%        |          1.232 |
|                 32 |            0.048 | ±0.8%         |           0.039 | ±0.3%        |          1.252 |

with the following env:
```
Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Transformers version: 4.47.0.dev0
Torch version: 2.5.0+cu118
GPU: NVIDIA A10G
```
Here is the [code](https://gist.github.com/qubvel/6c62984bf43e40efbc850fc4fbdf4d92), can you run it with your env to check? Also my code might be not the best way to benchmark, would appreciate if you share your code! Thanks for the ping! Looking great to me, just one comment @OmarManzoor : thank you for rebase and adding thresholds path for xpu. I verified it on my side against upstream pytorch xpu - both added tests work for me. Hey all! Sorry for the delay, we are in the middle of a huge refactoring in #35235, which is why I wanted to wait a bit, but good work should be rewarded, so let's merge this! Thanks for being patient 🤗 ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Related to #28005

- Adds sdpa for Beit model
- Also extends the support of sdpa to Data2VecVision model to ensure consistency


## Who can review?

@qubvel 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Let's add similar to the data2vec model Did you copy it from some model entirely or it have been modified? I mean it would be nice to add ""#Copied from"" if we could  Could you please add some comment what was modified with respect to the common test Actually this particular attention was a bit different because of the relative bias. So I modified it accordingly. The SelfAttention itself was not copied from anywhere. It would be great to add a fallback to `eager` attention if `output_attentions=True`, see other models for details, e.g. CLIP/SigLIP. Oh yes thank you for reminding 😄  @OmarManzoor we should also call `super().forward(...)` together with warning message 🤗  Sorry for missing no worries ```suggestion
                                        with sdpa_kernel(
```

@OmarManzoor : can you, please, rebase on top of latest master and reuse a change from #34889 merged today? This will generalize the test for other non-cuda devices by using `torch.nn.attention.sdpa_kernel`. We now have `sdpa_kernel` helper function to differentiate a call per torch version:
https://github.com/huggingface/transformers/blob/31299670cda29f25fbc655f6f166e7b8cc21c89f/tests/test_modeling_common.py#L190 Also from #34889. This will likely be needed to make test passing on XPU device. I can help to test on XPU if you will add this change into the PR or I can follow up later on for XPU with separate PR.

```suggestion
                                    if torch_device in [""cpu"", ""cuda""]:
                                        atol = atols[torch_device, enable_kernels, torch_dtype]
                                        rtol = rtols[torch_device, enable_kernels, torch_dtype]
                                    elif torch_device == ""xpu"":
                                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH
                                        # which is implemented on PyTorch level using aten operators and is
                                        # device agnostic with respect to implementation of each aten operator.
                                        atol = atols[""cuda"", False, torch_dtype]
                                        rtol = rtols[""cuda"", False, torch_dtype]
                                    else:
                                        atol = 1e-7
                                        rtol = 1e-4
``` Same as above, after rebase on top of latest main:
```suggestion
                                        with sdpa_kernel(
``` same as above:
```suggestion
                                    if torch_device in [""cpu"", ""cuda""]:
                                        atol = atols[torch_device, enable_kernels, torch_dtype]
                                        rtol = rtols[torch_device, enable_kernels, torch_dtype]
                                    elif torch_device == ""xpu"":
                                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH
                                        # which is implemented on PyTorch level using aten operators and is
                                        # device agnostic with respect to implementation of each aten operator.
                                        atol = atols[""cuda"", False, torch_dtype]
                                        rtol = rtols[""cuda"", False, torch_dtype]
                                    else:
                                        atol = 1e-7
                                        rtol = 1e-4
```"
35307,2024-12-17T09:09:41Z,2024-12-17T13:23:13Z,mokeddembillel,0,1,7,34,5,1,1,[],,0,15213.0,0,0,0,0,802044.117995,,0,7,0,False,[],It's missing a hyper link to your blog post / paper / models on the hub and just `make fix-copies`,It's missing a hyper link to your blog post / paper / models on the hub and just `make fix-copies`,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Adding Falcon3 model documentation

## Who can review?
@ArthurZucker
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
<!--Copyright 2024 The HuggingFace Team. All rights reserved.
```"
34110,2024-10-12T16:23:24Z,2024-12-17T13:46:05Z,MagnusS0,10,10,19,287,4,3,1,"['Vision', 'SDPA', 'run-slow']",856894.0,0,5692962.0,0,0,0,0,800672.261077,,0,19,0,False,"['qubvel', 'HuggingFaceDocBuilderDev', 'MagnusS0', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34110). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. ```FAILED tests/models/sam/test_modeling_sam.py::SamModelTest::test_sdpa_can_dispatch_non_composite_models - ValueError: The SDPA model should have SDPA attention layers``` can't merge until it's fixed! > `FAILED tests/models/sam/test_modeling_sam.py::SamModelTest::test_sdpa_can_dispatch_non_composite_models - ValueError: The SDPA model should have SDPA attention layers` can't merge until it's fixed!

Doing run slow I also get the following failed tests:
```
FAILED tests/models/sam/test_modeling_sam.py::SamModelTest::test_eager_matches_sdpa_inference_0_float16 - AttributeError: 'SamImageSegmentationOutput' object has no attribute 'hidde...
FAILED tests/models/sam/test_modeling_sam.py::SamModelTest::test_eager_matches_sdpa_inference_1_bfloat16 - AttributeError: 'SamImageSegmentationOutput' object has no attribute 'hidde...
FAILED tests/models/sam/test_modeling_sam.py::SamModelTest::test_eager_matches_sdpa_inference_2_float32 - AttributeError: 'SamImageSegmentationOutput' object has no attribute 'hidde...
FAILED tests/models/sam/test_modeling_sam.py::SamModelTest::test_sdpa_can_compile_dynamic - torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
FAILED tests/models/sam/test_modeling_sam.py::SamModelTest::test_sdpa_can_dispatch_non_composite_models - ValueError: The SDPA model should have SDPA attention layers
FAILED tests/models/sam/test_modeling_sam.py::SamModelTest::test_sdpa_can_dispatch_on_flash - RuntimeError: No available kernel. Aborting execution.
FAILED tests/models/sam/test_modeling_sam.py::SamModelIntegrationTest::test_inference_mask_generation_batched_points_batched_images - AssertionError: False is not true
FAILED tests/models/sam/test_modeling_sam.py::SamModelIntegrationTest::test_inference_mask_generation_no_point - AssertionError: False is not true
FAILED tests/models/sam/test_modeling_sam.py::SamModelIntegrationTest::test_inference_mask_generation_one_point_one_bb - AssertionError: False is not true
FAILED tests/models/sam/test_modeling_sam.py::SamModelIntegrationTest::test_inference_mask_generation_one_point_one_bb_zero - AssertionError: False is not true
```

- The 4 tests related to `test_inference_mask_generation` fail on the main branch as well. 
- I think `test_sdpa_can_dispatch_on_flash` should be skipped since the flash kernel can't be run with attention_mask's that is used to add the relative positional embeddings. 
- `test_eager_matches_sdpa_inference` is missing hidden states, can the vision_hidden_states be used for SAM in this test?

I haven't found the root cause of `test_sdpa_can_dispatch_non_composite_models`, running it outside the test framework it does not fail so I believe it might be related to how 'sdpa' is added to the configs (??)


I'll work on a fix for those I think I'll be able to solve.
 Apologies for the commit history mess. I had to rebase to clean things up, so the branch should now be up to date with the main branch and tests pass. Hi @MagnusS0, thanks for the update, can you please push an empty commit with the message `[run-slow] sam` to trigger slow tests in CI? Sorry, it should be the last commit, in that case I can approve the run 🙂 > Sorry, it should be the last commit, in that case I can approve the run 🙂

Ahh no problem, will commit again.  Just fyi there are 4 slow tests I expect will fail, as they also do that on main. 
They will pass if the `assertion` `atol=1e-4` is set to `atol=1e-3`. Would this be an fair fix?
 It looks like everything is fine in CI 🤗 Let's wait for a comment from @zucchini-nlp regarding setting attention implementation in config and I suppose it's ready to be merged Awesome 🤗  Probably just some small differences in my local setup causing the test to fail then. @ArthurZucker Could you please review this PR when you have a moment? It should be ready to merge 🤗 Thank you!
Looks good! 🤗  Thanks for adding, I think this should be automatically tests, but make sure to run the slow tests Thanks a lot for the addition, sorry for the delay. 
We are going to refactor a bit afterwards in #35235, but let's merge for now! 🤗 ","Looks good! 🤗  Thanks for adding, I think this should be automatically tests, but make sure to run the slow tests Thanks a lot for the addition, sorry for the delay. 
We are going to refactor a bit afterwards in #35235, but let's merge for now! 🤗 ","# What does this PR do?

This PR adds support for SDPA to the SAM (Segment Anything Model), following the implementation done by the PyTorch team in [segment-anything-fast](https://github.com/pytorch-labs/segment-anything-fast/).

I add two new classes:

- `SamSdpaAttention(SamAttention)`: For the decoder.
- `SamSdpaVisionAttention(SamVisionAttention)`: For the encoder.

### Performance Improvements (Updated 03.11.2024)
Time Reduction between Eager and SDPA modes for SAM-B and SlimSAM averaged across 10 runs in ms:

| **Batch Size** | **Eager (ms)**      | **SDPA (ms)**       | **Time Reduction (%)** |
|----------------|---------------------|---------------------|------------------------|
| **SAM-B**      |                     |                     |                        |
| 1              | 64.58 (±2.14)       | 30.95 (±0.07)       | 52.06%                 |
| 8              | 1,021.45 (±4.55)    | 227.44 (±0.67)      | 77.73%                 |
| **SlimSAM**    |                     |                     |                        |
| 1              | 50.51 (±1.19)       | 28.70 (±0.03)       | 43.16%                 |
| 8              | 913.97 (±6.73)      | 284.43 (±2.14)      | 68.87%                 |


Peak Memory Usage During Forward Pass:

| **Batch Size** | **Eager (MB)** | **SDPA (MB)** | **Memory Reduction (%)** |
|----------------|----------------|---------------|--------------------------|
| **SAM-B**      |                |               |                          |
| 1              | 2,151          | 875           | 59.31%                   |
| 8              | 16,007         | 3,913         | 75.54%                   |
| **SlimSAM**    |                |               |                          |
| 1              | 1,961          | 1,273         | 35.10%                   |
| 8              | 15,540         | 9,503         | 38.84%                   |


On the TrashNet 1.0 validation set I got a reduction around 30-35%, with no decrease in accuracy.
I also ran training on TrashNet 1.0 with no issues and about 1.7x speedup per epoch with SlimSAM. Also allowed gradient check-pointing that seemed to be missing from the model deceleration, worked without any changes.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@amyeroberts 
@qubvel 
I'll also tag @ArthurZucker since we just worked on this model last weekend.

### Some TODO
Should probably add some documentation for this, but let me know where is best place to add this. ","this should be the same as the parrent one no?  Let's not re-define it in that case! If we keep it the same as in the parent we have to calculate the attention weights manually which takes longer. We then also store the full attention matrix before adding it as positional bias in SDPA which cause spikes in memory. 

(A quick test showed close to 2x peak memory usage.)

PyTorch team write about it in this [blog](https://pytorch.org/blog/accelerating-generative-ai/). Oh nice then let's add a small comment about that!
 cc @zucchini-nlp not sure we have a better way to do this today right! Added a small comment about it in d486e627c64c6982ada52805de75857c86ad8c74 @MagnusS0 hey, no need to do this now, as each sub-config should get its attn implementation set when loading `from_pretrained()`. The to check correctness are `test_attn_implementation_composite_models` and `test_sdpa_can_dispatch_composite_models` and `test_flash_attn_2_can_dispatch_composite_models`  

Also, can you pls add the attributes for `sub_configs` as in this PR (https://github.com/huggingface/transformers/pull/34410/files) and enable tests for configs? it should be the `composite` model as SAM has several sub configs in the main config. You can add class attribute `_is_composite=True` to the tester class so the appropriate tests are called :) `sub_configs` added, as well as the config test. Added `_is_composite=True` and  `test_sdpa_can_dispatch_composite_models` but did not add `test_flash_attn_2_can_dispatch_composite_models` as flash attention won't work with the positional embedding.  Let me know if anything else is missing 🤗 cool, thanks, LGTM! Feel free to ping core maintainer for review :)"
35297,2024-12-16T16:11:32Z,2024-12-17T07:05:36Z,ArthurZucker,1,0,3,64,2,0,0,[],3089.0,0,53646.0,0,0,0,0,824701.944222,,1,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35297). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?
The order of modular conversion was wrong. Now we do have: 

```bash
No differences found for src/transformers/models/gemma/configuration_gemma.py.
No differences found for src/transformers/models/gemma/tokenization_gemma.py.
No differences found for src/transformers/models/gemma/modeling_gemma.py.
No differences found for src/transformers/models/llava_next_video/configuration_llava_next_video.py.
No differences found for src/transformers/models/llava_next_video/modeling_llava_next_video.py.
No differences found for src/transformers/models/ijepa/modeling_ijepa.py.
No differences found for src/transformers/models/instructblipvideo/configuration_instructblipvideo.py.
No differences found for src/transformers/models/instructblipvideo/modeling_instructblipvideo.py.
No differences found for src/transformers/models/starcoder2/modeling_starcoder2.py.
No differences found for src/transformers/models/olmo2/configuration_olmo2.py.
No differences found for src/transformers/models/olmo2/modeling_olmo2.py.
No differences found for src/transformers/models/gemma2/configuration_gemma2.py.
No differences found for src/transformers/models/gemma2/modeling_gemma2.py.
No differences found for src/transformers/models/aria/configuration_aria.py.
No differences found for src/transformers/models/aria/modeling_aria.py.
No differences found for src/transformers/models/aria/image_processing_aria.py.
No differences found for src/transformers/models/aria/processing_aria.py.
No differences found for src/transformers/models/cohere2/configuration_cohere2.py.
No differences found for src/transformers/models/cohere2/modeling_cohere2.py.
No differences found for src/transformers/models/glm/modeling_glm.py.
```
PS: removed the `examples` as it slows downs the make fix-copies for everyone
(meaning, gemma2 after gemma)",
35288,2024-12-16T07:43:12Z,2024-12-16T13:36:27Z,zhanluxianshen,1,0,1,6,1,1,1,[],52493.0,0,52494.0,0,0,0,0,856354.268306,,0,1,0,False,['zhanluxianshen'],"Thanks.Yep, this is clean. Thank you!","Yep, this is clean. Thank you!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33736,2024-09-26T18:06:54Z,2024-12-17T10:26:44Z,tonywu71,13,30,137,2206,22,3,0,"['New model', 'Vision', 'Multimodal', 'run-slow']",2955285.0,0,7057190.0,0,0,0,0,812637.19356,,0,137,0,False,"['tonywu71', 'HuggingFaceDocBuilderDev', 'yonigozlan', 'ArthurZucker']","**Deps used for the weight conversion (`requirements.txt`):**

```
accelerate==1.0.1
aiohappyeyeballs==2.4.3
aiohttp==3.10.10
aiosignal==1.3.1
appnope==0.1.4
asttokens==2.4.1
attrs==24.2.0
certifi==2024.8.30
charset-normalizer==3.4.0
colpali_engine==0.3.3
comm==0.2.2
datasets==3.0.2
debugpy==1.8.7
decorator==5.1.1
dill==0.3.8
executing==2.1.0
filelock==3.16.1
frozenlist==1.5.0
fsspec==2024.9.0
gitdb==4.0.11
GitPython==3.1.18
huggingface-hub==0.26.2
idna==3.10
ipykernel==6.29.5
ipython==8.29.0
isort==5.13.2
jedi==0.19.1
Jinja2==3.1.4
jupyter_client==8.6.3
jupyter_core==5.7.2
libcst==1.5.0
markdown-it-py==3.0.0
MarkupSafe==3.0.2
matplotlib-inline==0.1.7
mdurl==0.1.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
nest-asyncio==1.6.0
networkx==3.4.2
numpy==2.1.2
packaging==24.1
pandas==2.2.3
parso==0.8.4
pexpect==4.9.0
pillow==11.0.0
platformdirs==4.3.6
prompt_toolkit==3.0.48
propcache==0.2.0
psutil==6.1.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==18.0.0
Pygments==2.18.0
python-dateutil==2.9.0.post0
pytz==2024.2
PyYAML==6.0.2
pyzmq==26.2.0
regex==2024.9.11
requests==2.32.3
rich==13.9.3
ruff==0.5.1
safetensors==0.4.5
six==1.16.0
smmap==5.0.1
stack-data==0.6.3
sympy==1.13.1
tokenizers==0.20.1
torch==2.5.1
tornado==6.4.1
tqdm==4.66.6
traitlets==5.14.3
-e git+https://github.com/tonywu71/transformers.git@05df69d83bdc5398f590ca0bb623de330cec49dd#egg=transformers
typing_extensions==4.12.2
tzdata==2024.2
urllib3==1.26.20
wcwidth==0.2.13
xxhash==3.5.0
yarl==1.17.1
``` Hi @ArthurZucker, could you review my PR for ColPali please? 😁  @yonigozlan was kind enough to do multiple reviews of the PR already, so it should almost ready to ship!

However, I noticed a few bugs with modular that will need to be fixed before merging:

1. There is the bug shown below where the `ColPaliProcessorKwargs` are not properly updated in the processing file.
<img width=""965"" alt=""Screenshot 2024-10-31 at 09 06 32"" src=""https://github.com/user-attachments/assets/80922edd-c64e-4cd9-bf2e-06d5e60a4a4e"">

2. The docstrings seem not to be properly synchronized: see the forward documentation for `ColPaliForRetrieval` in  `src/transformers/models/colpali/modular_colpali.py` vs `src/transformers/models/colpali/modeling_colpali.py`.

Any chance you could use with these please? 🙏🏼 


 > Hi @ArthurZucker, could you review my PR for ColPali please? 😁 @yonigozlan was kind enough to do multiple reviews of the PR already, so it should almost ready to ship!
> 
> However, I noticed a few bugs with modular that will need to be fixed before merging:
> 
> 1. There is the bug shown below where the `ColPaliProcessorKwargs` are not properly updated in the processing file.
> 2. The docstrings seem not to be properly synchronized: see the forward documentation for `ColPaliForRetrieval` in  `src/transformers/models/colpali/modular_colpali.py` vs `src/transformers/models/colpali/modeling_colpali.py`.
> 
> Any chance you could use with these please? 🙏🏼

This was fixed thanks to https://github.com/huggingface/transformers/pull/3448! 👍🏼  Will review today! Sorry for the delay!  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33736). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @ArthurZucker, any news for this PR review please? 🤗  @ArthurZucker Quick reminder about this review as it's been pending for a while and seems to be in a good state :) One more thing: for some reason, running `make fix-copies` seems to incorrectly modify the `index.md` file, assigning ❌ to ColPali for the ""PyTorch support"" column. Any idea how to fix this?

<img width=""2114"" alt=""Screenshot 2024-11-15 at 09 39 41"" src=""https://github.com/user-attachments/assets/8857cc07-c013-4288-a8c2-af78d58a346b"">
 Thanks for the review, I'll be working on addressing your comments! Hey @ArthurZucker!
This should be ready for another round of review :) (the failing test seems unrelated).
Major changes are 
- Addition of a `MODEL_FOR_RETRIEVAL_MAPPING` to Transformers, as this would be the first retrieval model in Transformers.
- Instead of inheriting from PaliGemma via modular, we instead have a` self.vlm = AutoModelForImageTextToText...` as you suggested, where the vlm is defined in the config. This greatly simplify the modeling code, which doesn't use modular anymore. Almost there :hugs: with the config update we should be good to merge > Mmm okay, but in that case let's not have a very specifc change. vlm_config is just for paligemma. We just need a change in the ColPaliGemmaConfig not in modeling utils 

Is the last thing to do @ArthurZucker Fixed the issue with `text_config` and the slow tests ran successfully 🤗Thanks so much for working on this! Excited to see ColPali in Transformers 🤗.
Main comment for now is to try and inherit directly from `PaliGemmaForConditionalGeneration` to make full use of modular and to avoid instantiating `PaliGemmaForConditionalGeneration` inside the model Looks like something is going wrong with the files generated by Modular, but otherwise looks good!
For the processor and modeling tests, you can take a look at other model tests to see how they should be implemented Thanks for working on this, apart from some nits and the `ProcessingKwargs` issue, it looks almost ready to go for me! Thanks for sharing this intersting work! 
I think we can just add 1 class, kind of what we have for Llava, which would use AutoModel! 🤗  A few nits, almost good to go!","Thanks so much for working on this! Excited to see ColPali in Transformers 🤗.
Main comment for now is to try and inherit directly from `PaliGemmaForConditionalGeneration` to make full use of modular and to avoid instantiating `PaliGemmaForConditionalGeneration` inside the model Looks like something is going wrong with the files generated by Modular, but otherwise looks good!
For the processor and modeling tests, you can take a look at other model tests to see how they should be implemented Thanks for working on this, apart from some nits and the `ProcessingKwargs` issue, it looks almost ready to go for me! Thanks for sharing this intersting work! 
I think we can just add 1 class, kind of what we have for Llava, which would use AutoModel! 🤗  A few nits, almost good to go!","# What does this PR do?

Add [ColPali](https://doi.org/10.48550/arXiv.2407.01449) support in 🤗 `transformers`.

## Who can review?

@yonigozlan 😉 
@ArthurZucker 

## Additional details

- This PR uses the new [Modular 🤗 transformers](https://huggingface.co/docs/transformers/main/en/modular_transformers#modular-transformers) feature from [v4.45.0](https://github.com/huggingface/transformers/releases/tag/v4.45.0)
- The ColPali is mainly inspired from the [colpali-engine](https://github.com/illuin-tech/colpali) repository I'm maintaining with my co-authors. The initial code was taken from `colpali-engine==v0.3.0`.
- To prevent adding the `colpali-engine` dependency, the weights are directly loaded from an exported `state_dict` stored in [`vidore/colpali-v1.2-merged-state_dict`](https://huggingface.co/vidore/colpali-v1.2-merged-state_dict).
- The newly converted model weights are stored in [`vidore/colpali-v1.2-hf`](https://huggingface.co/vidore/colpali-v1.2-hf).
- I have contributed a small dataset for integration testing for visual retrievers: [`vidore/document-visual-retrieval-test`](https://huggingface.co/datasets/vidore/document-visual-retrieval-test). I believe this should be moved to `hf-internal-testing/document-visual-retrieval-test`.
- I had a lot of trouble with the weight conversion part. Turns out there was a reproducibility issue depending on the version of torch. For reproducibility, I've added the freezed deps below in the PR.

## Progress checklist

## TODO

- [x]  (Optional) Understood the model’s theoretical aspects
- [x]  Prepared 🤗 Transformers dev environment
- [x]  Set up debugging environment of the original repository
- [x]  Created script that successfully runs the forward() pass using the original repository and checkpoint
- [x]  Successfully added the model skeleton to 🤗 Transformers
- [x]  Successfully converted original checkpoint to 🤗 Transformers checkpoint
- [x]  Successfully ran forward() pass in 🤗 Transformers that gives identical output to original checkpoint
- [x]  Finished model tests in 🤗 Transformers
- [x]  Successfully added tokenizer in 🤗 Transformers
- [x]  Run end-to-end integration tests
- [x]  Finished docs
- [x]  Uploaded model weights to the Hub
- [x]  Submitted the pull request
- [x]  (Optional) Added a demo notebook → can be found in https://github.com/tonywu71/colpali-cookbooks
- [x] Update tests if we decide to migrate the custom test dataset to `hf-internal-testing/document-visual-retrieval-test` (done → [[url]](https://huggingface.co/datasets/hf-internal-testing/document-visual-retrieval-test))
- [x] ~~Wait for https://github.com/huggingface/transformers/pull/34477 to be merged (fix for modular script)~~ https://github.com/huggingface/transformers/pull/34487 has been merged and fixes this problem","```suggestion
``` I think we should try this with 
```suggestion
class ColPaliForRetrieval(PaliGemmaForConditionalGeneration):
```
As we should be using as much inheritance as we can in the ""modular"" philosophy, and we want to avoid instantiating `PaliGemmaForConditionalGeneration` inside this model We want to avoid this This can be placed inside the config, and  maybe given a more descriptive name like `text_projection_dim`? This wouldn't need to be overridden if we go with `class ColPaliForRetrieval(PaliGemmaForConditionalGeneration):` Could you try overriding the `__call__` function of `PaliGemma` processor instead of creating these new functions?  Here you would have to copy the whole forward function of `PaliGemmaForConditionalGeneration` I suppose Is this a post processing function to get the retrieval scores? Maybe we could rename it to something like `post_process_retrieval` to follow the formatting for other downstream tasks like object detection The reason we chose this implementation was to have a clean `forward` method. I tried to have something elegant with your proposed solution: wdyt? (I've got to test the model but I think I have to convert the existing weights first). I'll hypothesize what you might have in mind: a `__call__` method with both `images` and `queries` as input args. Am I wrong? If not, can you give an example of another processor I could get my inspiration from please? 😁  Yep something like that! We are in the process of uniformizing processors for multimodal models, so that would include ColPali. you can take a look at the processor of [PaliGemma ](https://github.com/huggingface/transformers/blob/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src/transformers/models/paligemma/processing_paligemma.py) to have an idea of how the __call__ function and the handling of kwargs should look like!  Looks good to me if it works! If you rewrite the call function for ColPaliProcessor, you could allow it to be used without an image and wouldn't need this workaround, unless I missed something? Any reason the casting is needed here? ```suggestion
from transformers import ColPaliForRetrieval, ColPaliProcessor
``` Could you also define a ColpaliModelTester and ConfigTester and include them here? you can look at other test_modeling classes such as the paligemma one to see how it's done ```suggestion
from ..paligemma.modeling_paligemma import PaliGemmaForConditionalGeneration
from ..paligemma.configuration_paligemma import PaliGemmaConfig
from ..paligemma.processing_paligemma import PaliGemmaProcessor
```
Could you try this to see if it fixes the problems with modular? `ColPaliForRetrievalOutput` would make more sense here I think It's not needed, it just makes typing more explicit for my IDE. But have to admit that it's quite non-standard yet, so I've removed it! Surprisingly enough, running `python utils/modular_model_converter.py --files_to_parse src/transformers/models/colpali/modular_colpali.py` with your snippet yields the following error 😅 

```
ValueError: Unable to find dependencies for PaliGemmaForConditionalGeneration in transformers.models.paligemma.modeling_paligemma. Here are the dependencies found: {'ColPaliCausalLMOutputWithPast': {'ModelOutput'}, 'ColPaliMultiModalProjector': {'nn.Module'}, 'ColPaliPreTrainedModel': {'PreTrainedModel', 'COLPALI_START_DOCSTRING'}, 'ColPaliForConditionalGeneration': {'_prepare_4d_causal_attention_mask_with_cache_position', 'nn.Module', 'ColPaliPreTrainedModel', 'ColPaliCausalLMOutputWithPast', 'COLPALI_START_DOCSTRING', 'GenerationMixin', 'ModelOutput', '_CONFIG_FOR_DOC', 'ColPaliMultiModalProjector', 'logger', 'ColPaliPreTrainedModel, ', 'PreTrainedModel', 'COLPALI_INPUTS_DOCSTRING'}}. (The automatic renaming might have gone wrong!)
``` Very surprising as they are the same imports After taking a good look at the PaliGemmaProcessor, I'd say it's feasible! I'll give it a try :) Solved thanks to https://github.com/huggingface/transformers/pull/34077 👌🏼  @yonigozlan TODO: As discussed, this code is still not updated properly by the convert modular script. I think it would be nice to make those variable names more explicit Where are these functions used? Might be nice to have a fine-grained check, with a hardcoded score from the original implementation for example Same here for variable names Not sure an assert makes sense here, maybe we could raise an error instead? Or is this check necessary at all? This is quite a high tolerance 😅. Aren't the MAEs zero?"
35272,2024-12-14T11:29:00Z,2024-12-16T17:23:34Z,zhanluxianshen,2,0,1,8,1,1,1,[],180282.0,0,211716.0,0,0,0,0,856385.868094,,0,1,0,False,"['Rocketknight1', 'zhanluxianshen']",cc @stevhliu  Thanks.Thanks for the fix!,Thanks for the fix!,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35271,2024-12-14T11:25:02Z,2024-12-16T17:22:35Z,HMJ0628,3,0,1,70,2,1,1,[],93.0,0,194253.0,0,0,0,0,874087.276551,,0,1,0,False,"['HMJ0628', 'Rocketknight1', 'asdkfjsd']","@asdkfjsd Please help me review it.Thank you! The translation is pretty good!I can understand it easily! cc @stevhliu Great, thanks for the translation!","Great, thanks for the translation!","# What does this PR do?
I have translated translate perf_infer_gpu_multi.md to Chinese. Please let me know if there are any corrections. Thank you very much!
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

",
35287,2024-12-16T05:33:13Z,2024-12-16T16:51:32Z,jla524,0,0,7,4,2,1,1,[],,0,40699.0,0,0,0,0,875951.158186,,0,7,0,False,[],"Good catch, thanks for fixing!","Good catch, thanks for fixing!","# What does this PR do?

Fix typos in translated docs as a follow up on https://github.com/huggingface/transformers/pull/35263

## Who can review?

@stevhliu
",
35295,2024-12-16T15:23:42Z,2024-12-16T15:52:48Z,eustlb,1,0,2,17,1,1,1,[],2138.0,0,2139.0,0,0,0,0,879082.903013,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35295). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thank you including in the patch!,Thank you including in the patch!,"# What does this PR do?

Fixes https://github.com/Vaibhavs10/insanely-fast-whisper/issues/255

https://github.com/huggingface/transformers/pull/34537 introduced casting to float64, which is not supported on the MPS backend.",
35293,2024-12-16T12:14:38Z,2024-12-16T13:18:52Z,ivarflakstad,1,0,2,108,3,1,1,[],1708.0,0,3856.0,0,0,0,0,888710.229065,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35293). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.OK, it's very nice not to be `pushy` 😆 ","OK, it's very nice not to be `pushy` 😆 ","# What does this PR do?
Temporarily disable the AMD push CI to reduce noise when debugging",
34724,2024-11-14T01:53:08Z,2024-11-19T11:30:45Z,li-plus,9,0,3,26,2,2,2,"['Multimodal', 'run-slow']",381706.0,0,2805242.0,0,0,0,0,889415.317634,,0,3,0,False,"['li-plus', 'qubvel', 'ShuaibinQi', 'HuggingFaceDocBuilderDev']","Hi @li-plus! Thanks for your contribution! 

Can you please enable gradient checkpointing tests for this model to make sure it works properly? I see [these](https://github.com/huggingface/transformers/blob/main/tests/models/qwen2_vl/test_modeling_qwen2_vl.py#L288-L304) tests are skipped on `main`, however, I was able to run them without issues locally. > Hi @li-plus! Thanks for your contribution!
> 
> Can you please enable gradient checkpointing tests for this model to make sure it works properly? I see [these](https://github.com/huggingface/transformers/blob/main/tests/models/qwen2_vl/test_modeling_qwen2_vl.py#L288-L304) tests are skipped on `main`, however, I was able to run them without issues locally.

@qubvel Thanks for advice. I've re-enable these gradient checkpointing tests for Qwen2VL in the latest commit. They run just fine on my machine. Thanks! Can you please also push an empty commit with the message `[run-slow] qwen2_vl` to trigger all model tests? We should be fine here, cause `test_training_gradient_checkpointing` is not a `slow` test, however just to double check everything else is fine 🙂  Thanks. Just pushed! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34724). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @qubvel It seems those failures are not related to this PR. Any idea? @li-plus 

Thanks your commit!
when I use gradient_checkpointing for Qwen2VisionTransformerPretrainedModel, I meet this Error:


AttributeError: 'Qwen2VisionTransformerPretrainedModel' object has no attribute '_gradient_checkpointing_func'. Did you mean: 'gradient_checkpointing'?


Is it necessary to implement this '_gradient_checkpointing_func' in  'Qwen2VisionTransformerPretrainedModel' ? > @li-plus
> 
> Thanks your commit! when I use gradient_checkpointing for Qwen2VisionTransformerPretrainedModel, I meet this Error:
> 
> AttributeError: 'Qwen2VisionTransformerPretrainedModel' object has no attribute '_gradient_checkpointing_func'. Did you mean: 'gradient_checkpointing'?
> 
> Is it necessary to implement this '_gradient_checkpointing_func' in 'Qwen2VisionTransformerPretrainedModel' ?

My  transfromers version is latest, and it is:
transformers ==4.47.0 @ShuaibinQi Hi, I did not reproduce this error using transformers 4.47.0 using this demo code:

```python
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info


processor = AutoProcessor.from_pretrained(""Qwen/Qwen2-VL-7B-Instruct"")

model = Qwen2VLForConditionalGeneration.from_pretrained(
    ""Qwen/Qwen2-VL-7B-Instruct"",
    torch_dtype=torch.bfloat16,
    attn_implementation=""flash_attention_2"",
    device_map=""cuda"",
)
model.gradient_checkpointing_enable()
model.train()

messages = [
    {
        ""role"": ""user"",
        ""content"": [
            {
                ""type"": ""image"",
                ""image"": ""https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"",
            },
            {""type"": ""text"", ""text"": ""Describe this image.""},
        ],
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors=""pt"",
)
inputs = inputs.to(""cuda"")

print(f'before forward {torch.cuda.memory_allocated()/1e9=:.3f} GB, {torch.cuda.memory_reserved()/1e9=:.3f} GB')
output = model(**inputs, use_cache=False)
print(f'after forward {torch.cuda.memory_allocated()/1e9=:.3f} GB, {torch.cuda.memory_reserved()/1e9=:.3f} GB')
output.logits.sum().backward()
print(f'after backward {torch.cuda.memory_allocated()/1e9=:.3f} GB, {torch.cuda.memory_reserved()/1e9=:.3f} GB')
```

Did you use `gradient_checkpointing_enable` to enable gradient checkpointing?
No worries, I checked, the same tests fail on main. Thanks for triggering slow tests!  Thanks indeed there is `supports_gradient_checkpointing`set to `True` good catch","No worries, I checked, the same tests fail on main. Thanks for triggering slow tests!  Thanks indeed there is `supports_gradient_checkpointing`set to `True` good catch","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Support gradient checkpointing for Qwen2VL ViT part. The current implementation in main branch only supports gradient checkpointing in language part. This PR further supports checkpointing vision encoder.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

cc @ArthurZucker, @amyeroberts, @qubvel

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35083,2024-12-04T11:18:32Z,2024-12-16T12:21:44Z,MekkCyber,1,0,1,9,1,1,1,[],1575.0,0,1040595.0,0,0,0,0,892138.816554,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35083). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice thanks for uploading the right model ! cc @Isotr0py ,Nice thanks for uploading the right model ! cc @Isotr0py ,"# What does this PR do?
The model used to test the ggml conversion of `falcon-7b` in `fp16` format is wrong : 

<img width=""1259"" alt=""image"" src=""https://github.com/user-attachments/assets/47ee7ff2-e8dd-40d5-83ba-6e056810f371"">
You can see that it contains some Q4 weights which is unexpected in a `fp16` model, and its size is only 4GB but it should be around 7x2 = 14GB. 
I did my own model conversion to gguf to fix the issue : 

<img width=""1261"" alt=""image"" src=""https://github.com/user-attachments/assets/5fcccb76-e8ae-4678-8ced-326b7e299d65"">

## Who can review ?
@SunMarc ",
35239,2024-12-12T15:08:05Z,2024-12-16T11:44:33Z,zucchini-nlp,1,0,3,7,2,1,1,[],1689.0,0,333388.0,0,0,0,0,894373.145036,,1,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35239). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for fixing this ! LGTM ! ,Thanks for fixing this ! LGTM ! ,"# What does this PR do?

As per title, after adding `no_split_modules` some tests started failing (those that were prev skipped). This PR fixes them",
34989,2024-11-28T10:41:37Z,2024-12-16T10:06:17Z,ydshieh,3,8,87,115,3,2,1,[],1656.0,0,1553082.0,0,0,0,0,900268.844705,,0,87,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker', 'ydshieh']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34989). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Let's fix the test to merge! will merge on Monday instead :-) Thanks for approvalVery nice work! A few suggestions  Thanks!,Very nice work! A few suggestions  Thanks!,"# What does this PR do?

See the new `collection_job` outputs in the artifacts tab.

This could ease the searching if a test is run or not, run in which jobs, their status in each job etc.","In circle-ci can't we just use something like `with` or require? It will automatically wait for the `run_test` to run, and then fetch results? 
(It could even be a github action no?) I think using the junit.xlm will be more ""foolproof"" than regex parsing no?  Job 2 `require` Job 1: if job 1 failed, job 2 won't be triggered (sad 😢 ). `when: always` can't apply for jobs in a workflow (only steps in a job) for CircleCI.


That is why such wait is implemented (although I don't like it). There is no regex in this script.

Regarding `junit.xlm`, yes probably more `foolproof`, but more post-process to be done I think:

- `xml`'s structure is more complex then `summary_short.txt`.
- it doesn't give something like `tests/generation/test_utils.py::GenerationIntegrationTests::test_generated_length_assisted_generation` that we could copy-paste and run directly.

`summary_short.txt` is working well in a very simple way. If we find `xlm` is necessary in the future, happy to make the change however.
 OK then you can merge but I would like for this test to not block merge  Ok sounds good!
 You mean if this new job fails, it shouldn't block the merge?

I will see if I can configure this 🤔  I am just doing 

https://github.com/huggingface/transformers/pull/34989/commits/d14de3db66b6b8c154cd0d5c5a109f1af394ce17

to make sure the `run` steps will always success. "
34785,2024-11-18T16:49:44Z,2024-12-15T19:00:36Z,yonigozlan,8,9,7,109,8,4,1,"['Vision', 'Processing']",1684.0,0,2340652.0,0,0,0,0,954612.170513,,1,7,0,False,"['qubvel', 'yonigozlan', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34785). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @yonigozlan! Thanks for opening a PR! Please check [this comment](https://github.com/huggingface/transformers/pull/31424#discussion_r1651356882) regarding enabling `use_fast=True` by default. The main concern was that fast and slow image processors might be not equivalent, so instead of forcing fast image processors we notify users that fast image processor is available. WDYT? Oh, I hadn't seen this, thanks for pointing it out @qubvel!
Indeed there are tiny differences between fast and slow image processors outputs, which could be confusing for users as it will result in minor differences in model outputs (not necessarily worse though). My main concern is that most users will ignore or won't notice the warning as nothing is going clearly wrong during inference, but they will miss out on using fast processors with our current vision models.  Since slow processors are a major speed bottleneck for many of them, this seems like a missed opportunity. Also it looks like there are currently no way to use fast processors in pipelines, whereas this PR would enable their used there too (I still have to check this).

I guess it comes down to prioritizing backward compatibility or inference speed.  Maybe we could add a `warning_once` in this PR, stating that while a slow processor was saved, Transformers v4.x automatically loads fast processors, so users might see minor output differences, and insist on the gains in inference speed.  Something like: 

```
A slow processor was saved with this model. However, since Transformers v4.x, we automatically load fast processors for improved inference speed. 
This may result in minor differences in outputs. To force the use of a slow processor, set `use_fast=False`.
```

This way, users are aware of the potential difference but also understand the benefit, and they can still choose to use the slow processor if needed.

Happy to discuss all of this @qubvel @ydshieh @molbap. @qubvel Agreed it might be better to do a progressive rollout and add more base tests in the meantime. I can change this PR to do that and also keep some of the refactoring of `image_processing_auto` in this PR as I feel there was some bits of confusing code (especially regarding not separating `image_processor_type` and `image_processor_class`).
I can also add support for fast image processors to output other format than torch tensors, unless there is any objection against that. > I can also add support for fast image processors to output other format than torch tensors, unless there is any objection against that.

Considering the usage of TF/Flax, I am not sure if it is worth the effort (if it will require more than expected effort). You can talk to core maintainers about this part. > @qubvel Agreed it might be better to do a progressive rollout and add more base tests in the meantime. I can change this PR to do that and also keep some of the refactoring of image_processing_auto in this PR as I feel there was some bits of confusing code (especially regarding not separating image_processor_type and image_processor_class).

@yonigozlan sounds great! > Inform users well ahead of time about the upcoming behavior change. For instance, announce that Fast image processors will become the default in 3–5 releases. Users who wish to retain the current behavior should explicitly set use_fast=False in their code.

I think we should start rolling this out (the warnings) and reduce to 2 releases for example (2 months) giving us time to tests! 

Absolutely no need to work on TF / Jax versions for now either! ➕  @ArthurZucker Changed the scope of this PR to fix the error when use_fast is set to True and no fast image processor is available. Also added warnings to start rolling out fast image processors by default.Thanks!

Have some comments as a first review. Fast image processing is indeed a compelling feature that could benefit many users. It looks similar to setting the default to `SDPA` over `eager` attention. While I haven’t noticed complaints about making SDPA the default, there were issues with CI breaking in different projects after SDPA was added to popular models like CLIP and Siglip.

For users experiencing slow processing speeds, exploring the option to set `use_fast=True` might be worthwhile. However, for those not facing performance challenges, it may not be necessary.

I don’t have strong objections to this feature, but I suggest the following approach to implement it effectively:

1. Add tests for thorough comparison between Slow and Fast image processors to ensure nothing breaks, especially for custom processors with non-default parameters. Tests should cover all Slow and Fast image processors (maybe a common tests?).

2. Inform users well ahead of time about the upcoming behavior change. For instance,  announce that Fast image processors will become the default in 3–5 releases. Users who wish to retain the current behavior should explicitly set `use_fast=False` in their code.

This gradual rollout can minimize potential issues while allowing users time to adapt. What do you think? Cool, yep let's make sure 4.48 has fast by default!","Thanks!

Have some comments as a first review. Fast image processing is indeed a compelling feature that could benefit many users. It looks similar to setting the default to `SDPA` over `eager` attention. While I haven’t noticed complaints about making SDPA the default, there were issues with CI breaking in different projects after SDPA was added to popular models like CLIP and Siglip.

For users experiencing slow processing speeds, exploring the option to set `use_fast=True` might be worthwhile. However, for those not facing performance challenges, it may not be necessary.

I don’t have strong objections to this feature, but I suggest the following approach to implement it effectively:

1. Add tests for thorough comparison between Slow and Fast image processors to ensure nothing breaks, especially for custom processors with non-default parameters. Tests should cover all Slow and Fast image processors (maybe a common tests?).

2. Inform users well ahead of time about the upcoming behavior change. For instance,  announce that Fast image processors will become the default in 3–5 releases. Users who wish to retain the current behavior should explicitly set `use_fast=False` in their code.

This gradual rollout can minimize potential issues while allowing users time to adapt. What do you think? Cool, yep let's make sure 4.48 has fast by default!","# What does this PR do?

Refactor parts of image_processing_auto to fall back on slow processor when use_fast is set to True and no fast processor is available.

Before, this would throw an error:
```python
processor = AutoImageProcessor.from_pretrained(""Salesforce/blip-image-captioning-large"", use_fast=True)
```
Now the following warning is displayed
```
`use_fast` is set to `True` but the image processor class does not have a fast version. Falling back to the slow version.
```
Also add warnings to start rolling out fast image processor by default (goal is v4.48). If use_fast is not set, and the checkpoint was saved with a slow processor, display:
```
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
```
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->



## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","This warning will be triggered a lot in the default behavior, as for now most image processors don't have a fast version. Happy to remove this if we think it's obvious to automatically fall back to the slow image processor when there is no fast one Otherwise, the fast vit image processor will crash in the default behavior (when `return_tensors` is not specified). This is now a bigger problem with fast image processors used by default. It's `warning_once` so should be fiine, right? Is this kind of breaking change?

(I am also curious what if a user is using TF/Flax model while their environment has torch/torchvision installed. Is the fast image processor will be used by default and will return torch tensor by default ..?) I guess this is a bug and here you fix it right? Oh I see, yes this might be a problem. In general, I'm not too sure why it was decided to constrain fast image processors to output only torch tensors. Would be glad to know if there was a reason for that, otherwise it might be something we would want to reconsider. Yes it was, sorry for the lack of explanation. It's actually now fixed by another PR this can be simplified a bit to check first if `image_processor_type + ""Fast""` is in the mapping, if yes we take, if no we don't. only need to call `get_image_processor_class_from_name` once  TBH I am not even sure TF is used for image models! Fine by me!"
34163,2024-10-14T17:12:24Z,2024-10-24T09:47:58Z,blueingman,6,9,5,106,2,2,2,[],56857.0,0,5339574.0,0,0,0,0,978330.633037,,0,5,0,False,"['LysandreJik', 'blueingman', 'Isotr0py']","Nice! Maybe @Isotr0py would you be willing to review this PR? :) OK, I will take a look tonight. :) I have updated the PR accordingly.Thank you very much for detailed guidance,and I 'll pay more attention to the details next time. @Isotr0py @LysandreJik  Thank you again for your careful suggestion! Hi @Isotr0py ,hope you're doing well. I have addressed the changes you suggested and updated the PR accordingly. When you have a moment, could you please review the updated code? Thank you for your time! The updated docs LGTM. :)

Also cc @LysandreJikThanks for your contributions! The translation overall LGTM. Just leave some nits to make sentence more coherent. Just a minor mistake. Otherwise LGTM. Thanks a lot!",Thanks for your contributions! The translation overall LGTM. Just leave some nits to make sentence more coherent. Just a minor mistake. Otherwise LGTM. Thanks a lot!,"# What does this PR do?
I am new to the open-source community, but I’m very eager to contribute to the Transformers project. I have translated the gguf.md file into Chinese. If there are any areas that need improvement or if I overlooked anything, please let me know


## Before submitting
- [y] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [y] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.


","```suggestion
    title: 与 GGUF 格式共享模型
``` ```suggestion
## 在 Transformers 中的支持
``` ```suggestion
根据分享在 Hub 上的较为热门的量化文件，初步支持以下量化类型：
``` ```suggestion
为了在`transformers`中加载`gguf`文件，你需要在 `from_pretrained`方法中为分词器和模型指定 `gguf_file`参数。下面是从同一个文件中加载分词器和模型的示例：
``` ```suggestion
目前支持以下在 Hub 上非常热门的模型架构：
``` ```suggestion
现在，你就已经可以结合 PyTorch 生态系统中的一系列其他工具，来使用完整的、未量化的模型了。
``` ```suggestion
该文件格式是一种“单文件格式”，通常单个文件就包含了配置属性、分词器词汇表和其他属性，同时还有模型中要加载的所有张量。这些文件根据文件的量化类型有不同的格式。我们在[这里](https://huggingface.co/docs/hub/en/gguf#quantization-types)进行了简要介绍。
``` ```suggestion
    title: 与 GGUF 格式的互操作性
```
The ""Interoperability"" should be translated to ""互操作性"" correctly, since it's a computer terminology. ```suggestion
为了将模型转换回`gguf`文件，我们建议使用`llama.cpp`中的[`convert-hf-to-gguf.py`文件](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py)。
```"
34936,2024-11-26T12:42:43Z,2024-11-26T18:09:52Z,blueingman,1,0,4,57,2,2,2,[],18264.0,0,1640554.0,0,0,0,0,978332.802396,,0,4,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34936). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice, this translation LGTM! Nice work, thanks!","Nice, this translation LGTM! Nice work, thanks!","# What does this PR do?

translated tiktoken.md into Chinese


Part of https://github.com/huggingface/transformers/issues/26803


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@stevhliu @Isotr0py  Would you be kind enough to review the PR when you have time? Any feedback or suggestions you have would be greatly appreciated.

",
34948,2024-11-27T02:50:05Z,2024-11-27T15:57:43Z,blueingman,2,2,9,60,2,2,2,[],30900.0,0,1589708.0,0,0,0,0,978337.292487,,0,9,0,False,"['HuggingFaceDocBuilderDev', 'blueingman']","> Overall looks good, just some nits.

Thank you for your careful suggestions! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34948). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Overall looks good, just some nits. LGTM, thanks for the translation!","Overall looks good, just some nits. LGTM, thanks for the translation!","# What does this PR do?
translated perf_train_special.md into Chinese

Part of https://github.com/huggingface/transformers/issues/26803


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.


@stevhliu @Isotr0py Would you mind taking a look when you have time?Thank you so much for your help again!


","```suggestion
之前，在 Mac 上训练模型仅限于使用 CPU 训练。不过随着PyTorch v1.12的发布，您可以通过在 Apple Silicon 芯片的 GPU 上训练模型来显著提高性能和训练速度。这是通过将 Apple 的 Metal 性能着色器 (Metal Performance Shaders, MPS) 作为后端集成到PyTorch中实现的。[MPS后端](https://pytorch.org/docs/stable/notes/mps.html) 将 PyTorch 操作视为自定义的 Metal 着色器来实现，并将对应模块部署到`mps`设备上。
```
We have better use ""Apple Silicon 芯片""(`Apple Silicon chip`) instead of ""Apple 芯片""(`Apple chip`) if Mac with x86_64 Intel CPU doesn't support mps. (Please correct me if I am wrong, because I don't have Mac) :) ```suggestion
# 在 Apple Silicon 芯片上进行 PyTorch 训练
```"
33200,2024-08-29T15:06:24Z,2024-10-29T09:36:04Z,sbucaille,7,30,32,170,5,7,1,"['bug', 'Vision']",621839.0,0,9321025.0,0,0,0,0,978842.671875,,0,32,0,False,"['HuggingFaceDocBuilderDev', 'qubvel', 'onuralpszr', 'sbucaille']","@amyeroberts comments addressed, tests are passing, let me know if I can do a final rebase for the merge The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33200). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @sbucaille, thanks for working on this and addressing comments, I will review this week! @qubvel thanks for the review, I've addressed your comments. I initially forgot to change the tests to reflect the changes in terms of SuperPoint's outputs, so this is also fixed. Let me know if there is anything else to do Hello, I was following this PR and curious that is it going to merge or any change still require ?  Hi @onuralpszr ,
Thanks for asking, there is no change required on my side ! @sbucaille thanks a lot for your contribution!Thanks for fixing and adding this method! 

Overall looks good - just a few things to add / update Thanks for iterating! 

Mostly small comments. Main one is about the indexing on `image_sizes` in the post-processing method  Thanks for working on this! Just some minor comments Thanks quick iteration, a few more comments Thank you for the fixes! Test diff looks fine now 👍  Thanks, looks good to me now!

cc @ArthurZucker for final review Feel free to merge if ready for you @qubvel 🤗 ","Thanks for fixing and adding this method! 

Overall looks good - just a few things to add / update Thanks for iterating! 

Mostly small comments. Main one is about the indexing on `image_sizes` in the post-processing method  Thanks for working on this! Just some minor comments Thanks quick iteration, a few more comments Thank you for the fixes! Test diff looks fine now 👍  Thanks, looks good to me now!

cc @ArthurZucker for final review Feel free to merge if ready for you @qubvel 🤗 ","# What does this PR do?

There is a bug highlighted by @merveenoyan where SuperPoint returns absolute keypoint coordinates at a wrong scale.
For example, an image of size (1000, 1000) will first be resized into (640, 480) by the image processor but SuperPoint will output keypoint coordinates between 0 and 640 for x and 0 and 480 for y.
This PR fixes this bug by returning keypoints in relative coordinates out of the SuperPoint forward method.
An additional ``post_process_keypoint_detection`` is added to the ``SuperPointImageProcessor`` to cast the keypoint coordinates into the original image sizes.
I also added an ""unwrapping"" logic where the output will be a list of ``keypoints, scores, descriptors`` for each image so that the user don't have to deal with the current mask mechanism. This is conditionned by a boolean argument in the signature.

I have not finished all the details but most of the implementation is there and I will add tests tomorrow but at least the PR is open as I promised Merve ☺️

Fixes #33825


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request), Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. 
-> On Slack
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?

## Who can review?

@amyeroberts @NielsRogge @merveenoyan ","Could you add type hinting and a docstring for this please?  We should add this as a documented method for the model's doc page e.g. [like here ](https://github.com/huggingface/transformers/blob/c409cd81777fb27aadc043ed3d8339dbc020fb3b/docs/source/en/model_doc/deformable_detr.md?plain=1#L61) We shouldn't be modifying inputs - this is a side-effect and can cause surprising to users  it's more pythonic to iterate over objects directly 

```suggestion
        for (image_mask, keypoints, scores, descriptors) in zip(outputs.mask, outputs.keypoints, outputs.scores, outputs.descriptors):
            indices = torch.nonzero(image_mask).squeeze(1)
            keypoints = keypoints[indices]
            scores = scores[indices]
            descriptors = descriptors[indices]
            results.append({""keypoints"": keypoints, ""scores"": scores, ""descriptors"": descriptors})
``` Technically a breaking change - but as this is more of a fix, I think it's OK  ```suggestion
You can then print the keypoints on the image of your choice to visualize the result:
``` ```suggestion
import matplotlib.pyplot as plt

plt.axis(""off"")
``` super small nit: I think we should also give an option to have this as a list of tuples to save users of the of converting it to tensor every time and check inside ourselves whether it's a tensor or not You are right, I also thought about it in the first place. I've added this option it should be either `image.shape[1]` or `image.size[1]` I think and not directly image itself

it would've been directly to have `image.size` from convenience standpoint IMHO i.e. each tuple would've been `image.size` and not indexed Good catch, it's a typo from a wrong copy paste, should be fixed now. Also, the fact that it is `tuple(image.size[1], image.size[0])` is because we require `(h,w)` in input (I guess it's standard from what I've seen in other post process methods in the repo) but `image.size` returns `(w, h)` I think? Otherwise we're just taking the first and second elements of the batch. 

If I'm right - I'm surprised this wasn't caught. Has this been tested on batch inputs? 

```suggestion
            keypoints[:, 0] = keypoints[:, 0] * image_size[:, 1]
            keypoints[:, 1] = keypoints[:, 1] * image_size[:, 0]
``` it's better to assert one then another separately. Otherwise, if the test fails you have to re-run with a breakpoint to figure out what's happened  Why mark as slow? It should be pretty fast  It would be nice to display the output image in the docs :)  I don't think there is a problem with this part. After checking, `image_size` is indeed a `(2,)` tensor, `keypoints` a `(n, 2)`, so `keypoints[:, 0] = keypoints [:, 0] * image_size[1]` multiplies all elements x elements from `keypoints` with the scalar from `image_size[1]` Fixed This is a leftover from when I used SuperPoint model to produce the keypoint detection output. I removed it Image added ! ```suggestion
image_sizes = [(image.height, image.width) for image in images]
``` Just wondering why are we iterating here. Let's maybe add print + output. In case it is used only for proper typehints
```python
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from .modeling_superpoint import SuperPointKeypointDescriptionOutput
``` Agreed with the current implementation, we are iterating over images, so image size is already for a single image Do we have somewhere description of the keypoints output format? I mean it would be great to have it in docs and code snippets, and maybe mention it in `post_process_keypoint_detection` method  The format is documented in `SuperPointKeypointDescriptionOutput` docstring, do you think I should still add this information somewhere else either way ? Ah yes you are right, it is indeed only used for typing, changed it I've tweaked the docs a bit to add printing, at first I didn't really know what to do with this part because the most interesting one is below 😄  Please add output type hint It looks like it's not `optional` based on the code below Can we make it without a loop + add a comment? Smth like:
```python
# keypoints are provided in (x, y) format, while image size is in (height, width)
masked_keypoints[:, 0] = masked_keypoints[:, :, 0] * image_size[:, 1].unsqueeze(-1)
masked_keypoints[:, 1] = masked_keypoints[:, :, 1] * image_size[:, 0].unsqueeze(-1)
```"
29967,2024-03-30T17:16:51Z,2024-05-01T15:58:01Z,NielsRogge,5,8,2,23,1,3,2,[],1163.0,0,22445361.0,0,0,0,0,979480.063776,,0,2,0,False,"['NielsRogge', 'qubvel', 'HuggingFaceDocBuilderDev', 'amyeroberts']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_29967). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @amyeroberts could we already merge this PR which addresses the first 3 points of https://github.com/huggingface/transformers/issues/29964? @NielsRogge Did you run and check re 100 epochs? https://github.com/huggingface/transformers/pull/29967#discussion_r1551806857 If not, then we should revert back until confirmed  With 100 epochs it converges better but still has pretty low metrics. Also, predictions are bad because we use it for inference with batch size = 1.
We can align it with https://github.com/huggingface/transformers/pull/30422 when it's merged to get better metrics. @amyeroberts people have reported to replicate it with 100 epochs here: https://github.com/huggingface/transformers/issues/30557#issuecomment-2086401493

Feel free to approve the PR in order to clarify this for now in the docs. Later on we could improve metrics moreThanks for working on this! 

Just some small nits & and a q about timm  Thanks for improving our task guides! ","Thanks for working on this! 

Just some small nits & and a q about timm  Thanks for improving our task guides! ","# What does this PR do?

This PR fixes some issues mentioned in #29964 ","I wouldn't add a specific device like this here - people will want to be able to copy/paste quickly without having to edit all the cuda references. I'd instead define `torch_device`, which is dependant on the device an available and then use that in all the `to` calls e.g. `.to(torch_device) Same here re device  Why remove the timm requirement? It seems fine to have if we're also requiring albumentations  The DETR models support the ""no_timm"" revision, which has been asked by several users. It allows to run the models with the AutoBackbone class rather than the Timm backbone. I understand the need for this in general, but I'm just wondering why change for this example given we already depend on third party libraries?  Simply because I prefer simplicity, so if users can run this tutorial without one additional dependency, I guess that's great for them Did you manage to investigate this? 100 epochs is very large. If we train for this many steps do we still see a change in metrics over the first 10 epochs? cc @qubvel perhaps this would be great to investigate as part of making it easier to train object detection models"
35009,2024-11-28T21:51:37Z,2024-12-12T14:47:05Z,keyboardAnt,12,3,14,239,2,4,2,[],39283.0,0,1381751.0,0,0,0,0,1031404.698072,,0,14,0,False,"['jmamou', 'keyboardAnt', 'HuggingFaceDocBuilderDev', 'danielkorat', 'ArthurZucker', 'zucchini-nlp']","@zucchini-nlp feel free to review it :-) The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35009). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Thanks @keyboardAnt ! LGTM as nothing was changed except for composing code into smaller functions.
> 
> The only question is the use of `self.prev_target_ids` which I've mentioned was removed in prev PR explaining it is not needed. This is what I got from reviewing the prev PR
> 
> > hmm, so to make sure, that means the prev impl when we checked prev_target_ids was not really correct? And we should check the length of already accepted input_ids
> > > yes
> 
> So do we need to save prev token ids from target model or we can re-use the current token ids, because the current token ids in any case will have the prev token ids as prefix with new accepted tokens appended at the end

For UAG, we need just the number of tokens in `self.prev_target_ids` (`prev_target_ids.shape[1]`) in order to know which target tokens were added after the target validation of the last speculative iteration and to convert them to the relevant draft tokens (instead of converting from the beginning). 
I propose to use similar approach as in [USD](https://github.com/huggingface/transformers/pull/35029) when we store only the previous number of target tokens _prev_target_seq_len. > we need just the number of tokens in self.prev_target_ids (prev_target_ids.shape[1]

Ah that makes sense if we don't care about the actual token ids used previously, because the tokens should be available without storing them. Then we can indeed store only the prev length > > we need just the number of tokens in self.prev_target_ids (prev_target_ids.shape[1]
> 
> Ah that makes sense if we don't care about the actual token ids used previously, because the tokens should be available without storing them. Then we can indeed store only the prev length

Thanks, @zucchini-nlp. I've replaced `self.prev_target_ids` with `self.prev_target_ids_len` and rebased `main` to keep the history clean. Can you please approve the awaiting workflows?

![image](https://github.com/user-attachments/assets/b6181978-089a-4590-bdea-e7f64eeb9465) > @keyboardAnt please apply the same fix as at [keyboardAnt#4](https://github.com/keyboardAnt/transformers/pull/4). else SD will not work when target and assistant are not on the same device

@keyboardAnt 
PR https://github.com/huggingface/transformers/pull/35116 fixes multi-gpu issue You can now request review from the core maintainer to merge this :) > You can now request review from the core maintainer to merge this :)

@ArthurZucker  Ah can you resolve conflicts? 🤗  > Ah can you resolve conflicts? 🤗

@ArthurZucker done! @ArthurZucker @zucchini-nlp 
conflicts were resolved 😃  ready for merge thanks, mergingThanks a lot for making the code more composable! LGTM as nothing changed in terms of functionality. Just left one comment as we had several PRs in parallel modifying the assisted generation code :) Thanks @keyboardAnt ! LGTM as nothing was changed except for composing code into smaller functions. 

The only question is the use of `self.prev_target_ids` which I've mentioned was removed in prev PR explaining it is not needed. This is what I got from reviewing the prev PR

> hmm, so to make sure, that means the prev impl when we checked prev_target_ids was not really correct? And we should check the length of already accepted input_ids
> > yes

So do we need to save prev token ids from target model or we can re-use the current token ids, because the current token ids in any case will have the prev token ids as prefix with new accepted tokens appended at the end  @keyboardAnt 
please apply the same fix as at https://github.com/keyboardAnt/transformers/pull/4.
else SD will not work when target and assistant are not on the same device Looks great! Thanks for the detailed explanations 🤗 
Merging!","Thanks a lot for making the code more composable! LGTM as nothing changed in terms of functionality. Just left one comment as we had several PRs in parallel modifying the assisted generation code :) Thanks @keyboardAnt ! LGTM as nothing was changed except for composing code into smaller functions. 

The only question is the use of `self.prev_target_ids` which I've mentioned was removed in prev PR explaining it is not needed. This is what I got from reviewing the prev PR

> hmm, so to make sure, that means the prev impl when we checked prev_target_ids was not really correct? And we should check the length of already accepted input_ids
> > yes

So do we need to save prev token ids from target model or we can re-use the current token ids, because the current token ids in any case will have the prev token ids as prefix with new accepted tokens appended at the end  @keyboardAnt 
please apply the same fix as at https://github.com/keyboardAnt/transformers/pull/4.
else SD will not work when target and assistant are not on the same device Looks great! Thanks for the detailed explanations 🤗 
Merging!","## What does this PR do?

This PR refactors the `AssistedCandidateGenerator` and `AssistedCandidateGeneratorDifferentTokenizers` classes to improve code readability, maintainability, and reusability. By breaking down large methods into smaller helper functions and reusing common logic, we enhance the modularity of these classes. This refactoring lays a cleaner foundation for upcoming features like `UniversalSpeculativeDecodingGenerator` without introducing any new functionality.

## Background

While working on Universal Speculative Decoding (https://github.com/huggingface/transformers/pull/34760), which introduces the `UniversalSpeculativeDecodingGenerator`, we identified opportunities to refactor existing code. The goal is to reuse core logic across different candidate generators and simplify the integration of new features that enable speculative decoding across models with different tokenizers.

By submitting this refactoring as a separate PR, we aim to:

- **Streamline the review process**: Focus on structural improvements without the complexity of new features.
- **Accelerate availability**: Make these improvements accessible to the community sooner.
- **Facilitate future development**: Provide a robust base for upcoming enhancements in the generation capabilities.

This refactor is a collaboration with @jmamou, who has already reviewed it (https://github.com/keyboardAnt/transformers/pull/1).

## Key Changes

### 1. Code Restructuring

- **Decomposed Large Methods**: Broke down the `get_candidates` methods in both classes into smaller, focused helper functions.
  - Example helper functions:
    - `_calculate_new_tokens`
    - `_update_past_and_masks`
    - `_prepare_generation_args`
    - `_generate_candidates`
- **Simplified Initialization**: Streamlined the `__init__` methods to remove redundancy and enhance clarity.

### 2. Improved Reusability

- **Modularized Operations**: Encapsulated unique functionalities in dedicated methods within each class.
  - For `AssistedCandidateGeneratorDifferentTokenizers`, methods like `_prepare_assistant_input_ids` and `_process_assistant_outputs` handle tokenizer-specific logic.
- **Reused Common Logic**: Leveraged inheritance and method overriding to share code between classes and reduce duplication.

### 3. Enhanced Readability

- **Clear Naming Conventions**: Renamed variables and methods for better clarity and understanding.
- **Consistent Formatting**: Applied uniform code formatting and style guidelines across the classes.
- **Documentation**: Added docstrings and comments to explain the purpose and functionality of methods.

## Motivation

This refactoring is motivated by the need to:

- **Reduce Technical Debt**: Clean up the codebase to make it more maintainable.
- **Facilitate Future Features**: Prepare the existing classes for integration with upcoming speculative decoding functionalities.
- **Encourage Community Contributions**: Make the code more approachable for contributors by improving readability and structure.

By isolating these changes, we enable reviewers to focus solely on the structural improvements without the added complexity of new features. This approach helps maintain a high code quality standard and simplifies the review and merging process.

## Dependencies

- **No New Dependencies**: This PR does not introduce any new dependencies. It solely refactors existing code.

## Before Submitting

- [x] **Part of Speculative Decoding Effort**: This PR is a preparatory step for enhancing speculative decoding capabilities (https://github.com/huggingface/transformers/pull/34760).
- [x] **Contributor Guidelines**: I have read and followed the [contributor guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request).
- [x] **No New Functionality**: This PR strictly involves refactoring without adding new features.
- [x] **Testing**: All existing tests pass successfully. No new tests are needed as functionality remains unchanged.

## Who Can Review?

The following reviewers are well-suited to review this PR: @gante, @ArthurZucker

---

This PR aims to strengthen the foundation for speculative decoding and other future enhancements by improving the existing code's structure and maintainability. We appreciate your time and look forward to your feedback.","the `prev_tokens` were removed recently in https://github.com/huggingface/transformers/pull/34823, any reason we have to add it back? thanks @zucchini-nlp !

We can remove it, we don't need it also for [USD](https://github.com/huggingface/transformers/pull/35029) Removed ✅ "
35242,2024-12-12T15:54:17Z,2024-12-13T22:46:49Z,asdkfjsd,2,0,2,87,2,2,2,[],47306.0,0,142780.0,0,0,0,0,1082216.655814,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'asdkfjsd']","Hello, I've completed a Chinese translation and would greatly appreciate it if you could take a moment to review it for me.  @Isotr0py Thank you very much! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35242). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Cool, looks good to me! Remember to request a review from another contributor who can read Chinese and review your translation :) The translation is pretty good!","Cool, looks good to me! Remember to request a review from another contributor who can read Chinese and review your translation :) The translation is pretty good!","# What does this PR do?

I have translated `docs/source/zh/perf_train_cpu.md` to Chinese. Please let me know if there are any corrections. Thank you very much!
<!-- Remove if not applicable -->

Fixed issue: [i18n-Chinese] Translating perf_train_cpu.md to Chinese #35230
URL : [https://github.com/huggingface/transformers/issues/35230](url)


## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
",
35105,2024-12-05T16:41:04Z,2024-12-05T17:15:48Z,molbap,1,0,44,478,5,2,2,['run-slow'],3323.0,0,739980.0,0,0,0,0,1087010.16046,,0,44,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35105). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thank you!,Thank you!,"# What does this PR do?

as title",
35096,2024-12-05T00:38:11Z,2024-12-05T19:06:54Z,jla524,1,0,1,32,12,1,1,[],58670.0,0,797720.0,0,0,0,0,1087043.605185,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35096). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for updating!,Thanks for updating!,"# What does this PR do?

updates python version to 3.9 in all translations


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@stevhliu",
35065,2024-12-03T18:52:47Z,2024-12-05T19:24:51Z,stevhliu,1,0,1,6,1,1,1,[],1632.0,0,904828.0,0,0,0,0,1087060.788525,,1,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35065). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Very clear! Thanks a lot ,Very clear! Thanks a lot ,"Fixes #35045 by clarifying that the `top_p`, `top_k` and `temperature` values are derived from a models generation config file, otherwise the default values are used.",
35120,2024-12-06T11:58:53Z,2024-12-06T13:42:51Z,qubvel,1,0,1,14,3,1,1,[],1616.0,0,670439.0,0,0,0,0,1087084.093303,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35120). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the fix!,Thanks for the fix!,"# What does this PR do?

Updates checkpoints path for I-JEPA model

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.",
35143,2024-12-07T20:21:15Z,2024-12-09T09:42:24Z,Uvi-12,1,0,2,6,1,1,1,[],136115.0,0,553877.0,0,0,0,0,1087104.602859,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35143). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"Fixes #35109 

This PR corrects the typos in prompts.py for 'acces' and 'those', to 'access' and 'these'.

@stevhliu",
34883,2024-11-22T16:32:08Z,2024-12-09T10:29:04Z,daniel-bogdoll,7,6,5,14,2,3,2,[],6493.0,0,1863599.0,0,0,0,0,1087130.982905,,1,5,0,False,"['qubvel', 'daniel-bogdoll']","Thanks @qubvel, sure thing! Which tests would I need to run to make sure modifications in the to() function of BatchFeature get tested?

Just to make sure, I assume you refer to https://github.com/huggingface/transformers/blob/54be2d7ae87e873482b984cc956e165ca4dc0ba3/src/transformers/feature_extraction_utils.py#L206 ? Yes, I refer to this one, but not sure it's properly tested anywhere, I was able to find only `SequenceFeatureExtractionTestMixin` Maybe we can do it as simple as 

```python
non_blocking = kwargs.get(""non_blocking"", False)
...
elif isinstance(v, torch.Tensor) and device is not None:
      new_data[k] = v.to(device=device, non_blocking=non_blocking)
...
``` That's how I would have tried it as well. But what about this block?

```
# Check if the args are a device or a dtype
        if device is None and len(args) > 0:
            # device should be always the first argument
            arg = args[0]
            if is_torch_dtype(arg):
                # The first argument is a dtype
                pass
            elif isinstance(arg, str) or is_torch_device(arg) or isinstance(arg, int):
                device = arg
            else:
                # it's something else
                raise ValueError(f""Attempting to cast a BatchFeature to type {str(arg)}. This is not supported."")
```

Here device is derived from `args` rather than `kwargs`. Should this be extended in some way to also consider deriving non_blocking? Not sure where or how this is used. >Here device is derived from args rather than kwargs. Should this be extended in some way to also consider deriving non_blocking? Not sure where or how this is used.

I don't think so, maybe at some moment, it is worth refactoring this method for more explicit args and kwargs. For now, we can add a note in docstring that `non_blocking` should be passed as a keyword argument. @qubvel Done! Thanks for the super-fast replies, was a pleasure! Tests fail now, though:

For the first one, as you stated here (https://github.com/huggingface/transformers/pull/34826#issuecomment-2491562222), it does not seem to be related.

https://app.circleci.com/pipelines/github/huggingface/transformers/111324/workflows/3351b194-4b9e-4a17-876b-85360fc7ff01/jobs/1482124?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-checks-link&utm_content=summary

```
FAILED
tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py::XLMRobertaXLModelTest::test_assisted_decoding_matches_greedy_search_1_same 
- AssertionError: False is not true
```


As the second one is a timeout issue, it also seems unrelated:

https://app.circleci.com/pipelines/github/huggingface/transformers/111324/workflows/3351b194-4b9e-4a17-876b-85360fc7ff01/jobs/1482127?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-checks-link&utm_content=summary

```
FAILED
tests/models/convbert/test_modeling_convbert.py::ConvBertModelTest::test_pipeline_fill_mask -
requests.exceptions.ReadTimeout: (ReadTimeoutError(""HTTPSConnectionPool(host='huggingface.co', port=443):
Read timed out. (read timeout=10)""), '(Request ID: 04e3d1b8-11fc-4791-ba74-3d7d67a5f3f2)')
``` @ArthurZucker or @LysandreJik please review when you have bandwidthHi @daniel-bogdoll, thanks for adding this! It looks great to me. Do you think it might be worth extending the same option to BatchFeature to ensure consistent capabilities? Thanks for updates! Looks great, just a small suggestion  Yeah sound super good!","Hi @daniel-bogdoll, thanks for adding this! It looks great to me. Do you think it might be worth extending the same option to BatchFeature to ensure consistent capabilities? Thanks for updates! Looks great, just a small suggestion  Yeah sound super good!","Option to set 'non_blocking' for to(device) operation in BatchEncoding for performance improvements. Defaults to 'false', thus no behavioral changes.

# What does this PR do?

This minor PR adds the non_blocking option to the to() function.

Previous: def to(self, device: Union[str, ""torch.device""]) -> ""BatchEncoding"":
New: def to(self, device: Union[str, ""torch.device""], non_blocking: bool = False) -> ""BatchEncoding"":

Since non_blocking defaults to 'False', this PR does not introduce behavioral changes. 

I realized, when utilizing Zero Shot Object Detection models, that it was not possible to set this option, leading to sub-optimal performance during inference.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review?

- vision models: @amyeroberts, @qubvel
","To force passing `non_blocking` as a keyword argument, so later we can introduce additional positional or keyword args without breaking BC, what do you think?

```suggestion
    def to(self, device: Union[str, ""torch.device""], *, non_blocking: bool = False) -> ""BatchEncoding"":
``` Sounds good! why do we need `*` ?  @qubvel suggested this to enforce it as a keyword argument for future backwards compatability. All arguments after the * are forced to be passed as keyword arguments: https://github.com/huggingface/transformers/pull/34883#discussion_r1856411789 Yes, only `device` can be passed as a positional argument with `*` introduced. This way, we will prevent anyone from using `batch_feature.to(""cuda"", True)` instead of `batch_feature.to(""cuda"", non_blocking=True)`. This would be useful in case we introduce more positional arguments in the future or need to change order, for example, with adding `dtype`. Thanks for explaining, good decision @qubvel ! 🤗 "
35170,2024-12-09T15:14:53Z,2024-12-11T13:12:34Z,LysandreJik,2,0,3,1980,97,1,1,[],1665.0,0,399400.0,0,0,0,0,1087165.13281,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35170). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. failed test is irrelevant and already addressed in another PR (not merged)In our tests I trust,In our tests I trust,,
35208,2024-12-11T11:42:39Z,2024-12-11T13:24:52Z,Cyrilvallez,5,0,1,218,1,1,1,[],1355.0,0,239307.0,0,0,0,0,1087192.571519,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'Cyrilvallez', 'ydshieh']","We probably also need to check how `check_modular_conversion` works to avoid it passes on PR but fail on main. But that could be in another PR and by me or anyone wants to work on it. BTW, it's always a good idea to trigger a slow CI :-) The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35208). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Humm, it seems very tricky to always avoid race conditions, it would probably require some check at merge time 🤔 I thought about this becoming an issue as we scale modular, but this is already an issue with the ""copied from"" (except if there is additional CI magic for copied from?). Here's the scenario:
- last commit on https://github.com/huggingface/transformers/pull/34858 pass all CIs as it should
- In the meantime, https://github.com/huggingface/transformers/pull/34157 is merged, relying on actual library version of Llama (not the modified one in https://github.com/huggingface/transformers/pull/34858)
- https://github.com/huggingface/transformers/pull/34858 is approved and merged (without any new commits, thus no new CI check)
- As Llama was modified in https://github.com/huggingface/transformers/pull/34858, Aria version is not longer up-to-date @Cyrilvallez Thanks for explaining. Yes, this happens from time to time. There is `Update branch` button, but click it all the time is also frustrating. We will try to think of solutions.Thanks",Thanks,"https://github.com/huggingface/transformers/pull/34858 modified Llama, but Aria relying on Llama got merged in the meantime. This correctly reapplies modular to Aria (order of RotaryEmbedding got changed because now the Attention does not need it anymore, thus it got closer to the class actually needing it)
",
35215,2024-12-11T17:45:16Z,2024-12-11T19:26:18Z,h3110Fr13nd,1,0,1,3,1,1,1,[],4519.0,0,217507.0,0,0,0,0,1087237.05955,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35215). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for filling in the docstring :),Thanks for filling in the docstring :),"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes the `make quality` issue due to Idefics3VisionConfig having this line
```python
    initializer_range (`<fill_type>`, *optional*, defaults to 0.02): <fill_docstring>
```
Replaced it with 
```python
    initializer_range (`float`, *optional*, defaults to 0.02):
            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
```

## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@stevhliu

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34968,2024-11-27T11:03:13Z,2024-12-11T14:38:42Z,ylacombe,5,0,13,12,4,2,2,['run-slow'],1893.0,0,1451249.0,0,0,0,0,1087218.063913,,0,13,0,False,"['HuggingFaceDocBuilderDev', 'ylacombe', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34968). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. let me check with the pipeline test - it's where I found this issue. it works now! Could you remove 2 lines below: `# TODO` and `@run_test_using_subprocess` before merge? Thanks

tests/pipelines/test_pipelines_text_to_audio.py
```
    # TODO: @ylacombe: `SeamlessM4TForTextToSpeech.generate` has issue with `generation_config`. See issue #34811
    @slow
    @require_torch
    @run_test_using_subprocess
    def test_medium_seamless_m4t_pt(self):
``` I've removed the TODO! You can ignore 

> tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py::SeamlessM4Tv2ModelWithSpeechInputTest::test_retain_grad_hidden_states_attentions

failing on multi-gpu runner.Thank you!

(a comment would be appreciated even if just a few words 🙏 ) LGTM","Thank you!

(a comment would be appreciated even if just a few words 🙏 ) LGTM","Fix #34811

cc @ydshieh ",
35226,2024-12-12T09:47:04Z,2024-12-12T13:45:04Z,Uvi-12,0,0,1,2,1,1,1,[],,0,159771.0,0,0,0,0,1087265.43715,,0,1,0,False,[],Thanks for the fix!,Thanks for the fix!,"Fixes #35225 

-This PR corrects 'indentifier' to 'identifier' in prompts.py

@stevhliu 
",
35216,2024-12-11T18:26:56Z,2024-12-12T13:59:24Z,Rocketknight1,1,0,1,4,1,1,1,[],1685.0,0,214952.0,0,0,0,0,1087294.102745,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35216). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"As @qgallouedec and @august-murr pointed out in https://github.com/huggingface/trl/pull/2455, the type hints for `apply_chat_template` are incorrect. Tools can be passed as `Callable` functions as well as JSON schema `dict` and this is documented [here](https://huggingface.co/docs/transformers/main/en/chat_templating#understanding-tool-schemas).

This PR corrects the type hints to follow the documentation/code!",
35157,2024-12-09T03:50:40Z,2024-12-13T18:23:00Z,winglian,3,1,4,1,1,3,3,[],41224.0,0,397941.0,0,0,0,0,1129681.179234,,0,4,0,False,"['winglian', 'HuggingFaceDocBuilderDev', 'ArthurZucker']","might have to broaden deepspeed for all zero cases? https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/engine.py#L2208-L2209 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35157). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. can you just run `make fixup` Since we should disable it for *all* DS (apparently), let's just go ahead and do that. I'll apply a similar fix in Accelerator. 

cc @SunMarc  LGTM with zach suggestion !  Thanks! We've also confirmed this fixes up all the fails users reported wrt deepspeed. cc @ArthurZucker for final post wing doing quality ;) sorry for breaking this ... and thanks for the fix!","Since we should disable it for *all* DS (apparently), let's just go ahead and do that. I'll apply a similar fix in Accelerator. 

cc @SunMarc  LGTM with zach suggestion !  Thanks! We've also confirmed this fixes up all the fails users reported wrt deepspeed. cc @ArthurZucker for final post wing doing quality ;) sorry for breaking this ... and thanks for the fix!","# What does this PR do?
Deepspeed 0.16 has assertions preventing the use of no_sync with zero 2/3. see https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/engine.py#L1986-L2004

it seems people are reporting this here https://github.com/microsoft/DeepSpeed/issues/6793, and I'm assuming that everyone is using accelerate/transformers as downgrading to deepspeed 0.15.4 makes it ""work"" for them.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@muellerzr 

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
                        if i != len(batch_samples) - 1 and not self.accelerator.distributed_type == DistributedType.DEEPSPEED
```"
35212,2024-12-11T15:35:30Z,2024-12-13T18:20:51Z,muellerzr,2,0,1,11,1,3,3,['bug'],1641.0,0,182726.0,0,0,0,0,1129806.480209,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35212). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Failures are unrelated! LGTM !  Testing this change locally, I can confirm that it enables my testing example to run successfully. There is an exception, namely when flash attention is being used, for some reason (perhaps it's an issue with my setup). But even so, this is an improvement and can be merged from my point of view.","LGTM !  Testing this change locally, I can confirm that it enables my testing example to run successfully. There is an exception, namely when flash attention is being used, for some reason (perhaps it's an issue with my setup). But even so, this is an improvement and can be merged from my point of view.","# What does this PR do?

https://github.com/huggingface/transformers/pull/34140 introduced a breaking change that users are reporting has many downstream issues:

* https://github.com/huggingface/peft/issues/2205
* https://github.com/axolotl-ai-cloud/axolotl/issues/2149

This PR reverts the breaking logic, while also maintaining the original test the user intended, in such a way that doesn't break downstream code. I'd highly recommend a patch with this fix @ArthurZucker 

Fixes https://github.com/huggingface/peft/issues/2205
Fixes https://github.com/axolotl-ai-cloud/axolotl/issues/2149


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker @winglian @SunMarc @BenjaminBossan ",
35231,2024-12-12T12:05:14Z,2024-12-13T18:12:00Z,HMJ0628,4,0,1,252,2,1,1,[],6567.0,0,108428.0,0,0,0,0,1130322.134801,,0,1,0,False,"['HMJ0628', 'HuggingFaceDocBuilderDev', 'Rocketknight1', 'asdkfjsd']",cc @stevhliu  OK，I will find another contributor to review. Your translation is simple and easy to understand for me. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35231). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for your translation! Remember to request a review from another contributor who can read the translated content!,Thanks for your translation! Remember to request a review from another contributor who can read the translated content!,"# What does this PR do?
I have translated agents_advanced.md to Chinese. Please let me know if there are any corrections. Thank you very much!
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35259,2024-12-13T14:35:31Z,2024-12-13T14:41:03Z,ivarflakstad,2,0,2,8,1,1,1,[],318.0,0,1667.0,0,0,0,0,1141666.268208,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']",merge as this is AMD-only docker file The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35259). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.👍 thank you,👍 thank you,"# What does this PR do?

There is a bug in the 6.0 rocm ubuntu image that causes `rocm-smi` to hang. This PR fixes that by updated the transformers-pytorch-amd-gpu image to use rocm 6.1 as the base image as well as aligning the torch wheel index accordingly.
The PR also adds `nvidia-ml-py` to the exclusion list specific to the AMD image.

Image has been tested on MI250


## Why 6.1?
6.3 seemed to be working fine, but the wheel index is not available yet.
6.2 introduced bugs related to multi-gpu, which is out of the scope of the current work effort.",
35119,2024-12-06T10:22:25Z,2024-12-13T13:36:23Z,ydshieh,2,2,2,2,1,1,0,[],1578.0,0,616440.0,0,0,0,0,1146879.641103,,1,2,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35119). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. merged as approved by Arthur offline,,"# What does this PR do?

> parse_pytest_errors_output (in `.circleci/parse_test_outputs.py`)

requires `rsfE` flag in `pytest`
","This is necessary to make

> parse_pytest_errors_output
(in `.circleci/parse_test_outputs.py`)

work otherwise error won't show in summary section"
35246,2024-12-12T17:14:09Z,2024-12-13T09:12:49Z,nhamanasu,3,0,2,1,1,1,1,[],2840.0,0,57520.0,0,0,0,0,1162697.379234,,0,2,0,False,"['Rocketknight1', 'zucchini-nlp', 'nhamanasu']","cc @ydshieh  - The failure of `tests_torch` will be resolved in this PR: https://github.com/huggingface/transformers/pull/35249 Fuyu skips the other assisted decoding methods due to specific way inputs have to be handled, afaik. So I'll be ok with skipping it, if @ydshieh agreesLGTM, thank you for the PR 🙏 ","LGTM, thank you for the PR 🙏 ","# What does this PR do?

skip Fuyu model from `test_prompt_lookup_decoding_matches_greedy_search` to fix `tests_generate`'s failure.

## Backgrounds
Recently, `test_generate` in CircleCI fails due to FuyuModelTest.
([reference] [tests_generate](https://app.circleci.com/pipelines/github/huggingface/transformers/113356/workflows/1e152e04-8f26-496a-9925-549ac7435e88/jobs/1514539?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-checks-link&utm_content=summary) - Failed)

The direct cause is `RuntimeError: The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 1`, perhaps caused by `eos_token_id should consist of positive integers, but is tensor([-1]). Your generation will not stop until the maximum length is reached. Depending on other flags, it may even crash.` 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

- generate: @zucchini-nlp
",
34910,2024-11-25T08:47:19Z,2024-12-13T13:33:45Z,faaany,12,3,16,68,34,4,3,[],23972.0,0,1572386.0,0,0,0,0,1147041.509715,,0,16,0,False,"['Rocketknight1', 'faaany', 'ydshieh']","I'm not sure we've observed this on our own test runners, but cc @ydshieh anyway Hi @faaany . Could you provide the command that you used to launch the test? Thank you But IMO, those classes like `SegformerImageProcessingTester` should not be a subclass of `unittest.Testcase`. They are not the classes to be tested but just helpers for the corresponding classes like `SegformerImageProcessingTest`.

Don't know why they are like that 😅  > Hi @faaany . Could you provide the command that you used to launch the test? Thank you

Below is my command example. Actually nothing special, I used to run with this command before and it worked. Could you help run this command on your machine and see how it goes? 

```bash
python -m pytest tests/models/s*
``` Sure. But I guess you mean `python -m pytest tests/models/*`?
BTW, in our CI workflow, each model is run in a separate (GitHub Action) job, rather than launched in the same process (like you do here), so there might have some difference.
 Hmm, not able to reproduce it in our CI env. But as mentioned, IMO, the changes should be remove the `unittest.TestCase` part in `class ASTFeatureExtractionTester(unittest.TestCase):` and similar place.

@faaany Would you be up to do this change? After that, we can check again if there is still some issues (running in your env.) and see what extra would be needed to do. > Hmm, not able to reproduce it in our CI env. But as mentioned, IMO, the changes should be remove the `unittest.TestCase` part in `class ASTFeatureExtractionTester(unittest.TestCase):` and similar place.
> 
> @faaany Would you be up to do this change? After that, we can check again if there is still some issues (running in your env.) and see what extra would be needed to do.

Sure, good ideas! I will try this out.  @ydshieh yes, after removing `unittest.TestCase`, it works in my env done. it works.  @Rocketknight1 Would you like to take final look and maybe give us a ✅ 🙏 ?

TL;DR: the `Tester` class is not the class to be tested, they are just some helpers. Hi @Rocketknight1, could you take a look? Thanks a lot! I think the failed tests are unrelated to my changes. Could you help retrigger the CI, when it passes? Glad it works in you env now, `XPU` must be happy this Friday!

Thanks again. This looks clean to me, but it's big enough that I should get a final review from a core maintainer! cc @ArthurZucker @LysandreJik 

Quick summary for them: Several ""Tester"" classes were subclasses of `unittest.TestCase`, but didn't actually have tests - they were just helpers for the actual test classes. This PR removes that subclassing, which fixes some issues. LGTM thanks","Glad it works in you env now, `XPU` must be happy this Friday!

Thanks again. This looks clean to me, but it's big enough that I should get a final review from a core maintainer! cc @ArthurZucker @LysandreJik 

Quick summary for them: Several ""Tester"" classes were subclasses of `unittest.TestCase`, but didn't actually have tests - they were just helpers for the actual test classes. This PR removes that subclassing, which fixes some issues. LGTM thanks","## What does this PR do?
I am not sure whether this is related to my test environment, but I got a lot of these kinds of failures when running tests as shown below: 

```bash
============================================================= ERRORS ==============================================================
__________________________ ERROR collecting tests/models/pix2struct/test_image_processing_pix2struct.py ___________________________
/usr/lib/python3.10/unittest/case.py:461: in __hash__
    return hash((type(self), self._testMethodName))
E   AttributeError: 'Pix2StructImageProcessingTester' object has no attribute '_testMethodName'
_____________________________ ERROR collecting tests/models/pixtral/test_image_processing_pixtral.py ______________________________
/usr/lib/python3.10/unittest/case.py:461: in __hash__
    return hash((type(self), self._testMethodName))
E   AttributeError: 'PixtralImageProcessingTester' object has no attribute '_testMethodName'
__________________________ ERROR collecting tests/models/pop2piano/test_feature_extraction_pop2piano.py ___________________________
/usr/lib/python3.10/unittest/case.py:461: in __hash__
    return hash((type(self), self._testMethodName))
E   AttributeError: 'Pop2PianoFeatureExtractionTester' object has no attribute '_testMethodName'
======================================================== warnings summary =========================================================
``` 
After the fix, tests run through. Could you take a look? Thanks a lot!

@amyeroberts @ydshieh 
","I think we can remove `()`, right? yes I think it got removed in style fixing."
35145,2024-12-07T20:35:01Z,2024-12-09T16:40:33Z,Uvi-12,1,0,2,8,1,1,1,[],134260.0,0,473478.0,0,0,0,0,1166687.730753,,0,2,0,False,['Uvi-12'],"@aymeric-roucher @stevhliu Please review the PR.LGTM, thank you!","LGTM, thank you!","Fixes #35144

-This PR corrects 'avilable' to 'available' in prompts.py

@stevhliu ",
35250,2024-12-12T19:35:01Z,2024-12-13T00:53:21Z,EricWinsorDSIT,1,0,1,4,1,1,1,[],11049.0,0,19101.0,0,0,0,0,1192666.462648,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35250). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Great catch, thanks!","Great catch, thanks!","# What does this PR do?

This PR fixes a typo in the first example jinja chat template. The template was missing a ' mark so attempting to use it in code causes a jinja syntax error.

## Before submitting
- [ x ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@stevhliu",
34966,2024-11-27T10:10:51Z,2024-11-27T15:32:50Z,ydshieh,6,0,1,6,1,1,1,[],1618.0,0,1335296.0,0,0,0,0,1206321.698191,,0,1,0,False,"['tomaarsen', 'loadams', 'HuggingFaceDocBuilderDev', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34966). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @ydshieh - FYI this change appears to have broken some unit tests in DeepSpeed, specifically where we download a model.  I'll look at ways to resolve this, I assume it is an issue in multiprocessing/threading. @loadams Thank you for informing us. Hope there is a way that could work in both cases 🙏 Let me know if there is anything we can help here. @ydshieh I'm also experiencing issues here for Sentence Transformers/Cross Encoder models, see https://github.com/UKPLab/sentence-transformers/issues/3129

In short: `multiprocessing.Process` never works when not inside of `__name__ == ""__main__""`. I recognize that most programs should be using that line, but I'd rather not force it on my users.

If one of my users loads any model that only has a `pytorch_model.bin`, then it'll fail, e.g.:
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer(""embaas/sentence-transformers-gte-base"")
```
or
```python
from sentence_transformers import CrossEncoder

model = CrossEncoder(""cross-encoder/ms-marco-MiniLM-L-6-v2"")
```
which internally call
```python
from transformers import AutoModel

model = AutoModel.from_pretrained(""embaas/sentence-transformers-gte-base"")
```
or
```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(""cross-encoder/ms-marco-MiniLM-L-6-v2"")
```

All of these get:
```
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the ""Safe importing of main module""
        section in https://docs.python.org/3/library/multiprocessing.html
```

- Tom Aarsen > Hi @ydshieh - FYI this change appears to have broken some unit tests in DeepSpeed, specifically where we download a model. I'll look at ways to resolve this, I assume it is an issue in multiprocessing/threading.

Hi @loadams FYI: it's reverted back to use `Thread` in #35236 (as there are other issues reported) > should be using that line, but I'd rather not force it on m

@ydshieh - thanks for the update!Thank you!",Thank you!,"# What does this PR do?

As discussed offline:

> Would it possible if we change Thread to Process below :pray: ?
I know it's not good to change code that is working and making sense for the sole purpose of testing, but Thread  (without join) here making some test(s) flaky failing
test_cached_model_has_minimum_calls_to_head
where we count the number of calls to Hub. Some other tests that (eventually) calling from_pretrained  entering Thread line  will affect test_cached_model_has_minimum_calls_to_head
because they are running at the same time in the same process.
There are other solutions, but the changes involves patching (a lot of) tests (so at conftest level which is not good neither IMO.
https://github.com/huggingface/transformers/blob/d5cf91b3462b5ed57260074a0708f09b16e787a8/src/transformers/modeling_utils.py#L3842

----------------------------------------------

To check the PR works (make sure tensorflow is available): 

> python gradio2.py

where `script.py`
```python
for i in range(20):
    import os
    os.system('python -m pytest -v tests/models/auto/test_modeling_auto.py tests/models/auto/test_modeling_tf_auto.py -k ""test_from_pretrained_identifier or test_cached_model_has_minimum_calls_to_head""')
    # or running several times on CI workflow: it's flaky and not easy to reproduce
    # or add `time.sleep(0.5)` at the beginning of `auto_convenanorsion`

```",
34719,2024-11-13T18:57:43Z,2024-12-13T07:23:31Z,horheynm,8,30,37,268,9,4,2,[],508896.0,0,2550348.0,2,0,0,0,1169258.352976,,0,37,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1', 'horheynm']","cc @SunMarc @MekkCyber for quantization PR is in a decent state to review. Will add tests for it to be finalized @SunMarc 
Hey Marc, this PR is ready > re: offline discussion check if the warnings we're seeing on this branch are specific to an uncompressed model vs compressed model

Ok this is addressed now - warning came from not calling apply_quantization_config which populates the scales and zp to state dicts.  @SunMarc 
PR comments are addressed, let me know if there are more to address! @ArthurZucker 
Could I get a review please! @ArthurZucker 
Could I get a review please! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34719). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.I see that the goal is to overwrite the `run_compressed` attribute in the quantization config. To do so, we have the [merge_quantization_configs](https://github.com/huggingface/transformers/blob/f297af55dfc27485189f352cd36b4683de12e0b3/src/transformers/quantizers/auto.py#L156) function and you mostly just need to create the [get_loading_attributes](https://github.com/huggingface/transformers/blob/f297af55dfc27485189f352cd36b4683de12e0b3/src/transformers/quantizers/auto.py#L180C63-L180C85) function. I think this will make the user experience better also. 

In the end, the user will only need to do: 

```
quantization_config = CompressedTensorsConfig(run_compressed=False)
model.from_pretrained(...,quantization_config=quantization_config) 
```
to load the uncompressed model  Thanks for the integration ! Left a few comments  re: offline discussion
check if the warnings we're seeing on this branch are specific to an uncompressed model vs compressed model Make sure the run_compressed argument is represented in the `to_dict` and `from_dict` functions, consider adding a test to verify LGTM ! Left a suggestion","I see that the goal is to overwrite the `run_compressed` attribute in the quantization config. To do so, we have the [merge_quantization_configs](https://github.com/huggingface/transformers/blob/f297af55dfc27485189f352cd36b4683de12e0b3/src/transformers/quantizers/auto.py#L156) function and you mostly just need to create the [get_loading_attributes](https://github.com/huggingface/transformers/blob/f297af55dfc27485189f352cd36b4683de12e0b3/src/transformers/quantizers/auto.py#L180C63-L180C85) function. I think this will make the user experience better also. 

In the end, the user will only need to do: 

```
quantization_config = CompressedTensorsConfig(run_compressed=False)
model.from_pretrained(...,quantization_config=quantization_config) 
```
to load the uncompressed model  Thanks for the integration ! Left a few comments  re: offline discussion
check if the warnings we're seeing on this branch are specific to an uncompressed model vs compressed model Make sure the run_compressed argument is represented in the `to_dict` and `from_dict` functions, consider adding a test to verify LGTM ! Left a suggestion","# What does this PR do?
Loading quantized model using compressed-tensors is currently hardcoded to run in `run_compressed` mode. 
This PR allows the model to be loaded in different ways


```python3

from transformers import AutoModelForCausalLM, AutoConfig
from transformers.utils.quantization_config import CompressedTensorsConfig

pretrained_model_name_or_path = ""neuralmagic/Llama-3.2-11B-Vision-Instruct-FP8-dynamic"" # static config file

quantization_config = CompressedTensorsConfig(run_compressed=False)
model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name_or_path,
    quantization_config=quantization_config
)

```
","need config for decompression -> find the filepath of model.safetensors If I understood correctly, you are trying to get the config file of the original model. Could you tell me why this is needed to decompress the model ?  nice  not sure to understand what it `format` method let's just use small models. 8b decompressed will be too big for the ci can you also check that we get the same output as the compressed model ?  The `model_path` should point to the compressed model on disk no? @horheynm  The only thing I need here is the path to the model safetensors. They are in a compressed format so by knowing the path I can decompress them If there is a better way to get the download folder path, lemme know  Format is the key to quantization confit, it tells what quant format to compress to - ex packed quantized (pack in int4,8 into int32 as contiguous array)  Ok will change, thanks! Ok sure thing! Yeah that will be disk if from_pretained(...) was provided a disk path. Otherwise disk will be the hf cache
Config in there is the path provided to the pre trained  config._name_or_path won't give you necessarily the path to the quantized model as you can see here:  https://huggingface.co/neuralmagic/Meta-Llama-3-8B-Instruct-FP8/blob/main/config.json#:~:text=%22_name_or_path%22%3A%20%22meta%2Dllama/Meta%2DLlama%2D3%2D8B%2DInstruct%22%2C @SunMarc 
`self.compressor.decompress(model_path=cache_path, model=model)`

:param model_path: path to compressed weights
:param model: pytorch model to load decompressed weights into
 If a model starts compressed, we then pass in the config to turn off `run_compressed`/decompress the model in `_process_model_after_weight_loading`, wont the `self.is_compressed` remain the same in its values and therefore, is_qat_trainable will be false? We should update the value right? nit: keep more concise form nit: remove unnecessary space nit: remove unnecessary space ```suggestion
        run_compressed (`bool`, *optional*, defaults to `True`): alter submodules in order to emulate compressed model execution if True, otherwise decompress submodules
``` ```suggestion
    def __init__(self, quantization_config: CompressedTensorsConfig, **kwargs):
``` If config is required in all cases, it should be defined in the function signature rather than being popped from kwargs Since there's no cases in which config should not be passed, we should put this directly in the function signature, rather than implicitly in the kwargs ```suggestion
    def is_trainable(self) -> bool:
``` Why remove `@property`? We can fix this by making is_compressed a `@property` ```suggestion
            run_compressed = self.is_compressed and self.run_compressed
            model, ct_quantization_config, run_compressed=run_compressed
```

Doesn't the previous logic have the effect of completely ignoring the run_compressed argument if the model is quantized?

Note that in the future, we plan to perform decompression through the `apply_quantization_config` function config is only needed for compressed tensors hfquantizer to get the names_or_path. 
I dont think its needed to make it default since no other hfquantizer is using it
   we dont need it all cases actually - only for compressed tensors hf quantizer. 
Individual quantizer specific args are in kwargs here ok"
35240,2024-12-12T15:13:32Z,2024-12-12T18:23:28Z,LysandreJik,1,0,3,382,19,1,0,[],2065.0,0,11398.0,0,0,0,0,1216060.481292,,1,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35240). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Offline approval from Arthur,Offline approval from Arthur,,
35103,2024-12-05T13:38:39Z,2024-12-12T14:07:06Z,RezaRahemtola,4,4,3,3,1,2,1,[],6794.0,0,610741.0,0,0,0,0,1227210.953277,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1', 'RezaRahemtola']","Hi @RezaRahemtola - the PR is a great idea, but we can't accept it as-is. `types.UnionType` was only added in Py3.10, and Py3.9 has not reached end-of-life yet. Any users still on 3.9 will get an error whenever this line is reached.

However, I agree that it's important that we support type hints like `str | None`. Can you figure out a way to do that without breaking compatibility? Indeed, just modified it in [this commit](https://github.com/huggingface/transformers/pull/35103/commits/2d2e26bee11f5babd2e61189fc34a96e75cfa2fe) to avoid breaking Python3.9 Hey @Rocketknight1, everything ready here if you can review when you have some time ;) The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35103). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Yes, this looks good now - thanks for the PR!","Yes, this looks good now - thanks for the PR!","# What does this PR do?

This PR fixes the parsing of type hints for chat templates by supporting the 3.10+ Union style (eg `str | None`, vs the older `Union[str, None]`).

The union was already detected in the `get_origin` function, and the arguments were also already parsed by `get_args` [here](https://github.com/huggingface/transformers/blob/main/src/transformers/utils/chat_template_utils.py#L89), but it wasn't handled in the condition block.

## Example

### Before:
```py
def get_current_temperature(location: str, unit: str | None = None) -> float:
    """"""
    Get the current temperature at a location.

    Args:
        location: The location to get the temperature for, in the format ""City, Country""
        unit: The unit to return the temperature in. (choices: [""celsius"", ""fahrenheit""])
    Returns:
        The current temperature at the specified location in the specified units, as a float.
    """"""
    return 22.0  # A real function should probably actually get the temperature!
```
was throwing this error:
```txt
Traceback (most recent call last):
  .....
  File ""...../tools.py"", line 73, in from_langchain
    function_parameters = _convert_type_hints_to_json_schema(langchain_tool._run)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".../site-packages/transformers/utils/chat_template_utils.py"", line 166, in _convert_type_hints_to_json_schema
    properties[param_name] = _parse_type_hint(param_type)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".../site-packages/transformers/utils/chat_template_utils.py"", line 151, in _parse_type_hint
    raise TypeHintParsingException(""Couldn't parse this type hint, likely due to a custom class or object: "", hint)
transformers.utils.chat_template_utils.TypeHintParsingException: (""Couldn't parse this type hint, likely due to a custom class or object: "", str | None)
```

and now works and generate the following schema:
```txt
{
  ""type"": ""function"",
  ""function"": {
    ""name"": ""get_current_temperature"",
    ""description"": ""Get the current temperature at a location."",
    ""parameters"": {
      ""type"": ""object"",
      ""properties"": {
        ""location"": {
          ""type"": ""string"",
          ""description"": ""The location to get the temperature for, in the format 'City, Country'""
        },
        ""unit"": {
          ""type"": ""string"",
          ""nullable"": True,
          ""enum"": [
            ""celsius"",
            ""fahrenheit""
          ],
          ""description"": ""The unit to return the temperature in.""
        }
      },
      ""required"": [
        ""location""
      ]
    },
    ""return"": {
      ""type"": ""number"",
      ""description"": ""The current temperature at the specified location in the specified units, as a float.""
    }
  }
}
```


<!-- Remove if not applicable -->

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

- chat templates: @Rocketknight1","```suggestion
    elif origin is Union or (hasattr(types, ""UnionType"") and origin is types.UnionType):
```
This looks good now! I made one change in the suggestion here. The reason is subtle and I'm not sure it matters, but the `in` operator compares by equality `==`, not identity `is`. In other words, `x in (a, b)` is equivalent to `x == a or x == b`, not `x is a or x is b`, which is what we want here.

One final request: Can you add a test in `tests/utils/test_chat_template_utils.py` with a function that uses type hints with `| None`, so we can verify that this works and doesn't regress later? Sure, I'll do that tomorrow.
But your CI tests are running with multiple Python versions, including 3.9 no?
Won't the test break in Python 3.9 as the type hint format isn't recognized as valid Python? Ah, you're totally right! I forgot `X | Y` types were only added in 3.10 as well.

In that case, we probably can't test this until 3.9 is retired - it's annoying, but there's not much we can do. Still, we can accept the PR to support those types, and just hope it doesn't regress until we can add a proper test for it. Alright, applied your suggestion and didn't add the test then!
I'll use this feature in my [company's project](https://github.com/Libertai/libertai-agents), I'll try to monitor it and submit another PR in case there's a regression 👍 "
35236,2024-12-12T13:49:07Z,2024-12-12T15:05:04Z,ydshieh,1,1,3,29,3,3,2,[],1871.0,0,4559.0,0,0,0,0,1227965.509366,,0,3,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35236). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Ok, this sounds good to me! Thanks for the quick fix @ydshieh! Looks good in principle!

I was about to say _it's no best practice to make such a big change just for testing purposes_, just before realizing the Process has been added for [testing purposes](https://github.com/huggingface/transformers/pull/34966) in the first place :smile: So I guess all good if CI passes :)","Ok, this sounds good to me! Thanks for the quick fix @ydshieh! Looks good in principle!

I was about to say _it's no best practice to make such a big change just for testing purposes_, just before realizing the Process has been added for [testing purposes](https://github.com/huggingface/transformers/pull/34966) in the first place :smile: So I guess all good if CI passes :)","# What does this PR do?

Fix #35228

On other platforms likke Mac / Windows, using `Process` (see #34966) is even much cost than `Thread` (on Linux, the cost is also higher but not that much).

In a CI environment, it may occurs many tests will eventually call `auto_conversion` (within `from_pretrained`), and the accumulation of using `Process` is too high and cause `PEFT (Mac / Windows) CI` running time increase a lot:

> Windows (~1h:22min) and MacOS (~52min) compared to Ubuntu (~22min)

This PR changes it back to using `Thread` and achieve the original goal of #34966 in another way.","This is to achieve what #34966 was trying to fix:

if the log is not from the same thread as the test thread itself, let's ignore it"
34511,2024-10-30T14:35:37Z,2024-11-04T12:47:34Z,muellerzr,2,13,17,119,3,6,7,[],1850.0,0,3626637.0,0,0,0,0,1318299.092421,,0,17,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34511). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @Milad335t just warning you to stop spamming or we'll have to block you 😢 Couple comments about the test! Tests look clean to me now, and I'm trusting you on the accelerate side of things! :sweat_smile:

cc @LysandreJik / @ArthurZucker for core maintainer review M Thanks, let's hope this gets stabilized! Thanks patching today!","Couple comments about the test! Tests look clean to me now, and I'm trusting you on the accelerate side of things! :sweat_smile:

cc @LysandreJik / @ArthurZucker for core maintainer review M Thanks, let's hope this gets stabilized! Thanks patching today!","# What does this PR do?

Alternative to https://github.com/huggingface/transformers/pull/34442

TL;DR we just need to remove `lru_cache` and everything will work fine. (and adds a test)

This PR also takes the full lessons from my article and adds it to the `Trainer` for a simpler solution to the grad accum calculation (we shouldn't rely on `accelerator` from now on bc it can't handle the nuances with the grad accum fix at the highest level API, so we use a lower level version instead)

Fixes https://github.com/huggingface/transformers/issues/34402

**Would recommend a patch after this**


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker @Rocketknight1 ","TinyLlama is very large for a test! We can probably reproduce the bug with a tiny-random model and a dataset that we just build as a list in the test, or something like that. Do we need the decorator? The test passes on Python < 3.11 anyway, so we can simplify the codebase and run it regardless! For an explanation on what we have going on here @Rocketknight1 , during DDP we use `model.no_sync()` to only communicate across all GPUs during the next step outside it (so we speed up training when not needed when doing gradient accumulation). `accelerator.no_sync()` is the lower-level `accumulate()` API which makes that op backed-independent (so on a single GPU it just does `nullcontext`) works, checking for `ParameterKind.VAR_KEYWOR` as type is probably better weird to me that we have to use -100 here, instead of a general parameter but whit was already the case IIRC we use `-100` for padding by default in the Trainer. I can align it to `self.processor` if it exists else `-100` if that's better?:) Actually our padding index is -100 everywhere.  okay sounds good then sorry No worries, it's weird for me too :) Why do we no longer need to shift labels `[""labels""][...,1:]`  when getting num_items_in_batch? sure, I also think we need to shift labels before computing the `num_items_in_batch`. Otherwise, the value is incorrect as the first element in labels may not be -100 I'm confused that the loss is no longer multiplied by the gradient accumulation steps here, because the loss has been multiplied by the data parallel size in https://github.com/huggingface/transformers/pull/34511/files#diff-ed55888e6665791fe92cc8fc0c499da54f4ace6738551cd9a2591881cda076deR3702-R3703 I see, it should be solved in https://github.com/huggingface/transformers/pull/35207"
35087,2024-12-04T15:04:25Z,2024-12-11T11:44:39Z,BenjaminBossan,4,0,2,101,2,1,1,[],2310.0,0,592817.0,0,0,0,0,1326391.582343,,1,2,0,False,"['BenjaminBossan', 'HuggingFaceDocBuilderDev', 'Rocketknight1']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35087). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker Please review or suggest a reviewer. @Rocketknight1 Do you know who could best review this PR? Sounds like a job for @muellerzr @SunMarc !Nice error :),Nice error :),"# What does this PR do?

Original issue: https://github.com/huggingface/peft/issues/2256

There is a potential error when using load_best_model_at_end=True with a prompt learning PEFT method. This is because Trainer uses `load_adapter` under the hood but with some prompt learning methods, there is an optimization on the saved model to remove parameters that are not required for inference, which in turn requires a change to the model architecture. This is why `load_adapter` will fail in such cases and users should instead set `load_best_model_at_end=False` and use `PeftModel.from_pretrained`. As this is not obvious, we now intercept the error and add a helpful error message.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?",
34564,2024-11-01T15:52:42Z,2024-12-11T12:40:31Z,qubvel,24,30,92,1561,25,6,1,"['New model', 'Vision', 'run-slow']",2729.0,0,3444469.0,0,0,0,0,1323043.247543,,0,92,0,False,"['qubvel', 'HuggingFaceDocBuilderDev', 'rwightman', 'LysandreJik']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34564). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @rwightman could you please make the first review in case you have bandwidth @qubvel I'm starting to work through it now, wanted to get eval working to check some familiar models and just wasted way too much time realizing I needed `--remove_unused_columns False ` for anything to work at all :/ ... that's a really poor setup when most datasets have an 'image' column and not a 'pixel_values' column (realize that's nothing to do with this PR, heh) :/

Annnyways, first pass of the code things looked sane but need to spend some time looking closer at the details and testing some cases. A few high level q...

* Does 'Wrapper' add any worthwhile value/info in the name vs
  * `TimmPreTrainedModel(PreTrainedModel)`
  * `TimmModel(TimmPretrainedModel)`
  * `TimmModelForImageClassification(..)`

* Is there a reason TimmWrapperModel has `.timm_model`, instead of something more generic like `.model`

* Any reservations in changing TimmWrapperModelForImageClassification to not use TimmWrapperModel? There are a few issues with the handling details for classifier, possible optimizations for forward call sequence and it'd probably be a bit cleaner to just duplicate a bit of redunant code and keep the two impl separate and a bit different.

* I thought we were going to set pretained=True so timm can load the weights, their are a number of weight adaptation / translation things that don't run if this isn't use, cannot change num_classes cleanly for instance. 

* What happens if we try to push these models to the hub? Do they get uploaded/written in a form that timm can read? @rwightman thanks for the review! Indeed there are some default params I'm also confused about, it's even more for object detection 🥲 

> Does 'Wrapper' add any worthwhile value/info in the name vs
TimmPreTrainedModel(PreTrainedModel)
TimmModel(TimmPretrainedModel)
TimmModelForImageClassification(..)

Left it as it was in the previous PR, however, `TimmModelForImageClassification` sounds better to me, I can rename it

> Is there a reason TimmWrapperModel has .timm_model, instead of something more generic like .model

The prefix ""timm_model"" is unique and is used in certain tests to identify when weights come from a timm model. It's also utilized in the `_fix_state_dict_key` method to determine whether to add the prefix when loading weights from the original checkpoint. For these reasons, I would prefer to keep it as ""timm_model""

> Any reservations in changing TimmWrapperModelForImageClassification to not use TimmWrapperModel? There are a few issues with the handling details for classifier, possible optimizations for forward call sequence and it'd probably be a bit cleaner to just duplicate a bit of redunant code and keep the two impl separate and a bit different.

Originally, this was implemented without `TimmWrapperModel` in `TimmWrapperModelForImageClassification`, but I introduced it to reduce code repetition. This approach also aligns better with common patterns in the transformers repo. Could you provide more details on the issues with the classifier? In any case, the code will remain the same for both models if we aim to maintain `output_hidden_states` functionality across them.

> I thought we were going to set pretained=True so timm can load the weights, their are a number of weight adaptation / translation things that don't run if this isn't use, cannot change num_classes cleanly for instance.


I left a comment in the thread about this. We use transformers for weight loading to leverage features like device_map, torch_dtype, and quantization. I’m also unsure how to disable weight loading through transformers if it’s handled by timm, as I haven’t seen any examples of this in the repo. I can dig into it further if needed.

Do you have an estimate of how many models involve weight renaming? Is there a way to update checkpoints without breaking older versions of timm? Alternatively, could we manage similar weight renaming directly in transformers? (Though I think this approach may be less robust.)

> What happens if we try to push these models to the hub? Do they get uploaded/written in a form that timm can read?

I’m not sure if it's currently compatible, but it would be great to enable it! The config should be compatible, however, the weights state dict will have the ""model.timm_model."" prefix. I can look into removing this prefix before saving. I will try to enable it and add a separate test for a few checkpoints. Thanks for bringing this up! > Originally, this was implemented without `TimmWrapperModel` in `TimmWrapperModelForImageClassification`, but I introduced it to reduce code repetition. This approach also aligns better with common patterns in the transformers repo. Could you provide more details on the issues with the classifier? In any case, the code will remain the same for both models if we aim to maintain `output_hidden_states` functionality across them.

I feel in this case there is a difference in alignment with other models, because both ImageClassification and base model wrap timm model instances that differ, instead of adding their own head to the same timm model. I see some options, depending on the mix of hidden state flags, head vs no head where it'd be appropriate to go through different forward calls, where more than just one argument might be appropriate to change on creation, etc.

Also, thinking about the future and other tasks. I feel there is a high probability say for supporting native timm object detection more flexibility is desired so it'd be safer to have it uncoupled.

That and there's a resisitance to significant changes in transformers after something is in there, so feel it's better to leave uncoupled to have additional flexibility and avoid being stuck with tricky decisions that might impact timm moving forward.
 > > I thought we were going to set pretained=True so timm can load the weights, their are a number of weight adaptation / translation things that don't run if this isn't use, cannot change num_classes cleanly for instance.
> 
> I left a comment in the thread about this. We use transformers for weight loading to leverage features like device_map, torch_dtype, and quantization. I’m also unsure how to disable weight loading through transformers if it’s handled by timm, as I haven’t seen any examples of this in the repo. I can dig into it further if needed.
> 
> Do you have an estimate of how many models involve weight renaming? Is there a way to update checkpoints without breaking older versions of timm? Alternatively, could we manage similar weight renaming directly in transformers? (Though I think this approach may be less robust.)

Aside from renaming, it just doesn't work right now, you can't load weights for a model if the head size changes for a new classification task. The wrapper only works if you use the imagenet classifier.

I realize timm doesn't have the dtype, lazy init features but it's better to have it work I feel. Can potentially look at supporting some of that through timm. It hasn't been a priority as there aren't too many very large models in timm.

If there's no way to do use pretrained=True on creation then will probably need to figure out how to add & call a method in timm once transformers has the state_dict and before it's loaded into the model, not sure if there's a spot for such a call in transformers? > Aside from renaming, it just doesn't work right now, you can't load weights for a model if the head size changes for a new classification task. The wrapper only works if you use the imagenet classifier.

Fixed! > If there's no way to do use pretrained=True on creation then will probably need to figure out how to add & call a method in timm once transformers has the state_dict and before it's loaded into the model, not sure if there's a spot for such a call in transformers?

If there would be any timm model/class-specific function that fixes state_dict I suppose we can call it before loading weights, similar to `_fix_state_dict_key` in the current implementation. However, it will be supported only for newer timm versions Thinking about this a bit more, there is another issue with the hub / transformers first weight loading. `timm` wasn't originally hub first, so the library itself is still the primary source of truth for some models, doing a hub based load won't work.

Example this model https://huggingface.co/laion/CLIP-ViT-B-16-laion2B-s34B-b88K ... is an OpenCLIP first model, but timm can load it if you use the model name 'vit_base_patch16_clip_224.laion2b' 

Indeed I feel that the wrapper should support all timm model names that work in timm, but right now if the model isn't on the hub w/ a timm primary config it isn't useable. Ideally it should work with both a hub model name OR any timm model name. The timm model name would require timm do the pretrained loading to resolve any translation to other hub name or weight source.

Some very popular examples of this

https://github.com/huggingface/pytorch-image-models/blob/51ac8d2efb926c6b7c34eeb1dc52bcf57999e2de/timm/models/vision_transformer.py#L1580-L1716 > > Aside from renaming, it just doesn't work right now, you can't load weights for a model if the head size changes for a new classification task. The wrapper only works if you use the imagenet classifier.
> 
> Fixed!

Yay, I was able to run fine-tune using run_image_classification.py after this fix. An observation the output files for that script aren't directly useable to push to hub.

There is no config matching timm format or name, there is a config output by the image preprocessor save process to a different filename that's a jumble of the timm config. 

Also, the state dict for the model has the timm_model prefix so it's not loadable in timm. Is there anyway to remove that prefix in the checkpoints? this would also make it more seamless doing local dir loads if someone had timm weights already and config files checked out.  Hi @rwightman, 

I've addressed some comments and pushed the following fixes:  

Config and weights are now compatible with native timm models.  
  - Removed the `timm_model` prefix.  
  - Disabled processor config saving, the configuration is now kept in `config.json`, consistent with the original timm checkpoint format.  

Here’s an example of a fine-tuned **TimmWrapper model** using `run_classification` that can be loaded directly into timm:  

```python
model = timm.create_model(""hf-hub:qubvel-hf/vit-base-beans"", pretrained=True)
```  

Regarding OpenCLIP models:  
Would it be possible to provide compatible copies of these checkpoints under the `timm/` org on the Hub? Specifically, reconverting them to the appropriate format.  

Thanks!   I've also noticed that some models from the provided [list](https://github.com/huggingface/pytorch-image-models/blob/51ac8d2efb926c6b7c34eeb1dc52bcf57999e2de/timm/models/vision_transformer.py#L1580-L1716) have transformers-like `config.json` along with `open_clip_config.json`, which allows them to be loaded with `CLIPForImageClassification`. Would it be possible to update other checkpoints similarly? We may confuse the users a bit, but updating model cards should help - Saved checkpoints are now timm-compatible
- Removed inheritance from the `ForImageClassification` model and added additional tests
- The renaming of `TimmWrapperModel` to `TimmModel` is not yet done. I'm uncertain if this might cause issues, as the model folder would need to be named `timm`, which could potentially create confusion with the `timm` package, maybe it's safer to have it as timm_wrapper

@rwightman, could you please review when you have the bandwidth? also please let me know what you think about uploading transformers compatible checkpoints to new or existing repos on the hub (see comments above) @qubvel thanks, I'll dig in and check out the latest updates tomorrow. A user of the timm backbone was complaining about the transformers weight handling wrt to 'gamma', etc .. can the fixes for this use case be extended to the backbone easily? > * Saved checkpoints are now timm-compatible
> * Removed inheritance from the `ForImageClassification` model and added additional tests
> * The renaming of `TimmWrapperModel` to `TimmModel` is not yet done. I'm uncertain if this might cause issues, as the model folder would need to be named `timm`, which could potentially create confusion with the `timm` package, maybe it's safer to have it as timm_wrapper
> 
> @rwightman, could you please review when you have the bandwidth? also please let me know what you think about uploading transformers compatible checkpoints to new or existing repos on the hub (see comments above)

Good point re the transformer naming resulting in a folder named just 'timm', yeah not a good idea, let's stick with wrapper naming as is. Parallels timm_backbone then too.  > Hi @rwightman,
> 
> I've addressed some comments and pushed the following fixes:
> 
> Config and weights are now compatible with native timm models.
> 
> * Removed the `timm_model` prefix.
> * Disabled processor config saving, the configuration is now kept in `config.json`, consistent with the original timm checkpoint format.
> 
> Here’s an example of a fine-tuned **TimmWrapper model** using `run_classification` that can be loaded directly into timm:
> 
> ```python
> model = timm.create_model(""hf-hub:qubvel-hf/vit-base-beans"", pretrained=True)
> ```

Great, I will try this out.


> 
> Regarding OpenCLIP models: Would it be possible to provide compatible copies of these checkpoints under the `timm/` org on the Hub? Specifically, reconverting them to the appropriate format.
> 
> Thanks!

It's just OpenCLIP specific though most instances of this are OpenCLIP weights... there are a few other cases where an organization has uploaded a model and I've directly referened a checkpoint and leveraged conversion functions in timm. 

```
    ""sam2_hiera_tiny.r896"": _cfg(
        hf_hub_id='facebook/sam2-hiera-tiny',
        hf_hub_filename='sam2_hiera_tiny.pt',
    ),
```

Would be possible to do the conversions, there are 60-70ish cases of this I think so a bit of work. > A user of the timm backbone was complaining about the transformers weight handling wrt to 'gamma', etc .. can the fixes for this use case be extended to the backbone easily?

yeah, probably the same trick will work, can have a look once this PR merged @qubvel just noticed this reply, in review chat above, notifications in review chats always throw me off so I'm pulling it out here

> is the `last_hidden_state` would be equivalent to no pooled output?
> 
> where the last hidden state is:
> 
> ```python
> last_hidden_state, hidden_states = self.timm_model.forward_intermediates(pixel_values, **kwargs)
> # or
> last_hidden_state = self.timm_model.forward_features(pixel_values, **kwargs)
> ```
> 
> If so, I suppose we can add some flag to avoid applying `forward_head` if someone don't want to have pooler output. It can be the same`BaseModelOutputWithPooling` with `pooler_output=None`


Yes, last_hidden_state is always unpooled output, they are supposed to be equivalent btw `forward_intermediates` and `forward_features`,  last_hidden_state in both cases should be possible to pass to forward_head() and produce the same result. 

You've already noticed, but forward_features exists in every single model and forward_intermediates only a few of them.

So for pooling is `BaseModelOutputWithPooling` .. is it normal to have `pooler_output=None`? Looking again at some convnets and looks like they always do include pooling output, I assumed that some might output BaseModelOutput but seems that's only intermediate stages. So maybe this is fine as is if that's typical in transformers.

Which brings to next point, I'm not intimately famaliar with the norms in Transformers, we should get another reviewer on here who is as my focus is on the timm bits.
 > So for pooling is BaseModelOutputWithPooling .. is it normal to have pooler_output=None? Looking again at some convnets and looks like they always do include pooling output, I assumed that some might output BaseModelOutput but seems that's only intermediate stages. So maybe this is fine as is if that's typical in transformers.

Agreed, looks a bit weird. In case of confusion, we can create `TimmWrapperModelOutput` and correctly specify that `pooler_output` is optional based on the config/init option. I will add an option and let`s see the core maintainers' opinion.  Thanks, this looks good! cc @molbap can you give the processor code a quick look just to double check? @LysandreJik @molbap @rwightman Thanks for the reviews! I believe all comments have been addressed. Do you have anything else in mind? It would be nice to move it forward. Ok awesome! At this point just a second quick look from @ArthurZucker and we're goodCopilot reviewed 11 out of 26 changed files in this pull request and generated no suggestions.
<details>
<summary>Files not reviewed (15)</summary>

* **src/transformers/configuration_utils.py**: Evaluated as low risk
* **examples/pytorch/image-classification/run_image_classification.py**: Evaluated as low risk
* **setup.py**: Evaluated as low risk
* **src/transformers/image_processing_base.py**: Evaluated as low risk
* **src/transformers/modeling_utils.py**: Evaluated as low risk
* **docs/source/en/index.md**: Evaluated as low risk
* **src/transformers/utils/__init__.py**: Evaluated as low risk
* **src/transformers/models/__init__.py**: Evaluated as low risk
* **src/transformers/models/auto/configuration_auto.py**: Evaluated as low risk
* **docs/source/en/_toctree.yml**: Evaluated as low risk
* **src/transformers/models/auto/modeling_auto.py**: Evaluated as low risk
* **src/transformers/dependency_versions_table.py**: Evaluated as low risk
* **src/transformers/utils/dummy_pt_objects.py**: Evaluated as low risk
* **src/transformers/__init__.py**: Evaluated as low risk
* **src/transformers/models/auto/image_processing_auto.py**: Evaluated as low risk
</details>


 @LysandreJik please review whenever you have bandwidth!

Some tests are failing, waiting for the update of `timm` version in CI docker images in a separate PR. This is starting to look nice!  Hey, took a quick look at the processor and ran it, found some stufff which I commented! Also looked at the whole PR, real nice work!  Super nice ! This is kind of a perfect integration with Auto API, congrats!","Copilot reviewed 11 out of 26 changed files in this pull request and generated no suggestions.
<details>
<summary>Files not reviewed (15)</summary>

* **src/transformers/configuration_utils.py**: Evaluated as low risk
* **examples/pytorch/image-classification/run_image_classification.py**: Evaluated as low risk
* **setup.py**: Evaluated as low risk
* **src/transformers/image_processing_base.py**: Evaluated as low risk
* **src/transformers/modeling_utils.py**: Evaluated as low risk
* **docs/source/en/index.md**: Evaluated as low risk
* **src/transformers/utils/__init__.py**: Evaluated as low risk
* **src/transformers/models/__init__.py**: Evaluated as low risk
* **src/transformers/models/auto/configuration_auto.py**: Evaluated as low risk
* **docs/source/en/_toctree.yml**: Evaluated as low risk
* **src/transformers/models/auto/modeling_auto.py**: Evaluated as low risk
* **src/transformers/dependency_versions_table.py**: Evaluated as low risk
* **src/transformers/utils/dummy_pt_objects.py**: Evaluated as low risk
* **src/transformers/__init__.py**: Evaluated as low risk
* **src/transformers/models/auto/image_processing_auto.py**: Evaluated as low risk
</details>


 @LysandreJik please review whenever you have bandwidth!

Some tests are failing, waiting for the update of `timm` version in CI docker images in a separate PR. This is starting to look nice!  Hey, took a quick look at the processor and ran it, found some stufff which I commented! Also looked at the whole PR, real nice work!  Super nice ! This is kind of a perfect integration with Auto API, congrats!","# What does this PR do?

Adds a TimmWrapper set of classes such that timm models can be loaded in as transformer models into the library. 

Continue of 
 - https://github.com/huggingface/transformers/pull/33687

### General Usage

```py
import torch
from urllib.request import urlopen
from PIL import Image
from transformers import AutoConfig, AutoModelForImageClassification, AutoImageProcessor

checkpoint = ""timm/resnet50.a1_in1k""
img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

image_processor = AutoImageProcessor.from_pretrained(checkpoint)
inputs = image_processor(img, return_tensors=""pt"")
model = AutoModelForImageClassification.from_pretrained(checkpoint)

with torch.no_grad():
    logits = model(**inputs).logits

top5_probabilities, top5_class_indices = torch.topk(logits.softmax(dim=1) * 100, k=5)
```


### Pipeline

Timm models can now be used in the image classification (if a classification model) and image feature extraction pipelines

```py
import torch
from urllib.request import urlopen
from PIL import Image

from transformers import pipeline

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))
pipe = pipeline(""image-classification"", model=""timm/resnet18.a1_in1k"")
print(pipe(img))
```

### Trainer

Timm models can now be loaded and trained with the trainer class. 

Example model trained with the trainer running the script command below:
https://huggingface.co/qubvel-hf/vit-base-beans

```
python run_image_classification.py \                
    --dataset_name beans \
    --output_dir ./beans_outputs/ \
    --remove_unused_columns False \
    --label_column_name labels \
    --do_train \
    --do_eval \
    --push_to_hub \
    --push_to_hub_model_id vit-base-beans \
    --learning_rate 2e-5 \
    --num_train_epochs 5 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --logging_strategy steps \
    --logging_steps 10 \
    --eval_strategy epoch \
    --save_strategy epoch \
    --load_best_model_at_end True \
    --save_total_limit 3 \
    --seed 1337 \
    --model_name_or_path timm/resnet18.a1_in1k \
    --ignore_mismatched_sizes
```

### Other features enabled

 - [x] Device map: 
 ```python
model = TimmWrapperForImageClassification.from_pretrained(checkpoint, device_map=""auto"")
 ```
 - [x] Torch dtype:
 ```python
model = TimmWrapperForImageClassification.from_pretrained(checkpoint, torch_dtype=""bfloat16"")
 ```
 - [x] Quantization:
 ```python
model = TimmWrapperForImageClassification.from_pretrained(checkpoint, load_in_4bit=True)
 ```
 - [x] Intermediate hidden states: `output_hidden_states=True` or `output_hidden_states=[1, 2, 3]` (to select specific hidden states)
```python
model = TimmWrapperForImageClassification.from_pretrained(checkpoint)
output = model(**intpus, output_hidden_states=True)
 ```
 
  - [x] Transformers TImmWrapper checkpoints are compatible with timm:
```python
model = timm.create_model(""hf-hub:qubvel-hf/vit-base-beans"", pretrained=True)
 ```
 
### TODO

 - [x] Gamma/beta renaming issue
 - [x] Update timm in CI 0.9.6 -> 1.0.11 to enable `output_hidden_states` tests
   - CI for slow-run takes longer to update images
 - [x] Weights are loaded by `transformers` instead of `timm`, which architectures are affected?
   - mostly OpenCLIP weights are affected (60-70 checkpoints), see the comment https://github.com/huggingface/transformers/pull/34564#issuecomment-2491972650
 - [x] Tests for image processor","There are 2 changes in this file:

1. state_dict keys renaming moved into separate method to be able to override it for TimmWrapper (disable gamma/beta renaming + add prefix)
2. metadata is None for timm checkpoints -> assuming these are pytorch checkpoints timm checkpoints store image processor config in `config.json` it's not clear why this is here, what it's attempting to do. timm has model specific init fns though they aren't separately callable right now, doing something like this that could overwrite timm defaults would change model behaviour don't think absence of a file is a good check Added this to initialize classifier, without this weights are not properly initialized, probably due to how the model is created in transformers

```python
from transformers import TimmWrapperForImageClassification

# --------------
# With init
# --------------

model = TimmWrapperForImageClassification.from_pretrained(""timm/resnet18.a1_in1k"", num_labels=10, ignore_mismatched_sizes=True)
print(model.timm_model.fc.weight[:3, :3])
# tensor([[-0.2117, -0.2422, -0.2540],
#         [-0.1106, -0.1856, -0.0152],
#         [-0.3430, -0.6446, -0.0530]], grad_fn=<SliceBackward0>)


# --------------
# Without init
# --------------

# patch with empty init weight function to check
def empty_init(self, module):
    pass
TimmWrapperForImageClassification._init_weights = empty_init

model = TimmWrapperForImageClassification.from_pretrained(""timm/resnet18.a1_in1k"", num_labels=10, ignore_mismatched_sizes=True)
print(model.timm_model.fc.weight[:3, :3])
# tensor([[0., 0., 0.],
#         [0., 0., 0.],
#         [0., 0., 0.]], grad_fn=<SliceBackward0>)
```

Ideally, we should get rid of this, but it's not common for transformers to load external models, so it might require more code changes. For now, its a simple fix to enable model loading with initialized classifier if shapes are mismatched. I'm not too familiar with the transformer norms, but what do you normally do in the case where you want a BaseModelOutput without pooling? Is there a normal argument you pass to enable / disable pooling? Or are we supposed to have a TimeWrapperWithPooling and TimmWrapperWithoutPooling and that's done by what class you create? People use both and imagine don't want to pool if it's not necessary for their use case... Looks like these tests are focused on models with classification heads with a classification wrapper...

Other scenarios to consider
* timm model w/ classification head used with TimmWrapperModel
* timm model w/o classification head use with TimmWrapperModel
* timm model w/o classification head use with TimmWrapperForImageClassification  (head needs to be added by wrapper)

Some models wihout heads to check
* vit_base_patch16_siglip_224.webli ... this is OpenCLIP model w/o timm config in hub though https://huggingface.co/timm/ViT-B-16-SigLIP 
* vit_small_patch16_224.dino is the `last_hidden_state` would be equivalent to no pooled output? 

where the last hidden state is:
```python
last_hidden_state, hidden_states = self.timm_model.forward_intermediates(pixel_values, **kwargs)
# or
last_hidden_state = self.timm_model.forward_features(pixel_values, **kwargs)
```

If so, I suppose we can add some flag to avoid applying `forward_head` if someone don't want to have pooler output. It can be the same`BaseModelOutputWithPooling` with `pooler_output=None` The `TimmWrapperModel` represents a model without a classification head, while the `TimmWrapperForImageClassification` includes a classification head. The wrapper itself does not add any heads, instead, the classification head is provided entirely by the original `timm` model. Such a behavior indeed is different from other `transformers` models, however, it allows load weights to classification head with no issues + make weights timm-backward-compatible. Good point regarding testing another model checkpoints and tests for `TimmWrapperModel` The `timm` models store the image processor configuration in `config.json` instead of `preprocessor_config.json`. This is why we check if another name is provided. Split and moved to methods of `PreTrainedModel` to override this block in TimmWrapper and avoid gamma/beta renaming. Added TimmWrapperModelOutput with optional pooling Really nice that there is no kwargs or whatever to load the model Ok with this This should eventually be completely removed, cc @ArthurZucker  Can you please add a comment saying that in case of no metadata, it's seen as a pytorch checkpoint You can use `CONFIG_NAME` instead I feel like there is probably a helper somewhere that would do this in a simpler manner, but this seems good to me the way it's written Given the method's objective, I would have it accept only a `pretrained_model_name` and I'd therefore remove this Good as long as we don't expect community checkpoints, but there are community checkpoints already, for example see the following: https://huggingface.co/prov-gigapath/prov-gigapath

I think we'll need a more robust check here This should be removed no? Should require timm as well Added https://github.com/huggingface/transformers/pull/34564/commits/7e0d2c6d5a32b42457ac9187a2f4d61972eee263 Indeed, this is a weak assumption. This function is only needed in the image processors auto class to load it from the config. In `AutoImageProcessor.from_pretrained` the only information we have is the model name, so the only way to make a robust check is to load files from the Hub.

I removed this function entirely and instead made a fallback for loading the timm image processor dict in the auto class of the image processor. To avoid loading the config for every model, I did it in the following way:

1) Try to load the image processor config as usual - most of the models will be fine, and we won't have any overhead here.
2) In case of an exception, try loading config.json and check if it's a timm checkpoint.

See https://github.com/huggingface/transformers/pull/34564/commits/ff6efde8d0d77baa83e9e7a264aec84618751d1b for details. I documented it in the code, let me know if you have doubts about this approach.

Works with
```python
image_processor = AutoImageProcessor.from_pretrained(""prov-gigapath/prov-gigapath"")
``` function removed with https://github.com/huggingface/transformers/commit/ff6efde8d0d77baa83e9e7a264aec84618751d1b (see comment below) function removed with https://github.com/huggingface/transformers/commit/ff6efde8d0d77baa83e9e7a264aec84618751d1b (see comment below) Thanks, removed in https://github.com/huggingface/transformers/pull/34564/commits/a476610cb5cc71e54975c47989d0ad1b680acfab Added in https://github.com/huggingface/transformers/pull/34564/commits/327095a8e120ff1dbbe06dbaf501fcdee8c6aa42"
34858,2024-11-21T14:16:33Z,2024-12-11T10:16:52Z,Cyrilvallez,4,6,11,899,31,2,1,[],2002.0,0,1718809.0,0,0,0,0,1326473.632498,,0,11,0,False,"['HuggingFaceDocBuilderDev', 'Cyrilvallez', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34858). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker should be good to go! Looks like there is a regression?

https://app.circleci.com/pipelines/github/huggingface/transformers/113239/workflows/d8800ea1-6599-4640-95cb-e9c558496c41/jobs/1512452 True! Aria relies on Llama and was merged in the meantime - https://github.com/huggingface/transformers/pull/35208 correctly reapplies new Llama change to AriaLet's make sure we only remove where it is needed, make mandatory and should be good to go! Nice, btw one thing we can include: default rope initialization should be in the function itself! ","Let's make sure we only remove where it is needed, make mandatory and should be good to go! Nice, btw one thing we can include: default rope initialization should be in the function itself! ","# What does this PR do?

This cleans-up the (expired) deprecated cycle for the rotary embeddings and fully move them to the Model instead of the Attention. Also removes deprecated EmbeddingClasses, and fix the tests accordingly.","should be mandatory! 
```suggestion
         position_embeddings: Tuple[torch.Tensor, torch.Tensor]
``` Unwanted change I think we did not have a deprecation cycle here! Nop, this is actually doing nothing (never used, no purpose here) so I cleaned it up! Indeed not linked to the rotary though, but as I'm cleaning stuff, let's remove it as the same time no changing the order is a breaking change!"
34850,2024-11-21T09:44:43Z,2024-12-11T10:03:31Z,zucchini-nlp,2,0,1,1,1,1,1,[],1557.0,0,1729128.0,0,0,0,0,1332464.709383,,1,1,0,False,"['HuggingFaceDocBuilderDev', 'zucchini-nlp']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34850). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Guess it can be merged without core maintainer's review, since it is a tiny change and approved by MarcLGTM ! ",LGTM ! ,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34681",
35139,2024-12-07T14:29:00Z,2024-12-10T23:16:37Z,HMJ0628,11,6,15,718,3,3,1,[],180227.0,0,290858.0,0,0,0,0,1371278.57244,,0,15,0,False,"['HMJ0628', 'stevhliu', 'HuggingFaceDocBuilderDev', 'Rocketknight1']","cc @stevhliu  Hi, no need to keep updating your branch! > Hi, no need to keep updating your branch!

Excuse me，what should I do to ？
 Should I pull request again？ You don't have to do anything! 🤗  Let the CI test below finish running and then I'll merge it for you! sorry，I have added agents to the toctree. Great! Last step to resolve the error is to remove the `transformers_agents.md` file (this is outdated now) and replace it in the toctree with your new `agents` file OK, I have finished it. > Last step to resolve the error is to remove the transformers_agents.md file (this is outdated now)

You still need to remove the transformers_agents.md file https://github.com/HMJ0628/transformers/blob/main/docs/source/zh/transformers_agents.md from your branch sorry, I have removed the file. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35139). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Very nice, thank you! As a reminder, you'll also need someone to review the translated content (I can't read Chinese) 🤗  Hello, can you add `agents` to the toctree please? Thanks for your translation! 🤗 ","Very nice, thank you! As a reminder, you'll also need someone to review the translated content (I can't read Chinese) 🤗  Hello, can you add `agents` to the toctree please? Thanks for your translation! 🤗 ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

I have translated agents.md to Chinese. Please let me know if there are any corrections. Thank you very much!

issue:[i18n-<languageCode>] Translating docs to Chinese Translate agents.md into Chinese #35135
Fixes # [(issue)](https://github.com/huggingface/transformers/issues/35135)


## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Would you mind also translating the warning (just the content and not the syntax)?

```md
> [!WARNING]
> Chinese translation here
``` This should also be translated ```suggestion
- 语音转文本：给定一个人讲述的音频录音，将其转录为文本（Whisper）
``` The tip hould also be translated Thank you for reminding me. I have translated these contents, and I will find someone else to help me check them. The translation works for me!
It's of real convenience to get the contents."
35188,2024-12-10T20:21:14Z,2024-12-10T23:16:01Z,johngrahamreynolds,1,0,3,9,1,1,1,[],5855.0,0,10488.0,0,0,0,0,1371315.580561,,0,3,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35188). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM, thanks for the fix!","LGTM, thanks for the fix!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR fixes issue #35174 related to an incorrect numeric definition of the Nvidia compute capability equal to and above which users will see the expedited benefit of tensor cores when passing the `pad_to_multiple_of` argument to any of the following data collators: {`DataCollatorWithPadding`, `DataCollatorForTokenClassification`, `DataCollatorForSeq2Seq`, `DataCollatorForLanguageModeling`}.

The changes update the relevant docstrings of the classes above in the `src/transformers/data/data_collator.py` file exactly as described in the original issue.


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests? - N/A


## Who can review?
@stevhliu

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35053,2024-12-03T03:36:13Z,2024-12-10T09:11:45Z,strangiato,5,0,1,6,1,1,1,[],25326.0,0,675307.0,0,0,0,0,1371597.733739,,0,1,0,False,"['qubvel', 'HuggingFaceDocBuilderDev', 'strangiato']","Hey @strangiato, for llama vision models the file is `consolidated.pth`

see 
https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct/tree/main/original It seems there is a discrepancy between the files that is published on HuggingFace for the .pth format, and the files published on meta directly (https://www.llama.com/llama-downloads/)

If you download the files directly form meta they are using `consolidated.00.pth` not `consolidated.pth`. Ok, then it's better to support both, smth like
```pyhton
path = ""consloidated.pth"" if os.path.exists(""consloidated.pth"") else ""consloidated.00.pth""
``` Updated the code to check if the 00.pth file exists and falls back to the non-numbered one in a similar what to what you suggested. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35053). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM, thanks !","LGTM, thanks !","# What does this PR do?

mllama conversion with shard_num 1 expects the file to be `consolidated.pth` but the Llama 3.2 11B uses `consolidated.00.pth` instead.

Fixes #35049 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel


",
33027,2024-08-22T20:19:48Z,2024-12-10T17:08:27Z,AhmedAlmaghz,5,22,32,464,5,2,2,[],8830156.0,0,9502944.0,1,0,0,0,1382947.469892,,0,32,0,False,"['HuggingFaceDocBuilderDev', 'AhmedAlmaghz', 'abodacs']","أخي عبدالله @abodacs  هذا ما تم إنجازه في هذا القسم 
إلى الامام إن شاء الله
هذا آخر ملف في هذا القسم



# DEVELOPER GUIDES

- [x]  Use fast tokenizers from 🤗 Tokenizers #33034
- [x]  Run inference with multilingual models #33048
- [x]  Use model-specific APIs #33030
- [x]  Share a custom model #33031
- [x]  Templates for chat models #33026
- [x]  Trainer #33080
- [x]  Run training on Amazon SageMaker #33071
- [x]  Export to ONNX #33072
- [x]  Export to TFLite #33077
- [x]  Export to TorchScript #33079
- [x]  Benchmarks #33023
- [x]  Notebooks with examples #33049
- [ ]  Community resources #33027
- [x]  Troubleshoot #33017
- [x]  Interoperability with GGUF files #33037
 @AhmedAlmaghz 
تسلم
 ممكن تلقى نظرة على
 
https://huggingface.co/docs/transformers/main/en/tiktoken
https://huggingface.co/docs/transformers/main/en/modular_transformers
https://huggingface.co/docs/transformers/main/en/how_to_hack_models

اعتقد انهم ليس لهم ترجمة > @AhmedAlmaghz تسلم ممكن تلقى نظرة على
> 
> https://huggingface.co/docs/transformers/main/en/tiktoken https://huggingface.co/docs/transformers/main/en/modular_transformers https://huggingface.co/docs/transformers/main/en/how_to_hack_models
> 
> اعتقد انهم ليس لهم ترجمة

تم إضافة ترجمة الملفات في طلب السحب هذا
الرجاء قم بمراجعتها.

هذه الملفات جديدة تم إضافتها حديثاً

اشكرك اخي عبدالله @abodacs على ملاحظاتك  القيمة.

اتمنى لك التوفيق Thank you @abodacs  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33027). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.السﻻم عليكم

تمت المراجعة جزاك الله خير

@AhmedAlmaghz  LGTM, thanks!","السﻻم عليكم

تمت المراجعة جزاك الله خير

@AhmedAlmaghz  LGTM, thanks!","
## What does this PR do?
Translated the `docs/source/ar/community.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ","```suggestion
تُجمّع هذه الصفحة الموارد المتعلقة بـ 🤗 Transformers التي طوّرها المجتمع.
``` ```suggestion
# مجتمع المطورين
``` ```suggestion
# كيفية تعديل أي نموذج من نماذج Transformers
``` ```suggestion
توفر مكتبة [🤗 Transformers](https://github.com/huggingface/transformers) مجموعة من النماذج المسبقة التدريب والأدوات لمعالجة اللغات الطبيعية، والرؤية، وما إلى ذلك. على الرغم من أن هذه النماذج تغطي مجموعة واسعة من التطبيقات، فقد تواجه حالات استخدام لا تدعمها المكتبة بشكل افتراضي. يُمكن للتخصيص أن يفتح إمكانيات جديدة، مثل إضافة طبقات جديدة، أو تعديل البنية المعمارية، أو تحسين آليات الانتباه. سيُوضح لك هذا الدليل كيفية تعديل نماذج Transformers الموجودة لتلبية احتياجاتك المحددة. الشيء الرائع هو أنك لست بحاجة إلى الخروج من إطار عمل Transformers لإجراء هذه التغييرات. ي يمكنك تعديل النماذج مباشرةً في Transformers والاستفادة من الميزات مثل [واجهة برمجة التطبيقات Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer)، و [PreTrainedModel](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel)، والضبط الدقيق الفعال باستخدام أدوات مثل [PEFT](https://huggingface.co/docs/peft/index).
``` ```suggestion
سنرشدك في هذا الدليل  لكيفية تخصيص نماذج Transformers الموجودة لتلبية متطلباتك، دون فقدان مزايا الإطار. ستتعلم كيفية:
``` ```suggestion
نموذج **Segment Anything (SAM)** هو نموذج رائد في مجال تجزئة الصور. في تنفيذه الافتراضي، يستخدم SAM إسقاطًا مجمعًا للاستعلام والمفتاح والقيمة (`qkv`) في آلية الانتباه الخاصة به. ومع ذلك، قد ترغب في ضبط مكونات محددة فقط من آلية الانتباه، مثل إسقاطات الاستعلام (`q`) والقيمة (`v`)، لتقليل عدد المعلمات القابلة للتدريب والموارد الحسابية المطلوبة.
``` ```suggestion
من خلال تقسيم الإسقاط المجمع `qkv` إلى إسقاطات منفصلة `q` و `k` و `v`، يمكنك تطبيق تقنيات مثل **LoRA** (Low-Rank Adaptation) على إسقاطي `q` و `v` فقط. يسمح لك هذا بما يلي:

- ضبط عدد أقل من المعلمات، مما يقلل من العبء الحسابي.
- تحقيق أداء أفضل من خلال التركيز على مكونات محددة.
- تجربة استراتيجيات تعديل مختلفة في آلية الانتباه.
``` ```suggestion
- **الإسقاطات المنفصلة:** يتم إزالة الإسقاط المُجمع `qkv`، وإنشاء إسقاطات خطية منفصلة `q` و `k` و `v`.
- **دالة استدعاء  تحميل الأوزان:** تقوم طريقة `_split_qkv_load_hook` بتقسيم أوزان `qkv` المسبقة التدريب إلى أوزان `q` و `k` و `v` منفصلة عند تحميل النموذج. يضمن هذا التوافق مع أي نموذج مسبق التدريب.
- **التنفيذ الأمامي:** يتم حساب الاستعلامات والمفاتيح والقيم بشكل منفصل، وتستمر آلية الانتباه كالمعتاد.
```





  ```suggestion
#### **الخطوة 2: استبدال فئة الانتباه الأصلية**
``` ```suggestion
## المساهمة بابداعاتك الخاصة
``` ```suggestion
إذا قمت بتطوير تعديﻻتك الخاصة لنماذج Transformers وترغب في مشاركتها، ففكر في المساهمة في هذه الوثيقة.
``` ```suggestion
مكتبة `transformers` هي إطار عمل ذو فلسفة محدد؛ يتم تعريف فلسفتنا في [الدليل المفاهيمي](./philosophy).

جوهر هذه الفلسفة يتمثل في مبدأ [نموذج واحد، ملف واحد](https://huggingface.co/blog/transformers-design-philosophy)
في المكتبة. الجانب السلبي لهذا المكون هو تقييده لوراثة واستيراد مكونات الملفات.
 ```suggestion
نتيجة لذلك، تتكرر مكونات النموذج عبر العديد من الملفات. يحتوي `transformers` على عدد كبير من طبقات الانتباه، يقارب عدد النماذج، والكثير منها متطابق.  يتسبب هذا في تباعد عمليات التنفيذ المستقلة مع تطبيق الإصلاحات والتغييرات.
``` ```suggestion
ولمعالجة ذلك، اعتمدنا مفهوم ""النسخ"" في المكتبة.  فبإضافة تعليق يُشير إلى أن التعليمات البرمجية هي نسخة من أخرى، نضمن من خلال أنظمة  CI والأوامر المحلية عدم تباعد النسخ.  لكن هذه العملية، رغم بساطتها، تُسبب إرهاقاً.  كما أنها تزيد العبء على المساهمين، وهو ما نهدف إلى تجاوزه.
``` ```suggestion
غالباً ما تتطلب مساهمات النماذج إضافة تعليمات برمجية (حوالي 1000 سطر)، ومعالج (حوالي 500 سطر)، واختبارات، ووثائق، إلخ. ونادراً ما تقل مساهمات النماذج عن 3000-5000 سطر من التعليمات البرمجية،  معظمها أكواد نمطية.  هذا يرفع مستوى  المساهمات،

ونهدف مع المحولات النمطية إلى خفض هذا المستوى إلى حدّ مقبول.
```
 ```suggestion
تُبسط أداة ""linter"" الوراثة، مُنشئةً جميع الملفات المفردة من الملف النمطي، مع الحفاظ على شفافيتها أمام مستخدمي Python. حاليًا، تُبسط الأداة مستوىً واحدًا من الوراثة
```   ```suggestion
- إذا ورثت فئة التكوين من فئة أخرى وأضافت/حذفت معامل، فسيتم إما الإشارة إلى الملف المولد مباشرةً
  (في حالة الإضافة) أو إزالته تمامًا (في حالة الحذف).
```


 ```suggestion
- إذا ورثت فئة من فئة أخرى، على سبيل المثال: `class GemmaModel(LlamaModel):`، تُستنتج التبعيات تلقائيًا
  سيتم استنتاج جميع الوحدات الفرعية تلقائيًا من الفئة الأصلية.
```
 ```suggestion
- إذا قمت بتعريف وظائف جديدة في الملف `modular` واستخدمتها داخل الفئات، فستستنتج أداة linter ذلك تلقائيًا

يجب أن تكون قادرًا على كتابة كل شيء (المجزىء اللغوي، ومُعالِج الصور، والنموذج، والتكوين) في الملف `modular`، وسيتم إنشاء الملفات المُقابلة تلقائيًا.
``` ```suggestion
هنا مثال سريع باستخدام BERT و RoBERTa. النموذجان مرتبطان ارتباطًا وثيقًا: يختلف تنفيذهما النموذجي في طبقة تضمين.

بدلاً من إعادة تعريف النموذج بالكامل، إليك كيف يبدو ملف `modular_roberta.py` لفئات النمذجة والتكوين (لأغراض المثال، يتم تجاهل المجزىء اللغوي في هذا الوقت حيث أنه مختلف جدًا).

```

  ```suggestion
من أجل تحميل ملفات `tiktoken` في `transformers`، تأكد من أن ملف `tokenizer.model` هو ملف tiktoken وسيتم تحميله تلقائيًا عند التحميل `from_pretrained`. إليك كيفية تحميل مجزىء لغوي ونموذج، والذي
``` ```suggestion
## إنشاء مجزىء لغوي tiktoken
```"
35171,2024-12-09T16:41:09Z,2024-12-10T19:36:25Z,stevhliu,1,0,1,10,5,1,1,[],1754.0,0,96920.0,0,0,0,0,1384490.543272,,1,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35171). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks!,Thanks!,Fixes broken links to FlashAttention,
34537,2024-10-31T15:18:47Z,2024-12-05T12:46:29Z,eustlb,1,17,27,198,4,4,2,['run-slow'],1662.0,0,3465988.0,0,0,0,0,1389965.554088,,0,27,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34537). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Justifications of the changes 🤗 LGTM @eustlb ! Thanks for working on this!

It's quite surprising to me that the 2nd case wasn't handled well by the current integration! However, the proposed solution sounds ok, as long as it pass the new integration tests! Great PR, Though you explained I am a bit outside the loop here 😉 sorry for the delay, good work!","Justifications of the changes 🤗 LGTM @eustlb ! Thanks for working on this!

It's quite surprising to me that the 2nd case wasn't handled well by the current integration! However, the proposed solution sounds ok, as long as it pass the new integration tests! Great PR, Though you explained I am a bit outside the loop here 😉 sorry for the delay, good work!","# What does this PR do?

Fixes #34472

## What's happening

A special case in timestamp offsets was not handled.
When predicting timestamps, Whisper follows two strategies (see [here](https://github.com/openai/whisper/blob/5979f03701209bb035a0a466f14131aeb1116cbb/whisper/transcribe.py#L367C17-L375C62)):
1. single timestamp at the end: predicted sequence ends with <|t1|><eos> → no speech after t1, seek to* end of the 30sec segment
2. double timestamp at the end: predicted sequence ends with <|t1|><|t2|><eos> →  seek to* t1 

>*Note: Whisper works on 30sec windows of audio. Above ""seek to"" means sliding this 30sec window to a new start position.

Case 1 is correctly handled in `_retrieve_segments` that is responsible of this seeking process during generation, making the generated segments correct (see snippet below) while it is not correctly handled in the tokenizer. 

<details>
  <summary>Snippet 🔧</summary>
  
```python
from datasets import load_dataset
from transformers import WhisperForConditionalGeneration, AutoProcessor
import numpy as np

# load model + processor
processor = AutoProcessor.from_pretrained(""openai/whisper-small.en"")
model = WhisperForConditionalGeneration.from_pretrained(""openai/whisper-small.en"")

# load dataset
dataset = load_dataset(""distil-whisper/librispeech_long"", ""clean"", split=""validation"")
sample = dataset[0][""audio""][""array""]
sampling_rate = dataset[0][""audio""][""sampling_rate""]

sample = [*sample[:15 * sampling_rate], *np.zeros(16 * sampling_rate).tolist(), *sample[15 * sampling_rate:]]
sample = np.array(sample)

# pre-process
inputs = processor(
    sample,
    sampling_rate=16_000,
    padding=""longest"",
    truncation=False,
    return_attention_mask=True,
    return_tensors=""pt"",
)

# inference
output = model.generate(**inputs, return_timestamps=True, return_segments=True)

# this is correct
print(""="" * 10, ""this is correct"", ""="" * 10)
for seg in output[""segments""][0]:
    print(f""{seg['start'].item():.2f} -> {seg['end'].item():.2f}: {processor.decode(seg['tokens'])}"")

# this is wrong
# pass token ids to processor's decode method
print(""="" * 10, ""this is wrong"", ""="" * 10)
result = processor.batch_decode(output[""sequences""], skip_special_tokens=True, output_offsets=True)
print(""\n"".join([f""{chunk['timestamp'][0]:.2f} -> {chunk['timestamp'][1]:.2f} : {chunk['text']}"" for chunk in result[0][""offsets""]]))
```

Returns:
```
========== this is correct ==========
0.00 -> 6.38:  Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.
6.38 -> 11.32:  Nor is Mr. Quilter's manner less interesting than his matter.
11.32 -> 15.00:  He tells us that at this festive season of the year,
30.00 -> 36.76:  With Christmas and roast beef looming before us, similes drawn from eating and its results
36.76 -> 39.80:  occur most readily to the mind.
39.80 -> 45.38:  He has grave doubts whether Sir Frederick Layton's work is really Greek after all and
45.38 -> 49.00:  can discover in it but little of rocky Ithaca.
49.00 -> 56.28:  Lenell's pictures are a sort of up-guards-and-atom paintings, and Mason's exquisite ittles
56.28 -> 64.12:  are as national as a jingo poem. Mr. Burkett fosters landscape's smile at one much in
64.12 -> 70.76:  the same way that Mr. Karker used to flash his teeth. And Mr. John Collier gives his
70.76 -> 77.16:  sitter a cheerful slap on the back before he says, like a shampoo or in a Turkish bath,
77.16 -> 78.16:  Next Man
========== this is wrong ==========
0.00 -> 6.38 :  Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.
6.38 -> 11.32 :  Nor is Mr. Quilter's manner less interesting than his matter.
11.32 -> 15.00 :  He tells us that at this festive season of the year,
15.00 -> 21.76 :  With Christmas and roast beef looming before us, similes drawn from eating and its results
21.76 -> 24.80 :  occur most readily to the mind.
24.80 -> 30.38 :  He has grave doubts whether Sir Frederick Layton's work is really Greek after all and
30.38 -> 34.00 :  can discover in it but little of rocky Ithaca.
34.00 -> 41.28 :  Lenell's pictures are a sort of up-guards-and-atom paintings, and Mason's exquisite ittles
41.28 -> 49.12 :  are as national as a jingo poem. Mr. Burkett fosters landscape's smile at one much in
49.12 -> 55.76 :  the same way that Mr. Karker used to flash his teeth. And Mr. John Collier gives his
55.76 -> 62.16 :  sitter a cheerful slap on the back before he says, like a shampoo or in a Turkish bath,
62.16 -> 63.16 :  Next Man
```
</details>

## Changes 

### on the existing
Most important change here is the one in `_retrieve_segments`. When finishing generation with <|t1|><|t1|> (case 2), we **must** add it to the tokens of the returned segment. Indeed, otherwise, when we will decode the concatenated sequence, we won't have any way to differentiate a single ending token (case 1) from a double ending token. 

### added 
A new `test_small_longform_timestamps_generation` test since this edge case was not catched before. IMO it's an important one to add since we can compare directly with OAI's expected output so it's pretty robust. Code to reproduce the expected output can be found [here](https://github.com/eustlb/reproduce-whisper-expected-outputs/blob/main/test_small_longform_timestamps_generation.py). This new test catch an edge case we did not test before: single timestamp ending (here at 15.0s), triggering seeking to end of segment (here 30.0s). Moreover, we test here both the segments through `generated_ids[""segments""]` that are built at generation time and the offsets with `processor.batch_decode(generated_ids[""sequences""], skip_special_tokens=True, output_offsets=True)` that are reconstructed from the concatenated segments.
 



","Original Whisper code base does such computations in float64. We need to ensure we do the same, especially wince we are comparing in the tests with the original Whisper outputs.  That's the only way to know latter that we have a double token ending segment It's done this way in the original codebase: summing float64 after the multiplication with the position. We avoid this way annoying floating-point arithmetic issues. This new test catch an edge case we did not test before: single timestamp ending (here at 15.0s), triggering seeking to end of segment (here 30.0s). Moreover, we test here both the segments through `generated_ids[""segments""]` that are built at generation time and the offsets with `processor.batch_decode(generated_ids[""sequences""], skip_special_tokens=True, output_offsets=True)` that are reconstructed from the concatenated segments. Are `token_ids` always in ascending order ? In that case, don't we always have `token_ids[i - 1] >= timestamp_begin and token_ids[i - 2] >= timestamp_begin` when entering the `if token >= timestamp_begin` loop, if i>=2 ? (nit) let's separate for readability
```suggestion
                idx_sliced_tokens = -1 if not is_last_slice or single_timestamp_ending else -2
                end_timestamp_pos = sliced_tokens[idx_sliced_tokens].item() - timestamp_begin
``` This looks a bit costly, I wonder if there's a cleaner way to compute all of this! 

Why do we need to get rid of the last timestamp when preparing the input ids ? So you're offsetting the last timestamp by one when the last two tokens are timestamp ? let's say we have `[..., T1, T2]`, you're doing `[..., T1, T2+1]` ? Aren't we supposed to always have two subsequent timestamp tokens now that you've dealt with single timestamp token in the generation file ? `slices` is a list of indexes along the generated sequence of tokens `seek_sequence`. Let's say we have 100 tokens and it was not single ending, meaning last two tokens are timestamps `[..., T1, T2]`. For this reason, `slices[-1] = 99` yet when we will slice after the segments with `seek_sequence[last_slice:current_slice]`, we want to make sure we include T2 in the slice (so that the we further know it is a double timestamp ending segment as explained in the PR's comment) →  by adding 1 to last slice, we ensure last iteration will slice 'seek_sequence[last_slice:100]` and that T2 will get included Not necessarily. After concatenating all the generated sequences for each 30sec segment and at the tokenizer decoding phase, we have two possibilities:
1. `[..., <t1>, <t2>, <0.0>, ...] `(last segment was double timestamp ending)
2. `[..., <t1>, <0.0>, ...] `(last segment was single timestamp ending)

Note that the only reason we can differentiate those two cases here is thanks to the above `slices[-1] += 1` that ensures `<t2>` is included when we are not single timestamp ending. So in 2. we have `token >= timestamp_begin` (the `<0.0>`), `token_ids[i - 1] >= timestamp_begin `but not `token_ids[i - 2] >= timestamp_begin` As mentioned above, we need to include the last timestamp in the case of double-ending timestamps (and not only the penultimate as done before) to enable the tokenizer to differentiate the two cases (single and double ending). Nevertheless, OAI does not have to worry about that because they don't concatenate all the tokens as we do, and for this when conditioning on the previous token, the last token in the case of double ending is omitted. To ensure we do the exact same, we need to remove it when preparing the `decoder_input_ids`. The issue is that any segment can be a double-ending one, they all need to be checked. Looks like it is not that costly (complexity is` O(batch_size * max number of segments)`). When testing, it has no measurable impact on inference speed (see long-form results [here](https://github.com/huggingface/transformers/pull/34135#issue-2584061232), our implem is on par with OAI's).  not sure if relevant but each call to `.item()` will do a device synch, maybe calling `.item()` once after loop will be better?  totally relevant, thanks !! This was unnecessary and is fixed in [5fba3e0](https://github.com/huggingface/transformers/pull/34537/commits/5fba3e0f0d9b422ce7b1aae9a527211749b89fce) I don't know if this is the right place or if there's an open issue, but this line now crashes on macs / the mps backend because float64 is not supported on apple silicon. I had to downgrade to 4.46.3 - is there some other way to fix this?"
35133,2024-12-07T10:56:20Z,2024-12-10T17:19:30Z,GaetanLepage,2,0,1,4,1,2,1,[],189021.0,0,284540.0,0,0,0,0,1390361.581327,,0,1,0,False,"['GaetanLepage', 'Rocketknight1']","cc @kwen2501 @ArthurZucker - I believe this is caused by the imports added in https://github.com/huggingface/transformers/pull/34184 > cc @kwen2501 @ArthurZucker - I believe this is caused by the imports added in #34184

Yes, indeed. We vendored a simple patch in nixpkgs: https://github.com/NixOS/nixpkgs/pull/362768/files.
It simply skip the import on darwin and this is good enough for us now.
All the downstream packages depending on `transformers` seem to work fine.LGTM I'll add it to the patch! Looks good, thanks!","LGTM I'll add it to the patch! Looks good, thanks!","# What does this PR do?

`torch.distributed` is sometimes unavailable (on some platforms such as darwin).
It leads to `transformers.models.auto.modeling_auto` to fail to import with:
```
ModuleNotFoundError: No module named 'torch._C._distributed_c10d'; 'torch._C' is not a package
```

I propose to implement a similar logic as in https://github.com/huggingface/accelerate/pull/2121 to load `torch.distributed` conditionally on its availability.
I was able to test the patch in [nixpkgs](https://github.com/NixOS/nixpkgs/) successfully.

Fixes #35129


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35137,2024-12-07T14:06:19Z,2024-12-10T17:58:47Z,asdkfjsd,4,2,5,379,2,2,1,[],181606.0,0,273149.0,0,0,0,0,1390353.90003,,0,5,0,False,"['HMJ0628', 'HuggingFaceDocBuilderDev', 'Rocketknight1', 'asdkfjsd']","cc @stevhliu  I have removed all the English original text (which was previously kept as comments in the document) and refined some of the Chinese expressions. Any suggestions or corrections are welcome. The translation is good and the meaning is clear. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35137). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the translation. Overall, I think it looks good, but its just a little difficult to read with both English/Chinese content.

Can you also find another contributor to review the translated content (I can't read Chinese)? Thanks for your contribution!","Thanks for the translation. Overall, I think it looks good, but its just a little difficult to read with both English/Chinese content.

Can you also find another contributor to review the translated content (I can't read Chinese)? Thanks for your contribution!","# What does this PR do?

I have translated Benchmarks.md to Chinese.  Please let me know if there are any corrections. Thank you very much!
According to the context, I translated  'Benchmark best practices' as 'Recommended Strategies for Benchmarking' in Chinese .

issue: [i18n-<languageCode>] Translating Benchmarks.md to Chinese #35134
Fixes # [https://github.com/huggingface/transformers/issues/35134](url)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

",I think removing all the English content here would make it easier to read Thank you for your advice. I will make improvements soon.
35181,2024-12-10T09:33:28Z,2024-12-10T17:08:56Z,henryhmko,2,0,2,16,6,1,1,[],22182.0,0,27328.0,0,0,0,0,1393346.59423,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']",cc @stevhliu ! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35181). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for all the fixes!,Thanks for all the fixes!,"# What does this PR do?

1. Fixes text typos in...
 - PEFT (`Tutorials`)
 - Automatic Speech Recognition (`Audio`)

2. Fixes code typos in...
 - Translation (`NLP`)
 - Summarization (`NLP`)
 - Question Answering (`NLP`)
 - Multiple Choice (`NLP`)



## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@stevhliu

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35060,2024-12-03T13:56:57Z,2024-12-10T15:30:10Z,MekkCyber,1,0,2,2,1,1,1,[],1642.0,0,610395.0,0,0,0,0,1399271.573576,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35060). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM ! ,LGTM ! ,"# What does this PR do?
Before the fix, there was a hidden issue where the `use_qkv_bias` parameter was not found in the configuration when using the GGUF format. As a result, the modeling code in transformers mistakenly assumed that there was no bias in the QKV layers.

## Who can review ?
@SunMarc @LysandreJik ",
35177,2024-12-10T04:50:34Z,2024-12-10T14:31:22Z,hgt312,0,0,1,2,1,1,1,[],,0,34848.0,0,0,0,0,1402801.969759,,0,1,0,False,[],"Yes, this is a much more correct init for layernorm. Thank you!","Yes, this is a much more correct init for layernorm. Thank you!","# What does this PR do?

Fix DBRX LayerNorm init method

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35182,2024-12-10T12:10:32Z,2024-12-10T13:52:20Z,xadupre,1,0,1,2,2,1,1,[],7741.0,0,7743.0,0,0,0,0,1403509.585458,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35182). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.This analysis makes sense, yes. I suspect the original intent was that the line would actually do `_masked_fill(~attention_mask, 0)`, which would ensure that all masked positions were fully set to 0, and didn't accidentally survive the first mask step as a tiny but non-zero value.

I agree that the line does nothing, and that the precaution is unneeded though, so thank you for the PR!","This analysis makes sense, yes. I suspect the original intent was that the line would actually do `_masked_fill(~attention_mask, 0)`, which would ensure that all masked positions were fully set to 0, and didn't accidentally survive the first mask step as a tiny but non-zero value.

I agree that the line does nothing, and that the precaution is unneeded though, so thank you for the PR!","# What does this PR do?

Both models Deberta and DebertaV2 executes one instruction with no impact on the result. It is the last line among the 

```python
attention_scores = attention_scores.masked_fill(~(attention_mask), torch.finfo(query_layer.dtype).min)
# bsz x height x length x dimension
attention_probs = nn.functional.softmax(attention_scores, dim=-1)
attention_probs.masked_fill(attention_mask, 0)  # no impact on what follows
```

The line can be removed safely. If we replace it by one inplace modification ``attention_probs.masked_fill(attention_mask, 0)`` then the attention_probs will be null because:

* line 1: replace all False mask values by - infinity
* line 2: compute softmax, then the replaced values become 0
* line 3: if modified with inplace modification, it would replace all True mask value by 0

At the end, the output tensor would be null. No unit test was added because the numerical results are not impacted.

Fixes #35162.",
27539,2023-11-16T14:08:45Z,2023-12-17T10:08:04Z,poedator,30,14,14,164,2,3,1,[],334130.0,0,33696477.0,0,0,0,0,1403684.518315,,0,14,0,False,"['Codys12', 'jpgard', 'PhilJd', 'UniverseFly', 'shentianxiao', 'poedator', 'ArthurZucker', 'patrickvonplaten', 'KexinFeng']","Generally, I don't have a problem with allowing to pass 4D attention masks! @poedator can you explain your use case a little bit for why you want to pass 4d attention masks?  @patrickvonplaten 
here is a use example:
Suppose one does beam search and has a starting prefix with tokens `11 22 33` in 4 beams. Now he needs to check candidates with tokens 44, 55, 66, and 77. Present code would pack the beams into a batch of shape (4, 4):

```
11 22 33  44
11 22 33  55
11 22 33  66
11 22 33  77
```
and run it with mask of all ones, passing such mask in 2D which gets expanded internally to 4D.

The proposed way would be to have a batch shaped (1, 7):
`11 22 33  44 55 66 77`
and the 4d mask would have a shape (1, 1, 4, 7) and look like this:
```
1  1  1  1  0  0  0 
1  1  1  0  1  0  0 
1  1  1  0  0  1  0 
1  1  1  0  0  0  1

with a positions tensor of [0, 1, 2, 3, 3, 3, 3]
```

At subsequent beam search iterations the mask will reflect which past tokens should the new tokens attend to.
Such mask needs to pass intact. 
This saves memory for past_key_values cache and thus allows beam search and other similar inference (like SpecInfer) of longer sequences with limited VRAM.

Another use case is kindly proposed by @UniverseFly below. Very interesting PR! Would this feature also enable SFT packing as mentioned in https://github.com/huggingface/trl/issues/805?


![](https://user-images.githubusercontent.com/26831266/272305004-93c690a8-7e9b-40ad-885f-d530996aa109.png)
 > Very interesting PR! Would this feature also enable SFT packing as mentioned in [huggingface/trl#805](https://github.com/huggingface/trl/issues/805)?
Sure it would. Just have a separate packing function somewhere - it is beyond the scope of this PR. 
Besides, one should be able to pack multiple series of sequences into a batch this way. 
 I tried this branch and the `model.forward` seems to work fairly well, but `model.generate` raises errors with the 4D attention mask (with Llama). After some checking, it might be due to the missing logic here:

https://github.com/huggingface/transformers/blob/53a7e7750ff088ffbd7d96c5aeed122cc96b6866/src/transformers/models/llama/modeling_llama.py#L1087-L1124 Generate looks like a harder challenge for your methods - each individual sequence will be expanding, thus you'd need to reorder past_kv and mask at each step. I believe that to implement it, you'd need to write custom `prepare_inputs_for_generation()`, and possibly some more logic. 
I'll be happy to test drive it. 
On my side I intend to write a PR for more efficient beam search after this PR merges. Hi, @ArthurZucker  
I limited this PR only to the mask code, proceeding with the tests. 

So far I have demo in [Colab with monkey patch based on this PR.](https://colab.research.google.com/drive/1PMcLNjjjK0Zwgg6rQJdutwolfIVCZfs6?usp=sharing) It shows a negligible difference in logits obtained the old and new ways. I dent to believe that this is a rounding error somewhere. Would you support it as the basis for the tests? 
BTW, where to put this new test?

Hi, @UniverseFly ,
Try the monkey patch from the [Colab notebook](https://colab.research.google.com/drive/1PMcLNjjjK0Zwgg6rQJdutwolfIVCZfs6?usp=sharing) - see if it works to implement your idea.   Thanks for this PR and the demo. It is very helpful in trying the [SpecInfer paper](https://arxiv.org/abs/2305.09781). Also in another recent progress on speculative decoding [look ahead decoding](https://lmsys.org/blog/2023-11-21-lookahead-decoding/) Fig 5, this PR will also be useful. Reviewing now 😉  - squashed all earlier commits into one
- added tests. Made a separate class to test with full model loading.
- added support for sdpa (following https://github.com/huggingface/transformers/pull/26572)
- `test_modeling_utils.py::AttentionMaskTester` and `::TestAttentionImplementation` tests pass
- new tests pass

@ArthurZucker, please review. Hopefully it is ready to merge. @ArthurZucker, pls give me a hint about `NameError: name 'torch' is not defined` error. Apparently a decorator or import is missing, but can't figure it out. The import and decorators seem in place... > Hi, @ArthurZucker I limited this PR only to the mask code, proceeding with the tests.
> 
> So far I have demo in [Colab with monkey patch based on this PR.](https://colab.research.google.com/drive/1PMcLNjjjK0Zwgg6rQJdutwolfIVCZfs6?usp=sharing) It shows a negligible difference in logits obtained the old and new ways. I dent to believe that this is a rounding error somewhere. Would you support it as the basis for the tests? BTW, where to put this new test?
> 
> Hi, @UniverseFly , Try the monkey patch from the [Colab notebook](https://colab.research.google.com/drive/1PMcLNjjjK0Zwgg6rQJdutwolfIVCZfs6?usp=sharing) - see if it works to implement your idea.

@poedator, Did you ever get the 4D beam search working with the monkey patch in the Colab notebook? I would be very interested if you were able to get this working already! (especially depth first kv cache updating) > @poedator, Did you ever get the 4D beam search working with the monkey patch in the Colab notebook? I would be very interested if you were able to get this working already! (especially depth first kv cache updating)
Hi, @Codys12! Thank you for the interest to the PR.
I made a working implementation of memory-efficient beam search using this 4D mask. Got as high as 256 beams of 32 tokens  with Llama 7 on A100 and this is far from the limit. There is no complete demo to share, but [this gist ](https://gist.github.com/poedator/c754247f3dca8f70b710186c9bc37032)has the beam search part of my code. Hope that you find it useful.
 having a look! > Sorry checking the test there are duplicate markers 😅 not sure they are needed no?

Earlier, I got frustrated with failing commits and added decorators everywhere. Now most of them are gone and it still passes CI checks. Thanks for the contribution! 🤗  @ArthurZucker , would you want to publish a blog post in HF blog with 4d attention use cases?
I propose to include:
- memory efficient beam search (my example, from tests)
- SFT packing as mentioned in https://github.com/huggingface/trl/issues/805, suggested by @UniverseFly 
- [look ahead decoding](https://lmsys.org/blog/2023-11-21-lookahead-decoding/), suggested by @KexinFeng 
 If you want feel free to do so! 🤗  Note that not all paths of this can be `torch.compile`d:

The following fails due to `torch.all(attention_mask == 1)`.

```
import torch
import torch.nn as nn
from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask_for_sdpa

class Model(nn.Module):
    def forward(self, inputs_embeds):
        batch_size, seq_length, _ = inputs_embeds.shape
        past_key_values_length = 10
        attention_mask = torch.tensor([1.])
        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
        )
        return attention_mask

model = Model()
model = torch.compile(model, fullgraph=True)
model(torch.ones([1,5, 32]))
``` @PhilJd,
`torch.all(attention_mask == 1)` was present even before this PR.
[see this line](https://github.com/huggingface/transformers/pull/27539/files#diff-b14be70e49c04e876d7cd745948bf1bee279bc0d6f2a71b1a18e3b5aff293bd1L343)
it comes form https://github.com/huggingface/transformers/pull/26572

have you tested the preceding commit? Ah sorry, just looked at the blame - yeah, the previous commit fails as well @fxmarty . `_prepare_4d_causal_attention_mask` is applied only if `self._use_flash_attention_2` is False (https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1039). Is it because 4D attention mask and flash attention 2 are not compatible? The function description should be updated to avoid confusion as `attention_mask` is not necessarily 2D now (https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_attn_mask_utils.py#L290) @shentianxiao , thank you for your attention to the 4D attention!

> `_prepare_4d_causal_attention_mask` is applied only if `self._use_flash_attention_2` is False (https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1039). Is it because 4D attention mask and flash attention 2 are not compatible?

it is not about compatibility, rather the flash_attention_2 code contrasted original mask vs modified mask coming from `_prepare_4d_causal_attention_mask()`

> The function description should be updated to avoid confusion as attention_mask is not necessarily 2D now (https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_attn_mask_utils.py#L290)

I agree, that the original mask may also be 4d-shaped now. I just started PR https://github.com/huggingface/transformers/pull/28151 with documentation updates - will make edits there. Hopefully the maintainers responsible for `flash_attention_2` will verify it. **IMPORTANT: this PR makes changes that can only used by few classes of models**
requirements to use:
- have `position_ids` argument in `.forward()` method
- use `modeling_attn_mask_utils.py::_prepare_4d_attention_mask()` function for 4d mask generation
- 

as of 20.12.2023, only a handful (under 20) of transformers model classes meet these criteria. Most of these classes are multimodal, which may require their own use cases for 4D masks. The pure language modelling classes fit to use the 4D mask changes from this PR are only `LlamaModel`, `FalconModel` and `XGLMModel`. I made a small blog post based on this PR. 
https://huggingface.co/blog/poedator/4d-masks
Big thanks to everyone who contributed and commented! Thanks for the amazing addition!! This is a great new feature.

Just wanted to ask a question to make sure I am using it properly. In the code [here](https://github.com/poedator/transformers/blob/d80cb9823cc8b774bb4f41ac59579edca8f79ff0/src/transformers/modeling_attn_mask_utils.py#L357), it looks like the 4D masks are expected to have shape `[batch_size, 1, seq_len, seq_len].` (I am inferring that the `1` in the `expected_shape` is the heads dimension so that the same mask is broadcast to all heads.) In the [blog post](https://huggingface.co/blog/poedator/4d-masks), it describes the attention masks as having shape `[heads, batch_size, input_ids_length, total_sequence_length]`.

My question is: **are the `heads` and `batch_size` dimensions transposed in the blog post**? It seems like we are actually expected to provide 4D masks where the first axis is batch size, the second is heads. The blog post implies the reverse. Since I am sometimes using a batch size of 1 in testing, this works either way, but I want to use it correctly and don't see the ""proper"" shape documented anywhere (perhaps it is documented somewhere and I missed it!).

Thanks! @jpgard ,
you are correct, there was an error in my blog post.
Changed it to `[batch_size, heads, input_ids_length, total_sequence_length]`
thank you for raising this! Great, thanks for the quick reply and for your hard work on this @poedator !! Has this been tested with flash attention 2? Works great for me without flash attention 2, but when using flash attention I get lots of messages of the form `../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [202,0,0], thread: [105,0,0] Assertion idx_dim >= 0 && idx_dim < index_size && ""index out of bounds"" failed.`

Lower chunk of the stack trace posted below.

```
 File ""/admin/home-jpgard/miniconda3/envs/rtfm/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py"", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File ""/admin/home-jpgard/miniconda3/envs/rtfm/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/admin/home-jpgard/miniconda3/envs/rtfm/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/admin/home-jpgard/miniconda3/envs/rtfm/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py"", line 549, in forward
    attn_output = self._flash_attention_forward(
  File ""/admin/home-jpgard/miniconda3/envs/rtfm/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py"", line 592, in _flash_attention_forward
    query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(
  File ""/admin/home-jpgard/miniconda3/envs/rtfm/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py"", line 631, in _upad_input
    query_layer = index_first_axis(
  File ""/admin/home-jpgard/miniconda3/envs/rtfm/lib/python3.8/site-packages/torch/autograd/function.py"", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File ""/admin/home-jpgard/miniconda3/envs/rtfm/lib/python3.8/site-packages/flash_attn/bert_padding.py"", line 17, in forward
    return torch.gather(
RuntimeError: CUDA error: device-side assert triggered
```

Would be great to be able to use FA2 with this PR as the speedups are much larger as sequence length grows -- so FA2 seems like the perfect accompaniment to e.g. ""packed"" training sequences enabled by this PR.Generally the PR looks good to me! (We'd need some tests here). 

@ArthurZucker wdyt? Looks alright! but there should not be changes to the forward of the models (IMO) LGTM, the test should go in the https://github.com/huggingface/transformers/blob/8eae5ea3da8fa666e538df7e56d0cb41c8cea3f8/tests/test_modeling_utils.py#L1481  Thanks, just a few testing nits and good to go  Thanks for your patience! 🤗  Sorry checking the test there are duplicate markers 😅 not sure they are needed no? ","Generally the PR looks good to me! (We'd need some tests here). 

@ArthurZucker wdyt? Looks alright! but there should not be changes to the forward of the models (IMO) LGTM, the test should go in the https://github.com/huggingface/transformers/blob/8eae5ea3da8fa666e538df7e56d0cb41c8cea3f8/tests/test_modeling_utils.py#L1481  Thanks, just a few testing nits and good to go  Thanks for your patience! 🤗  Sorry checking the test there are duplicate markers 😅 not sure they are needed no? ","This is implementation for feature request from #27493 [custom 4d attention_mask as transformers .forward() argument](https://github.com/huggingface/transformers/issues/27493).

1) Allowing 4d attention masks to pass thru `_prepare_4d_causal_attention_mask()` intact
2) support in OPT (need to build custom `positions` tensor)
3) support in Llama (while Llama can accept custom `position_ids`, I added code to generate them internally)

The benefits of the code are to enable more memory-efficient text generation with tree-based parallel decoding as described in [SpecInfer paper](https://arxiv.org/abs/2305.09781)

Tagging:
@gante (generate)
@patrickvonplaten (masks)
@younesbelkada @ArthurZucker (text models)

This PR is WiP:
- Will add tests
- Need advice on how to handle models beyond covered Llama and OPT
- May add example for memory-efficient generation

**IMPORTANT: this PR makes changes that can only used by few classes of models**
requirements to use:
- have `position_ids` argument in `.forward()` method
- use `modeling_attn_mask_utils.py::_prepare_4d_attention_mask()` function for 4d mask generation
- 

as of 20.12.2023, only a handful (under 20) of transformers model classes meet these criteria. Most of these classes are multimodal, which may require their own use cases for 4D masks. The pure language modelling classes fit to use the 4D mask changes from this PR are only `LlamaModel`, `FalconModel` and `XGLMModel`.","this logic should not go here, it's should go in the prepare inputs for generation, as it's purely specific to 4d beam search.  I agree, that this should be limited to just the mask code. Makes this PR more manageable. Llama can work without that, since it can accept `position_ids` argument. Hopefully the newer models will support this argument. (could HF make it a part of some model guidelines?) let's rather use torch_device  ```suggestion
        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(self.device)
```
the smaller the better for our CI torch device should be used if this needs cuda use `    @require_torch_gpu
`  ```suggestion
        torch.testing.assert_close(
``` I observed that fp16 tests are more noisy, so what I did is:
- retained fp32 testsm but used even smaller model
- added fp16 test with relaxed tolerances
- added fp16 testing option for the top tokens order. this one requires torch @ArthurZucker 
with extra decorators all tests pass now. Pls take a look these should not be needed and are duplicated same here same here.  ```suggestion
@slow
@require_torch_gpu
```
I would expect this to be enough as torch_gpu requires torch no? "
34389,2024-10-24T17:02:06Z,2024-12-10T13:18:24Z,gallilmaimon,17,7,27,96,4,3,2,['run-slow'],69968.0,0,4047378.0,0,0,0,0,1407182.545206,,1,27,0,False,"['gallilmaimon', 'HuggingFaceDocBuilderDev', 'ylacombe', 'ArthurZucker', 'avishaiElmakies']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34389). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > The integration test you did looks good. Let's make sure to add this in the integration tests of `test_modeling_hubert.py` !

Okay, do you prefer that I do it with the textlesslib dependency in the test itself? or save the output and then just compare the output of HubertModel to the saved results from textlesslib?

> 2. This is a correct observation. Sorry that I've missed your comment about it! Would you like to open another quick PR to correct this ?

Okay, I will open a new one about this separately.

> 4. you might want to rebase your branch on the main `transformers`. If it doesn't work, you can share some logs here, so that I can help you!

I did do the rebase I think, but will try again and let you know
 > Okay, do you prefer that I do it with the textlesslib dependency in the test itself? or save the output and then just compare the output of HubertModel to the saved results from textlesslib?

You can take a look at the [test modeling file](https://github.com/huggingface/transformers/blob/1d063793318b20654ebb850f48f43e0a247ab7bb/tests/models/hubert/test_modeling_hubert.py#L752) to get inspiration, we usually compute a few stats from the expected outputs, as well as a small sequence extracted from the expected outputs
 @ylacombe Hey, I think I addressed all of your comments. Let me know if anything else is needed :) @ylacombe Hey again, just a gentle reminder about this as I would be happy to integrate as soon as possible. Thanks again for your time and feedback! @ylacombe 
sorry to bother about this. But I would really love for this change to be added. @ylacombe - Hey again, just wondering if you had a chance to go over this so we can integrate this addition. There are several projects I know of which would build on this fix. Thanks again! Let's push again the empty commit: git commit --allow-empty -m ""[run-slow] hubert"" ! I don't think it has run yet cc @ArthurZucker , could you review when you have time?

@gallilmaimon, thanks again for the work on this PR! Excited to try the new checkpoint on downstream tasks. Have you been able to run some benchmarks against other Hubert checkpoints?
 > @gallilmaimon, thanks again for the work on this PR! Excited to try the new checkpoint on downstream tasks. Have you been able to run some benchmarks against other Hubert checkpoints?

@ylacombe - My main usage is for discretising the representations and using them to train SpeechLMs and the results there seem good as expected (similar to the TWIST paper and notably better than the 50 hz), I will open-source this all hopefully very soon once it is ready!

But might also be interesting to try it for other downstream usages:) Hey @ArthurZucker any chance you had an opportunity to review this PR? Would really love to integrate this :) Thanks! Requesting @Rocketknight1's because @ArthurZucker has limited responsibility for a few days My only request is to use official checkpoint path for the test, otherwise good job and sorry for being late on this review! > My only request is to use official checkpoint path for the test, otherwise good job and sorry for being late on this review!

Hey @ArthurZucker, thanks for the review! I am not sure I understand what you mean by ""official checkpoint"" as this was only released as part of Fairseq and not HF, thus we preformed the conversion (as part of our academic lab - SLPRL) for community use. We validated that the results are identical as shown by the test. We of course give full reference and credit in the model card. I am happy to put the weights anywhere needed and merge the PR! Hey @ArthurZucker, the model hasn't been officially released and can only be found by digging deep into the fairseq repositories. Since the model card is quite clean, gives full reference, and is hosted in the organisation of an academic research lab, I believe it should be OK to keep it like this, WDYT?  

@gallilmaimon, could you add the license (MIT I think?) to the model card metadata in the meantime? I've opened [a PR](https://huggingface.co/slprl/mhubert-base-25hz/discussions/1) to do this, if it's indeed MIT-licensed  Hey @ylacombe, I approved your PR as this is in fact MIT licensed and I also have a link to the original license in fairseq GitHub! I've asked some of the people on the research paper author lists if it would be possible to transfer the checkpoint to the Meta organization. In the meantime, let's merge, thanks for your excellent work!Hi @gallilmaimon, thanks for quickly opening this PR!

The integration test you did looks good. Let's make sure to add this in the integration tests of `test_modeling_hubert.py` !

Once it's done, you can also push an empty commit to launch the slow tests CI run: `git commit --allow-empty -m ""[run-slow] hubert""` !

To address your questions:
1. I think it's okay to remove the `Copied from` statement here
2. This is a correct observation. Sorry that I've missed your comment about it! Would you like to open another quick PR to correct this ? 
3. I think you did the deepspeed integration correctly : it's only applied when using `weight_norm`
4. you might want to rebase your branch on the main `transformers`. If it doesn't work, you can share some logs here, so that I can help you!

Let me know if you have further questions ! Hey @gallilmaimon , really sorry for the late review! Thanks for integrating my comments, it looks good to me now!

Also, thanks for adding the integration tests!  It goes a little bit against our philosophy, as usually this would need a new model (because we introduce a new code path)! 
We can stray a little bit here, or we can go about this using modular but it might be an overkill!","Hi @gallilmaimon, thanks for quickly opening this PR!

The integration test you did looks good. Let's make sure to add this in the integration tests of `test_modeling_hubert.py` !

Once it's done, you can also push an empty commit to launch the slow tests CI run: `git commit --allow-empty -m ""[run-slow] hubert""` !

To address your questions:
1. I think it's okay to remove the `Copied from` statement here
2. This is a correct observation. Sorry that I've missed your comment about it! Would you like to open another quick PR to correct this ? 
3. I think you did the deepspeed integration correctly : it's only applied when using `weight_norm`
4. you might want to rebase your branch on the main `transformers`. If it doesn't work, you can share some logs here, so that I can help you!

Let me know if you have further questions ! Hey @gallilmaimon , really sorry for the late review! Thanks for integrating my comments, it looks good to me now!

Also, thanks for adding the integration tests!  It goes a little bit against our philosophy, as usually this would need a new model (because we introduce a new code path)! 
We can stray a little bit here, or we can go about this using modular but it might be an overkill!","# What does this PR do?
This issue adds support for BatchNorm instead of weight norm in the HubertModel as in https://github.com/facebookresearch/fairseq/commit/4db264940f281a6f47558d17387b1455d4abd8d9

The conversion file was also adapted to support the conversion from fairseq to HF, and was used to convert the widely used Hubert-base-25hz introduced in https://arxiv.org/abs/2305.13009

Fixes #34229

We already uploaded the converted weights to the hub at - https://huggingface.co/slprl/mhubert-base-25hz , which allows to assert the conversion worked correctly (compared to the original publication in textlesslib as follows):
```py
# Asserting that results are identical to textless original
from transformers import HubertModel
from textless.data.speech_encoder import SpeechEncoder
import torchaudio

model = SpeechEncoder.by_name(dense_model_name='mhubert-base-25hz', quantizer_model_name='kmeans', vocab_size=500, deduplicate=False, need_f0=False)
hf_model = HubertModel.from_pretrained('slprl/mhubert-base-25hz')

wav = torchaudio.load(<WAV_PATH>)[0]

torch.allclose(model(wav)['dense'], hf_model(wav, output_hidden_states=True).hidden_states[11])
``` 

@ylacombe - would love your review and specifically there were several open questions I was wondering about:
1) This means that HubertPositionalConvEmbedding is no longer a copy of transformers.models.wav2vec2.modeling_wav2vec2 - I addressed this but removing the comment, would you prefer I also change wav2vec?
2) The conversion script convert_hubert_original_pytorch_checkpoint_to_pytorch.py didn't work (before the change) for the regular hubert-base-ls960h model because of layernorm naming changes, as discussed in #26796. I didn't fix this because this felt out of scope.
3) I would love some guidance or help with deepspeed because I wasn't sure if any changes were needed to support this.
4) I also got some error when running `make fixup` which has to do with a file I haven't changed - `src/transformers/models/glm/modeling_glm.py` and I didn't manage to understand why. This also happened when running make fixup on a clean branch with no changes at all so would appreciate any help.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@ylacombe 
@eustlb","I think we'd rather add a `self.batch_norm = None if not config.conv_pos_batch_norm else nn.BatchNorm1d(config.hidden_size)` that we'd use in the forward pass, rather than using `nn.Sequential` here Why do we precise `(for bf16 models)` out of curiosity ? To be honest I just copied this from the fairseq definition https://github.com/facebookresearch/fairseq/blob/ecbf110e1eb43861214b05fa001eff584954f65a/fairseq/models/hubert/hubert.py#L197

I can remove this if you prefer. I felt that the current method was more similar to the weight norm approach (and also similar to fairseq), but can change to your suggestion and update the conversion script as well Let's remove it then In `transformers`, we rather make everything explicit! would be nice to open a PR to the original repo and use pr branch revision in the mean time!"
35116,2024-12-06T09:29:54Z,2024-12-10T08:59:18Z,zucchini-nlp,3,2,3,3,1,3,1,[],1634.0,0,346563.0,0,0,0,0,1419929.850282,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'jmamou', 'zucchini-nlp']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35116). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @zucchini-nlp @ArthurZucker 
For AG with multi-GPU, it would be beneficial to include in the documentation that for optimal speedup, the assistant model should be placed on the default/first device of the target model (`target.device`). This setup avoids the overhead of transferring the candidate token IDs tensor from `assistant.device` to `target.device` after the speculative iteration and back to `assistant.device` with the validated token IDs following target validation @jmamou yes, feel free to add it in the docs in subsequent open PRs if relevantThanks! Regarding multi-gpu, we don't have fast tests, but we could add them with github actions.","Thanks! Regarding multi-gpu, we don't have fast tests, but we could add them with github actions.","# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/35099, we need to manually move inputs to the correct device as assistant and target models can be initialized on different devices. We already move the input to assistant's device when `get_candidate_inputs` but don't do it in target models' forward

Btw, seems like we are now having more troubles with certain generation techniques or cache not working in multi-gpu setting. And we don't have much tests for multiple gpus, so I am not sure if that's intended or we can add more integration tests. Also if those tests will be run as part of pull request CI or not. cc @gante for this question

cc @jmamou ","in a previous PR https://github.com/keyboardAnt/transformers/pull/4/files I proposed to do it just after `get_candidates` at the same place we move `candidate_logits` to `self.device` oke, might be better to move it there for easier readability :)"
35172,2024-12-09T16:47:07Z,2024-12-09T18:55:16Z,matthewdouglas,1,0,1,20,2,1,1,"['Tests', 'Quantization']",1701.0,0,7692.0,0,0,0,0,1473369.490353,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35172). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM ! Thanks for fixing ! ,LGTM ! Thanks for fixing ! ,"# What does this PR do?
Fixes [test failures ](https://github.com/huggingface/transformers/actions/runs/12227957829/job/34105744773#step:10:1172) for bitsandbytes that appear after `accelerate==1.2.0` was released. 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. 
        [Slack thread](https://huggingface.slack.com/archives/C06ALV91VML/p1733487585960999)
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?

## Who can review?
@SunMarc ",
35132,2024-12-07T06:21:32Z,2024-12-09T16:31:32Z,fzyzcjy,2,0,1,2,1,1,1,[],207255.0,0,235862.0,0,0,0,0,1455534.546235,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'fzyzcjy']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35132). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. You are welcome!Yes, this seems like an obvious error in the log call. Thank you!","Yes, this seems like an obvious error in the log call. Thank you!","# What does this PR do?

Hi thanks for the library! When looking at mlflow callback (I wanted to customize some detailed behavior for it), it seems a line of log has a typo.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35167,2024-12-09T12:49:16Z,2024-12-09T15:09:50Z,LysandreJik,2,0,1,874,42,1,1,[],8431.0,0,16105.0,0,0,0,0,1479228.074405,,0,1,0,False,"['qgallouedec', 'HuggingFaceDocBuilderDev']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35167). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hey @LysandreJik @ArthurZucker any pointer to explain this cleanup?🤗 ,🤗 ,,
11098,2021-04-06T20:22:05Z,2021-04-06T20:42:06Z,stas00,0,0,1,4,1,2,2,[],,0,116012152.0,0,0,0,0,1491213.504707,,0,1,0,False,[],Thanks! I'm removing the @LysandreJik mention in the merge as it would ping me when forks rebase otherwise,Thanks! I'm removing the @LysandreJik mention in the merge as it would ping me when forks rebase otherwise,"make the example work

@LysandreJik
",
35160,2024-12-09T08:28:27Z,2024-12-09T13:13:37Z,MekkCyber,1,0,1,4,1,1,1,[],1652.0,0,17112.0,0,0,0,0,1493871.515207,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35160). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?

Fix a typo in `eetq` tests, `modules_to_not_convert` instead of `linear_weights_not_to_quantize`

## Who can review ?

@SunMarc ",
35148,2024-12-08T09:25:34Z,2024-12-09T09:01:31Z,NielsRogge,1,0,1,18,1,1,1,[],1551.0,0,84957.0,0,0,0,0,1509000.065163,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35148). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for update!,Thanks for update!,"# What does this PR do?

This PR is a follow-up of #33125 to add a figure and add a resource section.",
34319,2024-10-22T20:19:14Z,2024-11-25T17:44:09Z,ViktorooReps,2,8,7,67,2,4,2,[],2904288.0,0,3967977.0,0,0,0,0,1647561.916784,,0,7,0,False,"['ViktorooReps', 'HuggingFaceDocBuilderDev']","@ArthurZucker could not properly run `make fixup` due to some dependency problems, I think should be all good now The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34319). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks, docs LGTM! Once @ArthurZucker has reviewed, we can merge 🙂  NIce PR! LGTM was not pinged so did not come back to it! Could you run `make fixup` and rebase to make sure you are up to date?  Thanks 🤗 ","Thanks, docs LGTM! Once @ArthurZucker has reviewed, we can merge 🙂  NIce PR! LGTM was not pinged so did not come back to it! Could you run `make fixup` and rebase to make sure you are up to date?  Thanks 🤗 ","# What does this PR do?

Documentation improvement on tiktoken integration + tiktoken conversion function.

<!-- Remove if not applicable -->

Fixes #34221 


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests? 


## Who can review?

@ArthurZucker
@stevhliu
","Maybe we can create a new section here that describes the purpose and motivation a little more.

```suggestion
## Create tiktoken tokenizer

The `tokenizer.model` file contains no information about additional tokens or pattern strings. If these are important, convert the tokenizer to `tokenizer.json`, the appropriate format for [`PreTrainedTokenizerFast`].

Generate the `tokenizer.model` file with [tiktoken.get_encoding](https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/registry.py#L63) and then convert it to `tokenizer.json` with [`convert_tiktoken_to_fast`].
``` ```suggestion
The resulting `tokenizer.json` file is saved to the specified directory and loaded with [`PreTrainedTokenizerFast`].

```py
PreTrainedTokenizerFast.from_pretrained(""config/save/dir"")
```
``` very nice! 
We should do :
```python
from tiktoken import get_encoding

# You can load your custom encoding or the one provided by OpenAI
encoding = get_encoding(""gpt2"")
```
in this function directly IMO! 

Also let's maybe place this under the `integration` folder ! 🤗  I wouldn't like to load the encoding inside the function to allow for custom encodings like https://github.com/openai/tiktoken?tab=readme-ov-file#extending-tiktoken 

We could allow `Encoding | str` as input and load the encoding with `get_encoding` if a str is passed. Moved it to `integration/tiktoken.py` let's split in two lines: 
```suggestion
    tokenizer = TikTokenConverter(
        vocab_file=save_file_absolute, pattern=encoding._pat_str, additional_special_tokens=encoding._special_tokens
    ).tokenizer()
    tokenizer.save(output_file_absolute)
``` let's revert this as it's not needed! To actually work, the tokenizer pipeline needs the ByteLevel and Split pre-tokenizers (with the pattern string), the ByteLevel post-processor, and the special tokens from the original encoding. It looks like the `.converted()` method on the original TikTokenConverter adds these already: https://github.com/huggingface/transformers/blob/98e8062df3cf82b367e8a243963e4afb0d9d3407/src/transformers/convert_slow_tokenizer.py#L1532

but the `convert_tiktoken_to_fast` wrapper just uses the `.tokenizer()` method, which saves only the actual merges. This isn't documented, so unless you read the source code and/or inspect the actual tokenizers file, you won't realize the full pipeline is incomplete until Unicode tokens or special tokens start breaking.

This took me a couple hours to figure out what went wrong (porting the Fish Speech 1.5 tokenizer from Tiktoken). Is there a reason `convert_tiktoken_to_fast` doesn't just use the `.converted()` method? If this isn't possible, could the docs at least explicitly call out what the remaining steps are to set up the BPE pipeline?"
35121,2024-12-06T12:20:08Z,2024-12-09T08:57:41Z,techkang,2,6,9,132,4,3,2,[],79648.0,0,247121.0,0,0,0,0,1509163.928628,,0,9,0,False,['techkang'],"@ArthurZucker The Wav2Vec2Model bug is because SpeechEncoderDecoderModel takes variable argument as forward paramters:
https://github.com/huggingface/transformers/blob/c8c8dffbe45ebef0a8dba4a51024e5e5e498596b/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L457
But it dispatches the paramater to it's encoder and decode which doesn't accept variable argument: https://github.com/huggingface/transformers/blob/c8c8dffbe45ebef0a8dba4a51024e5e5e498596b/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L489-L493
I think the better solution is to modify it's decode to accept variable argument. I proposed a new commit and the test succeed. Finally all unit test passed. Please check  again. @muellerzr @ArthurZucker Thanks! These solutions make sense and ran the tests myself. Left some nits for less-leeway on the test closeness. cc @ArthurZucker  Sorry @muellerzr but this does not solve:
```
FAILED examples/pytorch/test_pytorch_examples.py::ExamplesTests::test_run_speech_recognition_seq2seq - TypeError: Wav2Vec2Model.forward() got an unexpected keyword argument 'num_items_in_batch'
```
so I am not sure I understand. Related to #35113 and #35128. 
We can't merge with the broken test Thanks a lot @techkang ! 
I did not dive enough on the test, my bad 🤗 
Merging ASAP and doing the patch","Thanks! These solutions make sense and ran the tests myself. Left some nits for less-leeway on the test closeness. cc @ArthurZucker  Sorry @muellerzr but this does not solve:
```
FAILED examples/pytorch/test_pytorch_examples.py::ExamplesTests::test_run_speech_recognition_seq2seq - TypeError: Wav2Vec2Model.forward() got an unexpected keyword argument 'num_items_in_batch'
```
so I am not sure I understand. Related to #35113 and #35128. 
We can't merge with the broken test Thanks a lot @techkang ! 
I did not dive enough on the test, my bad 🤗 
Merging ASAP and doing the patch","# What does this PR do?
There are two ways to fix GA loss bugs:
1. Use `num_items_in_batch` in loss function defined by model. In this case, `model_accepts_loss_kwargs` is `True`.
2. The model doesn't have loss function or user has self-defined loss function, which is `compute_loss_func`.

However, previes unit test only test for the second condition. So I introduced a new unit test to cover the first condition and fix the bugs by the way.



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@muellerzr @ArthurZucker ","Let's be a bit more aggressive and do 0.15 (this passes). I still feel that's quite big but I can't figure out why (my tests showed 0.001 should be doable).

```suggestion
        self.assertLess(max(diff_truth), 0.15, f""Difference {max(diff_truth)} is not within 0.15"")
``` ```suggestion
        self.assertLess(max(diff_truth), 0.2, f""Difference {max(diff_truth)} is not within 0.2"")
```

Similarly we can be aggressive here too It is strange that I tested on both Mac and Windows that `max(diff_truth)` is 0.144. So maybe 0.15 may failed on some other machine. I managed to reduce the gap to 1e-4 by padding all input labels to the same length. However, this method did not work for the GPT-2 model. I will continue to explore other solutions. Done! I use TinyStories to narrow down gap to the same as you. The code is submitted. Okay, would be nice if we had a safetensors model here but alright. 
"
34946,2024-11-26T20:33:46Z,2024-12-06T12:39:45Z,agostinv,8,0,2,2,1,4,4,[],57929.0,0,835617.0,0,0,0,0,1755050.261032,,0,2,0,False,"['BenjaminBossan', 'agostinv', 'HuggingFaceDocBuilderDev', 'MekkCyber']","LGTM @agostinv, thanks for the feature ! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34946). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Great little addition, thanks. Is this change sufficient to enable PEFT LoRA with bitlinear? Do you have a snippet to show its usage? I could imagine that training and inference work out of the box with this change, but some features like merging don't work or need special handling in PEFT.

Edit: As `BitLinear` is not a subclass of `nn.Linear`, we probably need extra handling in PEFT to make it work. @BenjaminBossan You're exactly right! Based on my experience, training functions but merging is non-trivial (also should clarify I forked `peft` to have it deduce the weight matrix dimensionality when I was looking for a hacky solution while working on my own experiments). Personally am keeping my adaptors separate for academic experiments so far. 

[`src/transformers/integrations/bitnet.py`](src/transformers/integrations/bitnet.py) has a number functions that can be used/adapted to be helpers in this respect if users really want to maintain a fully 1.58b layer versus some mixed-precision weights and two parallel forward paths through the layer. Ultimately, this all has to be user-defined behavior anyways unless `peft` integrates direct support for BitLinear on its own, so it shouldn't be too big of a deal on the `transformers` side of thing for the moment.

The attributes allow us to get caught by the following code in [`peft/tuners/lora/layer`](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py):

https://github.com/huggingface/peft/blob/131efba5d48753a3355ecd4f3833ae010a0510d6/src/peft/tuners/lora/layer.py#L93-L101

As far as a quick example goes, I have the following snippet that's pretty ad-hoc but is generally based on the BitsAndBytes implementations for `peft` with LoRA. Currently using it for a small, private project. In fact, it probably needs some changes to function during inference. 

```python
import warnings
from typing import Any, Optional

import torch

from peft.tuners.tuners_utils import BaseTunerLayer, check_adapters_to_merge
from peft.utils.other import transpose

from peft.tuners.lora.layer import LoraLayer
    
class BitNetLinearLora(torch.nn.Module, LoraLayer):
    # Lora implemented in a dense layer
    def __init__(
        self,
        base_layer: torch.nn.Module,
        adapter_name: str,
        r: int = 0,
        lora_alpha: int = 1,
        lora_dropout: float = 0.0,
        init_lora_weights: bool = True,
        use_rslora: bool = False,
        use_dora: bool = False,
        **kwargs,
    ) -> None:
        super().__init__()
        LoraLayer.__init__(self, base_layer)
        self.fan_in_fan_out = False

        self._active_adapter = adapter_name
        self.update_layer(
            adapter_name,
            r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            init_lora_weights=init_lora_weights,
            use_rslora=use_rslora,
            use_dora=use_dora,
        )

    def merge(self, safe_merge: bool = False, adapter_names: Optional[list[str]] = None) -> None:
        raise NotImplementedError

    def unmerge(self) -> None:
        raise NotImplementedError

    def get_delta_weight(self, adapter):
        return (
            transpose(
                self.lora_B[adapter].weight @ self.lora_A[adapter].weight,
                False,
            )
            * self.scaling[adapter]
        )

    def _mixed_batch_forward(
        self, x: torch.Tensor, *args: Any, adapter_names: list[str], **kwargs: Any
    ) -> torch.Tensor:
        # This is a special method that handles the case when users pass the argument `adapter_names`. This is an
        # extra argument that allows mixing different adapters in the same batch at inference time.
        result = self.base_layer(x, *args, **kwargs)

        unique_adapters = set(adapter_names)
        sub_batch_indices_list = []
        for adapter in unique_adapters:
            sub_batch_indices_list.append([index for index, item in enumerate(adapter_names) if item == adapter])

        for i, active_adapter in enumerate(unique_adapters):
            if active_adapter == ""__base__"":
                continue
            if active_adapter not in self.lora_A.keys():
                continue

            lora_A = self.lora_A[active_adapter]
            lora_B = self.lora_B[active_adapter]
            dropout = self.lora_dropout[active_adapter]
            scaling = self.scaling[active_adapter]

            requires_conversion = not torch.is_autocast_enabled()
            if requires_conversion:
                expected_dtype = result.dtype
                x = x.to(lora_A.weight.dtype)

            # getting the sub-batch, passing it to LoRA layers and updating the corresponding indices of the linear
            # layer output
            sub_batch = x[sub_batch_indices_list[i]]
            output = lora_B(lora_A(dropout(sub_batch))) * scaling
            if requires_conversion:
                output = output.to(expected_dtype)
            result[sub_batch_indices_list[i]] += output

        return result

    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:
        self._check_forward_args(x, *args, **kwargs)
        adapter_names = kwargs.pop(""adapter_names"", None)

        if self.disable_adapters:
            if self.merged:
                self.unmerge()
            result = self.base_layer(x, *args, **kwargs)
        elif adapter_names is not None:
            result = self._mixed_batch_forward(x, *args, adapter_names=adapter_names, **kwargs)
        elif self.merged:
            result = self.base_layer(x, *args, **kwargs)
        else:
            result = self.base_layer(x, *args, **kwargs)
            # As per Tim Dettmers, for 4bit, we need to defensively clone here.
            # The reason is that in some cases, an error can occur that backprop
            # does not work on a manipulated view. This issue may be solved with
            # newer PyTorch versions but this would need extensive testing to be
            # sure.
            result = result.clone()

            for active_adapter in self.active_adapters:
                if active_adapter not in self.lora_A.keys():
                    continue
                lora_A = self.lora_A[active_adapter]
                lora_B = self.lora_B[active_adapter]
                dropout = self.lora_dropout[active_adapter]
                scaling = self.scaling[active_adapter]

                requires_conversion = not torch.is_autocast_enabled()
                if requires_conversion:
                    expected_dtype = result.dtype
                    x = x.to(lora_A.weight.dtype)

                if not self.use_dora[active_adapter]:
                    output = lora_B(lora_A(dropout(x))) * scaling
                else:
                    x = dropout(x)
                    output = self.lora_magnitude_vector[active_adapter](
                        x,
                        lora_A=lora_A,
                        lora_B=lora_B,
                        scaling=scaling,
                        base_layer=self.get_base_layer(),
                    )
                if requires_conversion:
                    output = output.to(expected_dtype)

                result = result + output

        return result

    def __repr__(self) -> str:
        rep = super().__repr__()
        return ""lora."" + rep

def dispatch_bitnet(target: torch.nn.Module, adapter_name: str, **kwargs):
    new_module = None

    if isinstance(target, BaseTunerLayer):
        target_base_layer = target.get_base_layer()
    else:
        target_base_layer = target

    bitnet_kwargs = kwargs.copy()
    new_module = BitNetLinearLora(target, adapter_name, **bitnet_kwargs)

    return new_module
``` @agostinv Pretty cool, thanks for providing more details and the code sample. If you're interested, we can look into adding bitnet support to PEFT directly, your example already looks quite good as is and merging support is not mandatory. > Makes sense! Can't we just pass them to the `super().__init__()` call? `super().__init__(in_features, out_features)` should be equivalent no?

I didn't think so, and after trying on a minimum snippet it didn't seem to work (unless I've misunderstood what you meant). Since `BitLinear` inherits from `nn.Module` there isn't defined behavior for these attributes as far as I can tell. Even if it did work, adding these as visible attributes in `BitLinear` initialization more closely aligns with `nn.Linear` construction anyways.

> @agostinv Pretty cool, thanks for providing more details and the code sample. If you're interested, we can look into adding bitnet support to PEFT directly, your example already looks quite good as is and merging support is not mandatory.

Not opposed at all to adding direct PEFT support, especially if it is in addition to this PR. Not including these attributes in `BitLinear` objects *might* lead to problems down the line if different compression methodologies are employed to avoid 2-bit quantization and get closer to 1.58b (via efficient LUTs for ""5 bits to 3 params"" compression for example), another reason why deducing the dimensions isn't an ideal solution. 

While I currently have disallowed merging in that code snippet (mostly because I doubt it would result in a usable adapter), it feels like an official implementation should have some support for users that want to explore it. Since merging isn't super complicated, I can just go ahead and implement the most straightforward version if you'd like (i.e. dequantizing the BitLinear weights, adding the adapter, then requantizing and storing the new scales). 
 @MekkCyber Sorry to ping you again, but do you know if any other steps are required before merging to main, here? Assuming the state of this PR is fine. @agostinv sorry forgot about it, merged !LGTM!  Makes sense! Can't we just pass them to the `super().__init__()` call? 
`super().__init__(in_features, out_features)` should be equivalent no?","LGTM!  Makes sense! Can't we just pass them to the `super().__init__()` call? 
`super().__init__(in_features, out_features)` should be equivalent no?","# What does this PR do?

This PR is an extremely simple two-liner (adding `in_features` and `out_features` as attributes to `BitLinear`) whose only purpose is to improve accessibility for `BitLinear` to users that want to employ `peft`. Currently, `BitLinear` is not usable with LoRAs in `peft` out-of-the-box.

The typical flow for enabling LoRAs for custom layers in `peft` is to construct a custom class that describes the LoRAs behavior and then registers it with a private API. The problem is that `peft` still needs additional information on input and output dimensionality via `in_features` and `out_features`, which `BitLinear` currently lacks. The current solution for this problem is to wrap `BitLinear` with another module that adds these attributes during initialization and then replace all instances of `BitLinear` with that new module. Alternatively, the LoRA source code would have to be revised to support `BitLinear` and derive the feature dimensions from its weight matrix. From the perspective of potential users, adding the aforementioned attributes improves accessibility and avoids requiring some hacky looking fixes from their end.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

Other checkmarks are left untouched, as they don't look relevant.

## Who can review?

- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber
- bitnet contributors: @MekkCyber 
",
35114,2024-12-06T08:37:41Z,2024-12-06T11:03:31Z,ydshieh,2,2,1,11,1,2,1,[],250.0,0,8752.0,0,0,0,0,1760881.923575,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']",cc @molbap FYI The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35114). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Top thanks @ydshieh!,Top thanks @ydshieh!,"# What does this PR do?

We need the token `CIRCLE_TOKEN` to be used in the generated CircleCI config file.","The token is defined within a context `TRANSFORMERS_CONTEXT` (set in CircleCI organization setting) If this is available, it means we are in a repo. that is not `transformers` (i.e. private forked repo.)

It is determined by (in `config.yml`)

```
    setup_and_quality_2:
        when:
            not:
                 equal: [<<pipeline.project.git_url>>, https://github.com/huggingface/transformers]
        jobs:
            ...
            - fetch_tests:
                # [reference] https://circleci.com/docs/contexts/
                context:
                    - TRANSFORMERS_CONTEXT
```"
34332,2024-10-23T06:00:23Z,2024-10-30T09:21:38Z,zucchini-nlp,14,3,10,252,15,2,1,[],124375.0,0,3812771.0,0,0,0,0,1767901.582676,,0,10,0,False,"['pspdada', 'HuggingFaceDocBuilderDev', 'yurkoff-mv', 'ArthurZucker', 'zucchini-nlp', 'DarkLight1337', 'agadetsky']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34332). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Might as well go for next patch release @ArthurZucker ? Yep we can do that!
 @ArthurZucker hi!
It seems that this bug https://github.com/huggingface/transformers/issues/34625 might be related to the PR Thanks for fixing #34379! However, I'm still unable to use LLaVA-NeXT with text-only input. See the failure in https://buildkite.com/vllm/ci-aws/builds/10881#01930542-f9c4-4f8e-9c23-c0e5f5ef0141 which occurs when `input_ids` are given but not `pixel_values`. This pull request might not be working as expected. cc: https://github.com/huggingface/transformers/pull/30962#issuecomment-2466293660 @zucchini-nlp is working on fixes #34332 was merged to adresse this! The error is reproduced on the **Pixtral-12B** model in the **Transformers v4.46.3**. However, in the **Transformers v4.45.2** everything works.

```
with open('./data/dog.jpg', 'rb') as fr:
        image_dog = fr.read()

with open('./data/mountain.jpg', 'rb') as fr:
    image_mountain = fr.read()

images = [image_dog , image_mountain]
images = [Image.open(BytesIO(image)) for image in images]

messages = [
        {
            ""role"": ""user"", ""content"": [
            {""type"": ""text"", ""content"": ""What is this animal?""},
            {""type"": ""image""},
            {""type"": ""text"", ""content"": ""Can it live here?""},
            {""type"": ""image""}
        ]
        }
    ]

text_prompt = self.processor.apply_chat_template(model_input['messages'], add_generation_prompt=True)
            inputs = self.processor(text=[text_prompt],
                                                  images=images,
                                                  padding=True,
                                                  return_tensors=""pt"",
                                                  ).to(self.device)

output_ids = self.model.generate(**inputs,
                                                      do_sample=False,
                                                      max_new_tokens=2048,
                                                      )
```
**Error:**
```
Image features and image tokens do not match: tokens: 248, features 494
```
![mountain](https://github.com/user-attachments/assets/5e3d2e01-7c86-44a1-a5ac-d07bbfc1f186)
![dog](https://github.com/user-attachments/assets/f8c72da2-2056-4396-a5ec-67d50afdc579)

 @yurkoff-mv this is more about the Pixtral model and probably not related to this PR, because this PR modifies processing code in LLaVA models only. Pixtral has its own processor which is responsible for expanding input text with the correct number of image tokens. Would you mind opening a new issue?

cc @Rocketknight1 in case you've encountered this error already since you've been working on Pixtral processing code  @zucchini-nlp, thank you for relpy.
I use the **LLaVa** model. The code worked in **Transformers v4.45.2**.
```
processor = AutoProcessor.from_pretrained(model_path)
processor.tokenizer.pad_token = self.processor.tokenizer.eos_token
model = LlavaForConditionalGeneration.from_pretrained(model_path,
                                                      quantization_config=bnb_config,
                                                      device_map='auto',
                                                      attn_implementation=""flash_attention_2"",
                                                      )
``` @yurkoff-mv yes, but the processor is different and the issue here stems from processing code. The processor is responsible to infer the correct amount of image tokens and add it in input ids, while the model only checks if the lengths of encoded images and the image tokens are same

I'll look into thta, but a new issue for its own is nice way to track it as it is not related to this PR Found it, related to https://github.com/huggingface/transformers/issues/34204 which left some edge cases apparently. In your script you can overcome it by passing text as `str` not `list`

We'll try to fix it soon > Thanks for fixing #34379! However, I'm still unable to use LLaVA-NeXT with text-only input. See the failure in https://buildkite.com/vllm/ci-aws/builds/10881#01930542-f9c4-4f8e-9c23-c0e5f5ef0141 which occurs when `input_ids` are given but not `pixel_values`.

I'm still getting this problem on v4.46.3. @DarkLight1337 yes, the text-only input is currently not supported and should be fixed by https://github.com/huggingface/transformers/pull/34502 :)Thanks, makes sense, we might want a small vlm test for these (to ensure new models properly raise this!)  Thanks for adding the test as well","Thanks, makes sense, we might want a small vlm test for these (to ensure new models properly raise this!)  Thanks for adding the test as well","# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34284#issuecomment-2427033072. Our tests didnt catch this because the tester has one image + one text inputs always. Actually we can try to get a mixture of different images/texts but that would be a whole new story of issues where some tests might slice on batch size and we'll lose the match between number of images and image tokens 

Also fixes https://github.com/huggingface/transformers/issues/34379","rebasing should fix this! I think this slows down inference no? `.item()` induces cuda cpu synch! 
Anyways not the point, but thanks for the fix. 
`n_image_features` takes into account padding? (are image features not padded to the batch?  yes, if the image is padded it is usually unpadded before we come to this point, e.g in llava-next. Hm, I don't think the slowdown will be drastic especially since we need to check once per input in the pre-fill stage"
34157,2024-10-14T14:03:53Z,2024-12-06T11:17:35Z,aymeric-roucher,12,30,141,6251,32,4,0,"['New model', 'Multimodal', 'Mixture of Experts']",485.0,0,4569224.0,0,0,0,0,1760039.160131,,0,141,0,False,"['HuggingFaceDocBuilderDev', 'aymeric-roucher', 'molbap', 'mobicham', 'Cyrilvallez', 'ArthurZucker', 'zucchini-nlp']","@ArthurZucker I have this error that we discussed in `python utils/check_modular_conversion.py`:
```
Traceback (most recent call last):
  File ""/home/ubuntu/transformers/utils/check_modular_conversion.py"", line 73, in <module>
    non_matching_files += compare_files(modular_file_path, args.fix_and_overwrite)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/transformers/utils/check_modular_conversion.py"", line 53, in compare_files
    generated_modeling_content = convert_modular_file(modular_file_path)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/transformers/utils/modular_model_converter.py"", line 1138, in convert_modular_file
    wrapper.visit(cst_transformers)
  File ""/home/ubuntu/venv/aria/lib/python3.12/site-packages/libcst/metadata/wrapper.py"", line 204, in visit
    return self.module.visit(visitor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/venv/aria/lib/python3.12/site-packages/libcst/_nodes/module.py"", line 89, in visit
    result = super(Module, self).visit(visitor)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/venv/aria/lib/python3.12/site-packages/libcst/_nodes/base.py"", line 236, in visit
    leave_result = visitor.on_leave(self, with_updated_children)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/venv/aria/lib/python3.12/site-packages/libcst/_visitors.py"", line 71, in on_leave
    updated_node = leave_func(original_node, updated_node)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/transformers/utils/modular_model_converter.py"", line 1111, in leave_Module
    self._recursively_add_all_new_needed_functions_in_files()
  File ""/home/ubuntu/transformers/utils/modular_model_converter.py"", line 1097, in _recursively_add_all_new_needed_functions_in_files
    dependency, body, self.all_definitions[dependency], parent=parent
                      ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
KeyError: 'select_best_resolution'
``` The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34157). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. cc @Cyrilvallez as well! I don't get [this error](https://app.circleci.com/pipelines/github/huggingface/transformers/110262/workflows/34139b4f-b97d-470a-967c-5b40b9634b89/jobs/1466607?invite=true#step-105-2319_118) @molbap :
With `from ...image_utils import PILImageResampling`, I get an ""unprotected import"" error, even though many other models have the exact same way of importing `PILImageResampling`, cf `image_processing_idefics3.py` for instance. For the record: import structure should be protected in the `__init__.py` ! Any timeline for this ? We would love to push a quantized version! @zucchini-nlp I still have [some errors in the modular conversion](https://app.circleci.com/pipelines/github/huggingface/transformers/111426/workflows/8ab670f8-d22a-4a06-bf07-c6a06aaea436/jobs/1483883) (which due to modular conversion not generating files correctly)

I also have an [unprotected import error](https://app.circleci.com/pipelines/github/huggingface/transformers/111426/workflows/8ab670f8-d22a-4a06-bf07-c6a06aaea436/jobs/1483886) that I don't find the cause for:
- I do have some unprotected `import torch` in `modeling_aria.py`, but idefics3 has the same thing.
- I have no torch imports in processing or image_processing files. No image imports either except for `from ...utils import ImageInput, PILImageResampling` but idefics3 also has the same.

Generating from input embeds seems broken, I'll investigate that. Meanwhile do you have ideas on how to solve the import issue above? @Cyrilvallez the modular does not export all files correctly, which makes [this test](https://app.circleci.com/pipelines/github/huggingface/transformers/111850/workflows/76270616-7da8-4616-94eb-47248e75a780/jobs/1490136/parallel-runs/0/steps/0-106) fail:

Do you have an idea how to solve this? It's the last big issue before being able to merge. @aymeric-roucher the import error is from https://github.com/huggingface/transformers/blob/add-aria/src/transformers/models/aria/__init__.py file. To get all the files correctly on init from modular, you can add a `__all__ = [""ObjectName1"", ""ObjectName""]` in the modular file at the end with all the objects that need high level import.

And the `__init__.py` content should be
```python
from typing import TYPE_CHECKING

from ...utils import _LazyModule
from ...utils.import_utils import define_import_structure


if TYPE_CHECKING:
    from .configuration_aria import *
    from .image_processing_aria import *
    from .modeling_aria import *
    from .processing_aria import *
else:
    import sys

    _file = globals()[""__file__""]
    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)
```

I think in that case everything should be done through modular, including processor and image processor @ArthurZucker the tests failing are either flaky tests failing on other models, due to modular export, or an unsolved import error that we'll investigate with @zucchini-nlp : so that's ready for final review! The battle continues ⚔️! Looking into the modular issue  @aymeric-roucher, I just pushed some modifications to the modular converter and the modular itself. I think it should now behave the way you want. I did not apply the modular_converter on the new modular so you can do it yourself and check the changes it produces more easily. Let me know if it solves everything or if you still experience some issues!Feel free to ping me again once you are done with your cleanup!  Left an initial batch of comments, mainly related to processing to make it in line with the rest of the codebase! + a couple nits. LMK if you have questions and I can take another look later :hugs:  Some changes needed in the processor to make it fall inline with the other VLMs out there - and I've got some issues with integration tests, let me know!","Feel free to ping me again once you are done with your cleanup!  Left an initial batch of comments, mainly related to processing to make it in line with the rest of the codebase! + a couple nits. LMK if you have questions and I can take another look later :hugs:  Some changes needed in the processor to make it fall inline with the other VLMs out there - and I've got some issues with integration tests, let me know!","# What does this PR do?

Add [rhymes-ai/Aria](https://huggingface.co/rhymes-ai/Aria) to transformers!

@ArthurZucker","@ArthurZucker I added the second part of the check here to pass the test, it does not seem to create any missed imports so I let it but maybe it has unwanted consequences. Nice then thanks!  Ha nice @aymeric-roucher, indeed I forgot about built-ins when adding this! Thanks for correcting it! If you only check for `if dependency in self.all_definitions`, it should be enough though no? Built-ins will never be added to `self.all_definitions` during visit, so it would avoid having the large list It's done! ✅ I tried to remove this by integrating it into `AriaTextMoELayer`, but that means we would have to remap weights, so for the sake of simplicity I kept it separated. cc @ArthurZucker  This is different from torch's built-in GLU becauses it uses `silu` instead of `sigmoid` activation: so I kept the definition here. @ArthurZucker I tried to have this function copied from somewhere else (like Idefics or Llava) but found no match, there are slight modifications here and there, so I had to keep it. If you pool form main you'll see that they are indeed gone, that's because now the one in `generate` should be general enough to handle most vLM cases. If not the case, then yep you keep it as is here!@ We should add it to our acitvation_functions Okay sounds good I tried to remove it and got this error:
```
Traceback (most recent call last):
  File ""/home/ubuntu/Aria/inference.py"", line 35, in <module>
    output = model.generate(
             ^^^^^^^^^^^^^^^
  File ""/home/ubuntu/venv/aria/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/transformers/src/transformers/generation/utils.py"", line 2203, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File ""/home/ubuntu/transformers/src/transformers/generation/utils.py"", line 3159, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/transformers/src/transformers/models/aria/modeling_aria.py"", line 3124, in prepare_inputs_for_generation
    and (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length
                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/transformers/src/transformers/configuration_utils.py"", line 202, in __getattribute__
    return super().__getattribute__(key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'AriaConfig' object has no attribute 'image_seq_length'
```
So the config used in the general `prepare_inputs_for_generation` does not handle accessing a subconfig dedicated to vision.

I'll have to keep the current implementation then! some duplicates Interesting, doesn't it suffice to specify the number of tokens per image in image_seq_length in your vision config? (from the error you shared) I'd be pro-setting these utils function on top of the `processing_aria` file, as several routines depend on looking up filenames of type `processing_...`, and they don't add that many loc to the file! unprotected PIL imports (and `torch` in the processing) will cause errors in tests where libs aren't available in the environment nits on naming, no abbreviations + consistency with the codebase:

embed_dim --> in_features, ff_dim --> hidden_features  inits should have type hinting, and it seems these parameters are config-derived: so the init should be solely relying on `config` as an arg. `norm_layer` can be thus defined using a lookup table. here most args can be derived from config: so config should be passed in the init and then variables initialized from it. In particular `kv_dim` depends on two config keys, number of kv heads and head dimension, so this should be explicit at the init level of the module that uses them Nits on naming: avoid abbreviations, `self.ln_ffn` should be more explicitly named for instance `self.layer_norm` is fine since there's just one in this module ```suggestion
        batch_size = x.shape[0]
``` Can we handle this check in the configuration file and then just pass `(std=config.initializer_range)` below? This model is from Rhymes.AI indeed, but it's contributed to transformers by you - your username goes here :)  these values shouldn't have to be defined twice here you don't need to call `__call__`, because the BaseImageProcessor has got you covered

```python
class BaseImageProcessor(ImageProcessingMixin):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def __call__(self, images, **kwargs) -> BatchFeature:
        """"""Preprocess an image or a batch of images.""""""
        return self.preprocess(images, **kwargs)
``` this should be the preprocess() method directly we have an `ImageInput` type from `image_utils` which should do it this should be contained within the preprocess method, see next comment not needed since we are not in remote code execution anymore It's in this loop that the self.transform method should be expanded instead of called, to be inline with the current way of implementing image processors. Before you say so, yes, it's a bit verbose as it is and we aim to reduce it - but having all of the models to rework follow the same pattern will make the rework much easier long-term nit, no abbreviations"
34915,2024-11-25T10:33:04Z,2024-12-05T15:37:47Z,ArthurZucker,7,0,4,9,1,1,1,[],1925.0,0,940197.0,0,0,0,0,1772915.911908,,0,4,0,False,"['techkang', 'HuggingFaceDocBuilderDev', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34915). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I think this pr introduced a new bug that if user use user defined loss funcion and `model_accepts_loss_kwargs` is `False`, `compute_loss` function cannot get the `num_items_in_batch` argument. Finally, user defined compute_loss_func will not receive this argument either. `model_accepts_loss_kwargs` is supposed to be determined at init time and only depends on the forward pass of the model I understand now, I misinterpreted the if condition earlier. However, in this PR, when model_accepts_loss_kwargs is True, it won't pass the num_items_in_batch parameter, which would make the GA loss modification functionality ineffective. Why is that? In newest code, run
```
export RUN_SLOW=True
pytest tests/trainer/test_trainer.py::TrainerIntegrationPrerunTest::test_gradient_accumulation_loss_alignment
```
will cause error:
```
======================================================= short test summary info =======================================================
FAILED tests/trainer/test_trainer.py::TrainerIntegrationPrerunTest::test_gradient_accumulation_loss_alignment - AssertionError: 0.9038000000000004 not less than 0.1 : Difference -0.9038000000000004 is not within 0.1
============================================== 1 failed, 2 warnings in 102.43s (0:01:42) ==============================================
``` Ah shit the if condition is reversed  Opened a PR for a fix, thanks!TIL! However, as you can see by the failing test, this doesn't always work 😅 (If we can get it to that's great, I think that's originally why I went with explicit rather than implicit)","TIL! However, as you can see by the failing test, this doesn't always work 😅 (If we can get it to that's great, I think that's originally why I went with explicit rather than implicit)","# What does this PR do?
Fixes #34577
`model_accepts_loss_kwargs` was wrongly looking at kwarg names, while you should only need kwargs (since the name can vary for FlashAttentionKwargs, LossKwargs etc)",
35085,2024-12-04T14:01:29Z,2024-12-05T15:00:41Z,Isotr0py,2,0,3,7,1,2,2,[],1192.0,0,97486.0,0,0,0,0,1825523.31575,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'Isotr0py']","I think there is not, I just realized this when trying to migrate `test_ggml.py` to use `Qwen2.5-0.5B` for dequantization tests, because it generates totally gibberish outputs. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35085). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM ! Thanks ! Is there an specific issue linked to this ?  Cool, thanks @Isotr0py!","LGTM ! Thanks ! Is there an specific issue linked to this ?  Cool, thanks @Isotr0py!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

- Some models like [Llama-3.2-1B-Instruct-IMat-GGUF](https://huggingface.co/legraphista/Llama-3.2-1B-Instruct-IMat-GGUF) use `tie_word_embeddings=True`, but current GGUF model conversion didn't handle it and set to default value `tie_word_embeddings=False`.
- This PR fix the `tie_word_embeddings` handling for GGUF models


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@SunMarc 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33125,2024-08-26T14:40:37Z,2024-12-05T15:14:46Z,jmtzt,25,30,47,1909,19,4,3,"['New model', 'Vision', 'run-slow', 'Modular']",82621.0,0,8742013.0,0,0,0,0,1818648.712544,,0,47,0,False,"['qubvel', 'amyeroberts', 'NielsRogge', 'HuggingFaceDocBuilderDev', 'jmtzt', 'ArthurZucker']","cc @amyeroberts and @qubvel  Thanks for reviewing the code, @amyeroberts! :)

I've just pushed the adjustments, and now the fx support seems to be working fine. Thanks again for checking @amyeroberts :) I've just pushed the adjustments, however some tests appear to be failing due to some OS errors inside CircleCI, do you know why's that?  Thanks for pushing again! Hmmmm - I'm not sure why the slow model tests aren't being picked up or run here cc @ydshieh  Yih-Dar found the issue - the actions won't be triggered whilst there are merge conflicts in the PR. Could you resolve these, then push another `[run-slow] ijepa` commit? This should hopefully run the workflow!  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33125). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks for looking into it. I fixed the merge conflicts, now the tests seem to have run without any issues. cc @amyeroberts  @jmtzt Great! Final step is to move the checkpoints to be under the official organization and updating the paths in the PR. Could you share a link to all the checkpoints for this model under your name on hf.co?  Sure, here you can find them:

- [ijepa_vith14_1k](https://huggingface.co/jmtzt/ijepa_huge_patch14_1k)
- [ijepa_vith14_22k](https://huggingface.co/jmtzt/ijepa_huge_patch14_22k)
- [ijepa_vith16_1k](https://huggingface.co/jmtzt/ijepa_huge_patch16_1k)
- [ijepa_vitg16_22k](https://huggingface.co/jmtzt/ijepa_giant_patch16_22k) @amyeroberts I've also updated the model cards :) Hi @jmtzt! Thanks for working on the model and sorry for the long wait, sometimes we are overwhelmed with issues and PRs! Don't hesitate to ping a few times! No problem @qubvel, is there anything else I should do from my end? Thanks :) Hi @jmtzt let us know if you need any help to push this one to the finish line! > Hi @jmtzt let us know if you need any help to push this one to the finish line!

Hi @NielsRogge, I just need some time to go over the new conversion script and modelling pattern, I believe this weekend I can make some progress on it and keep you posted. Thanks! hey @qubvel just pushed the adjustments according to the new modular structure, as suggested. Will add the snippets to the model cards soon... :) Thanks for iterating on the PR and addressing the comments!

While trying to use the converted checkpoint with the snippet from the model card I got the message that some weights were not initialized.

```python
import requests
from PIL import Image
from torch.nn.functional import cosine_similarity

from transformers import AutoModel, AutoProcessor

url_1 = ""http://images.cocodataset.org/val2017/000000039769.jpg""
url_2 = ""http://images.cocodataset.org/val2017/000000219578.jpg""
image_1 = Image.open(requests.get(url_1, stream=True).raw)
image_2 = Image.open(requests.get(url_2, stream=True).raw)

model_id = ""jmtzt/ijepa_vith14_1k""
processor = AutoProcessor.from_pretrained(model_id)
model = AutoModel.from_pretrained(model_id)

def infer(image):
    inputs = processor(image, return_tensors=""pt"")
    outputs = model(**inputs)
    return outputs.pooler_output

embed_1 = infer(image_1)
embed_2 = infer(image_2)

similarity = cosine_similarity(embed_1, embed_2)
print(similarity)
```

Message:
Some weights of IJepaModel were not initialized from the model checkpoint at jmtzt/ijepa_vith14_1k and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']

Were these weights missed? Without the full model initialized I suppose the output similarity is incorrect, isn't it? the original I-JEPA model doesn't have the pooling layer, so I think to get around this we might need to default the `add_pooling_layer` to `False` in its initialization, and modify this snippet accordingly to get the last hidden states rather than the `pooler_output` Ok, sounds good 👍  hi @qubvel, thanks for the support and reviewing the PR! Just pushed the suggested changes, let me know if it's alright now. @ArthurZucker please review whenever you have bandwidth! The model is similar to ViT, so the `Modular` is used here to remove the CLS token.

Checkpoints can be found here:
https://huggingface.co/jmtzt

(we will need to transfer them to [facebook](https://huggingface.co/facebook) org as soon as we ensure the code is in the final stage + rename all occurrences in code and model cards) Feel free to ping me again for another review! 🫡  > Feel free to ping me again for another review! 🫡

@ArthurZucker I've pushed the adjustments according to your comments, let me know if anything is missing. Thanks! :) @ArthurZucker can you transfer checkpoints to Facebook org? Also, code snippets need to be adjusted.
Checkpoints are here: https://huggingface.co/jmtzt
 @jmtzt congratulations on the model merged 🎉  was glad to collaborate with you on this!
Can you please confirm if we can transfer checkpoints to Facebook org from your account? thanks for the support @qubvel and @ArthurZucker :)

Sure, go ahead!Thanks for adding this model @jmtzt! 

Overall it's looking great. Did an initial review outlining some small things to update. 

Before merging, we'll need to make sure the slow model tests are passing. To trigger these, you'll need to push a commit (empty or otherwise) with the message `[run_slow] ijpea`. Me or another person at HF will then need to approve the workflow Thanks for iterating - looks great! 

Just a small nit. It seems the slow model tests didn't run -- not really sure what happened there. Could you try with `[run-slow] ijepa` instead of `[run_slow] ijepa`? 

After than, just a merge conflict to resolve and we're good to go!  Thanks for the contribution! The code looks good to me, here are some main questions I have

1. We are moving to a new conversion script pattern (see [mllama](https://github.com/huggingface/transformers/blob/04b4e441dcd0ed966e158c66dac870e67cab8ea8/src/transformers/models/mllama/convert_mllama_weights_to_hf.py)), can you please adapt your script too.
2. Modeling code is mostly reused from ViT model with with some minor changes. Despite it is not common for `transformers` to use inheritance, we recently introduced a new feature - modular transformers. With this new feature you actually can use inheritance, and then your code is going to be unpacked to a single model - single file format. More information and examples are in this [issue](https://github.com/huggingface/transformers/issues/33916). Would you have bandwidth to try the feature with this model? It should be a great candidate!
3. Regrading model cards, it would be great to add code snippets there!

Thanks, let me know if you need any further assistance or have any questions! Hi @jmtzt! Thanks for trying a new modular converter! Great job! I left a few comments below, but overall looks almost ready! Hi @jmtzt! Seems like almost everything is fine! Thanks for correcting snippets in docs and model cards! Snippets work fine on my side, I also did an experiment fine-tuning classification model - converges really good.

A few final nits regarding docstrings/constants and a suggestion regarding classification head and I will pass it to a core maintainers review, thank you for the great job! Thanks for addressing all the comments quickly! IMO it's ready for the next review

P.S. It may take a while since there are a lot of PRs for the final review in a line 🤗  Thanks a lot for the PR! Modular makes it easy to understand: basically VIT but no cls token embedding right? (checked that no models in transformers already has this!) Very nice! Camel casing is a bit wrong but would look super ugly otherwise!","Thanks for adding this model @jmtzt! 

Overall it's looking great. Did an initial review outlining some small things to update. 

Before merging, we'll need to make sure the slow model tests are passing. To trigger these, you'll need to push a commit (empty or otherwise) with the message `[run_slow] ijpea`. Me or another person at HF will then need to approve the workflow Thanks for iterating - looks great! 

Just a small nit. It seems the slow model tests didn't run -- not really sure what happened there. Could you try with `[run-slow] ijepa` instead of `[run_slow] ijepa`? 

After than, just a merge conflict to resolve and we're good to go!  Thanks for the contribution! The code looks good to me, here are some main questions I have

1. We are moving to a new conversion script pattern (see [mllama](https://github.com/huggingface/transformers/blob/04b4e441dcd0ed966e158c66dac870e67cab8ea8/src/transformers/models/mllama/convert_mllama_weights_to_hf.py)), can you please adapt your script too.
2. Modeling code is mostly reused from ViT model with with some minor changes. Despite it is not common for `transformers` to use inheritance, we recently introduced a new feature - modular transformers. With this new feature you actually can use inheritance, and then your code is going to be unpacked to a single model - single file format. More information and examples are in this [issue](https://github.com/huggingface/transformers/issues/33916). Would you have bandwidth to try the feature with this model? It should be a great candidate!
3. Regrading model cards, it would be great to add code snippets there!

Thanks, let me know if you need any further assistance or have any questions! Hi @jmtzt! Thanks for trying a new modular converter! Great job! I left a few comments below, but overall looks almost ready! Hi @jmtzt! Seems like almost everything is fine! Thanks for correcting snippets in docs and model cards! Snippets work fine on my side, I also did an experiment fine-tuning classification model - converges really good.

A few final nits regarding docstrings/constants and a suggestion regarding classification head and I will pass it to a core maintainers review, thank you for the great job! Thanks for addressing all the comments quickly! IMO it's ready for the next review

P.S. It may take a while since there are a lot of PRs for the final review in a line 🤗  Thanks a lot for the PR! Modular makes it easy to understand: basically VIT but no cls token embedding right? (checked that no models in transformers already has this!) Very nice! Camel casing is a bit wrong but would look super ugly otherwise!","# What does this PR do?

This PR adds [I-JEPA](https://github.com/facebookresearch/ijepa).

To-Do's:
- [x]  convert remaining checkpoints `ijepa_vith14_22k`, `ijepa_vith16_1k`, `ijepa_vitg16_22k`.
- [ ]  transfer checkpoints to the meta org","To be filled in  No need for these tags if we only have pytorch

```suggestion
``` nit - formatting here looks a bit weird - our line length is 120 

```suggestion
    sys.modules[__name__] = _LazyModule(__name__, globals()[""__file__""], _import_structure, module_spec=__spec__)
``` onnx configs are now the responsibility of the optimum library 

```suggestion
``` Has this been tested?  copyright headers should be updated to reflect this model  The input docstring here doesn't match the signature. nit - standard in the library is to not have asserts in the modeling code 

```suggestion
``` 
```suggestion
    from .configuration_ijepa import IJepaConfig
``` It needs to be apdated with recent code changes, the bug was fixed in interpolate pos embedding method. Why `interpolate_pos_encoding` was removed? that was an oversight from my side, since in the original `VisitonTransformerPredictor` from I-JEPA it's not used. hmm, has something changed in the formatting config? lots of reformatting changes in this file Lets use ViTEmbeddings's `interpolate_pos_encoding` method modified to be without cls token

```python
    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:
        """"""
        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution
        images. This method is also adapted to support torch.jit tracing.

        Adapted from:
        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and
        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211
        """"""

        num_patches = embeddings.shape[1]
        num_positions = self.position_embeddings.shape[1]

        # always interpolate when tracing to ensure the exported model works for dynamic input shapes
        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:
            return self.position_embeddings

        patch_pos_embed = self.position_embeddings

        dim = embeddings.shape[-1]

        new_height = height // self.patch_size
        new_width = width // self.patch_size

        sqrt_num_positions = torch_int(num_positions**0.5)
        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)
        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)

        patch_pos_embed = nn.functional.interpolate(
            patch_pos_embed,
            size=(new_height, new_width),
            mode=""bicubic"",
            align_corners=False,
        )

        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)

        return patch_pos_embed
``` Can you please add a comment on why ""IJepaModel"" was added to this list? we also have to save image processor
```python
image_processor.save_pretrained(output_dir)
```

So, anyone later can load it from checkpoint together with a model:
```python
from transformers import AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained(""meta/i-jepa-..."")
``` awesome! thanks for adjusting it We should modify `IJepaForImageClassification` model prefix according to the `base_model_prefix` too:

```python
class IJepaForImageClassification(IJepaPreTrainedModel, ViTForImageClassification):
    def __init__(self, config: IJepaConfig):
        super().__init__(config)
        self.ijepa = IJepaModel(config)  # <-------   vit -> ijepa
        self.post_init()
``` Should it be different for different checkpoints? not used anywhere It would be nice to have a code snippet here on how to make a prediction with the model! + adding the same snippet to model cards on the HF Hub should be modified in modular too, smth like

```python
_CHECKPOINT_FOR_DOC = ""facebook/ijepa_vith14_1k""
``` my local formatter overwrote the init file, I'll revert it this is due to the modular model converter overwriting the `IJEPA_START_DOCSTRING` variable, leading to it not matching the function signature and when we do the repo consistency check, it fails, this happens similarly to the original ViT model, since it's also in this ignore list. @qubvel I found out that changing this from `vit` to `ijepa` makes the `IJepaModelTest::test_model_outputs_equivalence` fail w/ the following error: `TypeError: 'tuple' object does not support item assignment`, not sure why tbh Do we have any checkpoints with pretrained classification heads? hmm, you could also change `base_model_prefix` accordingly, but they should be consistent to be able to load pretrained weights in both models I suppose with modular we can overwrite class/method docstring too, let me know if this is not the case changed this to an image feature extraction example now... the issue was not having the `add_pooling_layer=False` in the `self.ijepa` definition inside the `IJepaForImageClassification`"
34156,2024-10-14T11:01:05Z,2024-12-05T16:07:33Z,jmamou,8,7,32,179,4,5,1,[],161125.0,0,4511188.0,0,0,0,0,1829046.411367,,0,32,0,False,"['gante', 'jmamou', 'ArthurZucker']","> In addition to the comments: can you share the benchmark results here as well, for future reference?

A100
target: starcoder
draft: tiny_starcoder
dataset: MBPP

Heuristics: mean_inference_time=16.33ms
Fixed threshold for dynamic SL https://github.com/huggingface/transformers/pull/33258: mean_inference_time=14.03ms
**Adaptive** threshold for dynamic SL (current PR): **mean_inference_time=13.42ms**

I will run later benchmark from https://huggingface.co/blog/dynamic_speculation_lookahead @jmamou yeah, let's please run more benchmarks before (iterating on the PR and) merging 

In the odd chance it ends up being beneficial only in very specific circumstances, I'd rather **not** merge the technique to avoid adding complexity (which usually reduces our team's ability to work on more projects 🤗 ) > @jmamou yeah, let's please run more benchmarks before (iterating on the PR and) merging
> 
> In the odd chance it ends up being beneficial only in very specific circumstances, I'd rather **not** merge the technique to avoid adding complexity (which usually reduces our team's ability to work on more projects 🤗 )

@gante  
I have run benchmarks from  https://huggingface.co/spaces/joaogante/assisted_generation_benchmarks
https://github.com/gante/huggingface-demos/tree/main/experiments/faster_generation
Evaluated metric: throughput -- time per token in ms, lower is better
Device: RTX 3090; dtype applies to both models


Model | Assistant | dtype | task | sampling? | w/o assistant | disco 0.4 | adaptive disco | disco speedup | adaptive disco speedup
-- | -- | -- | -- | -- | -- | -- | -- | -- | --
openai/whisper-large-v2 | openai/whisper-tiny | fp16 | automatic speech recognition | no | 20.02 | 14.59 | 13.81 | 1.37 | 1.45
facebook/opt-6.7b | facebook/opt-125m | bf16 | summarization | no | 23.81 | 8.73 | 8.72 | 2.73 | 2.73
facebook/opt-6.7b | facebook/opt-125m | bf16 | summarization | yes (t=0,6) | 24.21 | 12.01 | 10.55 | 2.02 | 2.29
facebook/opt-6.7b | facebook/opt-125m | bf16 | open-ended generation | no | 22.14 | 14.19 | 14.14 | 1.56 | 1.57
facebook/opt-6.7b | facebook/opt-125m | bf16 | open-ended generation | yes (t=0,7) | 22.13 | 14.16 | 14.09 | 1.56 | 1.57
Salesforce/codegen-6B-mono | Salesforce/codegen-350M-mono | bf16 | code generation (python) | no | 30.88 | 26.8 | 26.95 | 1.15 | 1.15
Salesforce/codegen-6B-mono | Salesforce/codegen-350M-mono | bf16 | code generation (python) | yes (t=0,4) | 37.02 | 35.88 | 33.79 | 1.03 | 1.1
google/flan-t5-xl | google/flan-t5-small | bf16 | summarization | no | 24.76 | 20.11 | 20.1 | 1.23 | 1.23
google/flan-t5-xl | google/flan-t5-small | bf16 | summarization | yes (t=0,6) | 24.44 | 26.78 | 25.15 | 0.91 | 0.97

Model | Assistant | dtype | task | sampling? |   |   |   |   |  
-- | -- | -- | -- | -- | -- | -- | -- | -- | --
meta-llama/Llama-3.1-8B | meta-llama/Llama-3.2-1B | bf16 | summarization | no | 33.06 | 19.27 | 19.29 | 1.72 | 1.71
meta-llama/Llama-3.1-8B | meta-llama/Llama-3.2-1B | bf16 | summarization | yes (t=0,6) | 33.6 | 24.35 | 21.69 | 1.38 | 1.55
meta-llama/Llama-3.1-8B | meta-llama/Llama-3.2-1B | bf16 | open-ended generation | no | 31.25 | 33.2 | 33.1 | 0.94 | 0.94
meta-llama/Llama-3.1-8B | meta-llama/Llama-3.2-1B | bf16 | open-ended generation | yes (t=0,7) | 31.35 | 42.29 | 39.02 | 0.74 | 0.8
meta-llama/Llama-3.1-8B | meta-llama/Llama-3.2-1B | bf16 | code generation (python) | no | 27.98 | 19.49 | 19.72 | 1.44 | 1.42
meta-llama/Llama-3.1-8B | meta-llama/Llama-3.2-1B | bf16 | code generation (python) | yes (t=0,4) | 28.6 | 24.23 | 20.85 | 1.18 | 1.37


An improvement is observed when `do_sample=True`, likely because the threshold was set to 0.4 to optimize for greedy decoding. It seems that a lower threshold may be needed when sampling, highlighting the need to adapt the threshold as proposed in the PR ...
 @jmamou I'm convinced :D The benchmarks do show a consistent upgrade @ArthurZucker could you please review it? > LGTM! It's just missing a test !

@ArthurZucker 
done! `from transformers.generation.candidate_generator import AssistedCandidateGenerator` needs to protect it's import to torch! `candidate_generator.py` needs to check python availability! Let's GOOOOOOO! 🚀 In addition to the comments: can you share the benchmark results here as well, for future reference?  LGTM, thank you for the thorough benchmark 🤗  LGTM! It's just missing a test ! LGTM! It's just missing a test !","In addition to the comments: can you share the benchmark results here as well, for future reference?  LGTM, thank you for the thorough benchmark 🤗  LGTM! It's just missing a test ! LGTM! It's just missing a test !","# What does this PR do?
Following
https://github.com/huggingface/transformers/pull/33258
https://github.com/huggingface/transformers/pull/33657

The assistant's confidence threshold is adjusted throughout the speculative iterations to reduce the number of unnecessary draft and target forward passes. The costs are estimated based on the ROC curve, which considers the probability of the draft token and its match with the target. A cost of 25% is assigned to false positives and 75% to false negatives.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@gante @amyeroberts ","```suggestion
            (defined by `num_assistant_tokens`) is not yet reached. The assistant's confidence threshold is adjusted throughout the speculative iterations to reduce the number of unnecessary draft and target forward passes, biased towards avoiding false negatives.
```
(let's make the docstring short, and leave the details in the implementation) sklearn is not installed by default, and we don't want to introduce a hard dependency to use assisted generation.

1 - we'll have to wrap this import in an import guard, i.e.
```py
from ..utils import is_sklearn_available
if is_sklearn_available():
    from sklearn.metrics import roc_curve
```
otherwise folks without the library will fail to initialize this module
2 - given that's an optional import and that this PR's technique ""only"" boosts speed, my suggestion would be to use the boost if the library is available, and don't use it if not. That way, assisted generation always works and the user is not forced to install an extra dependency 🤗 In the docs, we should recommend to install sklearn to benefit from this boost. This block can be conditional to `is_sklearn_available`, correct? (if we don't have sklearn we don't adapt the threshold, and therefore we don't need to store this data) Same comment as above @gante where do you suggest to  add the recommendation in the docs? 
[here](https://huggingface.co/docs/transformers/main/en/generation_strategies#speculative-decoding) ?
 Consider vectorizing to flatten the for-loop @jmamou yes, that will be the correct place in docs"
35081,2024-12-04T09:28:44Z,2024-12-05T15:30:09Z,zucchini-nlp,1,0,1,8,4,1,1,[],1628.0,0,108085.0,0,0,0,0,1831291.743501,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35081). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Yeah I thought why are we adding this after, but was in a rush!","Yeah I thought why are we adding this after, but was in a rush!","# What does this PR do?

Follows the prev PR https://github.com/huggingface/transformers/pull/34876, as the prev one was appending image tokens after BOS. It should be the other way around. Sorry, the last commit to increase readability messed up concatenation order",
35062,2024-12-03T16:18:26Z,2024-12-05T16:02:28Z,ydshieh,3,20,23,1654,11,4,2,[],1784.0,0,171844.0,0,0,0,0,1829350.887499,,0,23,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35062). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thank you @Wauplin Answering a few questions here:

> Oh, I think I misunderstood the use case here then 😕 What's the goal of TemporaryHubRepo then?

Yes, it's a bit confusing. Actually so far it just prepares a `repo_id` to be used in the test, for example, `push_to_hub(repo_id)`. Of course we can create the repo. in `__init__` or `__enter__` (which seems more nature and make more sense!).

> (nit) be careful that this is a repo_name and not a repo_id per-se since it doesn't contain the namespace. 

so far both can be passed without problem (in the test, it's the actual repo_id with `/` passed). But yeah, you are right, it makes the API unnecessary more complicated.

> Also since self.random_id can be None, then the repo_name might not be unique.

In this case I will prepare a random id (just not implemented yet so far, sorry for confusion)

> What I would advice you to do is to remove both random_id and prefix from this helper to simplify things as much as possible for the users.
- For `repo_id` argument: I was trying to keep `f""{USER}/test-generation-config` stuff (already in `main` branch)
  - (kind have its value of keeping information)
- For `random_id` argument: to allow passing the id created from `tempfile.TemporaryDirectory`, so there is a correspondence between the local dir and hub repo.

But yes, none of these 2 are really necessary. Updated as suggested. @Wauplin Could you take another look and let me know how you think? Especially regarding the arguments for __init__. ThanksHey @ydshieh thanks for the ping. I've left a few comments to compare to what's been done in `huggingface_hub` (see https://github.com/huggingface/huggingface_hub/blob/main/tests/testing_utils.py). I think UX can be improved by simplifying the interface. Looks good! I still have a suspicion on the tmp repo not been deleted in case of failure. Apart from that, seems good to me Much cleaner! Thanks :hugs:  Wow that's very clean! Thanks for spending time on this @ydshieh!","Hey @ydshieh thanks for the ping. I've left a few comments to compare to what's been done in `huggingface_hub` (see https://github.com/huggingface/huggingface_hub/blob/main/tests/testing_utils.py). I think UX can be improved by simplifying the interface. Looks good! I still have a suspicion on the tmp repo not been deleted in case of failure. Apart from that, seems good to me Much cleaner! Thanks :hugs:  Wow that's very clean! Thanks for spending time on this @ydshieh!","# What does this PR do?

The goal is to avoid race condition in Hub repo. related tests. Based on #31973, this PR:

- remove `_try_delete_repo` and `try ... except ...` in many files
- create `TemporaryHubRepo` in `testing_utils.py` (similar to `tempfile.TemporaryDirectory`)
- apply the same fix to `test_trainer.py` (which is currently flaky)
","We don't really need to keep this - we can just create a repo. name.

But I just try to keep as it is just allow passing an (random) `id` to `TemporaryHubRepo`. I give the argument the name `random_id` as I want to emphasis it should be random, and it's the caller's responsibility to make sure it's random (if it is passed) I should prepare a `random_id` if it is not specified in the caller.

I would prefer to use `tempfile.TemporaryDirectory` way to create a random id (directory name) though, but it's not really necessary.

My final goal is actually to have something


```python
def __init__(self, prefix=""repo"", random_id=None, token=None, create_local_dir=False):
```
which will use `tempfile.TemporaryDirectory` to create local temp dir if specified as `True`, so we don't have to do

```python
        with tempfile.TemporaryDirectory() as tmp_dir:
            with TemporaryHubRepo(prefix=repo_id, random_id=Path(tmp_dir).name, token=self._token) as tmp_repo:
```

but probably it's not a good idea? we don't really create a Hub repo. at this point. therefore we have to allow this case. ```suggestion
        delete_repo(repo_id=self.repo_id, token=self.token, missing_ok=True)
```

You can also pass `missing_ok=True` as parameter (will catch the repo not found error for you) this is a mistake. will revert Oh, I think I misunderstood the use case here then :confused: What's the goal of `TemporaryHubRepo` then? To be updated not updated yet  - just copied from (nit) be careful that this is a `repo_name` and not a `repo_id` per-se since it doesn't contain the namespace. Also since `self.random_id` can be `None`, then the repo_name might not be unique.  in `huggingface_hub`, we use 

```py
def repo_name(id: Optional[str] = None, prefix: str = ""repo"") -> str:
    """"""
    Return a readable pseudo-unique repository name for tests.

    Example:
    ```py
    >>> repo_name()
    repo-2fe93f-16599646671840

    >>> repo_name(""my-space"", prefix='space')
    space-my-space-16599481979701
    """"""
    if id is None:
        id = uuid.uuid4().hex[:6]
    ts = int(time.time() * 10e3)
    return f""{prefix}-{id}-{ts}""
```

to make the repo name pseudo-unique. To be honest, `prefix: str = ""repo""` is there for legacy reasons but it doesn't bring anything useful to the user. What I would advice you to do is to remove both `random_id` and `prefix` from this helper to simplify things as much as possible for the users. @Wauplin Here probably it's better to allow `create_repo_kwargs`? it could be added later if necessary but I don't think it is right? In `huggingface_hub` we have extra kwarg typically for `repo_type` but if `transformers` only gonna play with model repos, then it's not needed. Better to add when there is a need IMO Not related to this PR but `huggingface_hub.HfFolder` is a deprecated class. Better to use `huggingface_hub.get_token` and `huggingface_hub.save_token` instead. Can be done in a later PR. I just realized that I'm not sure anymore if `__exit__` is called when a test fails :confused: Might be the case but not 100% sure. If not called, it means tmp repos won't be garbage collected as previously with the ""try/finally"". noted, thank you It will!  Thank god the python core members implement this !  ```suggestion
    with TemporaryHubRepo(token=self._token) as temp_repo:
        model.push_to_hub(tmp_repo.repo_id, token=self._token)
```

(token should be passed to both calls IIUC) ```suggestion
    def __init__(self, namespace: Optional[str]=None, token: Optiona[str]=None) -> None:
```
(nit)"
35001,2024-11-28T15:11:44Z,2024-12-05T15:11:09Z,MekkCyber,3,0,3,53,6,3,3,[],1245.0,0,604767.0,0,0,0,0,1832430.487329,,0,3,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev', 'ArthurZucker']",To merge a bit before the release  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35001). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. can you resolve the conflicts please? Nice cleanup !  release is soon,Nice cleanup !  release is soon,"# What does this PR do?
Deprecates the use of `is_quanto_available` after switching to `optimum-quanto` package instead

## Who can review ?
@SunMarc ",
34829,2024-11-20T15:12:49Z,2024-12-05T14:47:20Z,Cyrilvallez,2,3,3,402,1,2,1,[],2094.0,0,1294473.0,0,0,0,0,1833860.92807,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'Cyrilvallez']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34829). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. IMO it makes more sense now (the keyword is more descriptive, and this case is very unlikely to reappear in future conversions), but I can revert if you think it can break some backward stuff! 🤗Very nice thanks! Would be great to keep the `is-v3` to not have a  breaking change! ",Very nice thanks! Would be great to keep the `is-v3` to not have a  breaking change! ,"# What does this PR do?

Update/refactor Mistral conversion script to work with latest models.

cc @ArthurZucker 
","nice equivalent to is v3 no?
 Yes, but it's actually the opposite (`is_v3 = not modules_are_split`) because this corresponds to an old case, so I switched the default which makes much more sense now. But I can revert if needed"
34972,2024-11-27T14:09:53Z,2024-12-05T14:46:02Z,ArthurZucker,1,0,2,4,2,2,3,[],1788.0,0,693374.0,0,0,0,0,1833936.512697,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34972). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks! LGTM,Thanks! LGTM,"# What does this PR do?
Bump tokenizers version as python3.7 and 3.8 support was dropped",
34836,2024-11-20T21:11:38Z,2024-11-28T15:04:05Z,mgoin,15,20,15,634,10,3,1,"['Vision', 'optimization', 'Multimodal', 'Processing']",6028.0,0,1272926.0,0,0,0,0,1833879.638504,,0,15,0,False,"['qubvel', 'yonigozlan', 'mgoin', 'HuggingFaceDocBuilderDev', 'ArthurZucker']","Hi @mgoin! Sounds great! Thanks for working on this 🤗 

cc @yonigozlan maybe if you have bandwidth Thanks for the review and context @yonigozlan ! I will look into it later today. Yes you are correct about using it within a Processor, however I have tested this works within vLLM simply by adding `use_fast=True` to our `AutoProcessor.from_pretrained()` call [here](https://github.com/vllm-project/vllm/blob/da7e702c6fae521bf8633affb8fe7b834f5cb94b/vllm/transformers_utils/processor.py#L18-L22). No need to manually specify the Processor class.

One bug I noticed is that if I specify `use_fast=True` and there isn't a Fast version of the ImageProcessor available, I get an exception. I can look into this, but would be good to get clarity that this is unintended behavior. Oh great news that it already works with AutoProcessor. As I said this is the first fast image processor used in a processor so it was not guaranteed :).

> One bug I noticed is that if I specify use_fast=True and there isn't a Fast version of the ImageProcessor available, I get an exception. I can look into this, but would be good to get clarity that this is unintended behavior.

Yes this is the same right now when using ImageProcessingAuto. I don't think it should be that way though, especially as more and more people will want to use fast image processors by default. I'll open a PR to fix this.

Current plan is:
- keep use_fast to False by default in Auto classes during a deprecation cycle, fall back on slow image processor (with a warning) if use_fast is set to True and no fast image processor exists
- set use_fast to True by default in Auto classes after the deprecation cycle (by then most models will hopefully have a fast image processor), still fall back on slow if no fast image processor exists.

The deprecation cycle is needed as there are slight differences in outputs when using torchvision vs PIL, see this PR https://github.com/huggingface/transformers/pull/34785 for more info. Feel free to ping us for another round of review! 🚀  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34836). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks @yonigozlan and @ArthurZucker - this PR is ready for more review! I believe the failing test is unrelated Hi @yonigozlan what do you think about the use of `math.ceil`? > Hi @yonigozlan what do you think about the use of math.ceil?

Sorry to be annoying with this but I'd prefer `torch.ceil`. I'd say we leave the `get_resize_output_image_size` function as it is (with np.ceil) in `image_processing_pixtral`, and rewrite one with torch in `image_processing_pixtral_fast`.
 Sorry if I am misunderstanding something @yonigozlan  but `torch.ceil` only works on [`torch.Tensor`](https://pytorch.org/docs/stable/generated/torch.ceil.html). Since we are working with `height`/`width` as `int` primitives I cannot substitute `torch.ceil`. 

Are you suggesting that I wrap the values in a Tensor just to extract them back out as primitives? This is inefficient and unneeded to work with `torch.compile`

```python
height = int(np.ceil(height / ratio))
# versus
height = int(torch.ceil(torch.tensor(height / ratio)).item())
```

Considering [`torchvision.transforms.v2.functional.resize`](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.functional.resize.html) takes in size as a list of ints (`size: Optional[List[int]]`), I don't see the benefit of using `torch.Tensor` for the height and width. If using numpy is the concern, I think using `math.ceil` makes sense. Hi @mgoin,
Sorry about that, I went over the code too quickly and completely missed that we were dealing with floats and not arrays in the np.ceil. So what you did with `ImageInput` and `math.ceil` in `image_processing_pixtral.get_resize_output_image_size` makes perfect sense, you can ignore my last few messages🤗. 
LGTM then! :) Nice thanks for clarification! Hi @ArthurZucker would you mind signing off? Yep having a look! Thanks a lot @mgoin for this contribution! 🔥  > LGTM, sorry for the delay! My main question is for @yonigozlan , do we have compile tests for image processor fast?

I don't think so, I'll open a PR for that! Cool thanks! Thanks a lot @mgoin for working on this! Looks great to me, just mentioned some minor things to fix.

To be transparent, the current plan for fast image processors is to make a strong BaseImageProcessorFast and several image processing Mixins, in order to make adding new fast image processor much simpler. All that to say that this processor might change again in the future. Meanwhile, I think it would be great to have as is, because as you can see it makes a huge performance difference, and this fast image processor in particular doesn't require a huge diff (compared to the DETR ones for example).

Also, this would be the first fast image processor used in a processor, and I don't think there is a mechanism to use it with AutoProcessor yet (I might be wrong), so this is also something that will need to be added soon. 
But I'm curious then to know how you are using it with a PixtralProcessor, I'm guessing you manually instantiate it with:
```python
from transformers import PixtralProcessor, AutoImageProcessor, AutoTokenizer

fast_image_processor = AutoImageProcessor.from_pretrained(""mistral-community/pixtral-12b"", use_fast=True)
tokenizer = AutoTokenizer.from_pretrained(""mistral-community/pixtral-12b"")

processor = PixtralProcessor(fast_image_processor, tokenizer)
```
Is that correct?

Thanks again!
 Thanks for iterating!
LGTM after adding a `get_resize_output_image_size` with torch instead of numpy LGTM, sorry for the delay! My main question is for @yonigozlan , do we have compile tests for image processor fast? ","Thanks a lot @mgoin for working on this! Looks great to me, just mentioned some minor things to fix.

To be transparent, the current plan for fast image processors is to make a strong BaseImageProcessorFast and several image processing Mixins, in order to make adding new fast image processor much simpler. All that to say that this processor might change again in the future. Meanwhile, I think it would be great to have as is, because as you can see it makes a huge performance difference, and this fast image processor in particular doesn't require a huge diff (compared to the DETR ones for example).

Also, this would be the first fast image processor used in a processor, and I don't think there is a mechanism to use it with AutoProcessor yet (I might be wrong), so this is also something that will need to be added soon. 
But I'm curious then to know how you are using it with a PixtralProcessor, I'm guessing you manually instantiate it with:
```python
from transformers import PixtralProcessor, AutoImageProcessor, AutoTokenizer

fast_image_processor = AutoImageProcessor.from_pretrained(""mistral-community/pixtral-12b"", use_fast=True)
tokenizer = AutoTokenizer.from_pretrained(""mistral-community/pixtral-12b"")

processor = PixtralProcessor(fast_image_processor, tokenizer)
```
Is that correct?

Thanks again!
 Thanks for iterating!
LGTM after adding a `get_resize_output_image_size` with torch instead of numpy LGTM, sorry for the delay! My main question is for @yonigozlan , do we have compile tests for image processor fast? ","# What does this PR do?

This PR implements a fast image processor for Pixtral. Follows issue https://github.com/huggingface/transformers/issues/33810.

The key acceleration comes from replacing Pillow/Numpy tensors and functions (resize, rescale, normalize) with torch tensors and torchvisionv2 functions. It comes along with support for `torch.compile` and passing `device=""cuda""` during inference to process the input on GPU. One limitation is that only `return_tensors=""pt""` will be supported. 

Usage
```python
from transformers import AutoImageProcessor

slow_processor = AutoImageProcessor.from_pretrained(""mistral-community/pixtral-12b"", use_fast=False)
fast_processor = AutoImageProcessor.from_pretrained(""mistral-community/pixtral-12b"", use_fast=True)
compiled_processor = torch.compile(fast_processor, mode=""reduce-overhead"")
```

From simple benchmarking with a single image of size `[3, 876, 1300]`, I see 6x to 10x speedup

![image](https://github.com/user-attachments/assets/7674c923-4c76-4629-aac1-ffc7856824c4)

```
--------------------------------------------------
Slow Processor (PIL Image) Statistics (milliseconds):
          Mean: 23.680
        Median: 23.098
       Std Dev: 2.240
           Min: 21.824
           Max: 36.064

--------------------------------------------------
Fast Processor (PIL Image) Statistics (milliseconds):
          Mean: 3.759
        Median: 3.762
       Std Dev: 0.133
           Min: 3.556
           Max: 4.223

--------------------------------------------------
Compiled Processor (PIL Image) Statistics (milliseconds):
          Mean: 4.632
        Median: 4.794
       Std Dev: 1.086
           Min: 3.488
           Max: 11.707

--------------------------------------------------
Slow Processor (Torch Image) Statistics (milliseconds):
          Mean: 22.331
        Median: 21.878
       Std Dev: 1.821
           Min: 21.316
           Max: 36.603

--------------------------------------------------
Fast Processor (Torch Image) Statistics (milliseconds):
          Mean: 2.242
        Median: 2.209
       Std Dev: 0.164
           Min: 2.182
           Max: 3.803

--------------------------------------------------
Compiled Processor (Torch Image) Statistics (milliseconds):
          Mean: 2.125
        Median: 2.117
       Std Dev: 0.073
           Min: 2.062
           Max: 2.594
```


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
}

``` This is actually my fault, but the import structure for `ImageProcessorFast` should be safeguarded by a torchvision dependency. I just opened this PR https://github.com/huggingface/transformers/pull/34859 to fix this for DETR models. If you could do the same here that woulde be great :) Same here Same here and here This uses numpy functions so it would be better to rewrite it using torch/torchvision functions, as the goal of fast image processors is to keep all processing within the torch ecosystem. 
I also wonder if it might cause performance issues, especially when compiling. ```suggestion
    Constructs a fast Pixtral image processor.
```
:) I was thinking of writing a `validate_fast_preprocess_arguments` in `image_utils` that would combine this in one function. If you could do it here that would be great :). You'll have to do `make fix-copies` to put this in `dummy_torchvision_objects` once you made the changes I mentioned above ```suggestion
    from transformers import PixtralImageProcessor

    if is_torchvision_available():
        from transformers import PixtralImageProcessorFast
``` What comes after this should also be in the for loop Thanks, this is clearly an improvement - `BatchMixFeature` is essentially torch-only
- `convert_to_rgb` only does something if the image is a `PIL.Image.Image`, otherwise it just returns the image
- `get_resize_output_image_size` just uses `image.shape` so it doesn't make a difference for either numpy or torch

So I don't see where an issue would arise. I can update the type of the argument to make it clear Good catch! Sorry I meant only `get_resize_output_image_size`. `np.ceil` is also used there.
But agreed there are no issues with `BatchMixFeature ` and `convert_to_rgb`  Thanks for that 🤗 I still think it would be better to rewrite this function with `torch.ceil` instead of `np.ceil` Ah I didn't notice the np.ceil, thanks for pointing out Sorry I missed your comment response. Since `height`, `width`, and `ratio` are python scalars it doesn't make sense to use torch here. I can replace the `np.ceil` with just `math.ceil` though. Just saw this, indeed my mistake :)"
35059,2024-12-03T13:55:56Z,2024-12-05T08:50:27Z,ydshieh,1,0,4,12,4,1,1,[],1672.0,0,154473.0,0,0,0,0,1855275.908784,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35059). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?

informative info for pickle (same as #34154",
35067,2024-12-03T22:05:43Z,2024-12-04T17:18:44Z,stevhliu,1,1,2,84,11,1,1,[],1726.0,0,69196.0,0,0,0,0,1911166.812251,,1,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35067). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Needed indeed! ,Needed indeed! ,"Fixes #34743.

Increases visibility of `torch_dtype=""auto""` throughout the docs to ensure users are aware that `""auto""` loads weights in the dtype specified in `config.json`. Temporarily resolves the issue, refer to #34919 to track the more permanent solution.","Would even put this in ""warning"" of some sort!"
35055,2024-12-03T07:17:39Z,2024-12-04T15:48:34Z,faaany,2,1,2,3,1,1,1,[],64121.0,0,117056.0,0,0,0,0,1916590.87864,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'faaany']",done The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35055). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks!,Thanks!,"## What does this PR do?
Just as the title suggests. I think it is necessary to mention that offloading requires CUDA again, as users could directly jump to this section.

Documentation: @stevhliu","```suggestion
Cache offloading requires a CUDA GPU.
```"
34896,2024-11-23T16:31:38Z,2024-12-04T13:48:28Z,vasqu,10,30,20,620,6,2,2,[],2168.0,0,946422.0,0,0,0,0,1917987.95089,,0,20,0,False,"['vasqu', 'dame-cell', 'ArthurZucker']","CI failings seem unrelated, flaky tests (e.g. XLM, Qwen2VL) Possible TODO -> fallback to eager when using head mask in fa2, sdpa + add head mask in flex attention (should be possible via score mod)

Edit: Added now Yes I was testing it for Gemma as well it needed a transpose at the end as well 

If you don't mind could you check the pull request i did for gemma seems I keep failing some tests 

Also the gemma 2 now supports new stuff in the  configuration which confused me a lot
The attn logit soft capping  


 Also the model.config._attn_implementation is not really implemented correctly for example it does not  correctly uses the correct attn upon choosing one


Still working on the gemma flex attention pr might help with the docs as well
 @dame-cell I'll take a look tomorrow! I'm busted for today :) 

But as quick thing to let the loading be handled correctly look into my changes into the utils folder and modeling_utils. With those changes, loading should be handled correctly. Tbh, that's one of the main reasons why I think it might be better to split some PRs and get loading etc correctly first before we start adding.

Edit: One last thing to change would be to add `_supports_flex_attn = True` then like done for sdpa, fa2 > @dame-cell I'll take a look tomorrow! I'm busted for today :) 
> 
> But as quick thing to let the loading be handled correctly look into my changes into the utils folder and modeling_utils. With those changes, loading should be handled correctly. Tbh, that's one of the main reasons why I think it might be better to split some PRs and get loading etc correctly first before we start adding.

Hmmm ohh I get it I see thanks for letting me know 😀 Feel free to ping me once you feel like this is ready! 🤗  I think it should be ready @ArthurZucker just found something on the fly just a min ago, should be good to go :smile: 

Edit: the ci failure doesn't seem related to this PR I'll take a look tomorrow or the day after :)  @ArthurZucker updated per review, looking forward to the next round ;) @ArthurZucker Hopefully the last round :crossed_fingers: A collection of comments which partially show the issues I listed above Thanks for improving the API `_check_and_enable_flex_attn` was missing from my inital PR!  I think the PR gets slowly wrapped up; some things that should be made into separate PR(s) imo:
- [ ] Common tests
- [ ] Docs
- [ ] Tracking when attn implementation is manually changed within the config Very nice! 
Left a few small comments but almost ready to go! Just a list of TODOs (separate PRs):
- [ ] Deprecate FA peft integration checks
- [ ] Deprecate RoPE reconversion
- [ ] Docs
- [ ] Common Tests
- [ ] Track when attn implementation is manually changed

Otherwise, I think this PR is good now! LGTM now thanks a lot for the refactor!","A collection of comments which partially show the issues I listed above Thanks for improving the API `_check_and_enable_flex_attn` was missing from my inital PR!  I think the PR gets slowly wrapped up; some things that should be made into separate PR(s) imo:
- [ ] Common tests
- [ ] Docs
- [ ] Tracking when attn implementation is manually changed within the config Very nice! 
Left a few small comments but almost ready to go! Just a list of TODOs (separate PRs):
- [ ] Deprecate FA peft integration checks
- [ ] Deprecate RoPE reconversion
- [ ] Docs
- [ ] Common Tests
- [ ] Track when attn implementation is manually changed

Otherwise, I think this PR is good now! LGTM now thanks a lot for the refactor!","# What does this PR do?
Adds flex attention and the refactor according to #34809

However, I discovered several issues in the current version of gemma2 (#34282):
- It seems like that flex attention needs a transpose afterwards like sdpa
- Loading flex attn with from pretrained didn't work and hence, current tests use another attn implementation (eager or sdpa not sure again)
- Tests could gain from similar tests like sdpa :D for now it's a bit of a hassle to always have some integration test added when it could be a more general test for all subsequent models
- I'm not familiar with better transformers or limitations of flex attn --> added some todos in case we need to check in
- Flex attn doesn't support dropout (or maybe I've overlooked something)
- Setting `model.config._attn_implementation = ...` should be tracked somewhere and checked for sanity as done the first time - for now it silently overwrites and could cause some ugly errors (tested with changing to flash attention 2 while not having fa2 installed)
- Documentation should be added somewhere (prolly perf or something else)

So tbh, I'm not sure whether to split this PR into several ones, e.g. a gemma fix, general loading, general tests, docs, and then subsequent models, or not

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@ArthurZucker ","Likely linking to #34809 Just let it there since I'm not familiar with better transformers and if there needs to be a check or smthn Also here unsure if we will encounter bugs ;) Would love to have common tests in the future (instead) Removed since we handle this outside the forward for the attention masks; kept the buffers for BC so weights loading won't complain. I think this is missing in gemma2 as well: It's using the config but unsure if that's sufficient. No dropout in flex attn Transpose which is (possibly) missing in gemma2  This peft check also seems to be missing in gemma2 very nice! ```suggestion
```
better transformers has been deprecated for a while now! I think we are better off just explicitly writing this!  Le't s always cast, this is internally done pretty sure there is no difference we can also just use transpose!  Yeah. To be honest it should be done in `_flash_attention_forward` rather than here as it's the interface ! same comment here! do we have to declare this for compile ? otherwise would rather not write it That's indeed missing from gemma2 Potentially yes, but also because you need a higher version of torch for gemma2 AFAIR!  That's supper verbose 😢 we could just say that we are falling back to eager due to config missmatch or something! that's a great catch! but we can add it to the score-mod no? no this should only be checked in SDPA path!  And it should only be check in FA2 path! Added link to the issue Added this within FA2 only and made the check an lru cache so we don't suffer from performance downgrades. Sadly not, in the case of the head mask the order of ops doesn't matter since we completely turn the head to all zeros but dropout still depends on the correct distribution calculations and just then turns off some values.

The order of ops is: `dropout(softmax(score_mod(Q, K)))` --> we would introduce unwanted behaviour.

Edit: llama for ref https://github.com/huggingface/transformers/blob/0b5b5e6a70249837293499e9363a64765a57111c/src/transformers/models/llama/modeling_llama.py#L339-L340 I've added this to the FA2 interface, lmk if this is correct and/or if I should change things. Noticed that we can handle like the other implementations and only introduce a transpose here :D Done Changed it, was just a personal preference"
35007,2024-11-28T17:23:43Z,2024-12-04T13:43:36Z,VladOS95-cyber,4,1,5,118,8,1,1,[],122.0,0,505207.0,0,0,0,0,1924078.130697,,0,5,0,False,"['VladOS95-cyber', 'ArthurZucker']","Hey @ArthurZucker! This PR is ready for review, please, take a look. I think I should Fix Starcoder2 as well Thanks, sorry a bit slow this week, reviewing asap! Thanks @VladOS95-cyber 🤗 Lovely! Thanks for fixing and bringing more support!",Lovely! Thanks for fixing and bringing more support!,"# What does this PR do?

This PR uses the torch.distributed.tensor.parallel subpackage to implement Tensor Parallel for Qwen2, Qwen2Moe, Starcoder2.
Fix qkv states dims for Mistral
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Link: https://github.com/huggingface/transformers/issues/34789
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker 
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
    # Default tensor parallel plan for base model `Qwen2Moe`
```"
35056,2024-12-03T08:42:37Z,2024-12-04T14:13:11Z,Cyrilvallez,1,5,8,2529,13,2,1,[],1614.0,0,106236.0,0,0,0,0,1922315.229951,,0,8,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35056). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for addressing, we need to merge and now work on unitests!","Thanks for addressing, we need to merge and now work on unitests!","# What does this PR do?

This improves the modular converter when adding a new multi-modal model (with some `Vision` and `Text` part added to the model name) + allow correct renaming for all uppercase model names (e.g. `CLIP`, see [example](https://github.com/huggingface/transformers/compare/multimodal-modular?expand=1#diff-0363c40d762885e8a1376b54964b426b0640606f9497cb298137d8cce47ead93)).

A multi-modal model with one part (e.g. the `Text` part) being a copy/paste version of an all text model (e.g. `Llama`), can directly use the new prefix `NewModelText` instead of `NewModel`, and everything will be renamed accordingly in `Llama` (see [example](https://github.com/huggingface/transformers/compare/multimodal-modular?expand=1#diff-ecfccbecf7f2bf28a10ed66f42c28e4f9c80540fa41cb3be0f8b7c61eec065e3)). However, due to previous inconsistencies in naming, if inheriting from another multi-modal model which did not use the same patterns with the `Text` and `Vision` part, the renaming will not be possible, in which case default renaming (`OldModel` -> `NewModel`) will be used, and intermediate classes without the prefix will need to be manually added with the new prefix (see [example](https://github.com/huggingface/transformers/compare/multimodal-modular?expand=1#diff-85f4d9cb9e1a70fc20e54bbf023f477c8635a7d6fa23e5a425ca24b892dd924f)).  
In all cases, a warning will be raised to tell which prefix is used in case of potential ambiguous situation, and explain that intermediate classes may need to be manually added to the modular.","```suggestion
            # does not make sense, i.e. a class is used somewhere before being defined... like in CLIP) 
``` ah this is for config imports I suppose!
 LGTM as always would be nice to have a CLIP example in the comment! 🤗  Yes!"
34005,2024-10-07T13:16:56Z,2024-12-04T10:15:26Z,tshu-w,1,0,2,2,1,2,2,[],5000322.0,0,5006664.0,0,0,0,0,1930229.918917,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34005). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Seems fair! Sorry for the delay, forgot about this PR. LGTM, merging now","Seems fair! Sorry for the delay, forgot about this PR. LGTM, merging now","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@zucchini-nlp @gante 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35048,2024-12-02T23:25:01Z,2024-12-03T18:54:16Z,faaany,1,0,1,21,1,1,1,[],69924.0,0,70155.0,0,0,0,0,1991854.180732,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35048). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for updating everywhere! :D,Thanks for updating everywhere! :D,"## What does this PR do?
The same with other PRs. :D 

Documentation: @stevhliu


",
32306,2024-07-29T23:29:54Z,2024-08-14T14:14:25Z,jerryzh168,17,30,13,542,13,4,1,['Quantization'],216513.0,0,10980360.0,0,0,0,0,1967757.459646,,0,13,0,False,"['amyeroberts', 'jerryzh168', 'imba-tjd', 'SunMarc', 'HuggingFaceDocBuilderDev', 'msaroufim', 'sayakpaul', 'BenjaminBossan']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32306). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. cc @SunMarc  > Hi @jerryzh168, thanks for doing the integration ! I did a first pass and the pr looks pretty clean ! I've left a few comments. Also, can you share a bit about how serialization might look like and the issue that you are facing ? It's fine if we don't support it in this PR. This can come in a follow-up PR !

@SunMarc thanks for the review, I have addressed/replied to all the comments.

I talked about serialization briefly in https://github.com/jerryzh168/transformers/blob/torchao/docs/source/en/quantization/torchao.md

main thing is torchao is using tensor subclasses, and it does not work well with serialization code in huggingface, we have opened issue here: https://github.com/huggingface/transformers/issues/32364, the problem we are seeing now is huggingface is relying on `storage_id` to figure out the sharing relationship of tensors: https://github.com/huggingface/huggingface_hub/blob/97d5ef603f41314a52eb2d045ec966cf9fed6295/src/huggingface_hub/serialization/_torch.py#L479, and the assumption that all tensors have storage is been broken by subclassed tensors. we can recommend a more general solutions for this. cc @albanD

 @SunMarc thanks for the review, I addressed your comments, and had one remaining question (see inline) Is safetensors serialisation supported for the quantized weights?

Reference: https://github.com/huggingface/safetensors/issues/515#issuecomment-2271208072 > Is safetensors serialisation supported for the quantized weights?
> 
> Reference: [huggingface/safetensors#515 (comment)](https://github.com/huggingface/safetensors/issues/515#issuecomment-2271208072)

safetensor is not supported yet, still figuring out the plan there, but I'd expect us to support non-safe tensor serialization first Hey @SunMarc anything else we need to get this landed? Sort of related we're releasing ao 0.4 tomorrow and was hoping we could add this integration to our release notes Here's the PR with a couple of fix: https://github.com/jerryzh168/transformers/pull/1. Oh my bad, I pushed on your branch by mistake and it merged the PR. I didn't know I could push ;) Feel free to revert if you don't agree with some changes I made !  > Here's the PR with a couple of fix: [jerryzh168#1](https://github.com/jerryzh168/transformers/pull/1). Oh my bad, I pushed on your branch by mistake and it merged the PR. I didn't know I could push ;) Feel free to revert if you don't agree with some changes I made !

I see, that's fine, thanks for the fix! I thought `get_keys_to_not_convert` is model specific? does it apply to all models? `get_keys_to_not_convert` applies to all models. This is something we added for all our quantization methods and it seems to work pretty well ! I also changed a bit the torch_dtype logic !  I also fixed a new nits in the tests ! Hi everyone, I ran some experiments and it is actually possible to train a torchao-quantized model with PEFT, even though it is not explicitly supported in PEFT yet (working on it). The reason why it works is because the type of the layer is not changed by torchao, so for instance `nn.Linear` stays `nn.Linear`. Therefore, PEFT will apply a normal PEFT layer, e.g. `lora.Linear`.

I wrote some unit tests to check this and it looks like it works:

```python
import gc
import tempfile
import unittest

import torch
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer

class PeftTorchaoGPUTests(unittest.TestCase):
    supported_quant_types = [
        ""int8_weight_only"",
        ""int8_dynamic_activation_int8_weight"",
    ]

    def setUp(self):
        self.causal_lm_model_id = ""facebook/opt-125m""
        self.tokenizer = AutoTokenizer.from_pretrained(self.causal_lm_model_id)

    def tearDown(self):
        gc.collect()
        torch.cuda.empty_cache()

    def test_causal_lm_training_single_gpu_torchao(self):
        # call this with CUDA_VISIBLE_DEVICES=0
        from transformers import TorchAoConfig

        device = 0

        for quant_type in self.supported_quant_types:
            with tempfile.TemporaryDirectory() as tmp_dir:
                quantization_config = TorchAoConfig(quant_type=quant_type)
                model = AutoModelForCausalLM.from_pretrained(
                    self.causal_lm_model_id, device_map=device, quantization_config=quantization_config
                )
                model = prepare_model_for_kbit_training(model)

                config = LoraConfig(
                    r=16,
                    lora_alpha=32,
                    target_modules=[""q_proj"", ""v_proj""],
                    lora_dropout=0.05,
                    bias=""none"",
                    task_type=""CAUSAL_LM"",
                )
                model = get_peft_model(model, config)

                data = load_dataset(""ybelkada/english_quotes_copy"")
                data = data.map(lambda samples: self.tokenizer(samples[""quote""]), batched=True)

                trainer = Trainer(
                    model=model,
                    train_dataset=data[""train""],
                    args=TrainingArguments(
                        per_device_train_batch_size=4,
                        gradient_accumulation_steps=4,
                        warmup_steps=2,
                        max_steps=5,
                        learning_rate=2e-4,
                        logging_steps=1,
                        output_dir=tmp_dir,
                    ),
                    data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),
                )
                model.config.use_cache = False
                trainer.train()

    def test_causal_lm_training_multi_gpu_torchao(self):
        from transformers import TorchAoConfig

        for quant_type in self.supported_quant_types:
            with tempfile.TemporaryDirectory() as tmp_dir:
                quantization_config = TorchAoConfig(quant_type=quant_type)
                model = AutoModelForCausalLM.from_pretrained(
                    self.causal_lm_model_id,
                    device_map=""auto"",
                    quantization_config=quantization_config,
                    torch_dtype=torch.bfloat16,
                )

                assert set(model.hf_device_map.values()) == set(range(torch.cuda.device_count()))

                model = prepare_model_for_kbit_training(model)
                setattr(model, ""model_parallel"", True)
                setattr(model, ""is_parallelizable"", True)

                config = LoraConfig(
                    r=16,
                    lora_alpha=32,
                    target_modules=[""q_proj"", ""v_proj""],
                    lora_dropout=0.05,
                    bias=""none"",
                    task_type=""CAUSAL_LM"",
                )
                model = get_peft_model(model, config)

                data = load_dataset(""Abirate/english_quotes"")
                data = data.map(lambda samples: self.tokenizer(samples[""quote""]), batched=True)

                trainer = Trainer(
                    model=model,
                    train_dataset=data[""train""],
                    args=TrainingArguments(
                        per_device_train_batch_size=4,
                        gradient_accumulation_steps=4,
                        warmup_steps=2,
                        max_steps=5,
                        learning_rate=2e-4,
                        logging_steps=1,
                        output_dir=tmp_dir,
                    ),
                    data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),
                )
                model.config.use_cache = False
                trainer.train()
```

As you can see, `int8_weight_only` and `int8_dynamic_activation_int8_weight` work already. For `int4_weight_only`, I got an error unfortunately:

```
tests/test_gpu_examples.py:3079: in test_causal_lm_training_single_gpu_torchao
    trainer.train()
../../clones/transformers/src/transformers/trainer.py:1991: in train
    return inner_training_loop(
../../clones/transformers/src/transformers/trainer.py:2327: in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
../../clones/transformers/src/transformers/trainer.py:3452: in training_step
    self.accelerator.backward(loss, **kwargs)
../../../anaconda3/envs/peft/lib/python3.11/site-packages/accelerate/accelerator.py:2159: in backward
    loss.backward(**kwargs)
../../../anaconda3/envs/peft/lib/python3.11/site-packages/torch/_tensor.py:521: in backward
    torch.autograd.backward(
../../../anaconda3/envs/peft/lib/python3.11/site-packages/torch/autograd/__init__.py:289: in backward
    _engine_run_backward(
../../../anaconda3/envs/peft/lib/python3.11/site-packages/torch/autograd/graph.py:768: in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
../../../anaconda3/envs/peft/lib/python3.11/site-packages/torch/autograd/function.py:306: in apply
    return user_fn(self, *args)
../../../anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:313: in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
../../../anaconda3/envs/peft/lib/python3.11/site-packages/torch/autograd/__init__.py:289: in backward
    _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor([[[-0.4355, -0.8750,  0.2188,  ..., -0.8750, -0.4375,  0.0693],
         [ 0.0540, -1.1094,  0.0889,  ...,  0....,  0.2139,  ..., -0.5117, -0.3672,  0.2539]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>),)
args = ((tensor([[[-8.0466e-06, -4.8876e-05, -2.2888e-05,  ...,  6.0320e-05,
          -1.9073e-06, -9.4414e-05],
         [-...,  ...,  0.0000e+00,
           0.0000e+00,  0.0000e+00]]], device='cuda:0', dtype=torch.bfloat16),), False, False, ())
kwargs = {'accumulate_grad': True, 'allow_unreachable': True}, attach_logging_hooks = False

    def _engine_run_backward(t_outputs, *args, **kwargs):
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: derivative for aten::_weight_int4pack_mm is not implemented

```

For me to be able to use `Trainer`, I had to patch `is_trainable` to return `True`. So I wonder if it can be set to `True` if PEFT is available and int8 is being used. WDYT?

Note that I'll still look into a proper PEFT integration, since I'm not sure if features like merging would already work. But training and inference should be fine as is.

PS: I'm using torch 2.4.0, PEFT and transformers from source, in case it matters. Nice ! I'll create to PR for that  Hello. The current doc has strange content about the features that torchao supports. It would be good if someone can fix it.

https://huggingface.co/docs/transformers/quantization/overview

![图片](https://github.com/user-attachments/assets/49a49181-ffc9-4ffa-ba9e-cdee24ef595f)
 @imba-tjd which one are you referring to? @jerryzh168 Please see the screenshot in my above comment.
For example there is `🟢🔴` in one cell at the line of torchao. > @jerryzh168 Please see the screenshot in my above comment. For example there is `🟢🔴` in one cell at the line of torchao.

yeah this means partial support. but it might be clearer is we make it yellow and then add a note I guess, specifically, this means we support non-safetensor serialization (please feel free to submit a fix if you have time, otherwise I should be able to get to this a bit later)Hi @jerryzh168, thanks for doing the integration ! I did a first pass and the pr looks pretty clean ! I've left a few comments. Also, can you share a bit about how serialization might look like and the issue that you are facing ? It's fine if we don't support it in this PR. This can come in a follow-up PR !  Thanks for iterating @jerryzh168 ! We are practically there ! Left a few comments  Nice ! thanks for iterating ! I left a few comments. I think we can try to merge this today/tmr ! I'll open a PR to fix a few things (`modules_not_to_convert` and `create_quantized_params` ). Please fix the points I mentioned in my comments below. Also the test file disappeared, can you add it back ? ","Hi @jerryzh168, thanks for doing the integration ! I did a first pass and the pr looks pretty clean ! I've left a few comments. Also, can you share a bit about how serialization might look like and the issue that you are facing ? It's fine if we don't support it in this PR. This can come in a follow-up PR !  Thanks for iterating @jerryzh168 ! We are practically there ! Left a few comments  Nice ! thanks for iterating ! I left a few comments. I think we can try to merge this today/tmr ! I'll open a PR to fix a few things (`modules_not_to_convert` and `create_quantized_params` ). Please fix the points I mentioned in my comments below. Also the test file disappeared, can you add it back ? ","This PR adds [torchao](https://github.com/pytorch/ao) as an option for quantization, right now we expose three quantization technique: `int4_weight_only`, `int8_weight_only` and `int8_dynamic_activation_int8_weight` to user, it uses tensor subclass to implement weight quantization and `torch.compile` to generate efficient code for cuda.


Example:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import TorchAoConfig
from torchao.utils import benchmark_model
import torchao

DEVICE_TYPE = ""cuda""

def init_model_and_benchmark(model_id, torch_dtype=torch.bfloat16, quantization_config=None):
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    if quantization_config is not None:
        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=DEVICE_TYPE, torch_dtype=torch.bfloat16, quantization_config=quantization_config)
    else:
        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=DEVICE_TYPE, torch_dtype=torch.bfloat16)

    # sanity check: run the model
    input_text = ""What are we having for dinner?""
    input_ids = tokenizer(input_text, return_tensors=""pt"").to(DEVICE_TYPE)
    output = model.generate(**input_ids, max_new_tokens=1000)
    print(tokenizer.decode(output[0], skip_special_tokens=True))

    NUM_WARMUP = 10
    NUM_RUNS = 50

    if quantization_config is not None:
        torchao.quantization.utils.recommended_inductor_config_setter()

    model = torch.compile(model, mode=""max-autotune"")

    benchmark_model(model.generate, NUM_WARMUP, kwargs=input_ids, device_type=DEVICE_TYPE)
    print(""running benchmark"")
    results = benchmark_model(model.generate, NUM_RUNS, kwargs=input_ids, device_type=DEVICE_TYPE)
    return model, results

model_id = ""meta-llama/Llama-2-7b-chat-hf""
bf16_model, bf16_time = init_model_and_benchmark(model_id)
print(f""bf16: {bf16_time}"")

quantization_config = TorchAoConfig(quant_type=""int4_weight_only"", group_size=32)
int4wo_model, int4wo_time = init_model_and_benchmark(model_id)
print(f""int4wo: {int4wo_time} speedup: {bf16_time / int4wo_time}"")

quantization_config = TorchAoConfig(quant_type=""int8_weight_only"")
int8wo_model, int8wo_time = init_model_and_benchmark(model_id)
print(f""int8wo: {int8wo_time} speedup: {bf16_time / int8wo_time}"")

quantization_config = TorchAoConfig(quant_type=""int8_dynamic_activation_int8_weight"")
int8da_int8w_model, int8da_int8w_time = init_model_and_benchmark(model_id)
print(f""int8da_int8w: {int8da_int8w_time} speedup: {bf16_time / int8da_int8w_time}"")
```

 -->
Perf benchmark results (WARMUP=2, RUNS=10) A100 cuda machine:

```
bf16: 8167.34453125
int4wo: 6036.775390625 speedup: 1.3529316568467553
int8wo: 6974.3453125 speedup: 1.1710553701165052
int8da_int8w: 7945.5375 speedup: 1.0279159252913979
```

we don't always expect speedup here since we may need manual changes to model code before we can get speedup","can you show the full list of available options or link to it? will the int4 weight only example also work on cpu? We do have support for nf4 and QAT though, is the intent here to say the hf ao backend does not support this yet? is this a reasonable default? Most open models are released in fp16 or bf16 so upscaling to fp32 seems strange why do we need this? yeah just added to the docstring of `TorchAOConfig` yeah I think it might work as well, but we have most confidence in cuda QAT is still in prototype I think, oh I forgot about nf4, I haven't thought of how we expose nf4 yet, will check with Driss this is the same default using in hf (https://github.com/huggingface/transformers/blob/14ee2326e51cb210cec72f31b248cb722e9d5d1f/src/transformers/modeling_utils.py#L3043) not sure, this is copied from hqq quantizer, I can remove I think we haven't tested nf4 dtype with quantize_ API yet, maybe we can create an issue to make nf4 tensor work with `quantize_` API ```suggestion
        # Step 2: Update the module using the `quantize_` API from TorchAO
``` Let's keep the same convention as other configs. Could you propagate the modification ? 
```suggestion
## TorchAoConfig
``` Is it recommended to install the `nightly` version ? Note that you added a lot of check for torchao but I don't think this is compatible if you only have `torchao-nightly` installed no ?  Could you also update this table ? https://huggingface.co/docs/transformers/en/quantization/overview Same remark as above
```suggestion
from .quantizer_torchao import TorchAoHfQuantizer
``` let's do that in `update_torch_dtype` instead We don't really need to update the `torch_dtype` here in reality. With transformers, we will set it by default to `torch.float32 ` with `torch_dtype = None`.  This will always be not None
```suggestion
``` I think that we can the user about that in `update_torch_dtype`  Can perform a check for self.quant_type in a post_init method to check that the user passed the right value ?  what are the available kwargs for each methods ? It would be great to document the most important ones.  We don't need that since we are setting the module again after quantization  Is this an issue if we don't set the bias and leave the bias parameter on the `meta` device ? Can we still quantize the linear layers without issue ? If we do have this issue, then it will also be an issue if we have sharded checkpoints since the weights and the bias might not be on the same file.  just trying to make sure if there is new features in torchao it's immediately available to hf users, but I can change this to torchao as well.

oh right, I will make sure the package check is consistent

I'll update this to torchao for now. @SunMarc you mean assign `self.torch_dtype` in `update_torch_dtype`? OK removed this override we are only changing the weight Tensor during quantization, so technically we don't need to touch bias Tensor, this code is copied from hqq quantizer.

I feel we can enable sharded checkpoint in a separate PR if needed, or do you think I should filter weight Tensor and only set them here? you mean update the default dtype to `torch.bfloat16`? I'm planning to link to torchao so we don't duplicate things and also make sure everything are up to date: https://github.com/pytorch/ao/tree/main/torchao/quantization#other-available-quantization-techniques"
35047,2024-12-02T22:56:34Z,2024-12-03T18:53:45Z,faaany,1,0,12,20,1,1,1,[],71615.0,0,71831.0,0,0,0,0,1991886.89057,,0,12,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35047). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thank you!,Thank you!,"## What does this PR do?
This PR further improves the docs using device-agnostic API instead of cuda. 

Documentation: @stevhliu

",
35013,2024-11-29T09:24:57Z,2024-12-03T18:22:02Z,wwwbai,3,1,3,71,2,2,2,[],422.0,0,377825.0,0,0,0,0,1993790.331555,,0,3,0,False,"['wwwbai', 'HuggingFaceDocBuilderDev']","After three simple document translation PRs, I think I have fully understood the process for newcomers to contribute. Thank you @stevhliu @Isotr0py @blueingman for your help.If you could review this translated document , I would be particularly grateful!
 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35013). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Just a nit.

Thanks a lot!!Cool, thanks!  Just a nit.","Cool, thanks!  Just a nit.","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

I translated community.md into Chinese.


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@stevhliu
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
| [Hugging Face Transformers Glossary Flashcards](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards) | 这是一套基于 [Transformers文档术语表](glossary) 的抽认卡，它们已被整理成可以通过 [Anki](https://apps.ankiweb.net/) （一款专为长期知识保留而设计的开源、跨平台的应用）来进行学习和复习的形式。使用方法参见： [介绍如何使用抽认卡的视频](https://www.youtube.com/watch?v=Dji_h7PILrw)。 | [Darigov Research](https://www.darigovresearch.com/) |
```"
35054,2024-12-03T06:22:02Z,2024-12-03T17:18:39Z,faaany,1,0,1,2,1,1,1,[],35860.0,0,39398.0,0,0,0,0,1997593.348187,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35054). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Good catch, thank you!","Good catch, thank you!","## What does this PR do?
The current code example will give me the following error:

```bash
Traceback (most recent call last):
  File ""/home/sdp/fanli/transformers/playground.py"", line 110, in <module>
    for value in generator:
  File ""/home/sdp/fanli/transformers/playground.py"", line 80, in model_inference
    inputs = processor(
  File ""/home/sdp/fanli/transformers/src/transformers/models/idefics2/processing_idefics2.py"", line 227, in __call__
    raise ValueError(
ValueError: The total number of <image> tokens in the prompts should be the same as the number of images passed. Found 3 <image> tokens and 2 images.
```

This is because the `messages` were used in the previous example, while in this example an additional prompt is added. 


Documentation: @stevhliu
",
34454,2024-10-28T03:21:33Z,2024-12-03T13:58:54Z,sywangyi,4,0,3,14,2,1,1,['run-slow'],621929.0,0,3149604.0,0,0,0,0,2008616.96174,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'sywangyi', 'yao-matrix', 'Rocketknight1']","@amyeroberts  gentle ping @ylacombe @eustlb @Rocketknight1  gentle ping. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34454). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.This looks clean, and I'm sorry the review took so long! I should have just approved it the first time around, given that it clearly fixes the problem.","This looks clean, and I'm sorry the review took so long! I should have just approved it the first time around, given that it clearly fixes the problem.","…_disable

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)
https://github.com/huggingface/transformers/issues/33527


- speech models: @ylacombe, @eustlb
",
35043,2024-12-02T14:14:24Z,2024-12-03T12:56:59Z,ydshieh,1,1,1,4,1,2,1,[],1629.0,0,81757.0,0,0,0,0,2013293.519962,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35043). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks, that makes sense!","Thanks, that makes sense!","# What does this PR do?

`BertGeneration` doesn't prepare the attention mask correctly when it is used as a decoer + `use_cache=True`.

Running the script above:

The hidden states difference (the first item printed) looks

- `[0.0, 0.004747, 0.0062393]` with `main`
- `[0.0, 0.0, 0.0]` with this PR

This is also the reason why 

> tests/models/bert_generation/test_modeling_bert_generation.py::BertGenerationEncoderTest::test_assisted_decoding_matches_greedy_search_1_same

is flaky. With this PR, running 10000 times and all pass.

### Script

```python
import torch
from tests.models.bert_generation.test_modeling_bert_generation import BertGenerationEncoderTest, BertGenerationEncoderTester

torch_device = ""cpu""
self = BertGenerationEncoderTest()
self.setUp()
model_class = self.all_generative_model_classes[0]
config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)
config.is_decoder = True

inputs_dict['attention_mask'][0, :3] = 0
inputs_dict['attention_mask'][0, 3:] = 1

inputs_dict['use_cache'] = True
inputs_dict['output_hidden_states'] = True
inputs_dict['output_attentions'] = True

model = model_class(config).to(torch_device).eval()

outputs1 = model(**inputs_dict)

shorter_inputs_dict = {k: v for k, v in inputs_dict.items()}
shorter_inputs_dict['input_ids'] = inputs_dict['input_ids'][:, :-1]
shorter_inputs_dict['attention_mask'] = inputs_dict['attention_mask'][:, :-1]

outputs2 = model(**shorter_inputs_dict)
diffs = [float(torch.amax(torch.abs(outputs1.hidden_states[idx][:, -2] - outputs2.hidden_states[idx][:, -1]))) for idx in range(len(outputs1.hidden_states))]
print(diffs)

print(inputs_dict['attention_mask'])
# The attention of the last token in `outputs2` should be the same as of the token at position -2 in `outputs1`
print(outputs1['attentions'][0][0, 0][-2, :-1].detach().tolist())
print(outputs2['attentions'][0][0, 0][-1, :].detach().tolist())
```
","when `use_cache` is specified and the model is used as decoder, this prevents the model to prepare the correct attention mask and each token attends to every other tokens."
34548,2024-10-31T21:14:40Z,2024-12-03T12:14:52Z,aymeric-roucher,1,0,8,474,5,1,1,['Agents'],1570.0,0,2818815.0,0,0,0,0,2015820.326133,,0,8,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34548). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Looks good to me thank you @aymeric-roucher !,Looks good to me thank you @aymeric-roucher !,"# What does this PR do?

Add callbacks + monitoring options to agents, with the end goal of being usable in a monitoring platform such as Arize AI's [Phoenix](https://docs.arize.com/phoenix).",
33049,2024-08-22T20:21:48Z,2024-12-02T19:40:04Z,AhmedAlmaghz,3,0,5,145,2,2,2,[],7519725.0,0,8830170.0,0,0,0,0,2055638.551867,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'AhmedAlmaghz', 'Rocketknight1']","Thank you @abodacs 
Done update notebooks.md
Please, Review it again cc @stevhliu  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33049). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.السﻻم عليكم
https://huggingface.co/docs/transformers/main/en/notebooks
ليست هذا الترجمه للملف 

@AhmedAlmaghz  Thanks!","السﻻم عليكم
https://huggingface.co/docs/transformers/main/en/notebooks
ليست هذا الترجمه للملف 

@AhmedAlmaghz  Thanks!","
## What does this PR do?
Translated the `docs/source/ar/notebooks.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ",
34923,2024-11-25T15:57:40Z,2024-12-03T10:20:31Z,Cyrilvallez,7,13,25,111,6,4,1,[],1696.0,0,670973.0,0,0,0,0,2022683.624141,,0,25,0,False,"['HuggingFaceDocBuilderDev', 'Cyrilvallez', 'SilverSoldier', 'ArthurZucker', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34923). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hey @BenjaminBossan, could you check if this PR solves the issue in `peft`, or point me to the failing tests please?   Raised this point in issue #34906 as well, how can we pass custom arguments to compile in this case, say different backend or other parameters?
There should be a way to pass the user arguments for non-default cases. Yeah we can use `generate_kwargs` for this. The original implem was super minimal for big gains, but down to include that! another comments:

- (nit) I am not sure it's necessary to save the compile config (unlike our other config classes)
- I understand we might want to change compile options.  But now imagine a use case: I compile with option 1. Then with option 2. But now I want to check with option 1 again for some reason. **Question**: would  compiling with option 1 the second time take the same amount time of compiling with option 1 the first time?
 Thanks for looking into this as well @ydshieh! Regarding your question, `torch` is able to cache different graphs for the exact same function, so no, it will actually not re-compile even after the switch. Not sure how many times you can do it before starting to loose cache entries though. But the following:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, CompileConfig
import torch
import time
device = 3
import warnings
warnings.filterwarnings(""ignore"")

model = AutoModelForCausalLM.from_pretrained(""meta-llama/Meta-Llama-3-8B"", torch_dtype=torch.float16).to(device)
tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3-8B"")

sequence = ""Hey what's the plan""

inputs = tokenizer.encode(sequence, return_tensors='pt').to(device)
model.generation_config.temperature = 1.0
model.generation_config.top_p = 1.0

# Compile default config
t0 = time.time()
out = model.generate(inputs, do_sample=False, max_new_tokens=500, cache_implementation=""static"")
out = tokenizer.batch_decode(out, skip_special_tokens=True)[0]
dt = time.time() - t0
print(f'dt: {dt}')
print(model._last_compile_config.to_dict())


# Compile new config
t0 = time.time()
out = model.generate(inputs, do_sample=False, max_new_tokens=500, cache_implementation=""static"", compile_config=CompileConfig(dynamic=True))
out = tokenizer.batch_decode(out, skip_special_tokens=True)[0]
dt = time.time() - t0
print(f'dt: {dt}')
print(model._last_compile_config.to_dict())

# Back to 1st config
t0 = time.time()
out = model.generate(inputs, do_sample=False, max_new_tokens=500, cache_implementation=""static"")
out = tokenizer.batch_decode(out, skip_special_tokens=True)[0]
dt = time.time() - t0
print(f'dt: {dt}')
print(model._last_compile_config.to_dict())


# Back to 2nd config
t0 = time.time()
out = model.generate(inputs, do_sample=False, max_new_tokens=500, cache_implementation=""static"", compile_config=CompileConfig(dynamic=True))
out = tokenizer.batch_decode(out, skip_special_tokens=True)[0]
dt = time.time() - t0
print(f'dt: {dt}')
print(model._last_compile_config.to_dict())
```

actually does not re-compile for the last 2 `generate` calls, and use already compiled graphs  Confirming that [this script](https://gist.github.com/Cyrilvallez/aa8fc622d698f1425615a6651d8ff3a9) returns
```py
>>> Without static cache and compile: 27.079 s
>>> Using `torch.compile`.
>>> Compiling default config: 26.816 s
>>> Using compiled graph: 6.541 s
>>> Compiling new config: 24.327 s
>>> Using compiled new graph: 6.930 s
>>> Back to 1st config and graph: 6.528 s
```
on a machine with A100 GPU, which is the expected resultThanks, before merging can you make sure:
- this fixed the PEFT issue! (we are missing a fast test let's add it!)
- this still has the expected performance gains!
🤗 LGTM otherwise! Let's rather go with generation_config / generate kwargs that can be passed and used! > Hey @BenjaminBossan, could you check if this PR solves the issue in `peft`, or point me to the failing tests please?

I checked and this branch indeed resolves the failing tests, thanks! LGTM, could you confirm performance boost with a gist script shared here(like the one I shared) just to double check? :hugs: ","Thanks, before merging can you make sure:
- this fixed the PEFT issue! (we are missing a fast test let's add it!)
- this still has the expected performance gains!
🤗 LGTM otherwise! Let's rather go with generation_config / generate kwargs that can be passed and used! > Hey @BenjaminBossan, could you check if this PR solves the issue in `peft`, or point me to the failing tests please?

I checked and this branch indeed resolves the failing tests, thanks! LGTM, could you confirm performance boost with a gist script shared here(like the one I shared) just to double check? :hugs: ","# What does this PR do?

As discussed, I don't think we should rely on defining an inner function and compiling it for every call to `generate`.

This moves the compiled forward to `PreTrainedModel` instead for reuse, which makes the most sense for me. That way, every `PreTrainedModel` effectively has 2 forwards, and we can dynamically choose between non-compiled (prefill) and compiled (iterative decoding). This is similar to what PyTorch does internally when calling `model.compile()` inplace, except in their case they force the use of the compiled one after the call was invoked.

Let me know what you think @ArthurZucker 

Also address https://github.com/huggingface/transformers/issues/34906","Cool, let's maybe document this a bit? (*usage)
```suggestion
    @property
    def compiled_call(self):
        if not hasattr(self, ""_compiled_call""):
            self._compiled_call = torch.compile(self.__call__, mode=""reduce-overhead"", fullgraph=True)
        return self._compiled_call
``` Haha yes, I was asking myself whether I wanted to name it `compiled_forward` or `compiled_call`... Updated accordingly! This could also use [`functools.cached_property`](https://docs.python.org/3/library/functools.html#functools.cached_property) but it's also fine as is. Is this method used anywhere? Is `compiled_call` supposed to use this method? It was a first draft to set compile arguments and is now removed! I am wondering if it would (probably) make more sense to do

> self._compiled_call = None

at this line and delegate the actually compile only in `compiled_call` (as you already there) - when it is called (like you do in `generate`).

Having 2 places to call `torch.compile`  seems a slight strange to me (but it's ok though).

And if you decide to take my suggestion, probably the name `_set_compile_call` should be changed to `_set_compile_config` torch.compile does not compile, calling the compile funciton does compile I know. But what I mean here is not the underlying compile happening, but rather the call to `torch.compile` twice. 2 different methods calling `torch.compile` seems to me not very good style (for me, it's better if one method is only set config and another one take the job to call `torch.compile`)

But it's no big deal but just of a habit of making each method does its own job. You're right, but maybe then using `_set_compile_call` in `compiled_call` instead? That way, `set_compile_call` is the only place using `torch.compile`, and `compiled_call` is still only used for accessing the fnction? (with drawback call to `_set_compile` in case it does not already exist) even with that, in `generate` we still need to call `_set_compile_call` then `compiled_call` right? (as `compiled_call`) doesn't contain the argument. If this is the case, then it's odd to have `compiled_call` calling `_set_compile_call`.

Otherwise, if you think it's ok/good to change `compiled_call` to accept compile_config arugment, then sound good to me with your suggested change. oh, `compiled_call` is a property so not to take argument .... so the concern in the first paragraph in the above comment is there. anyway, it's implementation details and not affecting users. Don't take it too serious if the changes will take too much time. Yes, I thought it makes sense to make it a property so that `model.compiled_call(**inputs)` could always be used directly as an alternative `model.forward(**inputs)`"
35034,2024-12-01T08:48:32Z,2024-12-02T14:03:36Z,jla524,1,0,2,76,2,1,1,[],106895.0,0,135381.0,0,0,0,0,2065624.268441,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35034). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM, and thank you for the PR!","LGTM, and thank you for the PR!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Add type hints to functions added in https://github.com/huggingface/transformers/pull/34282

## Who can review?

@ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34908,2024-11-25T08:25:16Z,2024-12-02T19:42:41Z,wwwbai,6,5,9,35,2,3,2,[],24115.0,0,645445.0,0,0,0,0,2075357.94167,,0,9,0,False,"['wwwbai', 'stevhliu', 'HuggingFaceDocBuilderDev', 'Rocketknight1']","cc @stevhliu  @statelesshz Considering the great value of your feedback on my previous translation work, would you be so kind as to review it？
 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34908). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Maybe @blueingman or @Isotr0py would also be interested in reviewing, given your ongoing translation efforts 🙂  @Isotr0py @blueingman Thank you for your valuable suggestion!! I have added all the changes to my branch.

 @stevhliu Hi there,Apologies for the follow-up, but I wanted to check if my PR is ready for merge. If there are any issues, could you let me know? Thanks a lot!Minor fix to the toctree but otherwise LGTM! Can you ping or ask another contributor to review the translated content? I think this translation is quite good overall. My abilities are somewhat limited, so I’ve only made a few nitpicky suggestions. Overall looks good. Just some nits. Looks good, thanks!","Minor fix to the toctree but otherwise LGTM! Can you ping or ask another contributor to review the translated content? I think this translation is quite good overall. My abilities are somewhat limited, so I’ve only made a few nitpicky suggestions. Overall looks good. Just some nits. Looks good, thanks!","# What does this PR do?

I translated bertology.md into Chinese.This is my second attempt to contribute, and I hope to do better than last time.
## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
  - local: bertology
``` ```suggestion
为了助力这一新兴领域的发展，我们在BERT/GPT/GPT-2模型中增加了一些附加功能，方便人们访问其内部表示，这些功能主要借鉴了Paul Michel的杰出工作(https://arxiv.org/abs/1905.10650)：
``` ```suggestion
当前，一个新兴的研究领域正致力于探索大规模 transformer 模型（如BERT）的内部工作机制，一些人称之为“BERTology”。以下是这个领域的一些典型示例：
``` ```suggestion
- 检索注意力头的输出值和梯度，以便计算头的重要性得分并对头进行剪枝，详情可见论文：https://arxiv.org/abs/1905.10650。
``` ```suggestion
为了帮助您理解和使用这些功能，我们添加了一个具体的示例脚本：[bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py)，该脚本可以对一个在 GLUE 数据集上预训练的模型进行信息提取与剪枝。
```"
34776,2024-11-18T11:00:33Z,2024-12-02T19:40:20Z,faaany,2,0,5,7,1,1,1,[],1202312.0,0,1240787.0,0,0,0,0,2075499.242541,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'faaany']","Hi @stevhliu , I fixed the bug in the example code. Otherwise, I will get the following error: 

```bash
Traceback (most recent call last):
  File ""/home/sdp/fanli/transformers/playground.py"", line 55, in <module>
    inputs = processor(prompt, images=videos).to(model.device, model.dtype)
TypeError: LlavaProcessor.__call__() got multiple values for argument 'images'
```

And also the `sample_frames` has a bug. `video_1` contains 7 frames rather than 6. So I fixed both issues.  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34776). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the fix! ❤️ ",Thanks for the fix! ❤️ ,"## What does this PR do?
Add the missing import for Image and add comment for other hardware devices. 

There is also a bug in the code itself, which needs further debug. I will undraft, once I fixed the issue. 

@stevhliu
",
30650,2024-05-04T09:38:44Z,2024-07-22T17:24:43Z,yonigottesman,30,24,1,273,8,4,2,[],274457.0,0,18353853.0,0,0,0,0,2074542.609897,,0,1,0,False,"['psinger', 'amyeroberts', 'HuggingFaceDocBuilderDev', 'Boltzmachine', 'avicooper1', 'Rocketknight1', 'stceum', 'yonigottesman', 'ermu2001']","cc @lewtun and @xenova to this as well! My thoughts on your questions:

> Is this API fine? maybe we should return the a labels key in the dict already and not bother with the intermediate mask.

I prefer just returning the labels with masking applied, rather than returning the mask for the user to apply.

>Name of the new tag? currently `generation` but maybe should be `assistant_response`? or anything you like.

I think `generation` is fine - `assistant_response` is very long!

> I think maybe I should add a warning if a user runs with return_assistant_mask but the tokenizer chat template hasn't changed yet to support this new tag. That way users will know the are probably training on wrong tokens. 

Agreed! I guess the easiest way to check this is to just do a string search for `{% generation %}` tags? Be careful, because you'll also need to check for variants like `{-` 

> n 99% of finetuning examples I see people using the [trl trainer](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_trainer.py#L131) with `packing=True`. My new changes wont be usable easily if people use that parameter and maybe we should think of my API while taking into consideration a refactor of the `packing` affect.

Yes, there's already a [DataCollatorForCompletionOnlyLM](https://huggingface.co/docs/trl/en/sft_trainer#train-on-completions-only) which also requires `packing=False`. I feel like we can slot in with that easily enough!

I want to hear from @xenova and ideally someone using minijinja as well, though - how easily can we support this extension? Since it's only useful in training, maybe it's less critical to have it in `huggingface/jinja` or `TGI`, but at the very least we should be able to gracefully ignore the `generation` tags. > I prefer just returning the labels with masking applied, rather than returning the mask for the user to apply.

I agree, but then what should be the ignore label? -100 (pytorch)?. Im not sure its a good idea to add another parameter `ignore_label` I think `-100` is correct, yes! This is the standard value for Torch and Transformers, so we don't need an extra arg to change it. yea i just thought of non pytorch users where -100 is not the default. 
Anyways I updated the code to return `labels` The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_30650). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @yonigottesman Thanks for working on this, this is a feature I am very much looking forward to. Hope this can be merged soon. Yes, sorry for not checking in @yonigottesman! Do you have other features you want to add, or should we treat this as review-ready? @Rocketknight1 this is ready to be reviewed yes :)  On it! @yonigottesman while I'm reviewing can you rebase/resolve the merge conflict? It's nothing major, but it'll block us merging the PR until it's ready. (Edit: Probably better to rebase because your branch is a little out of date by now, a rebase will catch any other issues before merging) I agree it should be `assistant_mask` and not labels. I feel like the collator should be added here and not `trl` what do you think? Yes, agree! It's also fine to leave that for a separate PR, and just add the mask functionality in this PR. ok. fixed to now return mask Got it! Ping me whenever you're ready for re-review. ready 😀 This is amazing!! Looking forward to this new change! fixed your suggestions. Do you think the docstring should contain a small example ? like this is the phi template with the new token:
```
    ""{{ bos_token }}""
    ""{% for message in messages %}""
    ""{% if (message['role'] == 'user') %}""
    ""{{'<|user|>' + '\n' + message['content'] + '<|end|>' + '\n' + '<|assistant|>' + '\n'}}""
    ""{% elif (message['role'] == 'assistant') %}""
    ""{%generation%}""
    ""{{message['content'] + '<|end|>' + '\n'}}""
    ""{%endgeneration%}""
    ""{% endif %}""
    ""{% endfor %}""
``` Yes! We could also add a section to the end of the [chat template documentation](https://huggingface.co/docs/transformers/main/en/chat_templating), under ""Template Writing Tips"" - it's a good way to make this feature visible to people. The source for it is `en/chat_templating.md`

I've updated that doc recently, though - make sure you rebase before editing it! @yonigottesman Could you rebase on `main` to include upstream changes? This should resolve a lot of the current CI failures  @amyeroberts done thanks @amyeroberts. You are right the whole sharing of the two lists between the AssistantTracker and the code outside is fishy and dangerous. I came up with a more pythonic solution with context managers tell me what you think.
This way the lists are only created and managed outside the object and you need to ""activate"" the object with the lists to get it to work. no encapsulation broken by code touching inner object members, and kind of safe as long as we use it with `with` statement
 @amyeroberts anything else I should update? 😄  @yonigottesman There's been a update on main which should fix the hub tests. Could you try rebasing, this should hopefully resolve  Amazing contribution! 🎉 🎉 🎉 It helps me a lot!

> Here is an example of the new api:
> 
> ```
> template = (
>     ""{% for message in messages %}""
>     ""{% if (message['role'] != 'assistant') %}""
>     ""{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}""
>     ""{% elif (message['role'] == 'assistant')%}""
>     ""{{'<|im_start|>' + message['role'] + '\n'}}""
>     ""{% generation %}""
>     ""{{message['content'] + '<|im_end|>'}}""
>     ""{% endgeneration %}""
>     ""{{'\n'}}""
>     ""{% endif %}""
>     ""{% endfor %}""
> )
> dummy_conversation = [
>       {""role"": ""system"", ""content"": ""system message""},
>       {""role"": ""user"", ""content"": ""user message""},
>       {""role"": ""assistant"", ""content"": ""assistant\nmessage""},
>       {""role"": ""user"", ""content"": ""user message 2""},
>       {""role"": ""assistant"", ""content"": ""assistant message 2""},
> ]
> 
> output = tokenizer_r.apply_chat_template(
>     dummy_conversations,
>     chat_template=dummy_template,
>     tokenize=True,
>     return_assistant_mask=True,
>     return_dict=True,
>   )
> 
> labels = [output[""input_ids""][index] if mask == 1 else -100 for index, mask in enumerate(output[""assistant_mask""])]
> ```


Some spelling mistakes in this example:

> output = tokenizer_r.apply_chat_template(
>     dummy_conversations,
>     chat_template=dummy_template,
>     tokenize=True,
>     return_assistant_mask=True,
>     return_dict=True,
>   )

`dummy_conversation` instead of `dummy_conversations`, `template` instead of `dummy_template`

> labels = [output[""input_ids""][index] if mask == 1 else -100 for index, mask in enumerate(output[""assistant_mask""])]

`assistant_masks` instead of `assistant_mask`
 Thank you for your work on this!

I'm having some issues though. When I run the example script from the tests, I don't seem to get any assistant tokens:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3.1-8B-Instruct"")

dummy_template = (
    ""{% for message in messages %}""
    ""{% if (message['role'] != 'assistant') %}""
    ""{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}""
    ""{% elif (message['role'] == 'assistant')%}""
    ""{{'<|im_start|>' + message['role'] + '\n'}}""
    ""{% generation %}""
    ""{{message['content'] + '<|im_end|>'}}""
    ""{% endgeneration %}""
    ""{{'\n'}}""
    ""{% endif %}""
    ""{% endfor %}""
)
conversations = [
    [
        {""role"": ""system"", ""content"": ""system message""},
        {""role"": ""user"", ""content"": ""user message""},
        {""role"": ""assistant"", ""content"": ""start turn 1 assistant message. end turn 1""},
        {""role"": ""user"", ""content"": ""user message 2""},
        {""role"": ""assistant"", ""content"": ""start turn 2 assistant message. end turn 2""},
    ],
    [
        {""role"": ""system"", ""content"": ""system message 3""},
        {""role"": ""user"", ""content"": ""user message 3""},
        {""role"": ""assistant"", ""content"": ""start turn 3 assistant message. end turn 3""},
        {""role"": ""user"", ""content"": ""user message 4""},
        {""role"": ""assistant"", ""content"": ""start turn 4 assistant message. end turn 4""},
    ],
]

output = tokenizer.apply_chat_template(
    conversations[0],
    chat_template=dummy_template,
    tokenize=True,
    return_assistant_tokens_mask=True,
    return_dict=True,
)
print("""".join(map(str, output[""assistant_masks""])))
```

For me, this prints out `0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000`

I think this bug is being caused by other tokens being printed out before the `{% generation %}`, within the same turn. For example, if I change the chat template to:
```python
dummy_template = (
    ""{% for message in messages %}""
    ""{% if (message['role'] != 'assistant') %}""
    ""{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}""
    ""{% elif (message['role'] == 'assistant')%}""
    ""{% generation %}""
    ""{{'<|im_start|>' + message['role'] + '\n'}}""
    ""{{message['content'] + '<|im_end|>'}}""
    ""{% endgeneration %}""
    ""{{'\n'}}""
    ""{% endif %}""
    ""{% endfor %}""
)
```

it works correctly, printing out `0000000000000000000000000000000011111111111111111111111110000000000000000001111111111111111111111111`

I am running the latest version of transformers, 4.44.0 @avicooper1 there is a bug but its not about tokens before ""generation"" in the same turn. If you try a different tokenizer it will work.
There is something strange about the llama3 tokenizer (`PreTrainedTokenizerFast`) for some reason the `char_to_token` function isn't working as expected and my implementation is based on its result.
I opened an issue huggingface/tokenizers#1620.  Given the issue, is there a workaround to get the assistant mask? sadly for llama3 i dont think so :( 
other models that use the same tokenizer class `PreTrainedTokenizerFast` (but different config) do work for example `tiiuae/falcon-mamba-7b-instruct`. so i guess its something specific to the llama3 configuration  Unfortunately the fact that the template needs to contain the `{% generation %}` part makes it very inflexible to use. Would it be somehow possible to just generate the mask base on the provided `user` `assistant` inputs?Just did a review! Here's my comments:

- Overall looks good, and it's a really clever solution
- Performance is very good, the test runs in milliseconds on my machine
- Should we add support for this to one or more of the DataCollator classes?
- Some changes should be reverted, see the specific code comments

Finally, I'm not sure if we should be returning `labels` or an `assistant_mask` from `apply_chat_template()`. I think it makes sense to return masked labels from a data collator that supports this, but not from `apply_chat_template()` itself, because it's kind of weird for `apply_chat_template()` to be handling labels at all! I think it might be better if `apply_chat_template()` just returns a simpler mask, and then the data collators use that to do masking. This looks really good now! I made a couple of small suggestions, but I think we're ready for core maintainer review now, cc @amyeroberts. There are also some failing tests, but these are unrelated, and should be fixed if you rebase.

Also, to make Amy's job easier, a quick explanation: This PR allows chat templates to mark assistant generations in the template. Very often, training pipelines only want to train on those tokens, and not compute loss on other tokens (e.g. control tokens, user messages, system messages). 

The way it works is by adding a small Jinja extension to support `{% generation %}` blocks, and then combining the string offsets from these blocks with the string offsets from tokenization to create a mask array, which is included as one of the tokenization outputs alongside `input_ids` and `attention_mask`. Thanks for working on this - this will be a great addition! 

I'm not very familiar with Jinja, and so we might be constrainted by the design patterns there, but just from the review I'm concerned about the design of `AssistantTracker` and how it handles state  Apologies for the delay in getting back to this. 

LGTM - thanks for iterating! Just some small comments","Just did a review! Here's my comments:

- Overall looks good, and it's a really clever solution
- Performance is very good, the test runs in milliseconds on my machine
- Should we add support for this to one or more of the DataCollator classes?
- Some changes should be reverted, see the specific code comments

Finally, I'm not sure if we should be returning `labels` or an `assistant_mask` from `apply_chat_template()`. I think it makes sense to return masked labels from a data collator that supports this, but not from `apply_chat_template()` itself, because it's kind of weird for `apply_chat_template()` to be handling labels at all! I think it might be better if `apply_chat_template()` just returns a simpler mask, and then the data collators use that to do masking. This looks really good now! I made a couple of small suggestions, but I think we're ready for core maintainer review now, cc @amyeroberts. There are also some failing tests, but these are unrelated, and should be fixed if you rebase.

Also, to make Amy's job easier, a quick explanation: This PR allows chat templates to mark assistant generations in the template. Very often, training pipelines only want to train on those tokens, and not compute loss on other tokens (e.g. control tokens, user messages, system messages). 

The way it works is by adding a small Jinja extension to support `{% generation %}` blocks, and then combining the string offsets from these blocks with the string offsets from tokenization to create a mask array, which is included as one of the tokenization outputs alongside `input_ids` and `attention_mask`. Thanks for working on this - this will be a great addition! 

I'm not very familiar with Jinja, and so we might be constrainted by the design patterns there, but just from the review I'm concerned about the design of `AssistantTracker` and how it handles state  Apologies for the delay in getting back to this. 

LGTM - thanks for iterating! Just some small comments","# What does this PR do?
This PR addresses issue huggingface/transformers#28950 and enhances the functionality of the `tokenizer.apply_chat_template` method when finetuning on chat datasets.

The method tokenizer.apply_chat_template is recommended for maintaining consistency with the model's original template during both training and inference phases. This practice ensures that conversations are processed in a uniform manner.

Moreover, during the finetuning process on chat datasets, it is crucial to exclude tokens from the ""user"" or ""system"" segments of the conversation. This exclusion is necessary because including these tokens would train the model to predict not only the ""assistant"" responses but also potential user queries, which is undesirable (and strange).

Currently, the `tokenizer.apply_chat_template` method does not provide a way to identify which tokens belong to the ""assistant"" response. To address this, the PR introduces a new parameter called `return_assistant_mask`. This parameter returns a mask that identifies tokens generated by the assistant, allowing for the appropriate creation of a labels arrays with ignore (-100) values during training.

Additionally, this PR proposes the introduction of a new keyword `generation` (name open for discussion) in the jinja2 chat template. This keyword is used to encapsulate the assistant’s response within your chat template.

Here is an example of the new api:
```
template = (
    ""{% for message in messages %}""
    ""{% if (message['role'] != 'assistant') %}""
    ""{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}""
    ""{% elif (message['role'] == 'assistant')%}""
    ""{{'<|im_start|>' + message['role'] + '\n'}}""
    ""{% generation %}""
    ""{{message['content'] + '<|im_end|>'}}""
    ""{% endgeneration %}""
    ""{{'\n'}}""
    ""{% endif %}""
    ""{% endfor %}""
)
dummy_conversation = [
      {""role"": ""system"", ""content"": ""system message""},
      {""role"": ""user"", ""content"": ""user message""},
      {""role"": ""assistant"", ""content"": ""assistant\nmessage""},
      {""role"": ""user"", ""content"": ""user message 2""},
      {""role"": ""assistant"", ""content"": ""assistant message 2""},
]

output = tokenizer_r.apply_chat_template(
    dummy_conversations,
    chat_template=dummy_template,
    tokenize=True,
    return_assistant_mask=True,
    return_dict=True,
  )

labels = [output[""input_ids""][index] if mask == 1 else -100 for index, mask in enumerate(output[""assistant_mask""])]
```

There are some issues I would want to discuss during this pr:

* Is this API fine? maybe we should return the a `labels` key in the dict already and not bother with the intermediate mask.
* Name of the new tag? currently `generation` but maybe should be `assistant_response`? or anything you like.
* I think maybe I should add a warning if a user runs with `return_assistant_mask` but the tokenizer chat template hasn't changed yet to support this new tag. That way users will know the are probably training on wrong tokens.
* In 99% of finetuning examples I see people using the [trl trainer](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_trainer.py#L131) with `packing=True`. My new changes wont be usable easily if people use that parameter and maybe we should think of my API while taking into consideration a refactor of the `packing` affect.





## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Default chat templates are actually deprecated and will be removed - they were always a temporary measure to handle the transition before most models had repo-level chat templates!

Since this is being removed soon, I think I'd prefer not to make any changes to it, just to simplify the PR. Instead, I can add a section about this tag to the chat template docs, as I'm planning a refresh there, including updates about new features like RAG and tool use. ```suggestion
                `{% generation %}` keyword.
``` Shouldn't we only take this path when `return_masked_labels==True`? yea I thought it would be cleaner to just have one function call. I changed it to go though the old path if `False` thats also fine maybe better for performance deleted. 
maybe i can put an example here `src/transformers/models/llama/tokenization_llama.py::default_chat_template`? I thought it could be useful to have an example other than the doc Hmn, we're going to be removing all of the default chat templates, so I don't think that works! I think putting it in the docs is the only place that makes sense. Good test, but maybe update the regex to also check for `{%-`? You could maybe even just check for the word ""generation"" at all. ```suggestion
                the mask will contain 1. for user and system tokens, the mask will contain 0.
                This functionality is only available for chat templates that support it via the 
                `{% generation %}` keyword.
``` ```suggestion
                ""return_assistant_tokens_mask==True but chat template does not contain `{% generation %}` keyword.""
``` maybe?

```suggestion
                                # assistant_start_char is out of bounds maybe due to truncation.
``` let's use `block` here, as `i` is typically used for indexing 

```suggestion
        for block in compiled_template.generate(
            messages=messages,
            tools=tools,
            documents=documents,
            add_generation_prompt=add_generation_prompt,
            **template_kwargs,
        ):
            rendered_blocks.append(block)
``` This is v. dirty - `new_generation_trackers` should be responsible for returning objects which don't require copying elsewhere Missing type hints and docstrings  What type is `environment` here? I'm not very familiar with jinja, so it might be not knowing the standard patterns here, but it seems messy to accept and modify the env state, but not have it as part of the instance state Should have a docstring and type hints What is the purpose of returning these attributes rather than having e.g.:

```suggestion
            def reset_generation_trackers(self):
                self.rendered_blocks = []
                self.generation_indices = []
```

and then modifying the instance attributes directly? 

```
compiled_template.environment.rendered_blocks.append(x)
```
etc? 

This avoids surprising mutations Why only testing with the rust tokenizer? 
 This logic is redundant - for models which have a rust tokenizer, `self.tokenizers_list` will have two elements, so the logic below is run twice * It's better to return early and have the bulk of logic indented as little as possible
* If skipping, this should be done explicitly, otherwise we think things are passing when they're not 


```suggestion
                if not self.test_rust_tokenizer:
                    self.skipTest(reason=""No fast tokenizer defined"")
                
                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name)

                # check batched
                output = tokenizer_r.apply_chat_template(
                    conversations,
                    chat_template=dummy_template,
                    tokenize=True,
                    return_assistant_tokens_mask=True,
                    return_dict=True,
                )
                for i, conv in enumerate(conversations):
                    chat_string = tokenizer_r.apply_chat_template(
                        conversations[i], tokenize=False, chat_template=dummy_template
                    )
                    assistant_start = output.char_to_token(i, chat_string.index(assistant_wrappers[i][0][0]))
                    assistant_end = output.char_to_token(
                        i, chat_string.index(assistant_wrappers[i][0][1]) + len(assistant_wrappers[i][0][1]) - 1
                    )

                    assistant_start2 = output.char_to_token(i, chat_string.index(assistant_wrappers[i][1][0]))
                    assistant_end2 = output.char_to_token(
                        i, chat_string.index(assistant_wrappers[i][1][1]) + len(assistant_wrappers[i][1][1]) - 1
                    )
                    # assert labels in assistant indices
                    self.assertEqual(
                        output[""assistant_masks""][i][assistant_start : assistant_end + 1],
                        [1] * (assistant_end - assistant_start + 1),
                    )
                    self.assertEqual(
                        output[""assistant_masks""][i][assistant_start2 : assistant_end2 + 1],
                        [1] * (assistant_end2 - assistant_start2 + 1),
                    )
                    # assert 0 in user/system indices
                    self.assertEqual(all(l == 0 for l in output[""assistant_masks""][i][:assistant_start]), True)
                    self.assertEqual(
                        all(l == 0 for l in output[""assistant_masks""][i][assistant_end + 1 : assistant_start2]),
                        True,
                    )

                # check not batched
                output = tokenizer_r.apply_chat_template(
                    conversations[0],
                    chat_template=dummy_template,
                    tokenize=True,
                    return_assistant_tokens_mask=True,
                    return_dict=True,
                )

                chat_string = tokenizer_r.apply_chat_template(
                    conversations[0], tokenize=False, chat_template=dummy_template
                )
                assistant_start = output.char_to_token(0, chat_string.index(assistant_wrappers[0][0][0]))
                assistant_end = output.char_to_token(
                    0, chat_string.index(assistant_wrappers[0][0][1]) + len(assistant_wrappers[0][0][1]) - 1
                )
                assistant_start2 = output.char_to_token(0, chat_string.index(assistant_wrappers[0][1][0]))
                assistant_end2 = output.char_to_token(
                    0, chat_string.index(assistant_wrappers[0][1][1]) + len(assistant_wrappers[0][1][1]) - 1
                )

                # assert labels in assistant indices
                self.assertEqual(
                    output[""assistant_masks""][assistant_start : assistant_end + 1],
                    [1] * (assistant_end - assistant_start + 1),
                )
                self.assertEqual(
                    output[""assistant_masks""][assistant_start2 : assistant_end2 + 1],
                    [1] * (assistant_end2 - assistant_start2 + 1),
                )

                # assert -100 in user/system indices
                self.assertEqual(all(l == 0 for l in output[""assistant_masks""][:assistant_start]), True)
                self.assertEqual(
                    all(l == 0 for l in output[""assistant_masks""][assistant_end + 1 : assistant_start2]), True
                )
``` Could you add a comment here explaining what an ""assistant wrapper"" is? In particular, the numbering in front of the `<|im_end|>` token isn't clear to me  It would be better to check the logic structure consistent here with the checks above e.g. testing `all` versus checking a list constructed of the expected length of 0s or 1s. FWIW, I think the latter is better as although slower will be easier to debug if tests start failing  @amyeroberts @yonigottesman

In https://github.com/mlflow/mlflow/pull/12757, we found this line throws in python 3.8.

https://github.com/mlflow/mlflow/actions/runs/10056412801/job/27795200814?pr=12757#step:12:1016

```
    class AssistantTracker(Extension):
        # This extension is used to track the indices of assistant-generated tokens in the rendered chat
        tags = {""generation""}
    
        def __init__(self, environment: ImmutableSandboxedEnvironment):
            # The class is only initiated by jinja.
            super().__init__(environment)
            environment.extend(activate_tracker=self.activate_tracker)
            self._rendered_blocks = None
            self._generation_indices = None
    
        def parse(self, parser: jinja2.parser.Parser) -> jinja2.nodes.CallBlock:
            lineno = next(parser.stream).lineno
            body = parser.parse_statements([""name:endgeneration""], drop_needle=True)
            return nodes.CallBlock(self.call_method(""_generation_support""), [], [], body).set_lineno(lineno)
    
        @jinja2.pass_eval_context
        def _generation_support(self, context: jinja2.nodes.EvalContext, caller: jinja2.runtime.Macro) -> str:
            rv = caller()
            if self.is_active():
                # Only track generation indices if the tracker is active
                start_index = len("""".join(self._rendered_blocks))
                end_index = start_index + len(rv)
                self._generation_indices.append((start_index, end_index))
            return rv
    
        def is_active(self) -> bool:
            return self._rendered_blocks or self._generation_indices
    
        @contextmanager
>       def activate_tracker(self, rendered_blocks: list[int], generation_indices: list[int]):
E       TypeError: 'type' object is not subscriptable

__init__   = <function PreTrainedTokenizerBase._compile_jinja_template.<locals>.AssistantTracker.__init__ at 0x7f013dc78940>
__module__ = 'transformers.tokenization_utils_base'
__qualname__ = 'PreTrainedTokenizerBase._compile_jinja_template.<locals>.AssistantTracker'
_generation_support = <function PreTrainedTokenizerBase._compile_jinja_template.<locals>.AssistantTracker._generation_support at 0x7f013dc78790>
is_active  = <function PreTrainedTokenizerBase._compile_jinja_template.<locals>.AssistantTracker.is_active at 0x7f013dc785e0>
parse      = <function PreTrainedTokenizerBase._compile_jinja_template.<locals>.AssistantTracker.parse at 0x7f013dc78820>
tags       = {'generation'}
``` ```suggestion
            def activate_tracker(self, rendered_blocks: List[int], generation_indices: List[int]):
```

or `from __future__ import annotations` needs to be added. Thanks for flagging! Opening a PR to fix


https://github.com/huggingface/transformers/pull/32155/files"
35020,2024-11-29T21:34:03Z,2024-12-02T19:39:09Z,secrettoad,1,0,1,3,1,2,2,[],250623.0,0,252306.0,0,0,0,0,2075571.933864,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35020). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Cool, thanks for taking the initiative to fix this! 🤗 ","Cool, thanks for taking the initiative to fix this! 🤗 ","# Add docstring example for compute_loss_function in trainer.py

<!--

Addresses issue raised in https://github.com/huggingface/transformers/issues/34955 around a missing/incomplete doctoring. Replaced with suggestion from @stevhliu.

-->

Fixes #34955


## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [X] Did you write any new necessary tests?


## Who can review?

@stevhliu @Rocketknight1 
",
35035,2024-12-01T09:48:58Z,2024-12-02T15:26:34Z,henryhmko,1,0,3,18,8,2,2,[],103485.0,0,106657.0,0,0,0,0,2090726.593066,,0,3,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35035). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM LGTM, and thanks for the fixes!","LGTM LGTM, and thanks for the fixes!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

1. Fixed small typos in the following docs in `Tutorials`:
- `perf_train_gpu_many.md`
- `agents.md`
- `agents_advanced.md`
- `knowledge_distillation_for_image_classification.md`



2. Fixed typo in incorrect usage of `model.image_guided_detection` in `zero_shot_object_detection.md`:
- Original doc uses `labels` but it should not be using `labels`



3. Updated OLMoE model version name from `allenai/OLMoE-1B-7B-0824` to `allenai/OLMoE-1B-7B-0924` according to Allen AI's latest huggingface model name [as seen here](https://huggingface.co/allenai/OLMoE-1B-7B-0924).


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@stevhliu 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34889,2024-11-22T23:51:54Z,2024-12-02T15:21:05Z,dvrogozh,10,9,3,76,5,5,3,"['Tests', 'bug']",2271.0,0,834999.0,0,0,0,0,2089408.695154,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'dvrogozh', 'ydshieh']","Looks like Friday evening is not the best time to run ci. Pushed same code 3 times, seeing different errors on each run:). Not related to the change I think. Will continue on Monday :). Clarified XPU backend behavior for `torch.backends.cuda.sdp_kernel`. As of PyTorch 2.5 (and today PyTorch main), XPU backend supports only `torch.nn.attention.SDPBackend.MATH` implementation of which is device agnostic with respect to implementation of each individual aten operator. So, we can reuse CUDA or CPU MATH weights for XPU (for all cases in disrespect to SDP backend). That's the change I did in the last version. @faaany, @EikanWang : fyi. Thanks @dvrogozh ! LGTM.

Have you run (some of) the relevant tests on a GPU (cuda) and XPU (cc @faaany  for this part maybe) machine?


 > Have you run (some of) the relevant tests on a GPU (cuda) and XPU (cc @faaany for this part maybe) machine?

I ran `python3 -m pytest --pspec -k test_eager_matches_sdpa_inference tests/models`:
* On Nvidia A10, CUDA, passing
* On Intel PVC, upstream pytorch XPU (without IPEX), passing
* On Intel PVC, with IPEX 2.3.110+xpu (pytorch 2.3.1), pasing
I also think that Transformers ci executes these CUDA tests and they are passing.

Above being said, we synced with @faany offline and she did try this PR on her side. I believe she saw tests passing for her w/o IPEX, but she mentioned that some tests are failing for her with IPEX. The later point is different from test results I have. I suspect that's due to the different IPEX versions we tried: I think @faaany tried IPEX with later version (she's offline now, I will check with her later on that).

Note that without this PR `test_eager_matches_sdpa_inference` tests fail for both upstream pytorch XPU and IPEX. The difference between upstream pytorch XPU and IPEX is which attention algorithms are supported. I know the current status for upstream pytorch XPU since I discussed that with relevant folks already. But I did not have such discussion for IPEX - that's what I will need to do. I know that IPEX might register additional operations and algorithms on top of upstream pytorch XPU, but whether it does so for SDP attention and in which version - I do not know (though I do have some understanding from the results I see so far). For myself: need to follow up with #34941 (sdpa tests for beit) which is being reviewed in parallel. ```
FAILED tests/models/xlm/test_modeling_xlm.py::XLMModelTest::test_batching_equivalence - AssertionError: tensor(False) is not true : Batched and Single row outputs are not equal in XLMForQuestionAnswering for key=end_top_index. Difference=1.
```
Failure on ci seems unrelated. I also see some flakiness on main ci results. I tried to rebase couple times, but this did not help. No changes from last review. yes we do have some flaky tests. (trying to fix them but in a slow peace) There is no need to have a green CI if you think some failure are irrelevant. Ping me for a double check then we could wait for a core maintainer's review. > There is no need to have a green CI if you think some failure are irrelevant. Ping me for a double check then we could wait for a core maintainer's review.

Yeah, I think that's the case here. I rebased today w/o any changes. Another test failed this time, I believe unrelated as well. Sorry, I am just paranoid about green ci - I consider that's my responsibility to achieve that on my PRs.

```
FAILED tests/trainer/test_trainer.py::TrainerIntegrationWithHubTester::test_push_to_hub_tags - KeyError: 'url'
FAILED tests/trainer/test_trainer.py::TrainerIntegrationWithHubTester::test_push_to_hub_with_saves_each_epoch - AssertionError: no logs of level WARNING or higher triggered on root
``` The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34889). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thank you again! 

@faaany Do you have any further comments? Feel free to merge if it's alright with you @ydshieh ","Thank you again! 

@faaany Do you have any further comments? Feel free to merge if it's alright with you @ydshieh ","Included fixes:
* Use `torch.nn.attention.sdpa_kernel` instead of deprecated `torch.backends.cuda.sdp_kernel`
* Use `torch.amp.autocast` instead of deprecated `torch.cuda.amp.autocast` in nemotron
* Reuse CUDA MATH thresholds in for XPU (as of PyTorch 2.5 XPU backend supports only `torch.nn.attention.SDPBackend.MATH`)

Fixes: #34888
CC: @amyeroberts @ydshieh","why `enable_kernels` not set to False here?  the same here.  I missed it. Fixed. fixed nit:

although xpu only support MATH, does it means the results from XPU will be the same as, say CUDA? Otherwise, I don't see the reason to reuse CUDA threshold. In my understanding, at least for the recent versions of PyTorch (2.5 and upcoming 2.6 ) MATH should give identical results on any hardware including different GPU devices and CPU because algorithm is implemented on torch level and is device agnostic (well, up to aten operators implementation which is device specific, but they still should give same results). The only exception here is MPS which has separate branch in the code though also implemented at torch level. Here are relevant places in the sources:
* https://github.com/pytorch/pytorch/blob/5ececd4caa4ec1534c567ec9db6efb861b760685/aten/src/ATen/native/transformers/attention.cpp#L776 (see separate code branch for MPS right above this code)
* https://github.com/pytorch/pytorch/blob/5ececd4caa4ec1534c567ec9db6efb861b760685/aten/src/ATen/native/transformers/attention.cpp#L795 So, per above I think that current reuse of CUDA thresholds is reasonable... That being said, question is whether this is sustainable in a longer term or we soon will need to adjust thresholds for XPU? Well, we might need to. It's likely that upstream pytorch XPU will get implementation for one or both of attention algorithms and here I am not sure that these will behave same as CUDA. Plus, there is also IPEX for XPU which might behave different. And all that might be version dependent....

Anyhow, whether we reuse CUDA thresholds or not is a call we need to make here. I would in any case start by copying CUDA thresholds to XPU specific location. Note that we might need few iterations to settle everything down. sound reason able to reuse (for MATH) threshold now. Thank you for explaining! thanks for updating!"
35028,2024-11-30T18:38:43Z,2024-12-02T13:47:05Z,Bojun-Feng,1,0,1,6,2,1,1,[],155299.0,0,155303.0,0,0,0,0,2096697.208742,,0,1,0,False,['Rocketknight1'],Thanks for the fix!,,"# What does this PR do?

Fixes a typo in warning switching to `optimum-quanto`.

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34930,2024-11-25T20:23:08Z,2024-12-02T10:46:46Z,milesial,2,0,1,3,1,3,3,[],41418.0,0,570218.0,0,0,0,0,2107517.63429,,0,1,0,False,"['milesial', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34930). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. For the mllama vision encoder (which should be the same size for 11B and 90B), in TRT BF16 BS8:

Before this change: 7GB of activations
After this change: 3GB of activationsHey @milesial ! Makes sense, agree this should reduce memory usage. Do you have any actual numbers of how much less memory is used after this modification? Like a very short test with several model runs

Anyway LGTM, pinging the core maintainer here  :fire: great catch thanks for reporting and fixing!","Hey @milesial ! Makes sense, agree this should reduce memory usage. Do you have any actual numbers of how much less memory is used after this modification? Like a very short test with several model runs

Anyway LGTM, pinging the core maintainer here  :fire: great catch thanks for reporting and fixing!","Changes the order of ops from ""stack -> tensor index"" into ""list index -> stack"". This small change means the stack operation has now a lot fewer inputs. This can save a lot (multiple x) of runtime memory especially once the graph is exported to ONNX or TRT.

@zucchini-nlp

",
34779,2024-11-18T11:20:33Z,2024-11-20T06:46:13Z,zucchini-nlp,7,2,3,13,3,3,2,[],1590.0,0,1211441.0,0,0,0,0,2103649.928021,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'Scyther-07', 'zucchini-nlp']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34779). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. sorry @qubvel , wrong tag for review  Hey @zucchini-nlp,
I am running the following code but it is throwing me a TypeError:
```
from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig, LlavaNextProcessor
model_id = ""llava-hf/llava-v1.6-mistral-7b-hf""
processor = AutoProcessor.from_pretrained(model_id)
```

Error: 
`TypeError: LlavaNextProcessor.__init__() got an unexpected keyword argument 'image_token'`

Now, I looked at the commits of this PR and it looks good to me. The issue might be with your last PR [#33424](https://github.com/huggingface/transformers/pull/33424). Kindly look into this.  @Scyther-07 hey, which transformers version you are using?  > @Scyther-07 hey, which transformers version you are using?

It's 4.39.3. 
I just noticed that the above code works fine on Google Colab but throws an error in the Kaggle Notebook. I don't know what to make of it. I think I should shift to Colab.  @Scyther-07 hmm, the 4.39.3 should throw error indeed and you need at least v4.43 to bypass the error. In fact we are currently changing the way inputs for VLMs are processed, thus I'd recommend to use the latest transformers after release. It will be v4.47 around next 1-2 weeks, not released yet now :)  Yeah, it worked. Too foolish of me. Thanks for the help!Thanks for the fix! Just a question re types Thanks, does it fix #34625 entirely? (let's close it if so!)","Thanks for the fix! Just a question re types Thanks, does it fix #34625 entirely? (let's close it if so!)","# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34625. There was a small precision error in unpadding because the modeling code casts the size to list, while processing code works with tensors. This PR casts everything to list to match the calculations ","Can`original_size` be already a list? I suppose it might be the case, based on the type hint. Calling `tolist()` for a list will lead to an error. oh right, nice catch, we need a type check also"
34953,2024-11-27T08:12:32Z,2024-12-02T10:44:42Z,chenweize1998,3,2,2,9,3,4,3,[],12322.0,0,441130.0,0,0,0,0,2107643.552912,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'chenweize1998', 'zucchini-nlp']","Just revised the code according to the review.  Core maintainer's approve and we'll merge it! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34953). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Hey, thanks a lot for fixing this! I had it somewhere on a bigger PR to clean up all VLMs and ensure it works, but I think we can merge this now

Left one tiny comment but overall LGTM  Thanks @chenweize1998, we had the issue in https://github.com/huggingface/trl/pull/2413

cc @ArthurZucker for final review and merging Thanks! Adding a test would be welcome as well in another PR!","Hey, thanks a lot for fixing this! I had it somewhere on a bigger PR to clean up all VLMs and ensure it works, but I think we can merge this now

Left one tiny comment but overall LGTM  Thanks @chenweize1998, we had the issue in https://github.com/huggingface/trl/pull/2413

cc @ArthurZucker for final review and merging Thanks! Adding a test would be welcome as well in another PR!","

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #34952  (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","ultra nit: could be more readable as this 

```python
if not isinstance(image_size, (list, tuple)):
    # cast to list to avoid numerical precision errors when calculating unpadding
    image_size = image_size.tolist()
orig_height, orig_width = image_size
``` Resolve in the new commit. Thanks!"
35016,2024-11-29T13:12:05Z,2024-12-02T10:39:39Z,xenova,1,0,2,7,1,3,3,[],1871.0,0,250055.0,0,0,0,0,2107945.934708,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35016). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks, LGTM! LGTM Very intricate, thanks for adding the detailed comment!","Thanks, LGTM! LGTM Very intricate, thanks for adding the detailed comment!","# What does this PR do?

Follow-up to #34852. See https://github.com/huggingface/transformers/pull/34852#issuecomment-2507768365 for why this is necessary.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
32830,2024-08-15T01:18:22Z,2024-09-10T15:35:57Z,guangy10,19,30,3,377,10,5,2,['ExecuTorch'],2251.0,0,9287581.0,1,0,0,0,2271644.197849,,1,3,0,False,"['guangy10', 'poedator', 'HuggingFaceDocBuilderDev', 'gante', 'ArthurZucker']","Is the test failure relevant?
```
FAILED tests/generation/test_beam_search.py::ConstrainedBeamSearchTest::test_constrained_beam_scorer_finalize - 
ValueError: Each list in `nested_token_ids` can't be a complete subset of another list, but is [[19, 40], [19, 40]].
```
I don't get what this test does and what the error means > As we've discussed in other places, the cache config should live inside `GenerationConfig`, not in `Config` 🤗 There should be no changes in the model architecture files (e.g. `modeling_gemma.py`), otherwise we expose ourselves to a very lengthy feature propagation cycle -- we would have to edit hundreds of files.
> 
> In a nutshell, to enable this feature:
> 
> 1. `generation_config` can hold and serialize the desired cache config
> 2. `PreTrainedModel.from_pretrained` should accept `generation_config` of type `GenerationConfig`, and overwrite the default `generation_config`
> 3. `generate` uses that field when it is set


I have no preference where the `cache_config` lives, so I can move it to `GenerationConfig` per your feedback.

I explained why some modeling code change is necessary in order to support `torch.export()` [abave here](https://github.com/huggingface/transformers/pull/32830#discussion_r1722305907), and the while work should be similar to support `torch.compile`. Because `torch.export` is more restricted due to its boarder use-cases in a non-python env, there are limitations to the graph input/outputs. The idea is that if we could make such changes in the fundamental models like llama, bert, clip, etc. many other models based on those could leverage the work. @amyeroberts could share more contexts just in case. > Hey! Allowing configuration of the `staticCache` is most welcome! the other changes IMO do not belong in transformers and are way to heavy, while in `optimum` we could simply wrap arround transformers models as it's something we are more likely to push than change each modeling file

It makes sense to have `optimum` being the place where the actual lowering to on-device happens. This would align it well with other on-device workflows like onnx and tflite. Here in this PR what I'm trying to enable is purely for `torch.export`, similarly to the work for `torch.compile`. One difference between `torch.compile` and `torch.export` is that export is the entry point for the PyTorch 2.x based on-device workflow ie `ExecuTorch`. So getting a transformer model compatible with `torch.export` in a way that can be further lowered to `ExecuTorch` later via  `optimum` is what I'm trying to accomplish in the `transformers` repo. Totally understand the motivations! 
For now the changes are too ""drastic"" and constraining for general use-cases. 
The changes done for torch compile were actually also benefic for non compile case and fairly ""minimal"". 
+ #29753 where we had a LOT of issues with registering a causal mask @guangy10 my apologies for the acute review above -- the PR already had a positive review, I wanted to make sure it didn't go in at least without exploring alternatives 🤗 

As Arthur wrote, let's try to explore options that do not require modeling changes. We've learned from recent experience that seemingly harmless changes can quickly escalate into difficult issues -- for us and for our users  > @guangy10 my apologies for the acute review above -- the PR already had a positive review, I wanted to make sure it didn't go in at least without exploring alternatives 🤗
> 
> As Arthur wrote, let's try to explore options that do not require modeling changes. We've learned from recent experience that seemingly harmless changes can quickly escalate into difficult issues -- for us and for our users

Thank you for reviewing this PR. All feedback is appreciated! @ArthurZucker @gante This PR is updated according to comments about generation config plus what we've discussed in the meeting last week:
 1. Passed `generation_config` via `from_pretrained` and override the default `generation_config`.
 2. Implement the [preferred option](https://docs.google.com/document/d/1A1ieHBVMT0q6yE2q7Mz9-_RVfF5x9xMFu0RKa_vzQSc/edit#bookmark=id.xwm0p2hbn3o2) we discussed last week. That is, continue using the forward() adapter and define it in transformers where lives closest to the source of truth for all transformers models and use in all places where ""Export to ExecuTorch"" is suitable.
 
I was planning to split this PR into two parts then I realized it's easier to keep it in one so that you can see the full picture about how the mode is configured and exported. cc: @amyeroberts @qubvel for additional eyes on reviewing. Okay, all CIs are green 🚀 🚀  > Looks good! Would be nice to have a real world example

Yeah, there is an e2e demo with PR https://github.com/pytorch/executorch/pull/4723 in ExecuTorch repo. You can see `gemma2b` is working out-of-the-box with ~15 tokens/s using `convert_and_export` from `huggingface/transformers/integrations/executorch.py` we added in this PR .

After this PR is merged, I can start adding an integration test for gemma2b utilizing the exported program (not lowered to ExecuTorch since no ExecuTorch integration in transformers repo) to generate sequences The examples_tensorflow failure doesn't seem to be relevant haha I see @amyeroberts was reviewing this at the same time as I was, there are a few things we both suggested/mentioned

(@amyeroberts's comments take precedence, as she's in charge of overall library maintenance)  Cool! Thanks everyone for all the valuable feedback! Please take a look and let me know if you have further comments Just updated the copyright headers and rebased to latest main to resolve conflicts. I start seeing new failures on this PR. @amyeroberts @gante Any idea why, or it's irrelevant and safe to merge? Hi @guangy10 👋 

I wasn't sure what was missing at first, so I went into the code, found the issue, and pushed the fix (tl;dr it was missing the lazy import structure for `transformers.integrations`). I hope you don't mind 🤗 

EDIT: it was also missing an entry for the new doc page in our doc's toc The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32830). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Hi @guangy10 👋
> 
> I wasn't sure what was missing at first, so I went into the code, found the issue, and pushed the fix (tl;dr it was missing the lazy import structure for `transformers.integrations`). I hope you don't mind 🤗
> 
> EDIT: it was also missing an entry for the new doc page in our doc's toc

@gante Thanks! If no more comments, can we merge this PR? @guangy10 done! Thank you for iterating with us 🤗  This PR was not universally good - it requires unnecessary `cache_config` for cases when users just want to indicate `cache_implementation=static` and let the rest of cache config be set by `.generate()`. 
suggestion: undo this PR and set custom logic just for the ExecuTorch users.

more details in https://github.com/huggingface/transformers/issues/35026
@gante Hey! Allowing configuration of the `staticCache` is most welcome! the other changes IMO do not belong in transformers and are way to heavy, while in `optimum` we could simply wrap arround transformers models as it's something we are more likely to push than change each modeling file As we've discussed in other places, the cache config should live inside `GenerationConfig`, not in `Config` 🤗 There should be no changes in the model architecture files (e.g. `modeling_gemma.py`), otherwise we expose ourselves to a very lengthy feature propagation cycle -- we would have to edit hundreds of files.

In a nutshell, to enable this feature:
1. `generation_config` can hold and serialize the desired cache config
2. `PreTrainedModel.from_pretrained` should accept `generation_config` of type `GenerationConfig`, and overwrite the default `generation_config`
3. `generate` uses that field when it is set

 Looks good! Would be nice to have a real world example Thanks for adding this! 

Just gave an overall review: @ArthurZucker and @gante are the cache kings, so I'll leave it to them to cover anything on the cache design patterns. Overall looks good to me - just a few small comments and nits  Thank you for iterating, looks mostly good to me 💛 

I've added a few comments to make the PR fit with a few `transfomers` nuances, like our import structure or documentation Added a few suggestions to fix the import structure according to our expectations, otherwise LGTM 🤗  Thanks for iterating on this! 

+1 on @gante's comment's about handling the imports such that most of the code isn't indented under `is_torch_available`, and a small nit on the docs. 

Otherwise all looks good to me! ❤️ ","Hey! Allowing configuration of the `staticCache` is most welcome! the other changes IMO do not belong in transformers and are way to heavy, while in `optimum` we could simply wrap arround transformers models as it's something we are more likely to push than change each modeling file As we've discussed in other places, the cache config should live inside `GenerationConfig`, not in `Config` 🤗 There should be no changes in the model architecture files (e.g. `modeling_gemma.py`), otherwise we expose ourselves to a very lengthy feature propagation cycle -- we would have to edit hundreds of files.

In a nutshell, to enable this feature:
1. `generation_config` can hold and serialize the desired cache config
2. `PreTrainedModel.from_pretrained` should accept `generation_config` of type `GenerationConfig`, and overwrite the default `generation_config`
3. `generate` uses that field when it is set

 Looks good! Would be nice to have a real world example Thanks for adding this! 

Just gave an overall review: @ArthurZucker and @gante are the cache kings, so I'll leave it to them to cover anything on the cache design patterns. Overall looks good to me - just a few small comments and nits  Thank you for iterating, looks mostly good to me 💛 

I've added a few comments to make the PR fit with a few `transfomers` nuances, like our import structure or documentation Added a few suggestions to fix the import structure according to our expectations, otherwise LGTM 🤗  Thanks for iterating on this! 

+1 on @gante's comment's about handling the imports such that most of the code isn't indented under `is_torch_available`, and a small nit on the docs. 

Otherwise all looks good to me! ❤️ ","# What does this PR do?

This PR is to address #32500 for [""Export to ExecuTorch""](https://github.com/huggingface/transformers/issues/32253)

 - Enable the ability to load a model with options to statically config the model using `StaticCache`
 - Create a new integration point for `ExecuTorch` at `transformers/integrations/executorch.py` and hosts the wrapper module class and util `convert_and_export_with_cache` there 
 - Expose the module so that it can be used in other repos
 - Improve test

```
from transformers import convert_and_export_with_cache

model = AutoModelForCausalLM.from_pretrained(
    hf_model_repo,
    attn_implementation=""sdpa"",
    generation_config=GenerationConfig(
        use_cache=True,
        cache_implementation=cache_implementation,
        max_length=max_cache_len,
        cache_config={
            ""batch_size"": batch_size,
            ""max_cache_len"": max_cache_len,
        },
    ),
)

exported_prog = convert_and_export_with_cache(model)

# Further lower the exported program to ExecuTorch with delegates
```

**A real world example/demo:**
The test model `gemma-2b` is naturally exportable via `convert_and_export_with_cache`. 
The test model `gemma-2b` is also lowerable and runnable via `ExecuTorch` w/ 15 tokens/s on `XNNPACK` backend. Checkout https://github.com/pytorch/executorch/pull/4723 for details in `ExecuTorch` repo.


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? #32500
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker
@amyeroberts
@gante
","```suggestion
    def __init__(self, batch_size: int, max_cache_len: int, device=""cpu""):
        self.batch_size = batch_size
        self.max_cache_len = max_cache_len
``` See #32657 as it was a bit counter intuitive!  this is a more involved change, as we have to add something to the model. It won't really scale much as we would have to add this for every model (like llama) right?  this is kind of a breaking change as well, won't interact well with the rest of the lib `generate` needs to interacat and change the cache (assisted decoding for example)  Feel free to suggest a better place for this logics, which should be common for all causal llm. Ideally if we could add this to the more fundamental model like llama, many other llama-bases/like models can take advantage of it. In the real world, enabling `torch.export` will be similar to the `torch.compile` work where necessary changes in individual model can be involved. cc: @amyeroberts
 @ArthurZucker The reason of unsetting `past_key_values` in the `outputs` is due to the restriction in `torch.export()`. Neither the inputs nor outputs can have `Cache` object.
```
E    torch._dynamo.exc.UserError: It looks like one of the outputs with type `<class 'transformers.cache_utils.StaticCache'>` is not supported or pytree-flattenable.
E    Exported graphs inputs can only contain the following supported types: [<class 'torch.Tensor'>, <class 'torch.SymInt'>, <class 'torch.SymFloat'>, <class 'torch.SymBool'>, <class 'torch.ScriptObject'>, <class 'NoneType'>, <class 'complex'>, <class 'torch.dtype'>, <class 'str'>, <class 'bool'>, <class 'ellipsis'>, <class 'int'>, <class 'torch.layout'>, <class 'code'>, <class 'torch.memory_format'>, <class 'bytes'>, <class 'float'>, <class 'torch.device'>].
```

We should only unset the `outputs.past_key_values` when the model is used for export. What's your suggestion to move forward? Maybe we could have another field `set_exportable` in the `cache_config` and only set it when it's used for `torch.export` and hide unexportable params from I/O?  The changes we did for compile where overall pretty minimal, at some point we did store the causal mask but it was causing a lot of problem for a lot of people. 

Not sure where this should go, as it's specific for decoder only models, but `optimum` is where we usually add such changes / monkey patches!  Perhaps we could create a class that wraps any `XXXForCausalLM` model, and removes any incompatible output from `torch.export`'s perspective! Would that work for you?

On our end, it's very important to avoid making changes to the modeling code itself (i.e. everything in `modeling_xxx.py`). These files are used in many places, including outside the HF ecosystem, so we need to be out of options before making changes there 🤗  Addressed nitpick: is there a typo? exportatible? :) Aha, typo. Fixed now. it's a bit weird that the mask is 2d, 2d for us is usually batch x sequence_length, not sequence x target length  would need a bit of doc about what the inputs should be! 
For example should cache position be just the current positions? Why do we need input ids + a real word application!  I think we can also test output with / without the exported model?  ```suggestion
        input_ids = tokenizer([""Hey!"", ""Let's dive into the""], return_tensors=""pt"").to(device)[""input_ids""]
``` and maybe pad the inputs? (also should we not test with an input attention mask? (since we register a ""mask"" not sure if that would work Yeah, this just is an example input used to trace the model via `torch.export`. It doesn't have to be the meaningful token ids. Edited the code to generate default examples inputs if not provided. Inline doc improved. What do you mean test w/o the exported model? nit - typo? 

```suggestion
class TorchExportableModuleWithStaticCache(torch.nn.Module):
``` I think this check is OK for now, but unfortunately probably not sufficient to catch all of the correct models. We might want to have this set as part of a model's attributes  nit - this just formats correctly for our rendered docs 

```suggestion
        model (`PreTrainedModel`): The pretrained model to wrap. The model must have caching
        enabled and use a 'static' caching implementation.

    Attributes:
        model (`PreTrainedModel`): The underlying pretrained model.
        static_cache (`StaticCache`): A static cache instance used to store past key values for faster inference.
        is_causal (`bool`): Indicates whether the model architecture supports causal masking (e.g., causal language models).
``` nit 

```suggestion
            input_ids (`torch.Tensor`): Tensor representing current input token id to the module.
            cache_position (`torch.Tensor`): Tensor representing current input position in the cache.
``` I might be missing something - but where / when is `self.mask` set? nit. 

```suggestion
        model (`PreTrainedModel`): The pretrained model to be exported.
        example_input_ids (`torch.Tensor`): Example input token id used by `torch.export`.
        example_cache_position (`torch.Tensor`): Example current cache position used by `torch.export`.
``` torch is already imported

```suggestion
``` We have `is_torch_greater_or_equal_than_2_3` flag you can import from `pytorch_utils` for this :)  nit - to match with typo correction above 

```suggestion
            TorchExportableModuleWithStaticCache(model),
``` Could we rename this to something like `convert_and_export_with_cache`? As some of the models we'd like to make exportable will not have a cache  If we're going to override if `generation_config` is passed in, we can just shortcut and set this directly instead of going through `GenerationConfig.from_pretrained` first which might e.g. download files. "
34927,2024-11-25T18:43:34Z,2024-11-26T13:28:07Z,VladOS95-cyber,3,0,1,17,2,1,1,['Tensor Parallel'],65283.0,0,361154.0,0,0,0,0,2322559.427882,,0,1,0,False,"['DebarshiChanda', 'VladOS95-cyber', 'Rocketknight1']","cc @arthurzucker since I think he was handling the tensor parallelism issue - let me know if I should have pinged someone else instead! @VladOS95-cyber @ArthurZucker Hi guys, I had a small doubt. Why aren't the changes in the query_states, key_states and value_states applied to other attention classes (like `MistralFlashAttention2`, `MistralSdpaAttention`) as well? > @VladOS95-cyber @ArthurZucker Hi guys, I had a small doubt. Why aren't the changes in the query_states, key_states and value_states applied to other attention classes (like `MistralFlashAttention2`, `MistralSdpaAttention`) as well?

Hey @DebarshiChanda! Thanks for noticed that, i think you are right and we should apply the same infer logic for other attention classes. I am going to fix it in another PR https://github.com/huggingface/transformers/pull/35007LGTM thanks for adding! 🤗 ",LGTM thanks for adding! 🤗 ,"# What does this PR do?

This PR uses the torch.distributed.tensor.parallel subpackage to implement Tensor Parallel for Mistral.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Link:https://github.com/huggingface/transformers/issues/34789
- [] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35008,2024-11-28T18:31:06Z,2024-11-29T13:19:58Z,SamuelLarkin,1,0,1,2,1,1,1,[],69319.0,0,69320.0,0,0,0,0,2355942.062715,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35008). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the fix!,Thanks for the fix!,"# What does this PR do?

Fix a warning message
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@muellerzr and @SunMarc

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35006,2024-11-28T17:02:53Z,2024-11-29T13:44:57Z,alvarobartt,1,0,1,4,1,1,1,[],1574.0,0,74524.0,0,0,0,0,2356032.231266,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35006). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Yes, good spot! The existing docstring is quite confusing.","Yes, good spot! The existing docstring is quite confusing.","# What does this PR do?

This PR updates the `FillMaskPipeline.__call__` signature to remove the unused `*args` and updates the docstrings accordingly, to use `inputs` over `args`.

## Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request) Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review?

As it's related to a `pipeline` I'm assuming @Rocketknight1 (?), let me know otherwise!",
34852,2024-11-21T13:14:27Z,2024-11-26T15:14:36Z,xenova,9,0,3,2,1,2,2,"['ONNX', 'Vision', 'Multimodal', 'run-slow']",496.0,0,691109.0,0,0,0,0,2357953.527409,,0,3,0,False,"['qubvel', 'HuggingFaceDocBuilderDev', 'xenova', 'zucchini-nlp']","Failing tests seem unrelated to PR, but important to fix The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34852). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @xenova, thanks for the fix and detailed report! looks good to me!
Can you please push an empty commit with `[run-slow] qwen2_vl` to trigger slow tests as well?

cc @zucchini-nlp for vlms Not sure if caused by this PR, but there is a minor difference in slow test. Other failing slow tests seem unrelated to this PR (torch dynamo).

Is there a way to test whether this was also the case previously?

![image](https://github.com/user-attachments/assets/b70b242e-2e92-420c-a92e-122000efeb76)
 Hi @xenova! You can search in `#transformers-ci-daily-models` slack channel with the failed test name to see if a target is already there (or with the model name) Got it 👌 Indeed, they seem to have been failing already. Qwen2-VL generation seems broken now for FA2 👀  (`RuntimeError: cu_seqlens_q must have dtype int32`)  > Qwen2-VL generation seems broken now for FA2 👀 (`RuntimeError: cu_seqlens_q must have dtype int32`)

I'll open a PR to only do the cast when tracing 🫡  Fix: https://github.com/huggingface/transformers/pull/35016
I added a note to why this is necessary. Hopefully it isn't too verbose :)Cool, thanks for enabling onnx in more VLMs! The failing test is flaky so feel free to re-trigger it until it turns green Nice, a good catch! 🫡 ","Cool, thanks for enabling onnx in more VLMs! The failing test is flaky so feel free to re-trigger it until it turns green Nice, a good catch! 🫡 ","# What does this PR do?

This PR fixes onnx export support for the vision encoder of Qwen2-VL, which converts the `cu_seqlens` to `torch.int32`, leading to errors later on when using the values for slicing.

Error message:

```
onnx.onnx_cpp2py_export.shape_inference.InferenceError: [ShapeInferenceError] (op_type:Slice, node name: /blocks.0/attn/Slice_4): axes has inconsistent type tensor(int64)
```
See https://github.com/microsoft/onnxruntime/issues/13497 for more information.

Since `torch.cumsum` will automatically convert both `torch.int32` and `torch.int64` tensors to `torch.int64` output, I propose we convert it back to the dtype of the inputs.
```py
>>> import torch
>>> torch.tensor([1,2,3], dtype=torch.int32).cumsum(0).dtype
torch.int64
>>> torch.tensor([1,2,3], dtype=torch.int64).cumsum(0).dtype
torch.int64
```

The location of the issues:

https://github.com/huggingface/transformers/blob/c57eafdaa119eecae8557be4c626629bc1adc0fd/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L1044-L1046


Code to reproduce the issue:

```py

import requests
from PIL import Image
import torch
from transformers import (
    AutoProcessor,
    Qwen2VLForConditionalGeneration,
)

# Constants
VISION_MODEL_NAME = ""vision_encoder.onnx""

# Load model and processor
model_id = ""hf-internal-testing/tiny-random-Qwen2VLForConditionalGeneration""
model = Qwen2VLForConditionalGeneration.from_pretrained(model_id).eval()
processor = AutoProcessor.from_pretrained(model_id)

# Prepare inputs
url = ""https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg""
image = Image.open(requests.get(url, stream=True).raw)
conversation = [
    {
        ""role"": ""user"",
        ""content"": [
            { ""type"": ""image"" },
            { ""type"": ""text"", ""text"": ""Describe this image.""},
        ],
    },
]
images = [image]
text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
inputs = processor(text=[text_prompt], images=images, padding=True, return_tensors=""pt"")

## Vision model
vision_inputs = dict(
    pixel_values=inputs[""pixel_values""],
    grid_thw=inputs[""image_grid_thw""],
)
vision_inputs_positional = tuple(vision_inputs.values())
vision_outputs = model.visual.forward(*vision_inputs_positional)  # Test forward pass
torch.onnx.export(
    model.visual,
    args=vision_inputs_positional,
    f=VISION_MODEL_NAME,
    export_params=True,
    opset_version=14,
    do_constant_folding=True,
    input_names=list(vision_inputs.keys()),
    output_names=[""image_features""],
    dynamic_axes={
        ""pixel_values"": {
            0: ""batch_size * grid_t * grid_h * grid_w"",
            1: ""channel * temporal_patch_size * patch_size * patch_size"",
        },
        ""grid_thw"": {0: ""batch_size""},
        ""image_features"": {0: ""batch_size * grid_t * grid_h * grid_w""},
    },
)

# Load and check the exported model model
import onnx
model = onnx.load(VISION_MODEL_NAME)
onnx.checker.check_model(model, full_check=True)
inferred = onnx.shape_inference.infer_shapes(model, check_type=True)
```



<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@amyeroberts, @qubvel, @simonJJJ (original PR)

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
35005,2024-11-28T16:05:24Z,2024-11-29T12:46:59Z,qubvel,2,0,2,4,2,1,1,[],624.0,0,74495.0,0,0,0,0,2359511.119491,,0,2,0,False,"['qubvel', 'HuggingFaceDocBuilderDev']",@ArthurZucker please have a look at the docker image update The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35005). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGMT! 🤗 ,LGMT! 🤗 ,"# What does this PR do?

Bump timm version to use `forward_intermediate` in `TimmWrapper`
 - https://github.com/huggingface/transformers/pull/34564
 - https://github.com/huggingface/transformers/pull/34996 (CI images build here)

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

cc @ydshieh 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34587,2024-11-03T20:31:28Z,2024-11-29T11:58:11Z,tibor-reiss,1,16,6,166,2,4,2,"['Vision', 'Multimodal', 'Processing']",2215588.0,0,2215603.0,0,0,0,0,2362440.709489,,0,6,0,False,['qubvel'],"Thanks for working on this @tibor-reiss!Hi @tibor-reiss, thanks for working on this! And sorry for the delay, the team was on the offsite. Don't hesitate to ping for review a few times 🤗  @tibor-reiss thanks for working on this and adding tests 🤗 

@ArthurZucker please review whenever you have bandwidth, see also a comment below. thanks for the description! Regarding the breaking change, let's either properly break or deprecate!","Hi @tibor-reiss, thanks for working on this! And sorry for the delay, the team was on the offsite. Don't hesitate to ping for review a few times 🤗  @tibor-reiss thanks for working on this and adding tests 🤗 

@ArthurZucker please review whenever you have bandwidth, see also a comment below. thanks for the description! Regarding the breaking change, let's either properly break or deprecate!","Adds uniformized processors for TrOCR following #31911 

Added tests (which also pass in main branch).

@qubvel @molbap","As far as I see it was an image processor, not a tokenizer, wasn't it? Thanks for adding tests ❤️  Not sure we need this `*args`, may lead to silent missing of `text` if it passed not as a keyword argument  I'll add, it's acceptable to use this pattern to capture arguments that are absolutely necessary and are not capturable by any other means, and indeed it should typically be after at least `images` and `text` - are you trying to capture a specific arg? I think this is correct. This path is only taken if we are inside the `as_target_processor` context manager. So I simplified it - while keeping backward compatibility - by removing self.current_processor. Thanks for the discussion. I don't have any specific arg in mind, it's purely for backwards compatibility. I agree though that best would be to remove `*args` completely, but that would break BC, because it is used both when calling `tokenizer` and `image_processor`. If this is not a concern here, I am happy to remove it :)

Regarding `text`: from the old code: `text = kwargs.pop(""text"", None)`. If I am not mistaken, this is the only way to set `text` inside `__call__`. The proposed solution keeps this convention, i.e. if someone specifies `text` as positional, it will still not work. Unfortunately, it's not possible yet to combine variadic number of positional arguments with the keyword-only delimiter, so this is the best I was able to come up with. Got it, I would leave it as it was, otherwise, `@as_target_processor` has no effect to `current_processor`. 

I'm not even sure it is used somewhere, probably we should deprecate `as_target_processor (I might be missing smth). @molbap do you know if it is used somewhere? I see, maybe a better tradeoff would be to remove it or to force users to use keyword arguments for everything other than `images`, I mean use pure `*` in the signature instead of `*args`. We can also add 🚨 for current PR to indicate it is not fully BC

 I have put back `current_processor`. There is already a warning inside the context manager that it will be removed in transformers v5. I have removed args. Thanks for putting it back! Indeed, I missed the warning 😄  Comment for @ArthurZucker: 
Slight breaking change (`*`) to force users to use keyword arguments in case previously positional args were used. Otherwise, we might either miss some `args` or get it assigned to the wrong keyword parameters and getting not a clear error message. tiny comment: let's add maybe a `#TODO` here to keep in mind to deprecate this pattern in this future :pray:  Up to you, there is no deprecation cycle so this is not very actionnable, neither for us nor for the user! Removed the `*`.
cc @molbap @qubvel "
33023,2024-08-22T20:19:26Z,2024-11-26T17:23:11Z,AhmedAlmaghz,3,13,15,356,2,2,2,[],7519998.0,0,8495920.0,1,0,0,0,2390045.866905,,0,15,0,False,"['HuggingFaceDocBuilderDev', 'AhmedAlmaghz']","Thank you @abodacs  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33023). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thank you @stevhliu السﻻم عليكم

@AhmedAlmaghz 
تمت المراجعة جزاء الله خيرا Thanks! Thanks!","السﻻم عليكم

@AhmedAlmaghz 
تمت المراجعة جزاء الله خيرا Thanks! Thanks!","
## What does this PR do?
Translated the `docs/source/ar/benchmarks.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ","```suggestion
> ⚠️ تنبيه: أدوات قياس الأداء من Hugging Face أصبحت قديمة،ويُنصح باستخدام مكتبات خارجية لقياس سرعة وتعقيد الذاكرة لنماذج Transformer.
```
 ```suggestion
لنلق نظرة على كيفية تقييم أداء نماذج 🤗 Transformers، وأفضل الممارسات، ومعايير الأداء المتاحة بالفعل.

يُمكن العثور على دفتر ملاحظات يشرح بالتفصيل كيفية قياس أداء نماذج 🤗 Transformers [هنا](https://github.com/huggingface/notebooks/tree/main/examples/benchmark.ipynb).
```
  ```suggestion
تسمح الفئتان [`PyTorchBenchmark`] و [`TensorFlowBenchmark`] بتقييم أداء نماذج 🤗 Transformers بمرونة. تتيح لنا فئات التقييم قياس الأداء قياس _الاستخدام الأقصى للذاكرة_ و _الوقت اللازم_ لكل من _الاستدلال_ و _التدريب_.
```
  ```suggestion
> هنا، ييُعرَّف _الاستدلال_ بأنه تمريرة أمامية واحدة، ويتم تعريف _التدريب_ بأنه تمريرة أمامية واحدة وتمريرة خلفية واحدة.
```
  ```suggestion
تتوقع فئات تقييم الأداء [`PyTorchBenchmark`] و [`TensorFlowBenchmark`] كائنًا من النوع [`PyTorchBenchmarkArguments`] و [`TensorFlowBenchmarkArguments`]، على التوالي، للتنفيذ. [`PyTorchBenchmarkArguments`] و [`TensorFlowBenchmarkArguments`] هي فئات بيانات وتحتوي على جميع التكوينات ذات الصلة لفئة تقييم الأداء المقابلة. في المثال التالي، يتم توضيح كيفية تقييم أداء نموذج BERT من النوع _bert-base-cased_.
```
  ```suggestion
هنا، يتم تمرير ثلاثة معامﻻت إلى فئات بيانات حجة قياس الأداء، وهي `models` و `batch_sizes` و `sequence_lengths`. المعامل `models` مطلوبة وتتوقع `قائمة` من بمعرّفات النموذج من [مركز النماذج](https://huggingface.co/models) تحدد معامﻻت القائمة `batch_sizes` و `sequence_lengths` حجم `input_ids` الذي يتم قياس أداء النموذج عليه. هناك العديد من المعلمات الأخرى التي يمكن تكوينها عبر فئات بيانات معال قياس الأداء. لمزيد من التفاصيل حول هذه المعلمات، يمكنك إما الرجوع مباشرة إلى الملفات `src/transformers/benchmark/benchmark_args_utils.py`، `src/transformers/benchmark/benchmark_args.py` (لـ PyTorch) و `src/transformers/benchmark/benchmark_args_tf.py` (لـ Tensorflow). أو، بدلاً من ذلك، قم بتشغيل أوامر shell التالية من المجلد الرئيسي لطباعة قائمة وصفية بجميع المعلمات القابلة للتكوين لـ PyTorch و Tensorflow على التوالي.
```
 
 ```suggestion
يُمكن ببساطة تشغيل كائن التقييم الذي تم تهيئته عن طريق استدعاء `benchmark.run()`.
```

  ```suggestion
يُمكن بعد ذلك تشغيل كائن قياس الأداء الذي تم تهيئته عن طريق استدعاء `benchmark.run()`.
```

  ```suggestion
بشكل افتراضي، يتم تقييم _الوقت_ و _الذاكرة المطلوبة_ لـ _الاستدلال_. في مثال المخرجات أعلاه، يُظهر القسمان الأولان النتيجة المقابلة لـ _وقت الاستدلال_ و _ذاكرة الاستدلال_. بالإضافة إلى ذلك، يتم طباعة جميع المعلومات ذات الصلة حول بيئة الحوسبة، على سبيل المثال نوع وحدة معالجة الرسومات (GPU)، والنظام، وإصدارات المكتبة، وما إلى ذلك، في القسم الثالث تحت _معلومات البيئة_. يمكن حفظ هذه المعلومات بشكل اختياري في ملف _.csv_ عند إضافة المعامل `save_to_csv=True` إلى [`PyTorchBenchmarkArguments`] و [`TensorFlowBenchmarkArguments`] على التوالي. في هذه الحالة، يتم حفظ كل قسم في ملف _.csv_ منفصل. يمكن اختيارًا تحديد مسار كل ملف _.csv_ عبر فئات بيانات معامل قياس الأداء.
```
  ```suggestion
بدلاً من تقييم النماذج المدربة مسبقًا عبر معرّف النموذج، على سبيل المثال `google-bert/bert-base-uncased`، يُمكن للمستخدم بدلاً من ذلك قياس أداء تكوين عشوائي لأي فئة نموذج متاحة. في هذه الحالة، يجب إدراج ""قائمة"" من التكوينات مع معامل قياس الأداء كما هو موضح أدناه.
```

  ```suggestion
- يجب دائمًا ذكر معلومات البيئة عند مشاركة نتائج تقييم النموذج. يُمكن أن تختلف النتائج اختلافًا كبيرًا بين أجهزة GPU المختلفة وإصدارات المكتبات، وما إلى ذلك، لذلك فإن نتائج الاختبار بمفردها ليست مفيدة جدًا للمجتمع.
```

  You can use the doc-builders special syntax for warnings and tips. For example:

```md
> [!WARNING]
> your content here
``` ```md
> [!TIP]
> your content here
```"
33079,2024-08-22T20:24:43Z,2024-11-11T18:41:01Z,AhmedAlmaghz,5,24,26,333,3,2,1,[],6897016.0,0,8495601.0,1,0,0,0,2390048.88337,,0,26,0,False,"['HuggingFaceDocBuilderDev', 'AhmedAlmaghz']","Thank you @abodacs  The following pull requests have been added to the current pull request to be merged once:

- [x]  Troubleshoot #33017

@abodacs  @stevhliu 

Thank you أخي عبدالله @abodacs  هذا ما تم إنجازه في هذا القسم 
إلى الامام إن شاء الله



# DEVELOPER GUIDES

- [x]  Use fast tokenizers from 🤗 Tokenizers #33034
- [x]  Run inference with multilingual models #33048
- [x]  Use model-specific APIs #33030
- [x]  Share a custom model #33031
- [x]  Templates for chat models #33026
- [x]  Trainer #33080
- [x]  Run training on Amazon SageMaker #33071
- [x]  Export to ONNX #33072
- [x]  Export to TFLite #33077
- [x]  Export to TorchScript #33079
- [ ]  Benchmarks #33023
- [ ]  Notebooks with examples #33049
- [ ]  Community resources #33027
- [x]  Troubleshoot #33017
- [x]  Interoperability with GGUF files #33037
 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33079). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. All Done!  @stevhliu 

Thank youالسﻻم عليكم
@AhmedAlmaghz 
تمت المراجعة جزاء الله خيرا Just a few minor formatting issues then it should be ready :) Thanks again!","السﻻم عليكم
@AhmedAlmaghz 
تمت المراجعة جزاء الله خيرا Just a few minor formatting issues then it should be ready :) Thanks again!","
## What does this PR do?
Translated the `docs/source/ar/torchscript.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ","```suggestion
هذه هي بداية تجاربنا مع TorchScript ولا زلنا نستكشف قدراته مع نماذج المدخلات المتغيرة الحجم. إنه مجال اهتمامنا وسنعمق تحليلنا في الإصدارات القادمة، مع المزيد من الأمثلة البرمجية، وتنفيذ أكثر مرونة، ومقاييس مقارنة بين  الأكواد القائمة على Python مع أكواد TorchScript المُجمّعة.
```


 ```suggestion
> TorchScript هي طريقة لإنشاء نماذج قابلة للتسلسل والتحسين من تعليمات PyTorch البرمجية.

هناك وحدتان من PyTorch، [JIT and TRACE](https://pytorch.org/docs/stable/jit.html)، تتيحان للمطورين تصدير نماذجهم لإعادة استخدامها في برامج أخرى مثل برامج C++ المُحسّنة للأداء.

نقدم واجهة تتيح لك تصدير نماذج 🤗 Transformers إلى TorchScript بحيث يمكن إعادة استخدامها في بيئة مختلفة عن برامج Python القائمة إلى PyTorch. هنا نشرح كيفية تصدير نماذجنا واستخدامها باستخدام TorchScript.
```
  ```suggestion
يتطلب تصدير نموذج أمرين:

- تهيئة مثيل للنموذج باستخدام علامة `torchscript`
- تمرير مُدخلات وهمية (dummy inputs) خلال النموذج
```

  ```suggestion
## علامة TorchScript والأوزان المرتبطة
``` ```suggestion
علامة `torchscript` ضرورية لأن معظم نماذج اللغة 🤗 Transformers لها أوزان مرتبطة بين طبقة `Embedding` وطبقة `Decoding`. لا يسمح لك TorchScript بتصدير النماذج ذات الأوزان المرتبطة، لذلك من الضروري فصل الأوزان ونسخها مسبقًا.
```

  ```suggestion
النماذج المُهيأة باستخدام علامة `torchscript` لها طبقة `Embedding` وطبقة`Decoding` منفصلتين، مما يعني أنه لا ينبغي تدريبها لاحقًا. سيؤدي التدريب إلى عدم تزامن الطبقتين، مما يؤدي إلى نتائج غير متوقعة.

هذا لا ينطبق على النماذج التي لا تحتوي على رأس نموذج اللغة، حيث لا تملك أوزانًا مرتبطة. يمكن تصدير هذه النماذج بأمان دون علامة `torchscript`.
```
  ```suggestion
تُستخدم المُدخلات الوهمية لتمرير أمامي خلال النموذج. أثناء انتشار قيم المُدخلات عبر الطبقات، يتتبع PyTorch العمليات المختلفة التي يتم تنفيذها على كل مصفوفة(tensor). ثم يتم استخدام هذه العمليات المُسجلة بعد ذلك لإنشاء *أثر* النموذج.
```

 
 ```suggestion
يتم إنشاء التتبع بالنسبة لأبعاد المُدخلات. وبالتالي، فهو مُقيّد بأبعاد المُدخلات الوهمية، ولن يعمل لأي طول تسلسل أو حجم دفعة مختلف. عند المحاولة بحجم مختلف، يتم رفع الخطأ التالي:
```

  ```suggestion
`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`
``` ```suggestion
نوصي بتتبع النموذج باستخدام حجم مُدخلات وهمية لا يقل عن أكبر مُدخل سيتم تقديمه للنموذج أثناء الاستدلال. يمكن أن تساعد الحشوة(padding) في ملء القيم المفقودة. ومع ذلك، نظرًا لتتبع النموذج بحجم مُدخل أكبر، ستكون أبعاد المصفوفة ستكون كبيرة أيضًا، مما يؤدي عنه المزيد من الحسابات.
```
  ```suggestion
انتبه إلى إجمالي عدد العمليات المُنفذة على كل مُدخل وتابع الأداء عن كثب عند تصدير نماذج متغيرة طول التسلسل.
```
  ```suggestion
لتصدير `BertModel` باستخدام TorchScript، قم بتهيئة ـ `BertModel` من فئة `BertConfig` ثم احفظه على القرص تحت اسم الملف `traced_bert.pt`:
```
  ```suggestion
يمكنك الآن تحميل `BertModel` المُحفظ سابقًا، `traced_bert.pt`، من القرص واستخدامه على `dummy_input` المُهيأ سابقًا:
```

  ```suggestion
### استخدام نموذج مُتتبع للاستدلال
```

  ```suggestion
استخدم النموذج المُتتبع للاستدلال باستخدام أسلوب `__call__` الخاص به:
```

  ```suggestion
قدمت AWS عائلة [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) من اﻷجهزة لخفض التكلفة وأداء التعلم الآلي عالي الأداء في البيئة السحابية. تعمل أجهزة Inf1 بواسطة شريحة Inferentia من AWS، وهي مُسرّع أجهزة مُخصص، متخصص في أعباء عمل الاستدلال للتعلم العميق. [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) هي SDK لـ Inferentia التي تدعم تتبع نماذج المحولات وتحسينها للنشر على Inf1. توفر Neuron SDK ما يلي:
```

 
 ```suggestion
``` ```suggestion
1. واجهة برمجة تطبيقات سهلة الاستخدام مع تغيير سطر واحد من التعليمات البرمجية لتتبع نموذج TorchScript وتحسينه للاستدلال في البيئة السحابية.
2. تحسينات الأداء الجاهزة للاستخدام [تحسين التكلفة والأداء](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/>).
3. دعم نماذج Hugging Face المحولات المبنية باستخدام إما [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) أو [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html).
```

  ```suggestion
تعمل نماذج المحولات المستندة إلى بنية [BERT (تمثيلات الترميز ثنائية الاتجاه من المحولات)](https://huggingface.co/docs/transformers/main/model_doc/bert) أو متغيراتها مثل [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) و [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta) بشكل أفضل على Inf1 للمهام غير التوليدية مثل الإجابة على الأسئلة الاستخراجية، وتصنيف التسلسلات، وتصنيف الرموز (tokens). ومع ذلك، يمكن تكييف مهام توليد النصوص للعمل على Inf1 وفقًا لهذا [برنامج تعليمي AWS Neuron MarianMT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html). يمكن العثور على مزيد من المعلومات حول النماذج التي يمكن تحويلها جاهزة على Inferentia في قسم [ملاءمة بنية النموذج](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia) من وثائق Neuron.
```

  ```suggestion
### التبعيات (Dependencies)
``` Duplicate section? ```suggestion
```python
``` ```suggestion
```python
``` ```suggestion
```python
```"
34999,2024-11-28T14:24:29Z,2024-11-28T16:05:09Z,Wauplin,4,0,3,21,4,1,1,[],156.0,0,6043.0,0,0,0,0,2434021.700527,,0,3,0,False,"['Wauplin', 'HuggingFaceDocBuilderDev', 'ArthurZucker']","Note: CI seems to be failing but I don't think it's related to this PR
```py
> python3 .circleci/parse_test_outputs.py --file tests_output.txt --fail
   1 failed because `AssertionError: 0.0002062409 not less than or equal to 0.0002 : outputs.logits` -> Difference between PyTorch and TF is 0.00020624090393539518 (>= 0.0002) for Data2VecVisionForSemanticSegmentation
   1 failed because `AssertionError: 1.9396484e-05 not less than or equal to 1e-05 : outputs.hidden_states_1` -> Difference between PyTorch and TF is 1.9396484276512638e-05 (>= 1e-05) for SwiftFormerModel
Number of failed tests: 2
``` The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34999). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. (can I let you merge it? I don't have permissions myself since failing CI) Of course!Failing tests are indeed unrelated ! LGTM! ",Failing tests are indeed unrelated ! LGTM! ,"Until now, all repos were public by default when created without passing the `private` argument. This meant that passing `private=False` or `private=None` was strictly the same. This is not the case anymore. Enterprise Hub offers organizations to set a default visibility setting for new repos. This is useful for organizations forbidding public repos for security matters. This PR mostly updates some docstrings + the default value for `hub_private_repo` in Trainer's args.

This PR doesn't create any breaking change. The real update has been done server-side when introducing the new Enterprise Hub feature. Related to https://github.com/huggingface/huggingface_hub/pull/2679.",
35000,2024-11-28T14:40:53Z,2024-11-28T15:31:36Z,MekkCyber,1,0,4,6,1,1,1,[],1640.0,0,3046.0,0,0,0,0,2436034.944098,,0,4,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35000). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice, can you add a comment of why you added TORCH_CUDA_ARCH_LIST ? ","Nice, can you add a comment of why you added TORCH_CUDA_ARCH_LIST ? ","# What does this PR do?
Install `autogptq` from source for `pytorch==2.5.1` compatibility. I added the `TORCH_CUDA_ARCH_LIST=""7.5+PTX""` to make it compile for Tesla T4 gpus available for the CI.
## Who can review ?
@SunMarc ",
34973,2024-11-27T14:50:21Z,2024-11-28T15:05:19Z,qubvel,5,1,2,21,2,2,1,[],1727.0,0,87299.0,0,0,0,0,2437615.309112,,0,2,0,False,"['qubvel', 'HuggingFaceDocBuilderDev', 'ydshieh2', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34973). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. This is nice. But could you explain a bit how it works for someone don't know this thing at all 🙏 ? Hey @ydshieh2! I would recommend looking into 
 - https://docs.python.org/3.11/library/typing.html#typing.TypeVar
 - https://guicommits.com/python-generic-type-function-class/
 
it should be helpful! Yes, Type and TypeVar both exist in python3.9

- https://github.com/python/cpython/blob/fb0b642bf1aa3ec276304b7170deedd5040c1698/Lib/typing.py#L1750
- https://github.com/python/cpython/blob/fb0b642bf1aa3ec276304b7170deedd5040c1698/Lib/typing.py#L581 Cool! Feel free to merge then! LGTM, is this in typing from python 3.9 ?","LGTM, is this in typing from python 3.9 ?","# What does this PR do?

Currently, the return type annotation of the `from_pretrained` method is a generic `PreTrainedModel`. This results in a suboptimal experience when accessing model-specific methods, their parameters, and corresponding docstrings. This PR improves it

---
**Here is an example from VSCode IDE on `main`:**

<img width=""400"" alt=""Screenshot 2024-11-27 at 14 32 45"" src=""https://github.com/user-attachments/assets/8f0091af-1f0d-4834-8829-f456a598ae1a"">
<img width=""443"" alt=""Screenshot 2024-11-27 at 14 32 37"" src=""https://github.com/user-attachments/assets/0ce0167e-e167-4a83-8481-b54aafdcb382"">

---

**Here is a version with corrected type annotation:**

<img width=""416"" alt=""Screenshot 2024-11-27 at 14 28 53"" src=""https://github.com/user-attachments/assets/fd136c1d-bb15-4f14-8dcd-d22e5cec57db"">
<img width=""618"" alt=""Screenshot 2024-11-27 at 14 28 40"" src=""https://github.com/user-attachments/assets/33e74a43-a4b0-4ceb-a72d-6374c48eba57"">



<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","This is slightly incorrect because actually, we have `Union[ImageProcessor, Tuple[ImageProcessor, Dict]]`. However, I suppose it's almost never used **externally** with `return_unused_kwargs=True`, in which case it would return a tuple instead of `ImageProcessor`. Thus, it should improve the experience."
34943,2024-11-26T16:11:53Z,2024-11-28T14:34:38Z,ydshieh,1,1,2,2,1,2,1,[],1661.0,0,166967.0,0,0,0,0,2439456.061225,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34943). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM!,LGTM!,"# What does this PR do?

We check a test if failing between a range commits.
If it is a new test that is not in previous commits, the status from those previous commits should be marked as `passing` (as that test doesn't exist in those commits)

Otherwise `git bisect` would fail and we don't receive any report.

This is the case in today CI with the 

> tests/models/olmo2/test_modeling_olmo2.py::Olmo2ModelTest::test_generate_compile_1_end_to_end

(see PR #34864)",I made a mistake here. It should be what is being in the change of this PR
34742,2024-11-15T07:37:37Z,2024-11-25T08:45:35Z,jeongin601,10,5,9,10,1,3,1,[],6062.0,0,1149573.0,0,0,0,0,2438106.161367,,0,9,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'jeongin601', 'ArthurZucker', 'ydshieh']","cc @ArthurZucker @gante @zucchini-nlp  BTW seem to be related to #34274 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34742). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > One suggestion and we can merge!

I updated it! :) Thanks Hi @jeongin601, thank you for this PR ❤️ !

It seems this PR has some regression: below is the 3 tests failing. Would you be up to take a look? You can find the job run log [here](https://github.com/huggingface/transformers/actions/runs/12022766734/job/33515773773).

In any case, thank you in advance.


```python
        ""nemotron"": {
            ""single-gpu"": [
                {
                    ""test"": ""tests/models/nemotron/test_modeling_nemotron.py::NemotronModelTest::test_torchscript_output_attentions"",
                    ""commit"": ""318fe25f22a99ce1226f8d2aadc268b40f7e55af"",
                    ""pr_number"": 34742,
                    ""author"": ""jeongin601"",
                    ""merged_by"": ""zucchini-nlp""
                },
                {
                    ""test"": ""tests/models/nemotron/test_modeling_nemotron.py::NemotronModelTest::test_torchscript_output_hidden_state"",
                    ""commit"": ""318fe25f22a99ce1226f8d2aadc268b40f7e55af"",
                    ""pr_number"": 34742,
                    ""author"": ""jeongin601"",
                    ""merged_by"": ""zucchini-nlp""
                },
                {
                    ""test"": ""tests/models/nemotron/test_modeling_nemotron.py::NemotronModelTest::test_torchscript_simple"",
                    ""commit"": ""318fe25f22a99ce1226f8d2aadc268b40f7e55af"",
                    ""pr_number"": 34742,
                    ""author"": ""jeongin601"",
                    ""merged_by"": ""zucchini-nlp""
                }
            ]
        }
    },
```
 @ArthurZucker @zucchini-nlp A kind remind: don't hesitate to ask for slow CI 🙂 - let's use the tools we have to make our life easier🙏 

 Ah it makes sense, torchscript does not support `DynamicCache` class ! (AFAIR) ah ok. I will check than. But if we eventually move forward to `DynamicCache` and drop legacy cache, it would mean torchscript is not going to work for many models ..? Yeah 👀 unless used with optimum!Thanks for adding this! Let's remove the deprecation warning, otherwise LGTM! One suggestion and we can merge!","Thanks for adding this! Let's remove the deprecation warning, otherwise LGTM! One suggestion and we can merge!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #34739

## Problem

Current implementation does not enable key value caching of nemotron and minitron models.
This problem can be checked by a quick example code that generates key and value caches.

## Modification
I modified the code to enabled key value caching while prefill phase, in reference to` 'modeling_llama.py'` file.

## Key value caching example code
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Minitron model and tokenizer from Hugging Face
model_name = ""your-minitron-model-name""  # Replace with the actual Minitron model name
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Set the model to evaluation mode
model.eval()

# Sample input text
input_text = ""Hello, how are you?""

# Tokenize the input
input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids

# First forward pass (prefill phase)
with torch.no_grad():
    outputs = model(input_ids, use_cache=True)  # Set use_cache=True
    logits = outputs.logits
    past_key_values = outputs.past_key_values

# Check the output
print(""Logits shape:"", logits.shape)
print(""Number of layers in past_key_values:"", len(past_key_values))
print(""Shape of keys and values in the first layer:"")
print(""Key shape:"", past_key_values[0][0].shape)
print(""Value shape:"", past_key_values[0][1].shape)

# Add new input to test cache utilization
new_input_text = "" What about you?""
new_input_ids = tokenizer(new_input_text, return_tensors=""pt"").input_ids

# Pass the new input along with the previous key-value cache
with torch.no_grad():
    outputs_with_cache = model(new_input_ids, past_key_values=past_key_values, use_cache=True)

# Check results after caching
new_logits = outputs_with_cache.logits
new_past_key_values = outputs_with_cache.past_key_values

print(""New logits shape:"", new_logits.shape)
print(""Number of layers in new past_key_values:"", len(new_past_key_values))
```
### As-Is Result 
![스크린샷 2024-11-15 오후 1 15 33](https://github.com/user-attachments/assets/259002b2-f661-4d03-a865-4dbe21d21c8d)

Key value caching is not done.

### To-be Result
![스크린샷 2024-11-15 오후 4 15 44](https://github.com/user-attachments/assets/edb42ad8-2bc4-408c-8b0e-eda506b43728)

Key value caching is enabled

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.




@ArthurZucker
Can you please check my modification? :)

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","I don't think we need to add deprecation message for newly added models, we can support only new Cache objects  Thanks for reviewing my code! I removed the deprecation warning. :) sorry if I wasn't clear, I mean totally removing support for the `tuple` format and this the `from_legacy_cache`

I'll ping the core maintainer after that for the final review :) Oh, sorry I got it wrong. Now, I removed support for tuple shaped past_key_values. Is this what you meant?  ```suggestion
        if use_cache and past_key_values is None:
            past_key_values = DynamicCache()
```"
34921,2024-11-25T15:19:24Z,2024-11-28T14:05:56Z,zucchini-nlp,1,2,5,110,6,2,1,[],3214.0,0,254793.0,0,0,0,0,2441180.482461,,0,5,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34921). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Cool thanks!,Cool thanks!,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34718 and adds tests. Also had to fix some inconsistencies in the code so that the test passes for all models, for ex using `max_batch_size` instead of `batch_size` in all cache classes","cool, let's make sure all our codes uses max_batch_size as logger warning is incompatible with compile yep, I check the generation files and the repo, seems like all changes are done already. Plus the compile tests on llama still pass"
34520,2024-10-30T19:43:00Z,2024-11-28T14:05:16Z,horheynm,8,13,20,48,4,4,2,[],605792.0,0,2485337.0,0,0,0,0,2441221.742783,,0,20,0,False,"['SunMarc', 'dsikka', 'HuggingFaceDocBuilderDev', 'horheynm']","@SunMarc 
Yes we are quantizing the model using oneshot from compressed-tensors, loading that model using `AutoModelForCausalLM` and `HFQuantizer`. Once loaded we will be running the training - qat finetuning. The 'quantization' we run is fakequant.

We are not using LoRA adapters > Yes we are quantizing the model using oneshot from compressed-tensors, loading that model using AutoModelForCausalLM and HFQuantizer. Once loaded we will be running the training - qat finetuning. The 'quantization' we run is fakequant.
> 
> We are not using LoRA adapters

Nice thanks for confirming ! It would be nice to add the `is_qat_trainable` in the base class (`HfQuantizer`) and set it to False by default. Feel free to ping me when the PR is ready !  > > Yes we are quantizing the model using oneshot from compressed-tensors, loading that model using AutoModelForCausalLM and HFQuantizer. Once loaded we will be running the training - qat finetuning. The 'quantization' we run is fakequant.
> > We are not using LoRA adapters
> 
> Nice thanks for confirming ! It would be nice to add the `is_qat_trainable` in the base class (`HfQuantizer`) and set it to False by default. Feel free to ping me when the PR is ready !

Hey Mark, 

Its ready for review. 
There is a test failure but i think its from the api timeout.
it shows

```
1 failed because `requests.exceptions.ReadTimeout: (ReadTimeoutError(""HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)""), '(Request ID` -> f55e37eb-9159-4b2b-bb6e-e87085e7590b)')
```

I tried to rerun, but the option is not clickable from my end. Anything I can do from my end to rerun?

Thank you! > I tried to rerun, but the option is not clickable from my end. Anything I can do from my end to rerun?

No, don't worry. I will rerun it  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34520). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @SunMarc 
Hi Marc, 
This is ready to merge > Thanks! Given this:
> 
> > Using llm-compressor, we have a pathway to run oneshot (to quantize) and then finetune (QAT).
> 
> Just want to make sure we are not breaking (i.e. if we need a version check on this? )

Hi @ArthurZucker where are you recommending adding a version check? > Thanks! Given this:
> 
> > Using llm-compressor, we have a pathway to run oneshot (to quantize) and then finetune (QAT).
> 
> Just want to make sure we are not breaking (i.e. if we need a version check on this? )

This won't be breaking. We had a oneshot then finetune pathway in the past. We are now using HFQuantizer to load model instead of our own custom class (based on AutoModelForCausalLM) we used to have. So previous versions of llm-compressor (library to run oneshot and finetune) and compressed-tensors (library to compress/decompress tensors) will support this pipeline.

If for comfortability, we want to add a version check, I would be happy to add! 

Let me know!

Thanks for the PR ! Left a few suggestion. Could you explain a bit more how you are performing training with compressed-tensors models if you are not using peft ? Are you maybe doing qat or just adding custom lora layers by yourself ?  Thanks for the PR ! I left a few comments. Also, you didn't make any substantial  changes in trainer, is it expected ?  LGTM ! Just a nit about `is_qat_trainable` for compressed_tensors. I don't think we should set it to True by default. Maybe this is linked to the run_compressed var that you wanted to add in another PR  Thanks! Given this:
> Using llm-compressor, we have a pathway to run oneshot (to quantize) and then finetune (QAT). 

Just want to make sure we are not breaking (i.e. if we need a version check on this? ) Thanks for answering, it's fine sounds like we don't need a check, merging!","Thanks for the PR ! Left a few suggestion. Could you explain a bit more how you are performing training with compressed-tensors models if you are not using peft ? Are you maybe doing qat or just adding custom lora layers by yourself ?  Thanks for the PR ! I left a few comments. Also, you didn't make any substantial  changes in trainer, is it expected ?  LGTM ! Just a nit about `is_qat_trainable` for compressed_tensors. I don't think we should set it to True by default. Maybe this is linked to the run_compressed var that you wanted to add in another PR  Thanks! Given this:
> Using llm-compressor, we have a pathway to run oneshot (to quantize) and then finetune (QAT). 

Just want to make sure we are not breaking (i.e. if we need a version check on this? ) Thanks for answering, it's fine sounds like we don't need a check, merging!","# What does this PR do?
Using HFQuantizer, models that were quantized using `compressed-tensors` can be loaded.

The purpose of this pr is to allow quantized models to be loaded using the trainer pathway. 
Currently, if quantized (HFQuantizer is instantiated based on the quantization_config), then raise for training. 

Using llm-compressor, we have a pathway to run oneshot (to quantize) and then finetune (QAT). 


## Who can review?
@SunMarc @younesbelkada","to remove We can just add it to the `config_dict` like we did for `sparsity_config` above no ? I guess the issue was that config_dict was modified if `quantization_config` was in `config_dict` To check if the model is quantized, let's use `getattr(model, ""hf_quantizer"", None) is not None` instead. Then you can also get the quantization method from model.hf_quantizer. If you are doing qat, we can think about creating another property `is_qat_trainable`.  As said earlier, we should probably create another property `is_qat_trainable` if this is what you are doing. This property was used only for peft training. We might need to rebrand it as `is_peft_trainable`.  Maybe not needed as discussed above remove? We already store the quantization method name in `QuantizationMethod` class. Could you use that instead ?  ```suggestion
            quantization_config[""quant_method""] = QuantizationMethod.COMPRESSED_TENSORS
``` Both conditions are the same and the variable is not used at all in trainer  add description  This shouldn't be true by default no ? It depends how the model was loaded if i'm correct.  This depends on whether run_compressed is true or not. This function should be changed to be dependent on that case Sounds good. I don't mind leaving it to True for now but it would be nice to change that in the other PR where you introduce `run_compressed`. "
34823,2024-11-20T09:14:58Z,2024-11-28T14:04:24Z,xinpengzz,6,7,8,28,3,3,2,[],2096.0,0,708567.0,0,0,0,0,2441273.822327,,0,8,0,False,"['danielkorat', 'xinpengzz', 'ArthurZucker']","cc @mosheber  

The usage:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

prompt = ""Alice and Bob""
checkpoint = ""google/gemma-2-9b""
assistant_checkpoint = ""double7/vicuna-68m""

assistant_tokenizer = AutoTokenizer.from_pretrained(assistant_checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors=""pt"")

model = AutoModelForCausalLM.from_pretrained(checkpoint)
assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)

# assistant_model.generation_config.assistant_lookbehind = 10 # the default value
# assistant_model.generation_config.target_lookbehind = 10 # the default value

outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)
tokenizer.batch_decode(outputs, skip_special_tokens=True)

``` Hi @ArthurZucker @zucchini-nlp 👋  I have tested these changes on 2000 samples in the alpaca-cleaned dataset, and the output is identical to those without assistant decoding. main model is Qwen1.5-0.5B and draft model is vicuna-68m. Hi @ArthurZucker 👋 , Would it be convenient to review the code? Hey yeah sorry reviewing now!Thanks! Nice and simple, thanks for adding the arg descriptions! 🤗 ","Thanks! Nice and simple, thanks for adding the arg descriptions! 🤗 "," # What does this PR do?

co-Authors: @WenjingKangIntel

[Universal Assisted Generation](https://github.com/huggingface/transformers/pull/33383)(UAG) allowed the assisted generation with any assistant model regardless of its tokenizer.
This work is awesome and makes the different models work together possible !

This PR is to refine its code.
The code of the Universal Assisted Generation (UAG) contains some useless attributes and some processes that need improvement.

I have contacted with the @danielkorat, the one of authors of UAG, about it, and he suggested me to create a pull request to discuss it conveniently and detailedly.

Code changes:
- removed the useless attributes: `self.prev_tokens`, `self.prev_target_ids`.
- added a judge statement when the `start_assistant_look_index < 0`.

Todo:
- [x] the size of window (the number of past tokens) is an experience value. Perhaps this value could be set to be adjustable, or dynamic

## Before submitting
- [ ] ~~This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case)~~.
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] ~~Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case~~.
- [ ] ~~Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).~~
- [ ] ~~Did you write any new necessary tests?~~

## Who can review?

@danielkorat 
@gante 
and anyone interested in the program 😄 
","can we add these in the docstring with default values isn't this different from current impl because `input_ids` are not same as the previous `input_ids`? Yes. 
`self.prev_target_ids` contains the previous `input_ids` and candidate tokens (have been converted by target tokenizer), but not all candidate tokens are accepted by the target model. So the number of `self.prev_target_ids` is not equal to the number of `input_ids` in the current step.

Actually, we can directly use the `inpud_ids` to get the correct length of tokens, and there is no need to save config `self.prev_target_ids`.  good idea, I forgot it, and I will add them. hmm, so to make sure, that means the prev impl when we checked `prev_target_ids` was not really correct? And we should check the length of already accepted `input_ids`

If that is the case, okay, LGTM! Yes, it is. cool, requesting core maintainer review and then we can merge"
34568,2024-11-01T18:54:11Z,2024-11-28T13:48:06Z,OFSkean,0,0,1,4,1,2,2,['bug'],,0,2314435.0,0,0,0,0,2442253.498048,,0,1,0,False,[],Thanks for updating it! Thanks for fixing!,Thanks for updating it! Thanks for fixing!,"# What does this PR do?
The DINOv2 model was trained and released with a patch size of 14. This PR changes the default in DINOv2Config from patchsize=16 to patchsize=14 to reflect that.

Fixes #34292


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@qubvel 
",
34509,2024-10-30T13:19:48Z,2024-11-28T12:56:25Z,BenjaminBossan,6,0,2,51,2,2,1,[],1751.0,0,2504290.0,0,0,0,0,2445262.924009,,0,2,0,False,"['BenjaminBossan', 'HuggingFaceDocBuilderDev', 'Rocketknight1']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34509). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker could you please review or suggest a reviewer? Another ping @ArthurZucker for review or reviewer suggestion. Thanks for the review @Rocketknight1.

Regarding your suggestion about inferring `is_trainable` from the model. This would only make a difference for the case where the user explicitly puts the model into training mode, right? However, this would be highly unusual, as with PEFT, you typically want the base weights to be frozen and only train the PEFT weights. Therefore, I don't think this would help in a real world use case. And for the rare occasion that the user wants all parameters to be trainable for whatever reason, they could just call `model.train()` after calling `load_adapter`.

This doesn't mean that your suggestion can't be implemented, it would just add a bit of complexity where it's probably not needed. LMK what you think. Yeah, sorry, this is a good point - I forgot that in PEFT training the base model generally wouldn't be trainable. I think your approach in the PR is better! Technically, this would be a breaking change, but I'd consider this more like a bugfix, so if users are relying on the old behavior, they're relying on buggy behavior. It's not quite as clear cut, but e.g. this PR aligns transformers with how PEFT behaves. Up to you if you consider this a breaking change or not.Doing an initial review until we can find a core maintainer - the code and tests look good, but maybe we should default `is_trainable` to the model's current state - in other words, the arg `is_trainable` could default to `None`, and if the user doesn't manually set a value, then we set it equal to `model.training`. This means we'll load the adapter in trainable mode if `model.train()` is set but not if `model.eval()` is set.

Maybe there's a reason that's a bad idea, though, I'm not sure! LGTM but this seems to be breaking no? If so let's put a 🔴 on the PR title ! ","Doing an initial review until we can find a core maintainer - the code and tests look good, but maybe we should default `is_trainable` to the model's current state - in other words, the arg `is_trainable` could default to `None`, and if the user doesn't manually set a value, then we set it equal to `model.training`. This means we'll load the adapter in trainable mode if `model.train()` is set but not if `model.eval()` is set.

Maybe there's a reason that's a bad idea, though, I'm not sure! LGTM but this seems to be breaking no? If so let's put a 🔴 on the PR title ! ","# What does this PR do?

When calling `model.load_adapter` to load a PEFT adapter, by default the adapter should be set to eval mode. This is now correctly done. Users can still pass `is_trainable=True` to load the adapter in training mode.

Fixes #34469


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?",
34890,2024-11-23T02:10:57Z,2024-11-28T13:46:56Z,kylesayrs,2,1,8,9,1,3,3,"['bug', 'Accelerate']",213203.0,0,473760.0,0,0,0,0,2442323.971762,,0,8,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']",cc @muellerzr @SunMarc for accelerate The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34890). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for bringing the fix over here too! Left a suggestion to make it easier on code readers. Thanks for fixing this !  Very much welcome! Thanks for the super clear PR description! 🤗 ,Thanks for bringing the fix over here too! Left a suggestion to make it easier on code readers. Thanks for fixing this !  Very much welcome! Thanks for the super clear PR description! 🤗 ,"# What does this PR do? #
## Purpose ##
* Fix bug introduced by slight behavior change of the `get_state_dict_from_offload` introduced starting in accelerate 1.1.0
* This bug affects models which are executed on the GPU with partial cpu/disk offloading only

## Background ##
Starting in [accelerate 1.1.0](https://github.com/huggingface/accelerate/pull/3204), `get_state_dict_from_offload` moves state_dict tensors to the CPU by default.

In the case that a module has parameters on the GPU, `get_state_dict_from_offload` implements the following procedure:
* The module starts with a reference to the tensors on the GPU (1)
* When `get_state_dict_from_offload` is called, copies of the tensors are created on the CPU (2) which are returned later
* After the tensors are copied to the CPU, the reference to the GPU tensors (1) is dropped
* After the CPU tensors (2) are moved into the `state_dict`, the CPU tensors (2) are copied back to the GPU (3)

This procedure ensures memory efficiency due to how reference to GPU tensors (1) is dropped before GPU tensors (3) are allocated.

However, in the downstream case of transformers, `PretrainedModel.save_pretrained` keeps a copy of the original GPU tensors (1) for longer than they are needed. This long-lived reference to (1) results in references to both (1) and (3) being alive at the same time, leading to increased memory usage.

## Changes ##
* Remove references to tensors in the `state_dict` as they are loaded  by `get_state_dict_from_offload`
  * This ensures compatibility with accelerate versions which have been released prior to and after https://github.com/huggingface/accelerate/pull/3253

## Testing #
<details><summary>save_offloaded.py</summary>

```python3
import torch
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(""nvidia/Llama-3.1-Nemotron-70B-Instruct-HF"", torch_dtype=""auto"", device_map=""auto"")
print(model.hf_device_map)
""""""
{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layer
s.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.l
ayers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0
, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 0, 'model.lay
ers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0, 'model.layers.24': 0, 'model.layers.25': 0, 'model.layers.26': 0, 
'model.layers.27': 0, 'model.layers.28': 0, 'model.layers.29': 0, 'model.layers.30': 0, 'model.layers.31': 0, 'model.layer
s.32': 0, 'model.layers.33': 0, 'model.layers.34': 0, 'model.layers.35': 0, 'model.layers.36': 0, 'model.layers.37': 0, 'm
odel.layers.38': 0, 'model.layers.39': 0, 'model.layers.40': 0, 'model.layers.41': 0, 'model.layers.42': 'cpu', 'model.lay
ers.43': 'cpu', 'model.layers.44': 'cpu', 'model.layers.45': 'cpu', 'model.layers.46': 'cpu', 'model.layers.47': 'cpu', 'm
odel.layers.48': 'cpu', 'model.layers.49': 'cpu', 'model.layers.50': 'cpu', 'model.layers.51': 'cpu', 'model.layers.52': '
cpu', 'model.layers.53': 'cpu', 'model.layers.54': 'cpu', 'model.layers.55': 'cpu', 'model.layers.56': 'cpu', 'model.layer
s.57': 'cpu', 'model.layers.58': 'cpu', 'model.layers.59': 'cpu', 'model.layers.60': 'cpu', 'model.layers.61': 'cpu', 'mod
el.layers.62': 'cpu', 'model.layers.63': 'cpu', 'model.layers.64': 'cpu', 'model.layers.65': 'cpu', 'model.layers.66': 'cp
u', 'model.layers.67': 'cpu', 'model.layers.68': 'cpu', 'model.layers.69': 'cpu', 'model.layers.70': 'cpu', 'model.layers.71': 'cpu', 'model.layers.72': 'cpu', 'model.layers.73': 'cpu', 'model.layers.74': 'cpu', 'model.layers.75': 'cpu', 'model.layers.76': 'cpu', 'model.layers.77': 'cpu', 'model.layers.78': 'cpu', 'model.layers.79': 'cpu', 'model.norm': 'cpu', 'model.rotary_emb': 'cpu', 'lm_head': 'cpu'}
""""""

torch.cuda.memory._record_memory_history()
model.save_pretrained(""save_dir"")
torch.cuda.memory._dump_snapshot(f""align.pickle"")
torch.cuda.memory._record_memory_history(enabled=None)
```
</details>

### transformers <= 4.46.0 | accelerate < 1.1.0 ###
* Original references are not moved
<img width=""1720"" alt=""Screenshot 2024-11-22 at 22 08 35"" src=""https://github.com/user-attachments/assets/7f21c040-ed8f-495b-a8b4-e7a195f4cc61"">

### transformers <= 4.46.0 | accelerate == 1.1.0, 1.1.1 ###
* Results in bug
* Original references are not garbage collected
* Calling save_pretrained results in OOM (>80GiB)
<img width=""1650"" alt=""without_change"" src=""https://github.com/user-attachments/assets/6b88675d-2577-4be3-b04e-16f3efaa7872"">

### transformers > 4.46.0 (this branch) | accelerate == 1.1.0, 1.1.1 ###
* Original references are garbage collected and deallocated after each shard (note that peak represents tensors which are dereference but which have not been deallocated)
* Calling save_pretrained maintains existing memory usage (<68.9GiB)
<img width=""1719"" alt=""Screenshot 2024-11-23 at 10 26 30"" src=""https://github.com/user-attachments/assets/616ccdff-2336-4fd4-82f9-a6ca77b47b99"">

### transformers > 4.46.0 (this branch) | accelerate > 1.1.1 ###
* Original references are not moved
<img width=""1717"" alt=""Screenshot 2024-11-24 at 23 28 27"" src=""https://github.com/user-attachments/assets/042f1b2b-82b0-4815-824d-56aeb232997e"">

## Who can review?


","```suggestion
                # delete reference, see https://github.com/huggingface/transformers/pull/34890
                del state_dict[tensor]  
```"
34978,2024-11-27T18:37:22Z,2024-11-27T20:49:21Z,sergiopaniego,1,0,1,2,1,1,1,[],1933.0,0,60044.0,0,0,0,0,2451255.907459,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34978). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Good catch, thanks!","Good catch, thanks!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes typo in `description` for `VisitWebpageTool`

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@aymeric-roucher @stevhliu 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34957,2024-11-27T09:26:27Z,2024-11-27T16:19:34Z,yuanx749,2,0,1,2,1,2,1,[],1349.0,0,52558.0,0,0,0,0,2491797.875433,,0,1,0,False,"['yuanx749', 'HuggingFaceDocBuilderDev']","> Why is there a comma after ]

It's ok to have a comma after the last entry in a dict, as line 72 below. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34957). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Why is there a comma after ] Thanks for the fix!",Why is there a comma after ] Thanks for the fix!,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fix a typo found when playing with the example: the bracket `[` is not closed, which causes a SyntaxError.


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@stevhliu
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
21195,2023-01-19T14:29:54Z,2023-01-23T18:45:28Z,amyeroberts,1,0,4,61,8,1,1,[],892.0,0,58627073.0,0,0,0,0,2478275.927978,,1,4,0,False,['HuggingFaceDocBuilderDev'],"_The documentation is not available anymore as the PR was closed or merged._LGTM, thanks!","LGTM, thanks!","# What does this PR do?

Adds properties with deprecations warnings to image processors for backwards compatibility. This resolves issues users had when trying to reference a deprecated property e.g. `image_processor.reduce_labels`. 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?",
34880,2024-11-22T13:13:50Z,2024-11-27T15:47:14Z,maximizemaxwell,3,6,10,171,2,2,1,[],357695.0,0,452729.0,0,0,0,0,2509985.165025,,0,10,0,False,"['maximizemaxwell', 'HuggingFaceDocBuilderDev', '4N3MONE']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34880). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. 몇 가지 제안사항 남겨두었습니다. 번역 고생하셨습니다! Resolved!🤗Cool, thanks! We can merge once a PseudoLab team member has reviewed :)","Cool, thanks! We can merge once a PseudoLab team member has reviewed :)","# What does this PR do?

Translated the encoder-decoder.md file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179
## Before reviewing

  - [x]  Check for missing / redundant translations (번역 누락/중복 검사)
  - [x] Grammar Check (맞춤법 검사)
  - [x] Review or Add new terms to glossary (용어 확인 및 추가)
  - [x]  Check Inline TOC (e.g. [[lowercased-header]])
  - [ ]  Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

@junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang
## Before submitting

   - [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
   - [x]  Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
    Pull Request section?
   - [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
    to it if that's the case.
   - [x]  Did you make sure to update the documentation with your changes? Here are the
    [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
    [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
   - [ ]  Did you write any new necessary tests?

## Who can review? (Final)

@stevhliu May you please review this PR?","```suggestion
모델이 생성된 후에는 BART, T5 또는 기타 인코더-디코더 모델과 유사한 방식으로 미세 조정(fine-tuning)할 수 있습니다.
``` ```suggestion
>>> # forward 함수가 자동으로 적합한 decoder_input_ids를 생성합니다.
``` ```suggestion
>>> # 이 부분은 특정 모델의 구체적인 세부사항을 복사할 때에만 사용합니다.
``` ```suggestion
>>> # 파이토치 체크포인트에서 로드하는 해결 방법
``` ```suggestion
>>> # 자기회귀적으로 요약 생성 (기본적으로 그리디 디코딩 사용)
``` ```suggestion
>>> # 미세 조정된 seq2seq 모델과 대응하는 토크나이저 가져오기
```"
34971,2024-11-27T13:54:46Z,2024-11-27T14:15:35Z,ydshieh,2,0,1,8,4,1,1,[],886.0,0,1721.0,0,0,0,0,2526537.837476,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']","> PaliGemmaForConditionalGenerationModelTest::test_generate_with_static_cache

Might be flaky (1 failed in 100 run in my last check). I will look at this but open another PR for it. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34971). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.🫡 thanks for adressing!",🫡 thanks for adressing!,"# What does this PR do?

Some tests are failing after #34247 as they are calling `get_max_length` which calls `logger`, and torch dynamo hate it with

> torch._dynamo.exc.Unsupported: Logger not supported for non-export cases

Let's just call `get_max_cache_shape` as `get_max_length` is just `logger + get_max_cache_shape`

`PaliGemmaForConditionalGenerationModelTest::test_generate_with_static_cache` and `tests/models/granite/test_modeling_granite.py::GraniteModelTest::test_generate_from_inputs_embeds_with_static_cache ` are passing now but 

> GraniteMoeModelTest::test_generate_from_inputs_embeds_with_static_cache

still has data dependent operation issue and failed. That should be addressed in a separate PR.

",
34911,2024-11-25T08:51:31Z,2024-11-27T15:47:28Z,faaany,1,1,2,2,1,1,1,[],116011.0,0,197758.0,0,0,0,0,2521496.958001,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34911). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for improving!,Thanks for improving!,"## What does this PR do?
As a non-cuda user, it would be good to get explained that `release_memory()` is a device-agnostic implementation.

Documentation: @stevhliu","Would also be nice to add a link to `release_memory` for more curious users

```suggestion
From the Accelerate library, you can also use a device-agnostic utility method called [release_memory](https://github.com/huggingface/accelerate/blob/29be4788629b772a3b722076e433b5b3b5c85da3/src/accelerate/utils/memory.py#L63), which takes various hardware backends like XPU, MLU, NPU, MPS, and more into account.
```"
33957,2024-10-04T17:46:25Z,2024-11-26T14:18:05Z,Rocketknight1,19,0,33,162,4,2,1,[],1497.0,0,4651098.0,0,0,0,0,2528863.287368,,1,33,0,False,"['CISC', 'LysandreJik', 'HuggingFaceDocBuilderDev', 'Wauplin', 'julien-c', 'Rocketknight1', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33957). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Interesting! I think we would need to be extra careful as this can influence quite a lot of other libs, to which we need to open PRs before merging this one! 

FYI @LysandreJik and @Narsil  Also cc @zucchini-nlp  Oh, wow, this will affect a lot of other projects. Also keep in mind that the HF API exposes the chat templates through the `config.tokenizer_config` entry in `ModelInfo`.

I will release a chat template editor (Gradio space) soon that lets you easily view, modify, test and submit PRs (just to address a couple of OP bullet-points), so thanks to @Rocketknight1 for pointing me towards this PR, I will need to keep a close eye on changes. :) @CISC yes, I'm aware that this could be disruptive! When this PR is ready we'll discuss it internally and with the community, and maybe we might end up aborting it at that point (and I'll try not to let that delay the inverse templates PR too much either way) This is ready for review! There's one major question: Right now I'm saving `processor_chat_template.jinja` and `chat_template.jinja` as separate files to ensure no conflicts, even though most repos should only have one.

We could consider always using `chat_template.jinja` for both, however in that case we would need to override the loading behaviour on the `tokenizer`, if it's the child of a `processor`, because if the `processor` saves a chat template file then the `tokenizer` will also load that and use it to set its own chat template attribute as well, which might have strange side effects.

Other than that, this should be ready for review! (cc some possibly affected people @zucchini-nlp @Narsil @xenova @LysandreJik)

Failing tests in UDOP seem to be unrelated and I can't reproduce them locally @zucchini-nlp - agreed, but hopefully we can deprecate the others! I hope when we're finally finished, we just have `chat_template.jinja` and `processor_chat_template.jinja`. I was considering merging both into `chat_template.jinja` but that creates lots of weird side effects, and I decided against it. Update: This should be ready for final review now! The failing UDOP tests are not related to this PR, and I have another PR open to fix them here #34180

To be clear, **we will not move the chat templates when this PR is merged. This PR just adds support for saving/loading to the new location, but we will not save to the new file locations by default. This will give the many other frameworks that load chat templates time to adapt, and give users time to update to versions that support the new locations.**

cc affected people: @Narsil @xenova @Wauplin  Thanks for the ping @Rocketknight1! I'll check what it implies server-side to make the parsing of the chat template available in the API. High level feedback: to avoid any confusion in the future, shouldn't the long-term files be called `tokenizer_chat_template.jinja` and `processor_chat_template.jinja`?
Otherwise some model repos will end up with `chat_template.json`/`processor_chat_template.jinja` for processors and `tokenizer_config.json`/`chat_template.jinja` for tokenizers (for backward compatibility). Meaning that `chat_completion.jinja` and `chat_completion.json` won't have the same meaning.

Having a longer and more explicit final name would avoid confusion in my opinion.  I think that makes sense, yes! It'll be a bit longer but very explicit about which files affect which classes. I'll make that change if other reviewers agree as well. > I was considering merging both into chat_template.jinja but that creates lots of weird side effects, and I decided against it.

Hi @Rocketknight1 @zucchini-nlp Getting back to this topic as I'd like to understand more the reasoning behind this file structure. I have a few questions:
1. What are the side effects in play if we set a single jinja file for both tokenizers and processors?
2. Apart from the text-only vs text+image(+<any modality?>) difference, is there another fundamental difference between a tokenizer and a processor? Can it happen that a VLM has 1 template for text-only and 1 template for text+image? 
3. (maybe a stupid question) if we have extra information to store about a jinja template, could it make sense to define a header structure that could be added to jinja templates (as jinja comments) with high level metadata? typically to indicate if the template is suitable for tokenizers and/or processors (an alternative could be to use a ~yaml~ toml config file instead of pure jinja)
4. what are the main libraries/tools that have to be updated when we update this standard? At least transformers/transformers.js/TGI for sure. Any other one to think about? 

Without knowing all the technical details, I'd advocate for a truly single file chat template if possible. If that's the case, we would support things differently server-side, typically having a single `model_info.config.chat_template` field instead of `model_info.config.tokenizer_config.chat_template` and `model_info.config.processor_config.chat_template`. Discussed it further with @julien-c who advocated to support only the (expected) final version on the Hub instead of the intermediate `chat_template.json`/... alternatives, to nudge model authors to use the ""standard"" asap (meaning reverting the PR I merged yesterday on moon-landing).

And sorry if all of this has already been discussed elsewhere!  > Discussed it further with @julien-c who advocated to support only the (expected) final version on the Hub instead of the intermediate chat_template.json/... alternatives, to nudge model authors to use the ""standard"" asap

100% Hi @wauplin, 1. is a good question! The main reason I wanted separate templates is because I thought the side effects would be unpredictable. For example, many processors have `processor.chat_template` but `tokenizer.chat_template` is `None`. However, if we put all chat templates in `chat_template.json`, then when the `Tokenizer` is loaded for that model, it will also set `processor.tokenizer.chat_template`. This is **probably** safe, but it might break some stuff that I can't predict! I've published the initial version of [Chat Template Editor](https://huggingface.co/spaces/CISCai/chat-template-editor), ~~however the full functionality is currently blocked by [the gr.LoginButton not working](https://github.com/gradio-app/gradio/issues/9820)~~.

I've started work related to this and the inverse template PR, but it's currently disabled/hidden until finalized.

~~I have a small question I hope someone can provide an answer to however; this PR mentions processor chat templates, and I've tried locating repos containing them to see how they work and possibly add support for them, but I can't seem to find any?~~

Found [community Pixtral](https://huggingface.co/mistral-community/pixtral-12b) which was very helpful, added support for `chat_template.json` and multimodal messages. I got waylaid by a couple of urgent tasks, but this should finally be ready! The summary is:

- The only raw chat template file is `chat_template.jinja`, for both processors and tokenizers. This means that processors and tokenizers saved in the same repo cannot have different chat templates.
- For now, we continue saving in the old format by default, but we support loading the new format. We keep it like that for a couple of versions so that when we start saving the new format it doesn't break everyone's experience.
- When a tokenizer has multiple templates, it cannot be saved in the new format. This probably won't be changed - we want to gently deprecate multiple templates. PR description updated, and merging! Thanks @Rocketknight1 ! Is there a first repo example on the Hub to check that? Asking since we'll have to support this new (and definitive!) file in the server-side parsing  cc @Wauplin not yet, but I'll make one in `hf-internal-testing` today!Seems good to me, but for processors now we'll have three ways of saving and loading templates. Some people (including repos maintained by us) still have it the old way, as part of `processor_config.json`. Introducing one more template file might be confusing for users, so we have to maybe add this somewhere in the docs. I mean when we finally decide to roll out saving/loading from `chat_template.jinja` files by default Let's go with a single file systeme and define a good standard for it. If the chat template stays as it is today, it does not have to be ""in"" the tokenizer: it's a new form of pre-processing that comes before the tokenzier and as such we don't need different file. 

let's synch about what's the best format to use, the simpler the better LGTM but the PR description needs to be updated! ","Seems good to me, but for processors now we'll have three ways of saving and loading templates. Some people (including repos maintained by us) still have it the old way, as part of `processor_config.json`. Introducing one more template file might be confusing for users, so we have to maybe add this somewhere in the docs. I mean when we finally decide to roll out saving/loading from `chat_template.jinja` files by default Let's go with a single file systeme and define a good standard for it. If the chat template stays as it is today, it does not have to be ""in"" the tokenizer: it's a new form of pre-processing that comes before the tokenzier and as such we don't need different file. 

let's synch about what's the best format to use, the simpler the better LGTM but the PR description needs to be updated! ","We have several issues with chat templates because they're stored as single lines in the JSON config files:

- Impossible to review diffs
- Very hard to edit in the web UI (or in general)
- Differences between `processor` templates in `chat_template.json` and `tokenizer` templates in `tokenizer_config.json` causing confusion
- Some models use multiple templates, requiring a template dict, but we're trying to discourage that in future and move those models to single templates with conditional behaviour instead

The solution:

- Just move chat templates to a single `chat_template.jinja` file in the repo
- If multiple templates are required, then they should still be stored in the JSON file. This is not supported for `Processor` classes, so processors should always be able to save their template as a raw Jinja file. In general, we'll be gently deprecating multiple templates in future.
- If a `chat_template.jinja` file is present, it overrides the JSON files. If a tokenizer is loaded with both Jinja and JSON chat templates and resaved, it should save only the Jinja file, and not have any `chat_template` entry in `tokenizer_config.json`.

For now, we continue saving in the old format by default. I'll probably keep it this way for several versions before making the new format the default, to ensure that most users are able to load the new format before it becomes common. Until then, the new format should mostly be used for testing, to make sure it's ready for deployment when we do the switch.

Extremely draft PR for now, since it'll probably break lots of things!

TODO:
- [X] Add loading/saving single files in tokenizers
- [x] Add loading/saving single files in processors
- [x] Add tokenizer tests for saving + loading + push_to_hub working correctly
- [x] Add processor tests for saving + loading + push_to_hub working correctly
- [x] Ensure processors + tokenizers don't clash when both saving chat templates",
34969,2024-11-27T11:10:09Z,2024-11-27T13:10:47Z,MekkCyber,2,0,3,3,1,1,1,[],1586.0,0,7240.0,0,0,0,0,2530897.889554,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'MekkCyber']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34969). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. GPTQ tests use peft, and in the peft version installed in the docker, the `is_eetq_available` is not updated to handle the `shard_checkpoint` error yet that's why we need to install from sourceWhat happens if we install the latest peft in comparison ? I'm not sure to understand what kind of error you are getting  LGTM ! ",What happens if we install the latest peft in comparison ? I'm not sure to understand what kind of error you are getting  LGTM ! ,"# What does this PR do?
Install PEFT from source for the CI docker to avoid importing eetq as long as `shard_checkpoint` error is not solved : https://github.com/NetEase-FuXi/EETQ/issues/34

Here is the docker image with `peft` installed from source : https://github.com/huggingface/transformers/actions/runs/12047769639/job/33590976973

and Here is the github action before installing peft from source where we have an error related to `shard_checkpoint` : https://github.com/huggingface/transformers/actions/runs/12041342712/job/33573331770
## Who can review ?
@SunMarc ",
34942,2024-11-26T16:07:29Z,2024-11-27T10:50:48Z,ArthurZucker,1,2,5,22,3,2,0,[],2423.0,0,67401.0,0,0,0,0,2539298.003453,,0,5,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34942). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?
There were small issues, mostly because we were missing transpose, thanks @vasqu. 
This was undetected because you don't get the version you requested, again caught by @vasqu, and wiill be fixed by #34896","```suggestion
    return attn_output, attn_weights
```
Otherwise we only take the first batch element if i see it correctly 👀  Indeed, that's a typo!
"
34904,2024-11-25T03:43:58Z,2024-11-26T17:37:18Z,imba-tjd,2,0,1,14,7,1,1,[],40678.0,0,136413.0,0,0,0,0,2601297.356309,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']","cc @stevhliu  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34904). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Cool, thanks for updating!","Cool, thanks for updating!","- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).

see https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables#deprecated-environment-variables
",
34914,2024-11-25T10:28:57Z,2024-11-26T17:23:45Z,faaany,1,0,1,11,3,1,1,[],110484.0,0,111288.0,0,0,0,0,2602123.936264,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34914). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Good catch, thanks!","Good catch, thanks!","## What does this PR do?
This PR does 2 things:
- Use full path for `run_qa.py`
- Remove `to_bettertransformers()`

With `to_bettertransformers()`, I will get the following error:
```bash
The class `optimum.bettertransformers.transformation.BetterTransformer` is deprecated and will be removed in a future release.
Traceback (most recent call last):
  File ""/home/sdp/fanli/tmp/transformers/test_docs.py"", line 14, in <module>
    model = model.to_bettertransformer()
  File ""/home/sdp/fanli/tmp/transformers/src/transformers/modeling_utils.py"", line 4944, in to_bettertransformer
    return BetterTransformer.transform(self)
  File ""/home/sdp/miniforge3/envs/ipex-2.5-new/lib/python3.10/contextlib.py"", line 79, in inner
    return func(*args, **kwds)
  File ""/home/sdp/miniforge3/envs/ipex-2.5-new/lib/python3.10/site-packages/optimum/bettertransformer/transformation.py"", line 229, in transform
    raise ValueError(
ValueError: This model already uses BetterTransformer optimizations from Transformers (torch.nn.functional.scaled_dot_product_attention). As such, there is no need to use `model.to_bettertransformers()` or `BetterTransformer.transform(model)` from the Optimum library. Details: https://huggingface.co/docs/transformers/perf_infer_gpu_one#pytorch-scaled-dot-product-attention.
```
If we should keep `to_bettertransformers()`, just let me know. Thanks! @stevhliu

",
34913,2024-11-25T09:43:08Z,2024-11-26T17:23:34Z,faaany,1,0,1,3,1,1,1,[],113226.0,0,114026.0,0,0,0,0,2602135.933593,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34913). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice, thanks for updating everywhere 🤗 ","Nice, thanks for updating everywhere 🤗 ","## What does this PR do?
As the title suggests, we could automatically detect the underlying hardware accelerator instead of hard-coding. 

Documentation: @stevhliu

",
34870,2024-11-22T01:44:29Z,2024-11-26T17:22:34Z,vansin,2,0,5,2,1,1,1,[],37381.0,0,401886.0,0,0,0,0,2602195.350503,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']",cc @stevhliu! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34870). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the update!,Thanks for the update!,"Update the Python version in the Chinese README to match the English README. 
",
34629,2024-11-06T16:45:39Z,2024-11-26T14:05:42Z,zRzRzRzRzRzRzR,6,22,28,108,4,3,1,[],1188416.0,0,1718717.0,0,0,0,0,2613695.721659,,0,28,0,False,"['zRzRzRzRzRzRzR', 'Cyrilvallez', 'ArthurZucker']","Now this implementation is compatible with both GLM-Edge and GLM-4 models, @Cyrilvallez , I would like to know how to modify modular_glm.py to achieve automatic updates, because some parts of the implementation in modeling_glm.py need to add new parameters to work properly. I referred to your plan and modified it to look like this. I believe the mathematical logic of this implementation is equivalent. I would like to know if there are any improvements needed for this version, and also, I would like to know if @Cyrilvallez could guide me on how to modify modular_glm.py to make good changes.
```
query_states, key_states = apply_rotary_pos_emb(
            query_states, key_states, cos, sin, partial_rotary_factor=self.partial_rotary_factor
        )
```

Let it automatically generate to modeling_glm.py. cc @Cyrilvallez but I think it would help to remove unrelated changes! 🤗  This modification should meet the requirements, and I have tried to remove all unnecessary code. The remaining code is all the code that will be used. All good, merging!Hey! Sorry for the delay, as I've said we were all in Martinique for our offsite the past week! 
You can check my comments, but unless I'm very much mistaken or missing something, most of the changes you propose are no-ops, and you only need to change `q, q_pass`/`k, k_pass` in `apply_rptary_pos_emb`
BTW, you tagged the wrong Cyril in the PR 🤣 It's getting better! Still some issue in the rotary though I think
Then, once we agree on the changes, you'll need to apply the changes in `modular` instead of `modeling` 🤗 Nice! That's it! 🤗
Final comment is that we don't even need to change the signature of `apply_rotary_pos_embed` as we can retrieve the `rotary_dim` from `cos` and `sin`, I forgot before sorry! That way, we don't even have to modify the attention implementation, which is a big win for the modular!

Also, please remove the unrelated notebook change you added (I assume as a mistake) 🤗 Sorry, forgot a comment, you need to ensure the dim in an integer as well.

To automatically generate the modeling file from the modular, you can run 
```sh
python utils/modular_model_converter.py --files_to_parse src/transformers/models/glm/modular_glm.py
```
from the root of the `transformers` repo 🤗 All good, thanks for iterating! Thanks, small convention for q_rot, q_pass, and a nit! We can merge afterwards!","Hey! Sorry for the delay, as I've said we were all in Martinique for our offsite the past week! 
You can check my comments, but unless I'm very much mistaken or missing something, most of the changes you propose are no-ops, and you only need to change `q, q_pass`/`k, k_pass` in `apply_rptary_pos_emb`
BTW, you tagged the wrong Cyril in the PR 🤣 It's getting better! Still some issue in the rotary though I think
Then, once we agree on the changes, you'll need to apply the changes in `modular` instead of `modeling` 🤗 Nice! That's it! 🤗
Final comment is that we don't even need to change the signature of `apply_rotary_pos_embed` as we can retrieve the `rotary_dim` from `cos` and `sin`, I forgot before sorry! That way, we don't even have to modify the attention implementation, which is a big win for the modular!

Also, please remove the unrelated notebook change you added (I assume as a mistake) 🤗 Sorry, forgot a comment, you need to ensure the dim in an integer as well.

To automatically generate the modeling file from the modular, you can run 
```sh
python utils/modular_model_converter.py --files_to_parse src/transformers/models/glm/modular_glm.py
```
from the root of the `transformers` repo 🤗 All good, thanks for iterating! Thanks, small convention for q_rot, q_pass, and a nit! We can merge afterwards!","# What does this PR do?

This PR is to allow the new version of the GLM-4 model to use different rotary_pos_emb.
I am still researching how to modify modular_glm.py so that model_glm.py can automatically generate an additional parameter called apply_rotary_pos_emb.

## Who can review?

This PR may [Cyrilvallez](https://github.com/Cyrilvallez) to help.","I don't understand why you modified the `RotaryEmbedding` class here.  I don't get it here either, this is exactly the same as before without modifying the `RotaryEmbedding` class, but will only work with `rotary_percent=0.5` or `rotary_percent=1`, and is much more confusing IMO indeed this is for me the only part that should need to be modified. The rest should not need any modification  Let's add it to the config class directly, with default value 0.5 And let's use the name `partial_rotary_factor` instead of `rotary_percent`, for library consistency Because there are two different models, 0.5 and 1. In the config, the glm-edge series model needs to be set to 1.
https://huggingface.co/ZP2HF/glm-edge-4b-chat/blob/6a5e92d0092bba5f94abd471720238b6dda8f9de/config.json#L11
Here I have made annotations. Yes, I can make modifications to this part. Nice but you need to add the docstring as well Removing that is a breaking change, why did you remove it? In the Rotary it is `cat`'ed, but unless I'm mistaken your original implementation used `interleave` Here you don't need to modify anything, you are basically slicing up to the full length which is useless Yes, that's indeed the only change that should be applied to the RotaryEmbedding 🤗 ```suggestion
    # Keep half or full tensor for later concatenation
    rotary_dim = cos.shape[-1]
```
We actually don't need to pass the `rotary_factor` as an argument to the function! ```suggestion
def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
```
We actually don't need to pass the rotary_factor as an argument to the function! See next comment, that way we don't even have to modify the modular file for the Attentions! No need to pass the extra arg! See above Same Same No need to pass the extra arg! See above Same, can be removed ```suggestion
    # Keep half or full tensor for later concatenation
    rotary_dim = cos.shape[-1]
```
Same as above ```suggestion
            dim=int(config.head_dim * config.partial_rotary_factor),
```
You need `int` here as well ```suggestion
    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]
    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]
```
we usually use these notations!  ```suggestion
```
I don't think this is used no? "
34865,2024-11-21T21:37:34Z,2024-11-26T10:09:31Z,MekkCyber,1,1,9,10,1,1,1,[],3040.0,0,390719.0,0,0,0,0,2628179.466481,,0,9,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34865). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for skipping the tests for now. Left a comment  LGTM !,Thanks for skipping the tests for now. Left a comment  LGTM !,"# What does this PR do?

In the inference part of AQLM they use the type `torch.Any` which was deprecated in torch 2.5. This PR simply skips inference tests with AQLM models till the fix https://github.com/Vahe1994/AQLM/pull/139 is merged to fix the CI.

## Who can review ?
@SunMarc ","don't need to create a seperate function for this as this is something that we will remove. You can just use the decorator like this `@skip(""reason_for_skipping"")`"
34111,2024-10-12T17:57:39Z,2024-11-26T11:23:08Z,eustlb,5,30,43,478,1,3,3,['run-slow'],162529.0,0,3864329.0,0,0,0,0,2623765.16237,,0,43,0,False,"['HuggingFaceDocBuilderDev', 'eustlb', 'MahmoudAshraf97']","> These are the expected results when generating with OpenAI code, right? Are these the results when doing padding up until 30s (like we do) or adding a 30s zero padded audio (as OpenAI does)?

Indeed, these are the expected results generated with OAI code inferred on mel input features extracted through `WhisperFeatureExtractor`. I feel like here more details are required (also updating the PR's description).

In Transformers, we have two inputs possibilities for Whisper:
1. mel spectrogram with 3000 frames →  audio is first padded **to** 30sec with 0.0s and then we extract the mel
2. mel spectrogram with more than 3000 frames →  no need for padding 

--- 
**case 1**
With an audio <=30sec, the difference between our implementation and OAI is that we first pad to 30sec with 0.0s, then extract features and this will be the input to the model's forward, while OAI pads audio with adding 30sec 0.0s, extract features, slice the exact number of frames and then pads the mel spectrogram to 3000 frames with 0.0s.

To understand better, for an audio of 10secs:
**Transformers**: audio + 20sec of 0.0s →  mel spectrogram of shape [80, 3000] where [2000:] frames are close but not exactly 0.0s 
**OAI**: audio + 30sec of 0.0s →  mel spectrogram of shape [80, 4000] → sliced to the duration of the audio (so until frame 1000) and then padded with 0.0s: [2000:] frames are exactly 0.0s.

--- 
**case 2**
No differences (other than numerical difference due to STFT implementation).

---

About the implementation in the [simple whisper fork](https://github.com/eustlb/whisper/tree/transcribe-from-mel-spectrogram):
We just take the mel spectrogram and concat with 3000 frames of 0.0s. This emulates the 30sec of 0.0s added originally. 
For case 1, the duration considered by OAI is 30sec (see [this line](https://github.com/openai/whisper/blob/25639fc17ddc013d56c594bfbf7644f2185fad84/whisper/transcribe.py#L134C9-L134C46)) and therefore the audio segment that will be given to the forward is the exact mel input that was given. 
For case 2, likewise the duration considered is the one of the given mel input. 

With inferring OAI directly on the mel spectrogram (so either of exactly 3000 frames, either on more than 3000 frames), we ensure that each pass of the forward of OAI whisper and our gets the exact same mel spectrogram. This ensures that the expected result we have in the test are indeed results that should be expected given the same input mel with OAI implementation.

**Note:** For tests that required batched inference which is not supported by OAI implementation, I simply run it sequentially to get the outputs 

 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34111). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. As mentioned [here](https://github.com/huggingface/transformers/pull/34135#issuecomment-2422712988), we won't return special tokens anymore with generate for Whisper. Let's adapt the tests a bit for that. One can argue that for efficiency, if we are going to remove the features of the zero padding anyway, then there is no need to calculate them from the beginning, padding with `HOP_LENGTH` is sufficient to get the exact numerical values and we calculate 1 extra value instead of 3000 

The features resulting from the two padding strategies should be identical
```python
from whisper import log_mel_spectrogram
import torch

SAMPLING_RATE = 16000
N_SAMPLES = SAMPLING_RATE * 30
HOP_LENGTH = 160
N_FRAMES = N_SAMPLES // HOP_LENGTH


for i in range(100):
    audio_n_samples = torch.randint(low=1, high=30, size=(1,)).item() * SAMPLING_RATE

    audio = torch.rand(audio_n_samples)

    features_pad_hop_length = log_mel_spectrogram(audio, padding=HOP_LENGTH)

    features_pad_30s_zeros = log_mel_spectrogram(audio, padding=N_SAMPLES)

    assert torch.allclose(
        features_pad_hop_length[:, :-1],
        features_pad_30s_zeros[:, :-N_FRAMES],
    )
``` Hey @MahmoudAshraf97,

Thanks for your comment! It is a valid point. Nevertheless, this PR does not aim to address it: it is only checking and correcting Whisper tests, with a fixed feature extraction strategy meaning that we are feeding our implementation and OpenAI's one with the exact same features. Feel free to open a PR to modify `WhisperFeatureExtractor` :) ! Hey @eustlb, let me know when you need a proper review.

In the meantime, I've left some formatting comments and a question: These are the expected results when generating with OpenAI code, right? Are these the results when doing padding up until 30s (like we do) or adding a 30s zero padded audio (as OpenAI does)? Thanks for this PR @eustlb, this work, although a bit time consuming, should have been done a long time ago! Congratulations on doing it so thoroughly.

Also, thanks for also providing [code](https://github.com/eustlb/reproduce-whisper-expected-outputs) to reproduce every results. It'll greatly help future efforts on Whisper.

Most of the comments I've made are tiny, the PR looks great to me!

I think we might want to merge this at the same time or a bit before your other PR, but in the meantime, cc @ArthurZucker and @LysandreJik for a final review LGTM, thanks for the meticulous work! 
Let's update to main and merge this just a tiny bit before the other PRs. Also, let's run the slow Whisper test on this PR, so that we can verify that your two other PRs are fixing these new tests Thanks a lot for this PR. The `whisper` codebase evolved quite a lot since the first release and it's nice to freshen things up! 

Very nice that you have a full reproducing recipe, this is something that was quite lacking from me, thanks for improving our port! 🤗 ","Hey @eustlb, let me know when you need a proper review.

In the meantime, I've left some formatting comments and a question: These are the expected results when generating with OpenAI code, right? Are these the results when doing padding up until 30s (like we do) or adding a 30s zero padded audio (as OpenAI does)? Thanks for this PR @eustlb, this work, although a bit time consuming, should have been done a long time ago! Congratulations on doing it so thoroughly.

Also, thanks for also providing [code](https://github.com/eustlb/reproduce-whisper-expected-outputs) to reproduce every results. It'll greatly help future efforts on Whisper.

Most of the comments I've made are tiny, the PR looks great to me!

I think we might want to merge this at the same time or a bit before your other PR, but in the meantime, cc @ArthurZucker and @LysandreJik for a final review LGTM, thanks for the meticulous work! 
Let's update to main and merge this just a tiny bit before the other PRs. Also, let's run the slow Whisper test on this PR, so that we can verify that your two other PRs are fixing these new tests Thanks a lot for this PR. The `whisper` codebase evolved quite a lot since the first release and it's nice to freshen things up! 

Very nice that you have a full reproducing recipe, this is something that was quite lacking from me, thanks for improving our port! 🤗 ","# What does this PR do?

This PR fixes multiple errors in Whisper integration tests and expected outputs. 

To compute the correct excepted outputs, it is necessary to work from a [very simple fork of the original OpenAI Whisper implementation](https://github.com/eustlb/whisper/tree/transcribe-from-mel-spectrogram). Indeed, the extraction of the mel spectrogram in `WhisperFeatureExtractor` diverges slightly from OpenAI's one: we pad the audio array to 30sec/ to longest with 0.0s and then extract our spectrogram through batched STFT while OpenAI's one will add 30sec of 0.0s to the audio array (and not pad _to_ 30sec). This way, the are sure that model inputs for our and OpenAI's implementations are exactly the same. 

With this, we can use the following protocol to compute the expected outputs for the tests:
1. extract mel inputs using the test's implementation (so using the WhisperFeatureExtractor)
2. infer OpenAI's model through the above explained fork directly from the mel input

> [!IMPORTANT] 
> Code to reproduce the outputs for each of the verified tests can be found [here](https://github.com/eustlb/reproduce-whisper-expected-outputs). 

# Edit: some more details about why we work from a whisper fork 

In Transformers, we have two inputs possibilities for Whisper:
1. mel spectrogram with 3000 frames →  audio is first padded **to** 30sec with 0.0s and then we extract the mel
2. mel spectrogram with more than 3000 frames →  no need for padding 

--- 
**case 1**
With an audio <=30sec, the difference between our implementation and OAI is that we first pad to 30sec with 0.0s, then extract features and this will be the input to the model's forward, while OAI pads audio with adding 30sec 0.0s, extract features, slice the exact number of frames and then pads the mel spectrogram to 3000 frames with 0.0s.

To understand better, for an audio of 10secs:
**Transformers**: audio + 20sec of 0.0s →  mel spectrogram of shape [80, 3000] where **[2000:] frames are close but not exactly 0.0s** 
**OAI**: audio + 30sec of 0.0s →  mel spectrogram of shape [80, 4000] → sliced to the duration of the audio (so until frame 1000) and then padded with 0.0s: **[2000:] frames are exactly 0.0s.**

--- 
**case 2**
No differences (other than numerical difference due to STFT implementation).

---

About the implementation in the [simple whisper fork](https://github.com/eustlb/whisper/tree/transcribe-from-mel-spectrogram):
We just take the mel spectrogram and concat with 3000 frames of 0.0s. This emulates the 30sec of 0.0s added originally. 
For case 1, the duration considered by OAI is 30sec (see [this line](https://github.com/openai/whisper/blob/25639fc17ddc013d56c594bfbf7644f2185fad84/whisper/transcribe.py#L134C9-L134C46)) and therefore the audio segment that will be given to the forward is the exact mel input that was given. 
For case 2, likewise the duration considered is the one of the given mel input. 

With inferring OAI directly on the mel spectrogram (so either of exactly 3000 frames, either on more than 3000 frames), we ensure that each pass of the forward of OAI whisper and our gets the exact same mel spectrogram. This ensures that the expected result we have in the test are indeed results that should be expected given the same input mel with OAI implementation.

> [!NOTE]  
> For tests that required batched inference which is not supported by OAI implementation, I simply run it sequentially to get the outputs 

# TODO

## Tests to be verified and eventually corrected 
✅ for a correct test
❌ for an incorrect one

- [x] **test_tiny_timestamp_generation** ❌
- [x] **test_large_timestamp_generation** ❌
- [x] **test_default_multilingual_transcription_short_form** ✅
- [x] **test_default_multilingual_transcription_long_form** ❌ →  here we wan't the model to return input token ids and eos  
- [x] **test_whisper_shortform_single_batch_prev_cond** ❌
- [x] **test_whisper_shortform_multi_batch_hard_prev_cond** ❌
- [x] **test_whisper_longform_single_batch** ✅
- [x] **test_whisper_longform_prompt_ids** ✅ →  partially verified, `""all-segments""` has no equivalent in OAI implem
- [x] **test_whisper_longform_multi_batch** ✅
- [x] **test_whisper_longform_single_batch_prev_cond** ✅
- [x] **test_whisper_longform_multi_batch_prev_cond** ✅
- [x] **test_whisper_longform_multi_batch_hard** ❌
- [x] **test_whisper_longform_multi_batch_hard_prev_cond** ❌
- [x] **test_whisper_longform_single_batch_beam** ❌
- [x] **test_tiny_generation** ❌
- [x] **test_tiny_en_generation** ❌
- [x] **test_tiny_en_batched_generation** ❌
- [x] **test_tiny_longform_timestamps_generation** ❌
- [x] **test_large_generation** ❌
- [x] **test_large_batched_generation** ❌
- [x] **test_large_generation_multilingual** ❌
- [x] **test_small_longform_timestamps_generation** ✅

","(nit): Using `# fmt: skip` signals that the code line won't be change when doing `make fixup`

I'm also suggesting that we keep everything into the same line, but that's not an obligation 
```suggestion
        EXPECTED_OUTPUT = torch.tensor([50258, 50259, 50359, 50364, 2221, 13, 2326, 388, 391, 307, 264, 50244, 295, 264, 2808, 5359, 11, 293, 321, 366, 5404, 281, 2928, 702, 14943, 13, 50692, 50692, 6966, 307, 2221, 13, 2326, 388, 391, 311, 9060, 1570, 1880, 813, 702, 1871, 13, 50926, 50926, 634, 5112, 505, 300, 412, 341, 42729, 3196, 295, 264, 1064, 11, 365, 5272, 293, 12904, 9256, 450, 10539, 51208, 51208, 949, 505, 11, 14138, 10117, 490, 3936, 293, 1080, 3542, 5160, 881, 26336, 281, 264, 1575, 13, 51552, 51552, 634, 575, 12525, 22618, 1968, 6144, 35617, 7354, 1292, 6, 589, 307, 534, 10281, 934, 439, 11, 293, 51836, 50364, 393, 4411, 13, 50514, 50257]) # fmt: skip
```

 Or you can use `# fmt: off` and `# fmt: on` on multiple lines:
```suggestion
        # fmt: off
        EXPECTED_OUTPUT = torch.tensor([
            50258, 50259, 50359, 50364, 2221, 13, 2326, 388, 391, 307, 264, 50244, 295,
            264, 2808, 5359, 11, 293, 321, 366, 5404, 281, 2928,
            702, 14943, 13, 50692, 50692, 6966, 307, 2221, 13, 2326,
            388, 391, 311, 9060, 1570, 1880, 813, 702, 1871, 13,
            50926, 50926, 634, 5112, 505, 300, 412, 341, 42729, 3196,
            295, 264, 1064, 11, 365, 5272, 293, 12904, 9256, 450,
            10539, 51208, 51208, 949, 505, 11, 14138, 10117, 490, 3936,
            293, 1080, 3542, 5160, 881, 26336, 281, 264, 1575, 13,
            51552, 51552, 634, 575, 12525, 22618, 1968, 6144, 35617, 7354,
            1292, 6, 589, 307, 534, 10281, 934, 439, 11, 293,
            51836, 50364, 393, 4411, 13, 50514, 50257
        ])
        # fmt: on
``` Not sure to follow what changed let's also do add a verification with `skip_special_tokens=True` We should stay consistent
```suggestion
            '""Folks, if you watched the show and I hope you do, I spent a lot of time right over there. Tiredlessly studying the lineage of the days most important thoroughbred stories and whole-stiner headlines, working with the best trainers, money can buy to rear their comedy offspring with a hand that is stern yet gentle into the triple crown winning equine specimen. That is my nightly monologue, but sometimes, sometimes, folks, I break into an unincorporated veterinary genetics lab and grab whatever test tubes I can find and then under a grow light I got from a discarded chia pet. I mixed the pilfered DNA of a horse and whatever was in a tube labeled Keith Colan extra. Slurrying the concoction with caffeine pills and a microwave red bull, I screamed, sang a prayer to Janice, initiator of human life and God of transformation as a half horse, half man, freak. Seizes to life before me and the hideous collection of loose animal parts and corrupted man tissue that is my segment. Meanwhile!""
``` Some of these are already used by default, let's remove them to improve readability I prefer the mutiple lines version for improved readability. Thanks for `#fmt: off/ on`, I was not aware of that ! The add 0.0000s and 82.6000 correspond respectively to the `decoder_input_ids` and `eos` tokens that I've added in long form generation. Yet as discussed [here](https://github.com/huggingface/transformers/pull/34135#issuecomment-2410846832), this is likely to be removed and therefore this will be reverted to the original value.  Waiting to have [this](https://github.com/huggingface/transformers/pull/34135#issuecomment-2410846832) decided to eventually remove the special tokens. corrected! All the ones with numerical values here are not set by default (not in generate and not also in the model's generation_config.json). Concerning `return_timestamps` and `condition_on_prev`, I find it clearer to have them explicitly mentioned. decided to remove  We might still want to test that generating with `forced_decoder_ids` and `prompt_ids` give the right answer, aligned with the original Whisper.

Do we already have a test like that or should we do it ourselves ? This is a tiny nit, but if we kept everything on a single line, we'd be able to see the difference between the two EXPECTED_OUTPUT right away Same nit comment here What's the status on this comment? Should we remove it? (nit)

While we're at it, let's align this with how we assert in the rest of the transformers tests and get rid of `assert`:
```suggestion
       self.assertEqual(transcription, "" Mirchi mein ki tene vibinda prajatiya hai"")
``` (nit) same here
```suggestion
        self.assertEqual(transcription, "" How much is the difference between the girls?"")
``` Same comment than before: this test and the following one are checking that prompt ids and decoder ids are still in the generated output. Should we instead check that the overall generated tokens are aligned with the ones we'd have with the original implementation ?

 Great! Let's not forget to mention this somewhere in the docs We have tests that verify that the output is aligned when setting `prompt_ids=`: `test_whisper_longform_prompt_ids`, `test_generate_with_prompt_ids`. Yet, we do (and did) not have any when setting `forced_decoder_ids` (the one we had was just checking that the generated sequence had the forced ids in the beginning). Let me add one! It has indeed to be removed, good catch !  Indeed, like said above, we have tests that verify that the output is aligned when setting `prompt_ids=`: `test_whisper_longform_prompt_ids`, `test_generate_with_prompt_ids`. Yet, we do (and did) not have any when setting `forced_decoder_ids` (the one we had was just checking that the generated sequence had the forced ids in the beginning). Let me add one! I am thinking about setting it by default to True in Whisper's generate (and adding it to the doc this way) and remove it from here. WDYT?  applied in commit [2127991](https://github.com/huggingface/transformers/pull/34111/commits/2127991640963ea2efabcff1ca5d76cea1987cbd) applied in commit [2127991](https://github.com/huggingface/transformers/pull/34111/commits/2127991640963ea2efabcff1ca5d76cea1987cbd) applied in commit [2127991](https://github.com/huggingface/transformers/pull/34111/commits/2127991640963ea2efabcff1ca5d76cea1987cbd) test added in commit [e5064d9](https://github.com/huggingface/transformers/pull/34111/commits/e5064d9b74ce5847075d52245d8414dea427dffe) test added in commit [e5064d9](https://github.com/huggingface/transformers/pull/34111/commits/e5064d9b74ce5847075d52245d8414dea427dffe) applied in commit [c805b19](https://github.com/huggingface/transformers/pull/34111/commits/c805b19f2d310d44fd3d36d5505e87af4179ea41)"
33889,2024-10-02T14:07:06Z,2024-11-25T10:36:45Z,VictorAtIfInsurance,15,1,14,21,2,2,2,[],155907.0,0,4737267.0,0,0,0,0,2628661.091415,,0,14,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'ylacombe', 'VictorAtIfInsurance', 'Rocketknight1', 'ArthurZucker']","cc @Rocketknight1 for pipelines and @ylacombe for ASR Hey @VictorAtIfInsurance, thanks for this PR!
Could you please provide some motivation on the PR and code snippets, if necessary? 

From a quick look, I think we'd rather raise an Error when some parameters are unused, rather than just passing them through. Especially because we want to avoid silent errors or mis-usages. For example, let's say I misspelled the much-used `return_timestamps` to `retun_timestamps`. With your PR, there would be no warnings and no errors and the resulting dictionary would not be as expected!
  Hi,

I should've marked this as WIP. The test is not setup for properly testing the intention of the PR,  it should be unused inputs and not any unused parameter as is written in the test now.

One of the reasons for this PR is inconsistent behaviour when passing chunking_length_s as a parameter to the pipeline.

E.g. I wouldv'e expected the outputs from these two different pipe calls to have the same structure of the output, i.e. that the assert would pass, 
```
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
import numpy as np


device = ""cuda:0"" if torch.cuda.is_available() else ""cpu""
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

model_id = ""openai/whisper-large-v3-turbo""

model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
)
model.to(device)

processor = AutoProcessor.from_pretrained(model_id)

pipe = pipeline(
    ""automatic-speech-recognition"",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch_dtype,
    device=device,
)

data = {""raw"": np.zeros(100), ""sampling_rate"": 16_000, ""id"": 1}

chunk_output = pipe(data.copy(), chunk_length_s=30)
non_chunk_output = pipe(data.copy())

print(chunk_output)
print(non_chunk_output)
assert chunk_output.keys() == non_chunk_output.keys()
```

But printing the outputs gives

```
{'text': ' you'}
{'text': ' you', 'id': [1]}
```

I.e. the unused parameter in the input ""survives"" when not using chunking but is removed when using chunking. 

The behaviour from the non chunking pipeline call comes from this line, so would expect something similar for chunking as well

https://github.com/huggingface/transformers/blob/4a173b88b50598e1dfff1759b67156f3ef257202/src/transformers/pipelines/automatic_speech_recognition.py#L472

Where the behaviour of the non chunking pipeline is especially useful if you generate your input data through a generator where you can directly call the pipeline with the generator while also retrieve metadata returned from the generator, without the extra step of first retrieving the next item, and call the pipe separately. See example:

```
def generate_data():
    iter = 0
    while iter < 10:
        yield {""raw"": np.zeros(100), ""sampling_rate"": 16_000, ""id"": iter}
        iter += 1

print(""Non chunking"")
for output in pipe(generate_data()):
    print(output)

print()
print(""Chunking"")
for output in pipe(generate_data(), chunk_length_s=30):
    print(output)
```

which prints

```
Non chunking
{'text': ' you', 'id': [0]}
{'text': ' you', 'id': [1]}
{'text': ' you', 'id': [2]}
{'text': ' you', 'id': [3]}
{'text': ' you', 'id': [4]}
{'text': ' you', 'id': [5]}
{'text': ' you', 'id': [6]}
{'text': ' you', 'id': [7]}
{'text': ' you', 'id': [8]}
{'text': ' you', 'id': [9]}

Chunking
{'text': ' you'}
{'text': ' you'}
{'text': ' you'}
{'text': ' you'}
{'text': ' you'}
{'text': ' you'}
{'text': ' you'}
{'text': ' you'}
{'text': ' you'}
{'text': ' you'}
```

In these instances it wouldv'e been super useful to pass along the unused inputs.


I hope this makes sense and that you agree with my intended behaviour. I will update the tests accordingly in that case, and sorry for posting unfinished code for review. 

 @ylacombe

I've now added some code examples explaining my issue, and updated the test accordingly:)
 I think keeping the output consistent with different input kwargs definitely fits with our philosophy!

@VictorAtIfInsurance we're actually working on a project to standardize the inputs and outputs for our pipeline classes, to match an inference spec that will be common across lots of projects like TGI / transformers.js / etc. See [this PR](https://github.com/huggingface/transformers/pull/33769).

If this PR makes the pipeline always compliant with the Hub output spec, then I'm definitely in favour of it! Can you:

1) Rebase on the latest version of `main` to make sure your PR is doing the spec compliance tests
2) Add a compliance test when `chunk_length_s` is passed as well?

Other than that this PR looks good to me. Hi,

Thanks for the input. 

I rebased and added check against `AutomaticSpeechRecognitionOutput` using the `compare_pipeline_output_to_hub_spec` test utility method. 

As i change the output from the pipeline call the test fails locally, which is to be expected since it doesn't follow the output structure. So this is probably not the way to go forward. Though then the non-chunk version of ASR pipeline has a problem on main, see the second assert in `test_pipeline_output_on_unused_kwargs` in #34087. 

What would be the correct way to handle invalid kwargs using the new standardized inputs for pipelines. Raise an exception such as in `test_fail_on_invalid_input_kwargs` in the same PR? #34087 

Or maybe a warning would be better in that case as it could break backwards compatibility for the users?

What would be the best next step here @ylacombe? I think consistence is important, but maybe my proposed change is not the way to go as it deviates from the standardized approach.



 @ylacombe It's okay for models to return many output fields, but we're trying to get the pipelines to have standardized inputs/outputs, so that they can fit into a standard inference API.

However, in this case, the pipeline obviously has compliance issues already, as mentioned in #34087. I didn't realize this, I'm sorry! I don't think we should delay this PR because of issues in `main`, so for now, I think we can skip the compliance checks I suggested, and I'll work on making the outputs compliant later in a separate PR. @ylacombe I've removed the compliance check in the test, making the test pass. Let me know if I should update anything more here. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33889). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Rebased main as some tests failed due to missing files.  @Rocketknight1  Could we rerun the workflow? The last time it failed as I was missing some files from main, but now it should be rebased. @ylacombe are we ready for merge?:) Sure, though there were no merge conflicts @ylacombe would you mind merging this PR, I do not have write access to do it myself. Merged sorry for the wait!Hey @VictorAtIfInsurance, this actually makes sense, especially since we pass extra parameters when we don't chunk.

I'm pretty sure it doesn't align with the pipeline philosophy though, and we could maybe ask @Rocketknight1's opinion on this! Hey @VictorAtIfInsurance, indeed it is not compatible with `AutomaticSpeechRecognitionOutput`, that you can find [here](https://github.com/huggingface/huggingface_hub/blob/20d84915c35a541462dd416a25062bc2b2ad683d/src/huggingface_hub/inference/_generated/types/automatic_speech_recognition.py#L99-L116).

> I think keeping the output consistent with different input kwargs definitely fits with our philosophy!

@Rocketknight1, I'm not sure to follow, `AutomaticSpeechRecognitionOutput` are really constraining and some models (say `Whisper`) might return many more output fields! How can we reconcile both?

 LGTM, let's rebase on main ? Very nice thanks!","Hey @VictorAtIfInsurance, this actually makes sense, especially since we pass extra parameters when we don't chunk.

I'm pretty sure it doesn't align with the pipeline philosophy though, and we could maybe ask @Rocketknight1's opinion on this! Hey @VictorAtIfInsurance, indeed it is not compatible with `AutomaticSpeechRecognitionOutput`, that you can find [here](https://github.com/huggingface/huggingface_hub/blob/20d84915c35a541462dd416a25062bc2b2ad683d/src/huggingface_hub/inference/_generated/types/automatic_speech_recognition.py#L99-L116).

> I think keeping the output consistent with different input kwargs definitely fits with our philosophy!

@Rocketknight1, I'm not sure to follow, `AutomaticSpeechRecognitionOutput` are really constraining and some models (say `Whisper`) might return many more output fields! How can we reconcile both?

 LGTM, let's rebase on main ? Very nice thanks!","# What does this PR do?

Harmonizing behaviour for what input data is passthrough automatic speech recognition pipelines.

Today you can get different return dictionaries keys when using ASR pipeline with and without the call parameter `chunk_length_s`. With this parameter you will only get a dictionary with the transformed input data, while omitting this parameter allows you to retrieve any unused input data parameter back in the resulting dictionary. This inconsistency can be confusing as sometimes you need to keep track of which inputs corresponds to which outputs, while it is not obvious to a user why.

The following code snippet highlights these differences:

```
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
import numpy as np


device = ""cuda:0"" if torch.cuda.is_available() else ""cpu""
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

model_id = ""openai/whisper-large-v3-turbo""

model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
)
model.to(device)

processor = AutoProcessor.from_pretrained(model_id)

pipe = pipeline(
    ""automatic-speech-recognition"",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch_dtype,
    device=device,
)

data = {""raw"": np.zeros(100), ""sampling_rate"": 16_000, ""id"": 1}

chunk_output = pipe(data.copy(), chunk_length_s=30)
non_chunk_output = pipe(data.copy())

print(chunk_output)
print(non_chunk_output)
assert chunk_output.keys() == non_chunk_output.keys()
```

This will print the following and fail the assert.

```
{'text': ' you'}
{'text': ' you', 'id': [1]}
```


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",let's remove  this please
34916,2024-11-25T10:49:23Z,2024-11-26T07:20:06Z,zucchini-nlp,1,0,1,12,1,1,1,[],1875.0,0,73844.0,0,0,0,0,2638347.490505,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34916). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for pushing this!,Thanks for pushing this!,"# What does this PR do?

As a final step of updating models to work on non-legacy path, this PR adds default values in video llava model. Since the hub repo owner has not been active for several months now, I think all we can do is to hardcode the defaults. Especially when there is only one ckpt with this particular architecture, we should be good to go

Anyone is still free to tweak the params by passing new values in the init

PS. Some slow tests might be failing, related to https://huggingface.slack.com/archives/C01NE71C4F7/p1732216183109549 . All slow tests should be run and fixed after https://github.com/huggingface/transformers/pull/34502 finalizing work on VLM-side",
34799,2024-11-19T07:30:13Z,2024-11-25T15:59:38Z,jiqing-feng,9,4,14,63,2,3,2,[],283.0,0,581763.0,0,0,0,0,2660779.492822,,0,14,0,False,"['jiqing-feng', 'HuggingFaceDocBuilderDev', 'SunMarc', 'ArthurZucker']","BTW, I suppose transformers missed some static cache tests, do you have any instructions about where can I add this kind of test? Thanks! > BTW, I suppose transformers missed some static cache tests, do you have any instructions about where can I add this kind of test? Thanks!

All the tests related to the cache are in test_utils.py file. inside `GenerationTesterMixin`, you will find the test we perform on all models and in `GenerationIntegrationTests`, these are integration tests.  Hi @SunMarc . I left the [comment ](https://github.com/huggingface/transformers/pull/34799#discussion_r1849382212)to explain why llama model doesn't have this issue. BTW, I also added the low-precision static cache tests to avoid this kind of issue in the future, please review it. Thanks! > ➕ on marc's comment. The safe way to do this is to cast key and query to the cache's dtype no? And do this in the `cache_utils` rather than at the modeling level!


Yes, I have applied your suggestions, thanks!



> Thanks for the explanation ! LGTM ! Did you run the static cache tests you added to see if there are other models that requires this fix ?

The CI already contains the tests that I changed, so currently no other models require it. Besides, I have changed it into cache_utils which should be applied for all language models with static cache. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34799). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @SunMarc , please review the new changes, thanks! > Hi @SunMarc , please review the new changes, thanks!

All good from my side. Pinging @ArthurZucker  I think it was testing for float32 initally and @jiqing-feng added coverage for float16 @ArthurZucker  Sounds good then, merging!˜Thanks for the bug fix ! Left a comment  Thanks for the explanation ! LGTM ! Did you run the static cache tests you added to see if there are other models that requires this fix ?  ➕ on marc's comment. The safe way to do this is to cast key and query to the cache's dtype no? And do this in the `cache_utils` rather than at the modeling level!  Not completely sure we want to test for float32 as it's quite heavy ",Thanks for the bug fix ! Left a comment  Thanks for the explanation ! LGTM ! Did you run the static cache tests you added to see if there are other models that requires this fix ?  ➕ on marc's comment. The safe way to do this is to cast key and query to the cache's dtype no? And do this in the `cache_utils` rather than at the modeling level!  Not completely sure we want to test for float32 as it's quite heavy ,"Hi @SunMarc . This PR fixed the data type mismatch when using low-precision static cache. The following code can reproduce the bug:
```python
import torch
from transformers import pipeline

model_id = ""EleutherAI/gpt-j-6b""
model_kwargs = {""torch_dtype"": torch.bfloat16}

pipe = pipeline(""text-generation"", model=model_id, model_kwargs=model_kwargs)

generation_config = pipe.model.generation_config
generation_config.cache_implementation=""static""

print(pipe(""I am happy because"", generation_config=generation_config))
```

Output:
```
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Traceback (most recent call last):
  File ""/home/jiqingfe/test_gptj.py"", line 13, in <module>
    print(pipe(""I am happy because"", generation_config=generation_config))
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/text_generation.py"", line 272, in __call__
    return super().__call__(text_inputs, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/base.py"", line 1301, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/base.py"", line 1308, in run_single
    model_outputs = self.forward(model_inputs, **forward_params)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/base.py"", line 1208, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
  File ""/home/jiqingfe/transformers/src/transformers/pipelines/text_generation.py"", line 370, in _forward
    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
  File ""/opt/conda/envs/idp/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/generation/utils.py"", line 2263, in generate
    result = self._beam_search(
  File ""/home/jiqingfe/transformers/src/transformers/generation/utils.py"", line 3472, in _beam_search
    outputs = self(**model_inputs, return_dict=True)
  File ""/opt/conda/envs/idp/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/conda/envs/idp/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/gptj/modeling_gptj.py"", line 1098, in forward
    transformer_outputs = self.transformer(
  File ""/opt/conda/envs/idp/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/conda/envs/idp/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/gptj/modeling_gptj.py"", line 838, in forward
    outputs = block(
  File ""/opt/conda/envs/idp/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/conda/envs/idp/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/gptj/modeling_gptj.py"", line 453, in forward
    attn_outputs = self.attn(
  File ""/opt/conda/envs/idp/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/conda/envs/idp/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/models/gptj/modeling_gptj.py"", line 246, in forward
    key, value = layer_past.update(key, value, self.layer_idx, cache_kwargs)
  File ""/home/jiqingfe/transformers/src/transformers/cache_utils.py"", line 1220, in update
    k_out.index_copy_(2, cache_position, key_states)
RuntimeError: index_copy_(): self and source expected to have the same dtype, but got (self) BFloat16 and (source) Float
```
","could you explain why this is needed for this particular model and why this doesn't happen for llama for example ? Many models have approximately the same modeling code.  For llama, we can see sin and cos come from position_embeddings (bf16 tensor) which comes from [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L168). You can see llama's rotary embedding converts the data type. But for gptj, the position embeddings come from [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gptj/modeling_gptj.py#L63), it set the data type to float32, so data type miss-match happens when the input data type is bf16 or fp16. You also have to do it in the `NotImplementedError` logic below no ? Maybe do the cast right after we computed `k_out` and `v_out` Right, fixed."
34859,2024-11-21T15:13:50Z,2024-11-25T21:27:56Z,yonigozlan,1,0,2,322,22,1,1,[],1670.0,0,368046.0,0,0,0,0,2673880.093537,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34859). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM, I think we are missing a check in `quality` were we remove the dependencies, but alright for now

Would be nice to update the `__init__` to the new format as well ! ","LGTM, I think we are missing a check in `quality` were we remove the dependencies, but alright for now

Would be nice to update the `__init__` to the new format as well ! ","# What does this PR do?

Small fix to include torchvision dependency in import structure for DETR, RT-DETR, Deformable DETR Fast image processors

Not sure why I didn't see it before, or why it didn't break anything, but better safe than sorry :) 

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

## Who can review?

@ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34633,2024-11-07T01:09:32Z,2024-11-25T18:30:38Z,xuzifei-dmatrix,2,0,3,3,1,1,1,[],1617648.0,0,1618396.0,0,0,0,0,2683788.395274,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker']","This indeed fixes the issue of: 
```python
TraceError: Proxy object cannot be iterated. This can be attempted when the Proxy is used in a loop or as a *args or **kwargs function argument. See the torch.fx docs on pytorch.org for a more detailed explanation of what types of control flow can be traced, and check out the Proxy docstring for help troubleshooting Proxy iteration errors
``` The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34633). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks, do you mind running `make fix-copies`? 
 LGTM thanks! ","Thanks, do you mind running `make fix-copies`? 
 LGTM thanks! ","# What does this PR do?
This PR fixes the issue that gpt2 is no longer fx traceable in the latest version
Below code gives error complaining about control flows not traceable. This PR solves the issue.
```python
from transformers.utils.fx import HFTracer
from transformers import pipeline
import torch
from torch.fx import GraphModule

pipe = pipeline(
    task=""text-generation"",
    model=""openai-community/gpt2"",
    device=""cuda"",
)
x = torch.randint(1, 100, (1, 1024)).to(""cuda"")
past_key_values = pipe.model(x, use_cache=True).past_key_values
dummy_inputs = dict(input_ids=x, labels=x, past_key_values=past_key_values)

tracer = HFTracer()
gm = GraphModule(pipe.model, tracer.trace(pipe.model, dummy_inputs=dummy_inputs))
```
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@ArthurZucker 
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34871,2024-11-22T03:36:18Z,2024-11-25T17:04:52Z,MekkCyber,1,0,1,4,1,2,3,[],1657.0,0,307717.0,0,0,0,0,2689662.43052,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34871). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM ! Even if a kwargs was not used, it is not a reason to raise an error. I'm not sure why we have this kind of behavior from generate, especially when we are just passing the output from the tokenizer. ","LGTM ! Even if a kwargs was not used, it is not a reason to raise an error. I'm not sure why we have this kind of behavior from generate, especially when we are just passing the output from the tokenizer. ","# What does this PR do?
This PR fixes a failling test in `test_ggml.py` related to the function `test_falcon7b_q2_k`. The error that was occuring is 
```
The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)
```
## Who can review ?
@SunMarc ",
34924,2024-11-25T16:14:51Z,2024-11-25T16:38:20Z,MekkCyber,1,0,2,2,1,1,1,[],1655.0,0,1657.0,0,0,0,0,2691010.395579,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34924). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice to see the quantization CI updated with the latest torch. Could you link to the job that shows that the docker image was built successfully ? ,Nice to see the quantization CI updated with the latest torch. Could you link to the job that shows that the docker image was built successfully ? ,"# What does this PR do?
Upgrade the torch version to 2.5.1 in the dockerfile for the quantization CI
Job for docker image with torch 2.5.1 : https://github.com/huggingface/transformers/actions/runs/12013872016
## Who can review ?
@SunMarc ",
34247,2024-10-18T11:37:10Z,2024-11-22T14:33:35Z,ArthurZucker,1,4,12,18,1,2,0,[],1117510.0,0,3301825.0,0,0,0,0,2690703.401032,,1,12,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34247). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.On principle this LGTM, however in practice I am not experiencing any speedup (but aggravated performance) until the number of new tokens is quite high, of the order of ~2500/3000 (quick test with Llama 3.1 8B)
Note sure if it comes only from compilation time and warmup, or some graph breaks somewhere
Did you try to compare performances a bit?","On principle this LGTM, however in practice I am not experiencing any speedup (but aggravated performance) until the number of new tokens is quite high, of the order of ~2500/3000 (quick test with Llama 3.1 8B)
Note sure if it comes only from compilation time and warmup, or some graph breaks somewhere
Did you try to compare performances a bit?","# What does this PR do?
Add automatic compile for `static` cache. 
This can be tested with:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import time
device = ""cuda""
model = AutoModelForCausalLM.from_pretrained(""meta-llama/Meta-Llama-3-8B"", torch_dtype=torch.float16).to(device)
tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3-8B"")

sequence = ""Hey what's the plan""

inputs = tokenizer.encode(sequence, return_tensors='pt').to(device)
model.generation_config.temperature = 1.0
model.generation_config.top_p = 1.0

t0 = time.time()
out = model.generate(inputs, do_sample=False, max_new_tokens=500)
out = tokenizer.batch_decode(out, skip_special_tokens=True)[0]
dt = time.time() - t0
print(f'dt: {dt}', out)

t0 = time.time()
out = model.generate(inputs, do_sample=False, max_new_tokens=500, cache_implementation=""static"")
out = tokenizer.batch_decode(out, skip_special_tokens=True)[0]
dt = time.time() - t0
print(f'dt: {dt}', out)

t0 = time.time()
out = model.generate(inputs, do_sample=False, max_new_tokens=500, cache_implementation=""static"")
out = tokenizer.batch_decode(out, skip_special_tokens=True)[0]
dt = time.time() - t0
print(f'dt: {dt}', out)
```
which give 15sec for dynamic, 30 for the first generate, then 4seconds for the next one
","```suggestion
            if self.device.type == ""cuda"":
```
Here it should be the type, not the device itself @ArthurZucker This PR breaks some tests on PEFT :(

I checked why that is exactly, and I found that it has nothing to do with the compilation. The sole reason is that on 2nd iteration, we use this function, which effectively calls:

`self.forward(**model_inputs, return_dict=True)`

whereas on the first iteration (and before the PR), we would call:

`self(**model_inputs, return_dict=True)`

Is there any specific reason why `forward` is used? Using `__call__` looks correct to me. Should be solved with https://github.com/huggingface/transformers/pull/34923 🤗 Nice, thanks for the quick reply."
34877,2024-11-22T10:43:39Z,2024-11-25T16:20:41Z,ydshieh,1,0,1,2,1,1,1,[],1584.0,0,279423.0,0,0,0,0,2692317.37375,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34877). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?

This test assert `model.out_indices, (-2, -1)` and it was written on 2023/12.

However, we have

> out_indices = list(out_indices) if out_indices is not None else None

in `src/transformers/utils/backbone_utils.py` which is written on 2024/05

It's just time to update the test so we don't see it fails forever.",
34895,2024-11-23T16:08:56Z,2024-11-25T15:47:14Z,MekkCyber,1,0,2,22,1,1,1,[],1767.0,0,171500.0,0,0,0,0,2694324.298654,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34895). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM ! ,LGTM ! ,"# What does this PR do?
Fixes BitNet tests for the CI

## Who can review ?
@SunMarc ",
34922,2024-11-25T15:45:33Z,2024-11-25T15:49:57Z,BenjaminBossan,2,0,1,4,1,1,1,[],267.0,0,1634.0,0,0,0,0,2692793.364839,,0,1,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev']",Merging this as this is a small change related to quantization tests.  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34922). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for updating this ! ,Thanks for updating this ! ,"# What does this PR do?

The old AWQ version is failing with the latest (unreleased) transformers, giving the error:

> ImportError: cannot import name 'shard_checkpoint' from
'transformers.modeling_utils'

This has been resolved in awq v0.2.7:

https://github.com/casper-hansen/AutoAWQ/pull/644

Fixes part of the failing quantization CI:

- https://github.com/huggingface/transformers/actions/runs/12002230961/job/33454061676
- https://github.com/huggingface/transformers/actions/runs/12002230961/job/33454065681


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] ~~Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.~~ Discussed internally on Slack
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?",
34864,2024-11-21T19:49:39Z,2024-11-25T15:31:22Z,2015aroras,0,0,2,442,17,1,1,[],,0,330104.0,0,0,0,0,2695278.299396,,0,2,0,False,[],LGTM! This does look better 😄 ,LGTM! This does look better 😄 ,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

This PR renames the recently-added OLMo November 2024 model to OLMo2.

<!-- Remove if not applicable -->

<!-- Fixes # (issue) -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34917,2024-11-25T12:55:50Z,2024-11-25T15:19:29Z,dependabot[bot],1,0,1,2,1,1,1,"['dependencies', 'python']",1632.0,0,8621.0,0,0,0,0,2695991.203024,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34917). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"Bumps [tornado](https://github.com/tornadoweb/tornado) from 6.4.1 to 6.4.2.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tornadoweb/tornado/blob/v6.4.2/docs/releases.rst"">tornado's changelog</a>.</em></p>
<blockquote>
<h1>Release notes</h1>
<p>.. toctree::
:maxdepth: 2</p>
<p>releases/v6.4.2
releases/v6.4.1
releases/v6.4.0
releases/v6.3.3
releases/v6.3.2
releases/v6.3.1
releases/v6.3.0
releases/v6.2.0
releases/v6.1.0
releases/v6.0.4
releases/v6.0.3
releases/v6.0.2
releases/v6.0.1
releases/v6.0.0
releases/v5.1.1
releases/v5.1.0
releases/v5.0.2
releases/v5.0.1
releases/v5.0.0
releases/v4.5.3
releases/v4.5.2
releases/v4.5.1
releases/v4.5.0
releases/v4.4.3
releases/v4.4.2
releases/v4.4.1
releases/v4.4.0
releases/v4.3.0
releases/v4.2.1
releases/v4.2.0
releases/v4.1.0
releases/v4.0.2
releases/v4.0.1
releases/v4.0.0
releases/v3.2.2
releases/v3.2.1
releases/v3.2.0
releases/v3.1.1
releases/v3.1.0
releases/v3.0.2
releases/v3.0.1
releases/v3.0.0
releases/v2.4.1
releases/v2.4.0</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tornadoweb/tornado/commit/a5ecfab15e52202a46d34638aad93cddca86d87b""><code>a5ecfab</code></a> Bump version to 6.4.2</li>
<li><a href=""https://github.com/tornadoweb/tornado/commit/bc7df6bafdec61155e7bf385081feb205463857d""><code>bc7df6b</code></a> Fix tests with Twisted 24.7.0</li>
<li><a href=""https://github.com/tornadoweb/tornado/commit/d5ba4a1695fbf7c6a3e54313262639b198291533""><code>d5ba4a1</code></a> httputil: Fix quadratic performance of cookie parsing</li>
<li>See full diff in <a href=""https://github.com/tornadoweb/tornado/compare/v6.4.1...v6.4.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tornado&package-manager=pip&previous-version=6.4.1&new-version=6.4.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/huggingface/transformers/network/alerts).

</details>",
34804,2024-11-19T12:35:20Z,2024-11-25T14:29:52Z,tomaarsen,1,0,2,4,1,2,2,[],1656.0,0,527484.0,0,0,0,0,2696758.543219,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34804). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for fixing this, LGTM. Thanks!","Thanks for fixing this, LGTM. Thanks!","# What does this PR do?

`PeftAdapterMixin.active_adapter` has been deprecated since #26407 added multi-adapter support, but it's still being used in `PeftAdapterMixin.get_adapter_state_dict`. This results in a FutureWarning every time I save a checkpoint after training with `model.add_adapter(...)`.

This PR updates the latter to use the correct `PeftAdapterMixin.active_adapters()[0]` method instead, which is identical in behaviour, except minus the warning.

The deprecated `active_adapter`: https://github.com/huggingface/transformers/blob/54739a320e38bc86cd250303a35e68d5d3f14a83/src/transformers/integrations/peft.py#L433-L438

Additionally, the comment 
```
The model will use `self.active_adapter()`
```
in `enable_adapters` seemed to be a bit misleading - that method wasn't being used. For clarity, I removed it.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review?

@ArthurZucker @BenjaminBossan

- Tom Aarsen
",
34819,2024-11-20T01:52:30Z,2024-11-25T14:53:04Z,jla524,1,0,4,42,1,1,1,[],38692.0,0,478835.0,0,0,0,0,2697578.315801,,0,4,0,False,['MekkCyber'],LGTM !  Thanks for the fix 😊Thanks 🤗 ,Thanks 🤗 ,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #34818 (issue)

- replace `Qwen/Qwen2-450m-beta` which no long exists
- update expected tensor values and text completions
- tested on a 3090 system

## Who can review?

@ArthurZucker ",
34569,2024-11-01T23:38:23Z,2024-11-25T13:35:24Z,dszeto,3,0,2,8,1,1,1,[],842291.0,0,2038470.0,0,0,0,0,2701191.55132,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'AlonKellner-Jounce', 'dszeto']","I want to emphasize my support for this PR with a simply reproducible error (and a real use-case) that this PR fixes:
When running vLLM with huggingface models which have a `PreTrainedTokenizerFast` tokenizer, for example:
```shell
python3 -m vllm.entrypoints.openai.api_server --model jounce/dummy-phi3
```
Currently fails like so:
```shell
ERROR 11-11 09:00:29 engine.py:158]   File ""/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/detokenizer.py"", line 122, in decode_sequence_inplace
ERROR 11-11 09:00:29 engine.py:158]     read_offset) = detokenize_incrementally(
ERROR 11-11 09:00:29 engine.py:158]                    ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-11 09:00:29 engine.py:158]   File ""/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/detokenizer.py"", line 301, in detokenize_incrementally
ERROR 11-11 09:00:29 engine.py:158]     prefix_text = tokenizer.convert_tokens_to_string(
ERROR 11-11 09:00:29 engine.py:158]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-11 09:00:29 engine.py:158]   File ""/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_fast.py"", line 641, in convert_tokens_to_string
ERROR 11-11 09:00:29 engine.py:158]     return self.backend_tokenizer.decoder.decode(tokens)
ERROR 11-11 09:00:29 engine.py:158]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-11 09:00:29 engine.py:158] AttributeError: 'NoneType' object has no attribute 'decode'
```

Great PR, tnx :) Hey @ArthurZucker , would be awesome if you could take a quick look at this.

Please let me know if someone else should review. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34569). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.There are quite a few unrelated changes that made their way here, can you revert them so we can merge? 🤗 ","There are quite a few unrelated changes that made their way here, can you revert them so we can merge? 🤗 ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

In `convert_tokens_to_string` of `src/transformers/tokenization_utils_fast.py`, `self.backend_tokenizer.decoder` can be `None` when the tokenizer is trained by `tokenizers.trainers.WordLevelTrainer`. This fix adds a check and falls back to joining tokens with a space when no decoder is found. This follows the default behavior in the Rust implementation of Tokenizers.

Original question posted on https://discuss.huggingface.co/t/pretrainedtokenizerfast-convert-tokens-to-string-always-assumes-the-presence-of-decoder/114978

All existing tokenizer tests are passing. Style changes were made by `make fixup`.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ArthurZucker

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34891,2024-11-23T02:41:34Z,2024-11-25T13:05:59Z,wanxiangchwng,1,0,1,22,11,1,1,[],211903.0,0,211905.0,0,0,0,0,2702366.286874,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34891). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM, thank you!","LGTM, thank you!","# What does this PR do?

fix some typos

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34184,2024-10-15T23:34:27Z,2024-11-18T18:51:50Z,kwen2501,30,30,26,449,18,3,2,[],179.0,0,3510827.0,0,0,0,0,2697871.359072,,0,26,0,False,"['kmehant', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'kwen2501']","Cc: @lessw2020 @HamidShojanazeri
Cc: @jerryzh168  Also we would place 
```python
    device = torch.device(f""cuda:{rank}"")
    dist.init_process_group(""nccl"", device_id=device)
    device_mesh = dist.init_device_mesh(""cuda"", (world_size,))

    with device:
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
        )
```
potentially inside `from_pretrained`, this way we automatically set things for users, with them only having to change the way they call their script (with torchrun or not)  WDYT? We will also add this to the `configuration_llama.py`:
```python
DEFAULT_TP_PLAN = {
    ""model.layer.*.q_proj"":""column_parallel""
}
```
this way we can init lama config with it! 
 cc @SunMarc as per our discussion! Thanks @ArthurZucker @SunMarc for your review.

Good idea to move `tp_plan` to `configuration_llama.py`. I made that change in the current version. Indeed `modeling_llama.py` becomes cleaner now, and it will not have dependency on `torch.distributed...`. 

Since the top-level model can have different `base_model_prefix` (`""model""`, `""transformer""`, etc), I could not directly put FQNs like `""model.layers.*""` in `configuration_llama.py`. So I prepare a `_base_model_tp_plan` starting with `""layers.*""` facing the base model `LlamaModel`. This works because `model.tensor_parallel()` searches for and applies `_tp_plan` recursively. That way we can serve all top models regardless of what prefix name they give to `LlamaModel`, and we can also serve a bare-metal `LlamaModel`.

I also made the TP styles ""type neutral"" in the config as you suggested. Translation to torch TP types is done only when applying TP. Re moving distributed / device_mesh init code inside `from_pretrained`:

It is a good direction in making the workflow even simpler for users. As in, wouldn't it be nice if the model is distributed after it is created? I think a UI design would be helpful as to how users express the intention of ""please create the model in a distributed manner"".

For example, would `from_pretrained` accepts a related argument, like:
```
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_mesh=device_mesh,
)
```
If that kwarg is given, I think the intention is clear enough, and we can call `tensor_parallel()` for the user. Or even better, initialize partial weights only -- this is supported by the proposed torch TP too, but it will require slightly more work in integration with the weight loading part of `from_pretained`. I can try making a demo in a next PR if you are interested.

Re launcher: I might slightly prefer the above ""explicit expression"" to ""implicit dependency"" on `torchrun`, for keeping HF transformers ""launcher neutral"", as some users may use SLURM, `mp.spawn`, etc. Another reason was that `init_process_group` and `init_device_mesh` can take some kwargs (such as timeout, options, etc), which might be a bit big for `from_pretrained` to absorb if it wants to call them inside.

Regardless, I agree that it would be very nice if we can create distributed models out of `from_pretrained"" directly, and I am really excite to help that way :)  Summarizing my discussion with @kmehant re collaboration plan between this PR and https://github.com/huggingface/transformers/pull/34194:
- Both PRs agree on the interest in having PyTorch TP in transformers :)
- We agree that the integration can hopefully happen at a broad base so that it can scale to a set of models, thus we agree on picking `PretrainedModel` as an integration point.
- [#34194](https://github.com/huggingface/transformers/pull/34194) has changes on the trainer side. And there is a corresponding [PR](https://github.com/huggingface/accelerate/pull/3173) for changes on Accelerate side. So those two PRs focus on workflow-level integration. So hopefully this PR can serve as a base for those two PRs and once landed, unblock the other two PRs. Sounds good, let's focus on this one first then!  Dumb question: is there a way to bypass the `check_repository_consistency` test? Thanks! @ArthurZucker @SunMarc  Okay, I just temporarily disabled the ""Copied from"" comments in some derivative models. Will add them back once we extend TP plans to those models. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34184). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks @ArthurZucker . I tried that method, but it doesn't seem to work. 
Specifically, the test is checking for consistency between `modeling_gemma.py` and `modular_gemma.py`. I edited neither, but the CI complains that they are not consistent. Here is the [link](https://app.circleci.com/pipelines/github/huggingface/transformers/109404/workflows/7fdcf506-4981-470c-8b3c-17e809855f1a/jobs/1453707). Ah that is because you probably modified `modeling` directly, when these two have a `modular` file, which has the source of truth! @ArthurZucker Fixed code consistency issue via `make fix-copies`. Thanks!
All tests seems passed now (except three awaiting approval). I wonder if you have more comments?
Also cc: @kmehant @SunMarc @muellerzr  I will handle the fix copies afterwards don't worry about it! Thanks for your comments @ArthurZucker .
The reason I haven't put `init_process_group` or `init_device_mesh` into `from_pretrained` are two folds:

1. I was not sure if I should make `from_pretrained` a collective API.
Both `init_process_group` and `init_device_mesh` have collectivity requirements. If `from_pretrained` calls them inside, it will have that requirement too. So I was hoping to let `init_device_mesh` undertake that requirement and cut `from_pretrained` loose from it. It also means that when user's program hit a hang, the debug / support load is on PyTorch, not on HF. The user could have also called `init_process_group` before calling `from_pretrained`; if we call it a second time, it may error out.

2. I was not sure if the user would like to shard the model over the entire ""world"".
In a multi-dimensional scenario, the user may want to TP'lize the model only onto a dimension of the world instead of the entire one (which `init_process_group` creates). This implies that the ""slice"" must come from the user. Of course, that's not the requirement of the example code, but just want to keep ourselves ""future proof."" 

That said, I agree that we can probably make the program simpler by moving the `.tensor_parallel()` API into `from_pretrained`. Mixing with 1 and 2, how about the following design:
```
tp_mesh = DeviceMesh(...)  # tp_mesh can be smaller than world
model = AutoModelForCausalLM.from_pretrained(
    model_id, tp_mesh=tp_mesh
)
```
This will achieve both 1 and 2:
- `DeviceMesh(...)` will undertake the collectivity requirement, `from_pretrained` will be free from it.
- User can specify which dimension of the world to apply TP via `tp_mesh`. Chatted with @SunMarc offline. Will move the `DeviceMesh` part in. Hi @ArthurZucker @SunMarc 
I made the three changes suggested by @ArthurZucker in the comment above.
1. `init_device_mesh` is now moved into `from_pretrained`. When user sets argument `tp_plan=""auto""`, the creation will be triggered and `from_pretrained` will return a TP'lized model. That is, the following UI is now supported:
```
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    tp_plan=""auto"",
)
```
2. `_tp_plan` is now a cls attr of `PreTrainedModel` and I moved `self._tp_plan = config.base_model_tp_plan` from each model's code surface to `PreTrainedModel.post_init()`, so that it is hidden / cleaner.
3. Removed `base_model_tp_plan` from `to_dict` and `to_diff_dict` so that it won't be serialized / saved atm.

Would appreciate your review. Thanks! And this is benchmarking per @SunMarc suggestion:
![Meta-Llama-3-8B-Instruct, seqlen = 512, python, w_ compile](https://github.com/user-attachments/assets/c8672aca-8b18-441e-abe3-a50fdb33db11)
 @ArthurZucker @SunMarc Wondering if you could take another look to see if I addressed your comments? Thanks very much! > Hi @ArthurZucker @SunMarc I made the three changes suggested by @ArthurZucker in the comment above.
> 
> 1. `init_device_mesh` is now moved into `from_pretrained`. When user sets argument `tp_plan=""auto""`, the creation will be triggered and `from_pretrained` will return a TP'lized model. That is, the following UI is now supported:
> 
> ```
> model = AutoModelForCausalLM.from_pretrained(
>     model_id,
>     tp_plan=""auto"",
> )
> ```
> 
> 2. `_tp_plan` is now a cls attr of `PreTrainedModel` and I moved `self._tp_plan = config.base_model_tp_plan` from each model's code surface to `PreTrainedModel.post_init()`, so that it is hidden / cleaner.
> 3. Removed `base_model_tp_plan` from `to_dict` and `to_diff_dict` so that it won't be serialized / saved atm.
> 
> Would appreciate your review. Thanks!

Sounds absolutely great! :hugs: @kwen2501 @ArthurZucker 
Do we want to address this minor support - https://github.com/huggingface/transformers/pull/34184#discussion_r1827185086 in this PR? This will unblock accelerate PR.  @kmehant Thanks for the suggestion. Understood. Agree with the need for a programatic approach.
For the `.tensor_parallel` method itself, it may not be very easy to return a True/False because `nn.Module.apply` does not return anything than the module itself.
Workflow-wise, I wonder if it would be clearer / safer to do:
```
if model.has_tp_plan():
    model.tensor_parallel()
```
If that addresses your need, I am happy to add a `has_tp_plan` util.  > I love it! TBH We can almost merge as is, the main issue / next PR is adding tests. We need to make sure there are no weird interactions! 🤗 Let's add the graph to the doc somewhere as well !

Good idea. Just looked [here](https://huggingface.co/docs/transformers/testing#distributed-training), seems easy to follow the example.  https://github.com/huggingface/transformers/pull/34184#issuecomment-2475352157

@kwen2501 Absolutely, this will be helpful, Thanks Sounds good! @ArthurZucker Thanks again for the suggestions.
1. A test has been added at `tests/tp/test_tp.py`, run command:
```
CUDA_VISIBLE_DEVICES=0,1 pytest -sv tests/tp/test_tp.py
```
2. A new doc has been added: `perf_infer_gpu_multi.md`, titled ""Multi-GPU inference"".
A use example and the benchmark figure have been added to the doc.
@ArthurZucker can you please help merge this [image upload](https://huggingface.co/datasets/huggingface/documentation-images/discussions/392) for doc? Thanks!
3. @kmehant The `.has_tp_plan` property has been added. Merged! let me review one last time 🤗  @ArthurZucker CI is all green now. (Previous failure seems to come from infra instability). Is there anything else I should add before merge? Thanks!Looks really really nice! I think the only thing I would change is to have the `tp_plan` loadable and potentially defined in the config. Given that it does not hold any weights, it would allow other frameworks to rely on it as well! 🤗  Looks a lot better! 
I think we want to have some small tests to make sure we properly serialize the TP-Plan, and also properly dispatch the model (slow tests, we can probably add them ourselves!)
- would need some documentation as well, we can do that in a follow up PR for sure



On of the most important question now is: What's the approach regarding `ForCausalLM`. 

This might be for @SunMarc as it's `transformers` internal: 

`_base_model_tp_plan` will be consumed by the `PreTrainedModel` ➡ this way we can enforce some checks: 
- check that all models that inherit from `PreTrainedModel` have a TP plan 
- check nesting (ForCausalLM has a module that has a TP Plan, if it has other module they should have a TP plan 
 Hey! So basically places where you are disabling it , you just need to do something like: 
`# Copied from transformers.models.llama.modeling_llama.LlamaModel with Llama->Olmoe,lm_head->new_name` for example

and before anything run `make fix-copies` which would apply your changes!  The only two missing things for me are this:
```
    device = torch.device(f""cuda:{rank}"")
    dist.init_process_group(""nccl"", device_id=device)
    device_mesh = dist.init_device_mesh(""cuda"", (world_size,))

    with device:
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
        )

```

that should be done here: 
https://github.com/huggingface/transformers/blob/006e86931247d7bacf94ad00ada1fb6baf11e94b/src/transformers/modeling_utils.py#L4075-L4097 

WIth a `tp_plan = ""auto""` or `tp_plan =base_model_tp_plan`
And, `PreTrainedModel` that has cls attr _tp_plan default to `config.base_model_tp_plan | cls._tp_plan` I love it! TBH We can almost merge as is, the main issue / next PR is adding tests. 
We need to make sure there are no weird interactions! 🤗 
Let's add the graph to the doc somewhere as well ! Great work! 🔥 some small things left to do but super close to merge!","Looks really really nice! I think the only thing I would change is to have the `tp_plan` loadable and potentially defined in the config. Given that it does not hold any weights, it would allow other frameworks to rely on it as well! 🤗  Looks a lot better! 
I think we want to have some small tests to make sure we properly serialize the TP-Plan, and also properly dispatch the model (slow tests, we can probably add them ourselves!)
- would need some documentation as well, we can do that in a follow up PR for sure



On of the most important question now is: What's the approach regarding `ForCausalLM`. 

This might be for @SunMarc as it's `transformers` internal: 

`_base_model_tp_plan` will be consumed by the `PreTrainedModel` ➡ this way we can enforce some checks: 
- check that all models that inherit from `PreTrainedModel` have a TP plan 
- check nesting (ForCausalLM has a module that has a TP Plan, if it has other module they should have a TP plan 
 Hey! So basically places where you are disabling it , you just need to do something like: 
`# Copied from transformers.models.llama.modeling_llama.LlamaModel with Llama->Olmoe,lm_head->new_name` for example

and before anything run `make fix-copies` which would apply your changes!  The only two missing things for me are this:
```
    device = torch.device(f""cuda:{rank}"")
    dist.init_process_group(""nccl"", device_id=device)
    device_mesh = dist.init_device_mesh(""cuda"", (world_size,))

    with device:
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
        )

```

that should be done here: 
https://github.com/huggingface/transformers/blob/006e86931247d7bacf94ad00ada1fb6baf11e94b/src/transformers/modeling_utils.py#L4075-L4097 

WIth a `tp_plan = ""auto""` or `tp_plan =base_model_tp_plan`
And, `PreTrainedModel` that has cls attr _tp_plan default to `config.base_model_tp_plan | cls._tp_plan` I love it! TBH We can almost merge as is, the main issue / next PR is adding tests. 
We need to make sure there are no weird interactions! 🤗 
Let's add the graph to the doc somewhere as well ! Great work! 🔥 some small things left to do but super close to merge!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

This PR uses the `torch.distributed.tensor.parallel` subpackage to implement Tensor Parallel for Llama (as an example).

The motivation is multi-fold:

1. to make modeling code simple as single-worker case:
all manual TP implementations under `if self.config.pretraining_tp > 1` can be removed.

3. to make tensor parallelism easily accessible by users:
added a `model.tensor_parallel(device_mesh)` method that allows users to turn a single-proc model into a parallel model. !- Please guide me to a right place to put this function/method if `PreTrainedModel` is not a preferred place. -!

### Note: 
This is just a demo. The removal of `if self.config.pretraining_tp > 1` may break workflows elsewhere, but it is just intended to calculate how much code can be saved, and hopefully it would be possible to direct it to the mechanism in this PR.

### User script:
```
import os
import torch
import torch.distributed as dist

from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = ""meta-llama/Meta-Llama-3-8B-Instruct""

def main(rank, world_size) -> None:
    device = torch.device(f""cuda:{rank}"")
    dist.init_process_group(""nccl"", device_id=device)

    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        tp_plan=""auto"",
    )

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    prompt = ""Can I help""

    inputs = tokenizer(prompt, return_tensors=""pt"").input_ids.to(device)
    outputs = model(inputs)

    next_token_logits = outputs[0][:, -1, :]
    next_token = torch.argmax(next_token_logits, dim=-1)
    response = tokenizer.batch_decode(next_token)
    print(f""{rank=}: {response}"")


if __name__ == ""__main__"":
    main(int(os.environ[""RANK""]), int(os.environ[""WORLD_SIZE""]))

```

Launch command:
`torchrun --standalone --nproc-per-node 4 tp_hf.py`

Output:
```
Can I help
rank=3: [' you']
rank=1: [' you']
rank=2: [' you']
rank=0: [' you']
```

### Test
```
CUDA_VISIBLE_DEVICES=0,1 pytest -sv tests/tp/test_tp.py
```

### Doc
`perf_infer_gpu_multi.md`

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

Models:

- text models: @ArthurZucker @younesbelkada @fxmarty 

Library:

- trainer: @muellerzr and @SunMarc

Integrations:

- Big Model Inference: @SunMarc

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)

Cc PyTorch Distributed Team:
@gnadathur @wanchaol @fduwjj @wz337 @wconstab @tianyu-l
","A mapping from tp style to the correct function might be better. 
We are also thinking of defining a TensorParallelConfig, your feedback is welcome here, as I don't know the variety of classes / args that might be used!  Thanks for the comment! Indeed a mapping style would look better. 
The only caveat is that the returned value here is an object rather than a constant or class (see the `()` behind `ColwiseParallel`). If we prepare a map, we may be always returning the *same* object -- the `parallelize_module` API may be able to support it I guess, I am just not sure if there is a contract guaranteeing that today. Do we want to allow this for external use by removing `_` so that we can allow users to define tp plan tweaks from `config.json`? 

Given that, shall we as well allow for providing custom tp plan as input to `LlamaConfig()` that overrides the default? Trying to think out loud here, 

Do we want keep all the tp_plans as part of the config classes? We pull them here from config as needed, WDYT?

My thinking goes like adding to the LlamaConfig something like


```
addon_tp  = {""LlamaForCausalLM"": {""lm_head"": ""colwise_rep""}}
```

or

```
llama_for_causal_lm_addon_tp_plan = {""lm_head"": ""colwise_rep""}
```

So here in the actual model class we pull in like we are pulling in for LlamaModel class (decoder layers)

This suggestion allows having all tp_plan confined to the config classes and allows users to configure it from outside either from config.json file or by when using LlamaConfig as is.  Pitching in :)

We should be able to use the same object since it applies required parallel operation to the module and returns a new copy - https://github.com/pytorch/pytorch/blob/86d4b7d60b264cae5a04a1b20719bcd7a5752a4c/torch/distributed/tensor/parallel/api.py#L95 

Have also tested it empirically while benchmarking (https://github.com/huggingface/transformers/pull/34194)

Thanks! Yeah, good idea. We can make this public once we prove things work.  That's a good question. The setting in the config class in current PR is a bit conservative in that it tries to align with the config's definition:
```
class LlamaConfig(PretrainedConfig):
    This is the configuration class to store the configuration of a [`LlamaModel`].
``` Yep, `base_model_tp_plan` should be supported as input to the PreTrainedConfig!  Actually up to debate.
The thing is, `LlamaConfig` is used to init all `LlamaPreTrainedModel`, thus warrants that you can have a need for multiple extra keys (add ons). As such, I think it makes more sense to have them in the class itself. 
But, indeed confining to the config would also be easier. I am just worried about extra add ons that would be ""visible"" when you have a model that is unrelated (like a SequenceClassification model). 

We can fetch the `_tp_plan` from the class on post init or `super().__init__(config)`  SOunds good! So, this variable `base_model_tp_plan` has to be added to PreTrainedConfig https://github.com/huggingface/transformers/blob/a769ed45e17c44fd17b85c025863c4e4f2f73634/src/transformers/configuration_utils.py#L50 with a default value as an empty dict `{}` which I believe is best possible default for any config sub class inheriting. > I think it makes more sense to have them in the class itself

Trying to understand, when we confine plans entirely to the model class instead of config classes, how does it allow users to modify tp plans through `config.json`?

An orthogonal thought, do you think we should instead introduce separate config file like we are doing for `FSDP config` where we pass in `fsdp_config` and here it could be `tp_config`?  Ah the thing is, we might not want to `serialize` it in the config.json, but they can pass a TP-Plan when initializing as kwargs Yeah we could also think about a separate file, BUT I think going for something close to our `device_map` will be easier. If there is a strong push, then we can have a config as well! I see @ArthurZucker, I thought we wanted to use config.json to both serialise and deserialise for users motivated from your comment - https://github.com/huggingface/transformers/issues/32470#issuecomment-2275768887 :) Thanks @kmehant @ArthurZucker for the suggestion. I moved `base_model_tp_plan` to `PretrainedConfig` in the latest commit. @kwen2501 Is there a way to know if tensor_parallel took effect or not? For instances like there was no usable tp plan found in any sub module so no TP took place etc. Returning that indication would help in better downstream usage (like as part of accelerate)

cc: @ArthurZucker  let's make sure it's not serialized for now (remove it in to_dict in save_pretrained) Yeah, I talked internally about it, we could have a different `tp_plan.json` but it's a runtime thing so would rather not serialize it for now! Ah I see what you want here! 
We don't have to put it here! Since it's a PreTrainedModel, pretrained model can have a `_tp_plan` default to `config.base_model_tp_plan | cls._tp_plan` Got it @ArthurZucker, thanks for letting me know, I will be happy to add support for `tp_plan.json` in future PRs :) related thread regarding the need for this - https://github.com/huggingface/accelerate/pull/3173#discussion_r1821213564 
 I added logging in this method. You can see if by setting `TRANSFORMERS_VERBOSITY=DEBUG`.
 
@kwen2501 
Logging is great, but need some indication that can be consumed programatically :) Yep done! ```suggestion
        if self._tp_plan is None:
``` feels simpler let's raise n error instead Oh, the reason for `self.base_model is self` is that we are attaching the base_model_tp_plan to the base model only. If we attach it to `LlamaForCausalLM`, the FQNs won't match, because the `base_model_tp_plan` FQNs start with ""layers"", not ""model.layers"". Oh, here the assert is more for catching internal error than user error.
It is like ""I swear this cannot happen” but it seems to happen anyway.
If someone runs python with `-O` flag, this assert can be optimized away :) "
34887,2024-11-22T22:12:55Z,2024-11-25T12:54:55Z,dependabot[bot],1,0,1,2,1,1,1,"['dependencies', 'python']",1599.0,0,225722.0,0,0,0,0,2704669.472673,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34887). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"Bumps [tornado](https://github.com/tornadoweb/tornado) from 6.4.1 to 6.4.2.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tornadoweb/tornado/blob/v6.4.2/docs/releases.rst"">tornado's changelog</a>.</em></p>
<blockquote>
<h1>Release notes</h1>
<p>.. toctree::
:maxdepth: 2</p>
<p>releases/v6.4.2
releases/v6.4.1
releases/v6.4.0
releases/v6.3.3
releases/v6.3.2
releases/v6.3.1
releases/v6.3.0
releases/v6.2.0
releases/v6.1.0
releases/v6.0.4
releases/v6.0.3
releases/v6.0.2
releases/v6.0.1
releases/v6.0.0
releases/v5.1.1
releases/v5.1.0
releases/v5.0.2
releases/v5.0.1
releases/v5.0.0
releases/v4.5.3
releases/v4.5.2
releases/v4.5.1
releases/v4.5.0
releases/v4.4.3
releases/v4.4.2
releases/v4.4.1
releases/v4.4.0
releases/v4.3.0
releases/v4.2.1
releases/v4.2.0
releases/v4.1.0
releases/v4.0.2
releases/v4.0.1
releases/v4.0.0
releases/v3.2.2
releases/v3.2.1
releases/v3.2.0
releases/v3.1.1
releases/v3.1.0
releases/v3.0.2
releases/v3.0.1
releases/v3.0.0
releases/v2.4.1
releases/v2.4.0</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tornadoweb/tornado/commit/a5ecfab15e52202a46d34638aad93cddca86d87b""><code>a5ecfab</code></a> Bump version to 6.4.2</li>
<li><a href=""https://github.com/tornadoweb/tornado/commit/bc7df6bafdec61155e7bf385081feb205463857d""><code>bc7df6b</code></a> Fix tests with Twisted 24.7.0</li>
<li><a href=""https://github.com/tornadoweb/tornado/commit/d5ba4a1695fbf7c6a3e54313262639b198291533""><code>d5ba4a1</code></a> httputil: Fix quadratic performance of cookie parsing</li>
<li>See full diff in <a href=""https://github.com/tornadoweb/tornado/compare/v6.4.1...v6.4.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tornado&package-manager=pip&previous-version=6.4.1&new-version=6.4.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/huggingface/transformers/network/alerts).

</details>",
33269,2024-09-02T20:27:16Z,2024-11-25T12:51:27Z,meliksahturker,5,0,1,4,1,1,1,[],5333949.0,0,7230251.0,0,0,0,0,2704880.376288,,0,1,0,False,"['meliksahturker', 'fzyzcjy', 'Rocketknight1']","+1 I am seeing the same problem. Looking forward to the fix! Hi, a few months has passed and I would appreciate it if this can be merged! /cc @ArthurZucker  cc @meliksahturker - it looks like the PR is good, but a test to ensure the issue doesn't regress would be great! Let us know if you need any help with that. Thanks and looking forward to it! This is blocking my usage of position_ids + fa2 :( > cc @meliksahturker - it looks like the PR is good, but a test to ensure the issue doesn't regress would be great! Let us know if you need any help with that.

I took a look to the existing [test](https://github.com/huggingface/transformers/pull/31629/files#diff-a6ebb0bccfe159ca855e9d9b5d6fbbeb62fced0c0296de0993ba39ef5c862cf0R4330) of the function, however I haven't been able to find the time to work on it.
I think it's best and the fastest if @RhuiDih would take a look and perhaps add the batched test since he implemented the function and the existing tests.LGTM, if we don't have one yet, let's add a test with batching! 
 Completely fine, sorry we should have merged sooner! 
Will be part of the release 😉 ","LGTM, if we don't have one yet, let's add a test with batching! 
 Completely fine, sorry we should have merged sooner! 
Will be part of the release 😉 ","# What does this PR do?
It resolves the bug reported in [issue 33268](https://github.com/huggingface/transformers/issues/33268) by calling contiguous() before view() for key and value within **prepare_fa2_from_position_ids** function.
Related error stems from that the tensor is not contiguous in the memory, hence is fixed by simply calling contiguous() function.

The same error is encountered and discussed in:
[PyTorch forum](https://discuss.pytorch.org/t/view-size-is-not-compatible-with-input-tensors-size-and-stride/121488/2)
[Huggingface forum](https://discuss.huggingface.co/t/albertformaskedlm-error-view-size-is-not-compatible/1261)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@RhuiDih
@ArthurZucker",
22105,2023-03-11T10:43:48Z,2024-11-25T09:43:16Z,ArthurZucker,19,2,43,2200,10,2,1,['WIP'],906.0,1,54001201.0,0,0,0,0,2711338.682278,,1,43,0,False,"['HuggingFaceDocBuilderDev', 'Bachstelze', 'qherreros', 'hriaz17', 'serenachou', 'ArthurZucker', 'joshdevins', 'zynaa']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_22105). All of your documentation changes will be reflected on that endpoint. Hey @ArthurZucker any updates on this? ETA for when it will be merged into main? Hey! Just got back from holidays, this should be my main focus in the coming days!  Sorry! Seem like I had to postpone this! If anyone want to take over feel free to do it, otherwise will be my priority once https://github.com/huggingface/transformers/pull/23909 is merge! Regarding the `z_steps` in `DebertaV2Model`: I think this code is relevant for the [enhanced mask decoder of the generator model](https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/apps/models/masked_language_model.py#L51)

```python
if attention_mask.dim() <= 2:
    extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
    att_mask = extended_attention_mask.byte()
    attention_mask = att_mask * att_mask.squeeze(-2).unsqueeze(-1)
elif attention_mask.dim() == 3:
    attention_mask = attention_mask.unsqueeze(1)
target_mask = target_ids > 0
hidden_states = encoder_layers[-2]
if not self.position_biased_input:
    layers = [encoder.layer[-1] for _ in range(2)]
    z_states += hidden_states
    query_states = z_states
    query_mask = attention_mask
    outputs = []
    rel_embeddings = encoder.get_rel_embedding()

    for layer in layers:
        # TODO: pass relative pos ids
        output = layer(hidden_states, query_mask, return_att=False, query_states=query_states,
                       relative_pos=relative_pos, rel_embeddings=rel_embeddings)
        query_states = output
        outputs.append(query_states)
else:
    outputs = [encoder_layers[-1]]
```

As far as I can tell, they hardcoded z_steps to 2 here. Although it should still be left as 0 for the discriminator. Adding the z_steps to the config seems like a good idea. 

`z_states` represents [the position embeddings](https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/apps/models/masked_language_model.py#L111), which are non-zero if `position_biased_input` is set to `True`. They are passed from the [embedding layer](https://github.com/microsoft/DeBERTa/blob/master/DeBERTa/deberta/bert.py#L269). So in order to properly implement this, I think we need to return the position embeddings here:

```python
class DebertaV2Embeddings(nn.Module):
    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, mask=None, inputs_embeds=None):
        ...

        return embeddings, position_embeddings
```

and implement the `z_steps` like this:

```python
class DebertaV2Model(DebertaV2PreTrainedModel):
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutput]:
        ...

        embedding_output, position_embedding_output = self.embeddings(
            input_ids=input_ids,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            mask=attention_mask,
            inputs_embeds=inputs_embeds,
        )
        ...

        if self.z_steps > 0:
            hidden_states = encoded_layers[-2]
            layers = [self.encoder.layer[-1] for _ in range(self.z_steps)]
            position_embedding_output += hidden_states
            query_states = position_embedding_output
            query_mask = self.encoder.get_attention_mask(attention_mask)
            rel_embeddings = self.encoder.get_rel_embedding()
            rel_pos = self.encoder.get_rel_pos(embedding_output)
            for layer in layers:
                query_states = layer(
                    hidden_states,
                    query_mask,
                    output_attentions=False,
                    query_states=query_states,
                    relative_pos=rel_pos,
                    rel_embeddings=rel_embeddings,
                )
                encoded_layers = encoded_layers + (query_states,)
``` What is the status?
The logs of the checks are expired. #27734 should help with some of the issues in the mean time Any update on this? We're blocked by some of the issues that this is meant to fix. Sepcifically:
* https://github.com/huggingface/transformers/issues/22790
* https://github.com/huggingface/transformers/issues/27586
* https://github.com/huggingface/transformers/issues/20815
* https://github.com/huggingface/transformers/pull/27734 Hi there, just checking in @ArthurZucker on whether there's any progress here please?  Hey hey! Sorry I ended up dropping this, let me get back to you next week! I reviewed #27734, will take it over this weekend if possible.  Anyone up for the task can try to do it as well 🤗  Hey @ArthurZucker any update on the status of this and related issues. Curious if this will also cover DeBERTa v3 now as well. Hey @ArthurZucker !
Thank you so much for your work. I'm reviewing your changes as it does impact my code on some project I have. I saw that you swap `StableDropout` with `Dropout` in various places. I'm not entirely sure of the impact but it seems to diverge from microsoft implementation. Did you face an issue with `StableDropout` and tracing ? Or is it something else? I'm not super familiar with this part of the code, but if I'll try to help you if I can.

Thanks again Yep, this decision is based on the fact that `StableDropout` is supposed to be just dropout but more efficient. 
This function is hard to trace / compile and is 4 years old. 
The idea is that if we remove this and XSoftmax, performance gains are so much better and code so much simpler, but equivalent, that is is worth it! LGTM! Every test we have on our end work with this PR. Thanks @ArthurZucker  cc @michaelbenayoun I am not sure I know how to avoid the HFProxy issue, do you have pointers?  Had to skip some pt-tf equivalence tests. The slow tests ran for me and are passing. 
If anyone has a problem will be quick to fix! Nice one, thanks @ArthurZucker !Ok! Can you document in DeBERTa's documentation page the evolution that the integration had? I think it's important that users have easily accessible information about the initial contribution, and how this refactor contributes to improving every aspect of DeBERTa.

Thanks!","Ok! Can you document in DeBERTa's documentation page the evolution that the integration had? I think it's important that users have easily accessible information about the initial contribution, and how this refactor contributes to improving every aspect of DeBERTa.

Thanks!","# What does this PR do?
Refactor both Deberta and DebertaV2 to make them more compatible with the overall transformers library

Should fix a bunch of issues related to torch-scripting with Deberta:
fixes #15216
fixes #15673
fixes #16456
fixes #18659
fixes #21300 
fixes #20815
fixes #12436 
fixes #18674 
","Does this also support the specific dropout/softmax classes or are we dropping them entirely? We are dropping them completely! 
"
34554,2024-11-01T02:38:02Z,2024-11-25T10:27:14Z,techkang,2,0,2,4,1,2,2,[],40700.0,0,2101752.0,0,0,0,0,2713534.509189,,1,2,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']","cc @muellerzr  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34554). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the fix! Sorry for seeing this so late, merging to make sure it's in the release!","Thanks for the fix! Sorry for seeing this so late, merging to make sure it's in the release!","# What does this PR do?
Fix https://github.com/huggingface/transformers/issues/34503
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34874,2024-11-22T07:35:59Z,2024-11-25T10:20:20Z,zucchini-nlp,2,0,1,249,1,1,1,[],1598.0,0,269061.0,0,0,0,0,2713949.423622,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'zucchini-nlp']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34874). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Added! Well it is more fixing than breaking I hope heheAs it is a bit breaking let's add 🔴 to the PR tittle! ,As it is a bit breaking let's add 🔴 to the PR tittle! ,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34689. Separated into a new PR from https://github.com/huggingface/transformers/pull/34274",
34876,2024-11-22T09:39:50Z,2024-11-25T09:41:56Z,zucchini-nlp,1,2,6,77,7,2,1,[],1644.0,0,259326.0,0,0,0,0,2716253.891721,,0,6,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34876). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks, can we just define something like: 
```python 
if input_ids is None:
    start_tokens = [[self.config.text_config.bos_token_id]]
    if getattr(self.config, ""video_token_index"", None) is not None:
        start_tokens += [[self.config.video_token_index]]
    input_ids = torch.tensor(start_tokens, dtype=torch.long, device=image_embeds.device)
```","Thanks, can we just define something like: 
```python 
if input_ids is None:
    start_tokens = [[self.config.text_config.bos_token_id]]
    if getattr(self.config, ""video_token_index"", None) is not None:
        start_tokens += [[self.config.video_token_index]]
    input_ids = torch.tensor(start_tokens, dtype=torch.long, device=image_embeds.device)
```","# What does this PR do?

Some tests started failing after updating the hub configs and weights so that we can remove the legacy math on BLIP models. This PR fixes the tests, mainly the reason was that we didn't think about generation from image without text which failed to slice the `input_ids` and merged with `image_embeddings`

","because the new configs hold both image and video models which are loaded as two different Model Classes in transformers. So the we have two special tokens now and the image token is pre-last, while video token is last id ```suggestion
                    torch.tensor([[self.config.image_token_index]], device=image_embeds.device, dtype=torch.long)
```
otherwise this is the legacy constructor !
```python
In [3]: torch.LongTensor(1, device=""mps"")
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[3], line 1
----> 1 torch.LongTensor(1, device=""mps"")

RuntimeError: legacy constructor expects device type: cpu but device type: mps was passed
```
and let's split in 2 lines 😉 "
34274,2024-10-21T08:15:26Z,2024-11-25T09:11:33Z,zucchini-nlp,1,18,15,121,7,3,1,[],348264.0,0,3027367.0,0,0,0,0,2718078.260012,,1,15,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34274). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Two items to check :D  Thanks LGTM but let's separate unrelated changes! Thanks 🤗 ,Two items to check :D  Thanks LGTM but let's separate unrelated changes! Thanks 🤗 ,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34206. As per title we would have to initialize empty cache whenever `use_cache=True`. Additionally `MllamaForCausalLM` was not loading correctly for me, so I modified the base model prefix","From the docstring of `PreTrainedModel`, regarding `base_model_prefix`: 

```
A string indicating the attribute associated to the base model in derived classes of the same architecture adding modules on top of the base model.
```

How is the hierarchy in the model weights? I'm assuming it's
```
model -|- language_model --------------|- model - (...)
       |                               |- lm_head
       |- vision_model-(...)
       |- multi_modal_projector-(...)
           
```

If that's the case, then I agree with the change, assuming we also change `self.model` to `self.language_model` in this class

(make sure all slow tests pass!) Make sure `py.test tests/models -k test_past_key_values_format` passes :) If the missing `rope_deltas` is the only issue, can we instead:
1 - replace `prepare_config_and_inputs_for_common()` by `prepare_config_and_inputs_for_generate()` in the base test
2 - Overload `prepare_config_and_inputs_for_generate` in this model tester, to also return `rope_deltas`

(much smaller code diff -> easier to maintain) Yes, that is how the checkpoints looks like and when I loaded the model it didn't load correctly if the base prefix is not fixed. The slow tests unfortunately can't be run because the model is read-token protected and EU has no access to them 🥲  But I tested with open mirrored weights   Hmm, but we would need access to the model outputs from prev step in that case, which we don't have because `generate()` returns only some kwargs. And if we want to feed the original inputs with cache to the `prepare_config_and_inputs_for_generate`, we'll need to prepare `repo_deltas` again in the same way passed Fair :)

Can we change `self.model` to `self.language_model`? Some parts of the codebase call variants of `inner_model = getattr(model, model.base_model_prefix)` that would mean we change the checkpoint state dict keys right? 🤔  anyway, lemme verify this and tell if it is possible without touching checkpoint > that would mean we change the checkpoint state dict keys right?

Uhmmm possibly? Not sure 😅 The tests that save and reload would break if it would not be BC, I think (chatted on slack about exploring how to standardize `qwen2_vl` + `generate`) Indeed, I found why it was changed, it was to get the self.base_model method for preparing causal mask in https://github.com/huggingface/transformers/pull/33677 hehe

and yes, w/o changing state dict keys we cannot call it ""model"". Imo even if we change the official state dict, there are many mirrors/finetunes which will be BC breaking compared to the first model release. So the better way i think is to bring back the base-model-prefix as it was.

I'm thinking maybe we can have a default method for `update_causal_mask` and `prepare_attention_mask` which will be the fallback if the base-model has no such method defined? 🤔 

EDIT: but wait, Arthur might disagree as he wanted to have attn preparation is all model files instead of having one copy in general modeling file. In that case, we might need to get smth better than `getatte(self, base_model_prefix)` as it doesn't work when same checkpoint is loaded as CausalLM and as ConditionalLM :(  🤔 

Regarding the `generate`-specific problem: if `base_model = getattr(self, self.base_model_prefix, None)` in the generalist `prepare_inputs_for_generation` (or its downstream usage) is the issue, then my recommendation would be to overwrite `prepare_inputs_for_generation` in mllama -- more specifically, in the classes where it doesn't work 

Alternatively, we could define `_prepare_4d_causal_attention_mask_with_cache_position` in all model classes -- write it once in the innermost class, then the child classes would define this function as a parent call.

Any of these solutions would work well for me :) (with a preference for the second: when we rewrite `prepare_inputs_for_generation`, we know for sure we'll have extra maintenance in the future) done, and also enabled compile tests for the CausalLM class to test that it works It should indeed be `langauge_model` but probably in a different PR as it is unrelated! this does make sense as it's helping users, and an old APi, but let's promote init of a cache and passing it! 🤗  We could but still I think it is a lot easier if users want a forward pass with cache, and do not want extra lines of code for importing and passing the cache object. So i think we'd better keep the default cache for now oke, will make a new PR missing torch jit tracing escape here no? "
33509,2024-09-16T13:28:19Z,2024-09-18T14:57:39Z,Rocketknight1,8,0,1,8,1,2,2,[],748.0,0,6007579.0,0,0,0,0,2743094.005013,,0,1,0,False,"['ylacombe', 'monica-sekoyan', 'Holmes-GU', 'Rocketknight1', 'Vaibhavs10']","cc: @ylacombe here Hey @Rocketknight1, thanks for opening the PR!
In theory, I don't see any problems with this fix, have you been able to run every slow ASR pipeline tests here ?  @ylacombe there are some slow tests that I can't get working on my local machine (even on `main`). However, of the tests that run, they all work with this PR as well! Let me know if you want me to run them! @ylacombe Sure!  Okay, cool! cc @LysandreJik for core maintainer review. Hi @Rocketknight1, 
I think `return_attention_mask` should also be added to `chunk_iter` function, so the warning will be removed even when we specify `chunk_length_s`. > cc @sanchit-gandhi - this PR just sets `return_attention_mask=True` on the preprocessors in the `automatic_speech_recognition` pipeline to avoid warnings caused by missing attention masks. It works okay in my testing, but please let me know if you think it could cause any problems!
> 
> Fixes #33498

Hi, where can I set `return_attention_mask=True`?So, I've first opened a PR to fix some of the slow tests that were not passing due to how data was loaded #33545. 
Your PR doesn't add any failing tests as compared to main and the changes make sense, so I think it should be ok to merge ! Thanks for the PR @Rocketknight1!","So, I've first opened a PR to fix some of the slow tests that were not passing due to how data was loaded #33545. 
Your PR doesn't add any failing tests as compared to main and the changes make sense, so I think it should be ok to merge ! Thanks for the PR @Rocketknight1!","cc @sanchit-gandhi - this PR just sets `return_attention_mask=True` on the preprocessors in the `automatic_speech_recognition` pipeline to avoid warnings caused by missing attention masks. It works okay in my testing, but please let me know if you think it could cause any problems!

Fixes #33498",
34632,2024-11-06T19:32:49Z,2024-11-25T09:03:44Z,dvrogozh,9,22,1,22,1,6,4,[],598722.0,0,1603861.0,0,0,0,0,2718542.891463,,0,1,0,False,"['mikaylagawarecki', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'ydshieh', 'dvrogozh']","@muellerzr, @SunMarc, @ArthurZucker : can you, please, help comment on this PR? see issue #34631 on details. I am getting 

> FAILED tests/trainer/test_trainer.py::TrainerIntegrationTest::test_can_resume_training - AttributeError: module 'numpy' has no attribute 'dtypes'. Did you mean: 'dtype'?

when running

> python3 -m pytest  tests/trainer/test_trainer.py::TrainerIntegrationTest::test_can_resume_training

against this PR.

 @ydshieh : this might be due to numpy version. dtypes was added in 1.25 according to https://numpy.org/doc/2.1/reference/routines.dtypes.html#module-numpy.dtypes. Locally I have 1.26.4. Which version do you have?

I will work on using context manager since there is an alignment on that and also tune a list per versioning of numpy. On our CI runner , I get `numpy=1.24.3` The numpy GLOBALs for dtypes that need to be allowlisted might need an if statement depending on whether version < 1.25 or not, there's some documentation on this here https://pytorch.org/docs/main/notes/serialization.html#troubleshooting-weights-only The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34632). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Thanks! Just a documentation suggestion but this all looks correct

@muellerz : done, added a link to Accelerate PR.  > LGTM ! Just a nit

@SunMarc : addressed, reused approach from accelerate on `numpy.core` deprecation. Thanks for fixing 🤗 Nice ! Thanks for adding this ! Left a comment  cc @muellerzr if you can have a look as well! Thanks! Just a documentation suggestion but this all looks correct LGTM ! Just a nit  Thanks",Nice ! Thanks for adding this ! Left a comment  cc @muellerzr if you can have a look as well! Thanks! Just a documentation suggestion but this all looks correct LGTM ! Just a nit  Thanks,"Starting from version 2.4 PyTorch introduces a stricter check for the objects which can be loaded with `torch.load()`. Starting from version 2.6 loading with `weights_only=True` requires allowlisting of such objects.

This commit adds allowlist of some `numpy` objects used to load model checkpoints. Usage is restricted by context manager. User can still call `torch.serialization.add_safe_globals()` to add other objects into the safe globals list.

Accelerate library also stepped into same problem and addressed it with PR-3036.

Fixes: #34631
See: https://github.com/pytorch/pytorch/pull/137602
See: https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.add_safe_globals
See: https://github.com/huggingface/accelerate/pull/3036

CC: @muellerzr @SunMarc","While this fixes the test failures described in #34631, there are few things to discuss for potentially better implementation:
* I have added allowed list globally. The better approach might be to use context manager like this:
```
with torch.serialization.safe_globals([np.core.multiarray._reconstruct, np.ndarray, np.dtype, np.dtypes.UInt32DType]):
    checkpoint_rng_state = torch.load(rng_file)
```
* Using context manager might require changes in other places in Huggingface which I did not identify yet, need to run more tests.
* I guess I did not allowlisted all what might be needed... This makes me think that we might need a change in Transformers API to allow end users to specify additional list items especially if context manager will be used.
* Another option is to set safe globals on a higher level. In a sense of failing tests described in #34631 this would be in test itself. I personally think this might not be the best approach - for example, tests will need to import `numpy` to allow list its objects. At the moment usage of `numpy` is hidden inside Transformers implementation.
* And yet another option is to ask PyTorch to allowlist certain things. In the case of identified failures - some numpy objects.

@muellerzr, @SunMarc : please, let me know your opinion. @albanD : can you, please, comment whether it makes sense to add any of `numpy` symbols into Pytorch allowed list for safe_globals? I would submit issue / PR on PyTorch side if you believe this makes sense. I think using a context manager might be better to better control what we allow in `safe_globals`.  We can have a global list then add a local list in the context manager if needed. Also, this way we can also clean the `safe_globals` when we exit from the context manager, a bit like in accelerate.  cc @muellerzr  I agree with `using a context manager`.

In the long term, it might be better if we are able to avoid using `torch.load(weights_only=False)`. But guess we still need to maintain BC. > @albanD : can you, please, comment whether it makes sense to add any of numpy symbols into Pytorch allowed list for safe_globals? I would submit issue / PR on PyTorch side if you believe this makes sense.

Alban is out till end November, but from our earlier discussions we had decided not to allowlist the numpy related dtypes by default > Alban is out till end November, but from our earlier discussions we had decided not to allowlist the numpy related dtypes by default

Thank you @mikaylagawarecki. That's the guidance I was looking for. We could have a `SAFE_TRANSFORMERS_GLOBAL` with these no? this way people can easily update them? 
TBH I prefer the context manager but want to have the least duplication as possible! Should I add any other numpy dtypes in the list? As of now I spotted only `np.unit32` in the Transformers list as the one needed. Switched to context manager. I found that calling `torch.serialization.add_safe_globals()` still works to add additional safe global staff. `SAFE_TRANSFORMERS_GLOBAL` can also be considered. Let me know if you see the need. The only one I don't see from accelerate is `encode`, however if things pass here without it it's accelerate specific and we don't need to worry about it I'd also add in https://github.com/huggingface/accelerate/pull/3036 to this list added Transformer tests did pass on my side without adding `encode`. This indeed seems accelerate specific. np.core is deprecated. Maybe you can add the modification we did in accelerate https://github.com/huggingface/accelerate/pull/3247 done. just a nit: should it be `""2.6.0""` here or it's really necessary being `""2.4.0""`? Switched to `version < 2.6.0a0`. Indeed, on switching to context manager I overlooked that it was introduced later. Overall:
* `torch.serialization.add_safe_globals` appeared in pytorch 2.4
* `torch.serialization.safe_globals` (context manager) appeared in 2.5
* And pytorch 2.6 flipped default of `weights_only` in `torch.load` from `False` to `True`

Overall, it indeed does not make sense to have this code working for versions earlier than 2.6 unless we will start calling `torch.load` with explicit `weights_only=True`. Hi! A tiny question: how to get `2.6.0a0` installed. I know how to install night but it gets `dev202411xx` instead of `a0`  Anyway, good to use `a0` here for now. Once 2.6 is released, we can change it to `2.6`. > Hi! A tiny question: how to get 2.6.0a0 installed. 

I am getting this building from sources. And `<2.6.0` does not work for me on my build. So, `2.6.0a0` is my best effort to get the check working for my current build. I did not know that nightly builds get `dev202411xx`, I thought they also give `a0`. I wonder will the check still work for nightly? I checked. `<2.6.0a0` won't work with nightly. So, I switched to a check I ones spotted in a code by Narsil. This should handle both cases, building from sources and using 2.6 nightly (I checked - works for both on my side):
```
if version.parse(torch.__version__).release < version.parse(""2.6"").release:
```"
31865,2024-07-09T14:15:50Z,2024-08-22T13:07:09Z,SunMarc,6,0,13,29,3,2,2,[],1222.0,0,11918743.0,0,0,0,0,2790680.348495,,0,13,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev', 'fzyzcjy', 'msaroufim']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_31865). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. There's also an `AdamWFp8` btw and it's the fastest one we've found when the HW supports it https://github.com/pytorch/ao/tree/main/torchao/prototype/low_bit_optim#benchmarks

Also cc @gau-nernst this is very exciting! > There's also an AdamWFp8 btw and it's the fastest one we've found when the HW supports it https://github.com/pytorch/ao/tree/main/torchao/prototype/low_bit_optim#benchmarks

Nice ! I'll add it in a separate PR !  Heads up @SunMarc we just released torchao 0.4! https://github.com/pytorch/ao/releases/tag/v0.4.0 Nice ! I'll merge it as soon as we merge the torchao quantization PR in transformers as there is some overlap !  Hi, is it OK if I PR to add the 8bit counterpart? (see https://github.com/huggingface/transformers/issues/34893 for details) Thanks!Nice! LG2M, cc @msaroufim :) Thanks for adding! ","Nice! LG2M, cc @msaroufim :) Thanks for adding! ","# What does this PR do ? 

This PR adds the 4-bit optimizer from torchao library into HF Trainer. For now, it requires the main branch of `torchao` and torch >=2.3 (maybe we can wait a bit before merging). For those who wants to try, you can pass `optim=""adamw_torch_4bit""` in `TrainingArguments`. 

Since we already have the 8-bit optimizer from bnb that works well, i'm not adding it. 

Related thread : https://x.com/marksaroufim/status/1809398186198593566

cc @muellerzr as you might be interested",
34314,2024-10-22T14:16:52Z,2024-10-24T09:16:55Z,ydshieh,2,0,2,31,11,2,2,[],1663.0,0,2760872.0,0,0,0,0,2876489.977438,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34314). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I will merge once I am able to build the images with python 3.9GG finally! Cool!,GG finally! Cool!,"# What does this PR do?

Python 3.8 reached EOL on Oct. 07, 2024

https://devguide.python.org/versions/",
34866,2024-11-21T22:11:27Z,2024-11-22T23:14:24Z,yonigozlan,1,2,2,298,3,2,2,[],1659.0,0,90177.0,0,0,0,0,2926711.158604,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34866). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Indeed the regex is broken and will stop after the first underscore (`_`) of the file type! And it is indeed easier to grab the name directly from the folder instead of parsing the last part 🤗
Thanks for the fix! SG then, I think the regex was to make it more ""general"" but this is simpler and better, thanks!","Indeed the regex is broken and will stop after the first underscore (`_`) of the file type! And it is indeed easier to grab the name directly from the folder instead of parsing the last part 🤗
Thanks for the fix! SG then, I think the regex was to make it more ""general"" but this is simpler and better, thanks!","# What does this PR do?

I need to add a method to an existing image processor in the GotOcr2 PR https://github.com/huggingface/transformers/pull/34721
Right now there seems to be a bug where the model name is not correctly deduced from the file name when we inherit from a module in an image_processing* file. With the example added, I get this error:
```
Traceback (most recent call last):
  File ""/home/ubuntu/models_implem/transformers/utils/modular_model_converter.py"", line 1547, in <module>
    converted_files = convert_modular_file(file_name, args.old_model_name, args.new_model_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/models_implem/transformers/utils/modular_model_converter.py"", line 1483, in convert_modular_file
    wrapper.visit(cst_transformers)
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/metadata/wrapper.py"", line 204, in visit
    return self.module.visit(visitor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/_nodes/module.py"", line 89, in visit
    result = super(Module, self).visit(visitor)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/_nodes/base.py"", line 233, in visit
    visitor.on_leave(self)
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/_visitors.py"", line 137, in on_leave
    leave_func(original_node)
  File ""/home/ubuntu/models_implem/transformers/utils/modular_model_converter.py"", line 1199, in leave_Module
    renamed_module = module.visit(renamer)
                     ^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/_nodes/module.py"", line 89, in visit
    result = super(Module, self).visit(visitor)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/_nodes/base.py"", line 227, in visit
    _CSTNodeSelfT, self._visit_and_replace_children(visitor)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/_nodes/module.py"", line 74, in _visit_and_replace_children
    body=visit_body_sequence(self, ""body"", self.body, visitor),
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/_nodes/internal.py"", line 227, in visit_body_sequence
    return tuple(visit_body_iterable(parent, fieldname, children, visitor))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/_nodes/internal.py"", line 193, in visit_body_iterable
    new_child = child.visit(visitor)
                ^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/_nodes/base.py"", line 236, in visit
    leave_result = visitor.on_leave(self, with_updated_children)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/matchers/_visitors.py"", line 515, in on_leave
    retval = CSTTransformer.on_leave(self, original_node, updated_node)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/hf_311_121/lib/python3.11/site-packages/libcst/_visitors.py"", line 71, in on_leave
    updated_node = leave_func(original_node, updated_node)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/models_implem/transformers/utils/modular_model_converter.py"", line 122, in leave_ClassDef
    new_name = convert_to_camelcase(updated_node.name.value, self.old_name, self.default_old_name)
                                                                            ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ReplaceNameTransformer' object has no attribute 'default_old_name'
```

Things work as expected with the following change:
https://github.com/huggingface/transformers/blob/4e90b99ed916300b80bac9db793f2a96b2a87122/utils/modular_model_converter.py#L1195

to

```python
file_model_name = file.split(""."")[-2]
```

As unless I'm missing something, the model name can be deduced from the folder name in which modeling/processing/config files are located?


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@Cyrilvallez @ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","wondering if the issue is not just that the file should be `img_proc_model` cf the camel casing No, the issue really is the regex: `transformers.models.blip.image_processing_blip` gives `processing_blip` instead of `blip` with the current regex"
34854,2024-11-21T13:31:15Z,2024-11-22T16:13:31Z,BenjaminBossan,1,0,3,26,2,2,2,[],1606.0,0,96703.0,0,0,0,0,2951397.356588,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34854). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for fixing this ! LGTM!  Makes sense! Thanks @BenjaminBossan ,Thanks for fixing this ! LGTM!  Makes sense! Thanks @BenjaminBossan ,"# What does this PR do?

EETQ tries to import the `shard_checkpoint` function from transformers but the function has been removed. Therefore, trying to use EETQ currently results in an import error. This fix results in EETQ tests being skipped if there is an import error.

The issue has been reported to EETQ:

https://github.com/NetEase-FuXi/EETQ/issues/34

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?",
34863,2024-11-21T18:24:53Z,2024-11-22T16:18:49Z,MekkCyber,1,0,1,2,1,1,1,[],1691.0,0,78836.0,0,0,0,0,2951646.89546,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34863). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM ! ,LGTM ! ,"# What does this PR do?
Simple fix  to fix the test for bitnet in the CI. Instead of using a gated repo we use a public one.

## Who can review ?
@SunMarc ",
34553,2024-11-01T00:11:26Z,2024-11-22T15:02:38Z,keyboardAnt,4,4,14,53,1,3,2,[],349947.0,0,1869916.0,0,0,0,0,2954175.453082,,2,14,0,False,"['ArthurZucker', 'keyboardAnt']","> Thank you for working on improving our tests 💛
> 
> A question: is this test somewhat fast to run (<5s)? If yes, amazing! If no, let's either a) reduce the number in `range` or b) tag the test as `@slow` [note: tests with `@slow` are usually run daily, so bad commits may squeeze in]

The test itself takes 1.89 s, and when you run it with pytest (`pytest tests/generation/test_utils.py::UtilsFunctionsTest::test_speculative_sampling_target_distribution`), it's not more than 2.57 s. Although it was only tested locally on my laptop, I believe it’s safe to keep it with the rest of the <5s tests. All checks have successfully passed (screenshot below). Are there any additional workflows to run before merging?

<img width=""916"" alt=""image"" src=""https://github.com/user-attachments/assets/6e4215ce-cdc9-4e91-aac0-74095fabf3c4""> @gante @ArthurZucker 
Would appreciate your help with finalizing the review Yep sorry for the delay, merging!Thank you for working on improving our tests 💛 

A question: is this test somewhat fast to run (<5s)? If yes, amazing! If no, let's either a) reduce the number in `range` or b) tag the test as `@slow` [note: tests with `@slow` are usually run daily, so bad commits may squeeze in] Thanks","Thank you for working on improving our tests 💛 

A question: is this test somewhat fast to run (<5s)? If yes, amazing! If no, let's either a) reduce the number in `range` or b) tag the test as `@slow` [note: tests with `@slow` are usually run daily, so bad commits may squeeze in] Thanks","### What does this PR do?
This PR introduces a test for speculative decoding to ensure the target distribution is preserved, addressing potential issues similar to #32867. The added test (`test_speculative_sampling_target_distribution`) validates that tokens are generated according to their intended likelihood, as defined in the logits, ensuring that the speculative decoding process adheres to expected distributions. Additionally, this is a foundational step toward supporting advanced speculative decoding algorithms, such as token-tree-based rejection sampling, which will enhance flexibility and performance in future implementations.

### Motivation and Context
The speculative decoding process has previously encountered issues where the target distribution was not preserved (e.g., in issues #32867 and #33534). This PR implements a test to safeguard against such inconsistencies by verifying that:
- The most likely tokens are chosen more frequently than less probable ones.
- Tokens are selected in alignment with the predefined candidate and new logits.

This enhancement not only improves the reliability of speculative sampling by enforcing distributional accuracy but also prepares the ground for implementing more advanced speculative decoding techniques, like token-trees-based sampling.

This PR is an initial step toward advancements in [Universal Assisted Generation](https://huggingface.co/blog/universal_assisted_generation). In collaboration with @Orenpereg, @danielkorat, @mosheber, @jmamou, and @MosheWasserb, we're preparing for a new speculative decoding function that this test will verify for losslessness in target distribution preservation.

### Dependencies
No additional dependencies are required.

### Linked Issues
#32867, #33534

### Before Submitting Checklist
- [x] I have read the [contributor guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request).
- [x] Documentation updates are not needed as this is a test enhancement.
- [x] New test coverage has been added to verify the speculative sampling behavior.

### Who can review?
@gante","Nit: let's make it in one line, so we can quickly compare indexes with other tensors.

(you'll have to remove the comma after the last `-inf`, otherwise the `make fixup` command will make it revert back to this format) I changed the formatting as requested, but `ruff`'s formatting check then failed the CI. (`make fixup` still reformats it into a column, even after removing the last comma you mentioned) you can use `# fmt: off` and `# fmt: on` Thanks @ArthurZucker. I changed all these inline comments to block comments, and it solved the issue while keeping the ruff checks on. 👍 "
34857,2024-11-21T14:15:38Z,2024-11-22T15:34:38Z,andimarafioti,5,6,2,40,1,3,2,"['Vision', 'Processing']",1794.0,0,91144.0,0,0,0,0,2954295.584349,,1,2,0,False,"['qubvel', 'mfarre', 'kashif', 'HuggingFaceDocBuilderDev', 'andimarafioti']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34857). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. thanks @andimarafioti  LGTM, I can load the dataset nicely now @andimarafioti LGTM:  I reran vlmevalkit with the current changes and I can confirm there is no regression To clarify, here I'm not changing where or if RGB conversion happens. Yeah, the idea is that we don’t actually need this code because the image will be converted to RGB. However, since maintaining this order is important for quality, it seems fine to me.Hi @andimarafioti! Thanks for the update, just a note regarding converting to RGB Agreed that RGB better be done if `do_convert_rbg` by updating the helper function from utils, otherwise LGTM","Hi @andimarafioti! Thanks for the update, just a note regarding converting to RGB Agreed that RGB better be done if `do_convert_rbg` by updating the helper function from utils, otherwise LGTM","# What does this PR do?

This PR improves a few smol issues we had with Idefics 3:

1) We couldn't use the model with images larger than 5*364. This was the default max_image_size. The method where this was computed took a parameter as input, but it was never used. It would also raise an error if we wanted to resize to a larger size. I changed this for a default value of 4k resolution, as this is already considerably larger than what we trained on, ie, anything larger is pretty outrageous.
2) We couldn't train with datasets that contained grayscale images since the input_data_format wasn't properly parsed. I fixed this by switching around the processing order. Now, if the images are grayscale, I add a channel to the end or start of the images. Then, the input_data_format can be correctly inferred if it is none. 
3) Finally, when converting to pil_image, we were not passing the input_data_format. For images that have 4 channels, this was breaking the processing. Since we already have the input_data_format in these functions, I added it. 

- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

- vision models: @qubvel

","There is a method that converts PIL images to RGB format (see `do_convert_rgb` param in other image processors). It's better to use it + extend to numpy arrays if needed. Yes I know, the order of the conversion here is actually important as RGB conversion before image_splitting was hurting the model performance quite a bit D:  Which is why I'm not converting to RGB until later in the pipeline > Yes I know, the order of the conversion here is actually important as RGB conversion before image_splitting was hurting the model performance quite a bit D:

haha, that's interesting, do you have any clarification for such a behavior? 😄  My observation is that the order here makes the resulting images are slightly different (if you just plot the differences you see large values on edges in the images). They look the same to me, but clearly not to the model since it was trained with a different pipeline that more closely resembles this.  Ok, got it, thanks for clarifying 🤗 "
34856,2024-11-21T14:07:31Z,2024-11-22T09:06:29Z,konradkalita,0,2,2,3,1,3,2,[],,0,68338.0,0,0,0,0,2977589.086802,,0,2,0,False,[],"Nice ! Could you add a test in the bnb tests to check if it works as expected ?  makes sense, thanks for updating","Nice ! Could you add a test in the bnb tests to check if it works as expected ?  makes sense, thanks for updating","# What does this PR do?

Fixes #34847


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

 @SunMarc
","Quantizer seems better place for it than PreTrainedModel.

Not sure if that's 100% exhaustive, but it was enough to solve my issue. Nice, sounds good to me ! "
34816,2024-11-19T17:15:49Z,2024-11-22T09:05:26Z,loadams,6,0,7,8,2,3,2,[],1835.0,0,229778.0,0,0,0,0,2977652.458499,,1,7,0,False,"['qubvel', 'HuggingFaceDocBuilderDev', 'pglorio', 'ArthurZucker', 'loadams']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34816). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks for this! I tried these fixes but I ran into this error:
```
ImportError: cannot import name 'Replicate' from 'torch.distributed.tensor' (/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/__init__.py)
```
when using torch 2.5.0, which was issued from [this line](https://github.com/huggingface/transformers/blob/e1f6ff7d9183e45246d08e07703465a9210191b0/src/transformers/pytorch_utils.py#L44). I was able to make it work with torch 2.5.1. @pglorio imports work fine for me with `torch==2.5.0` > Thanks, let's rather use `is_torch_greater_or_equal(""2.5"")` from
> 
> https://github.com/huggingface/transformers/blob/6aba68c0b9508b8c8fce025bd5492151258cd57a/src/transformers/utils/import_utils.py#L932-L938
> 
> And good to go!

Thanks @ArthurZucker - I've made that change but left the new function I added for checking torch>= 2.5, though it probably makes sense to refactor to not use those functions in the future.

The tests look to be unrelated failures though?

 > LGTM ! I reran the tests. These are unrelated to the PR

Thanks @SunMarc  THanks, merging!Thanks for catching 🤗  Thanks, let's rather use `is_torch_greater_or_equal(""2.5"")` from https://github.com/huggingface/transformers/blob/6aba68c0b9508b8c8fce025bd5492151258cd57a/src/transformers/utils/import_utils.py#L932-L938 And good to go! 
 LGTM ! I reran the tests. These are unrelated to the PR ","Thanks for catching 🤗  Thanks, let's rather use `is_torch_greater_or_equal(""2.5"")` from https://github.com/huggingface/transformers/blob/6aba68c0b9508b8c8fce025bd5492151258cd57a/src/transformers/utils/import_utils.py#L932-L938 And good to go! 
 LGTM ! I reran the tests. These are unrelated to the PR ","# What does this PR do?

Supplements #34800, the checks for torch.distributed.tensor require torch >= 2.5 as the comment states, this adds the check for it and updates the check.

Fixes #34795

## Who can review?

@ArthurZucker @muellerzr",
31272,2024-06-06T05:28:53Z,2024-06-06T11:41:32Z,statelesshz,1,0,1,4,1,1,1,[],25404.0,0,14611045.0,0,0,0,0,2981201.490508,,0,1,0,False,['ydshieh'],"Thanks for the fix. I am also always trapped by this .Thanks for fixing! 

I should have caught it in review too ","Thanks for fixing! 

I should have caught it in review too ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

I'm sorry for incorrectly using int type to set environment variables in the previous PR. This PR addresses the problem.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->

cc @ydshieh

",
32210,2024-07-25T03:13:28Z,2024-07-25T09:04:04Z,statelesshz,0,0,1,10,1,1,1,[],,0,10385567.0,0,0,0,0,2981205.234352,,0,1,0,False,[],Thanks for your PR @statelesshz ,Thanks for your PR @statelesshz ,"
# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

If I understand correctly, the minimum supported version of PyTorch for transformers is now 1.11.0. Therefore, this PR removes unnecessary guard code. 
The relevant PR is as follows：
- https://github.com/huggingface/transformers/pull/28207
- https://github.com/huggingface/transformers/pull/8979




## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->

cc @ydshieh
",
34849,2024-11-21T09:44:38Z,2024-11-22T07:25:14Z,zucchini-nlp,1,0,1,10,1,2,2,[],1636.0,0,78036.0,0,0,0,0,2983667.470676,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34849). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM!,LGTM!,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34630

cc @sumedhghaisas2 ",
34048,2024-10-09T18:34:25Z,2024-10-11T15:11:19Z,gante,2,5,6,774,25,4,2,[],84.0,0,3760662.0,0,0,0,0,2984454.880149,,0,6,0,False,['gante'],"@zucchini-nlp this PR may have a conflict with your encoder-decoder+compile PR 👀  Ran the following slow tests before merging:
- Llama 
- BART
- T5 (same failures as main)Thanks! I will update my PR when this one gets merged. Left a tiny question about Blip-2, overall LGTM as long as the tests don't complain 🧼 🧼 🧼 🧼  Very nice! Could you please reply? 
Much appreciated!","Thanks! I will update my PR when this one gets merged. Left a tiny question about Blip-2, overall LGTM as long as the tests don't complain 🧼 🧼 🧼 🧼  Very nice! Could you please reply? 
Much appreciated!","# What does this PR do?

Part of step 6 in https://github.com/huggingface/transformers/issues/32685
Follow-up to https://github.com/huggingface/transformers/pull/33870

This PR:
- Adds a minor change to `GenerationMixin.prepare_inputs_for_generation` to use `decoder_input_ids` in encoder-decoder models
- Deletes almost all `prepare_inputs_for_generation` in encoder-decoder llms 🔪 😎 ","is it okay we're losing this? Seems like BLIP was forcefully passing this kwarg for later setting the cache?

O think we don't have tests for BlipText, neither for VLM part so we can't rely on tests for BLIP 😭  (I'll work on it soon, rn I'm working on Idefics models and BLIP will be next) uhmm perhaps -- `is_decoder=True` is the default everywhere (in `forward`, in the config), but the user could force it to `False`. Going to revert

(I suspect this class is never used with `is_decoder=True`, but too late to fix that :D ) yeah, blip is a difficult case, better keep it overriden hehe Should I add this to my 
```AutoAdapterModel```
to generate in adapters using T5? If you mean the tests, you should not need to add it anywhere as it is ran only to test the correctness of new modifications. 

In general it is advised to post question [in the forum](https://discuss.huggingface.co/latest) if it is not a bug or feature request"
34134,2024-10-13T10:53:25Z,2024-11-20T10:32:07Z,tibor-reiss,4,4,5,19,2,3,2,[],165197.0,0,3399370.0,0,0,0,0,3027807.592644,,1,5,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'tibor-reiss']","cc @SunMarc for initial review The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34134). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Thanks for adding this @tibor-reiss! Can you also add a test so that this doesn't happen in the future ?

@SunMarc I was not sure where to add the test, so please feel free to move it around. Additionally, the TINY_* models are float32, so I went with the models from the issues' example (they are quite small) - let me know if there is a better way. Friendly ping @fxmarty-amdThanks for adding this @tibor-reiss! Can you also add a test so that this doesn't happen in the future ?  Thanks for iterating ! Left a few suggestions. Does this solve your issue @fxmarty-amd =) ?  Looks good! Sorry for being so late on my review and thanks for the contribution! 🤗 ",Thanks for adding this @tibor-reiss! Can you also add a test so that this doesn't happen in the future ?  Thanks for iterating ! Left a few suggestions. Does this solve your issue @fxmarty-amd =) ?  Looks good! Sorry for being so late on my review and thanks for the contribution! 🤗 ,"Fixes #34091

The models given in the issues' example have different torch_dtype which results in a different handling in `check_support_param_buffer_assignment`.
fxmarty/small-llama-testing: torch_dtype=float32
fxmarty/tiny-llama-fast-tokenizer: torch_dtype=float16


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@LysandreJik @fxmarty @muellerzr
","```suggestion
            self.assertTrue(is_on_meta(model_id, dtype))
``` Since we are loading the model on meta device, this test should be rather quick. Also it requires torch.  
```suggestion
    @require_torch
``` This was my first instinct as well, and it is definitely more readable. However, if there are multiple asserts which fail, this would only catch the first failure, thus making it necessary to iterate by running the test several times and fixing one by one. I have fixed thousands of tests, so I prefer the approach of one assert per test, but I can also live with proposed solution - so please feel free to keep the last commit or remove it :) Makes sense! Both sounds good to me! "
34642,2024-11-07T14:46:17Z,2024-11-20T17:02:58Z,corentin-ryr,2,4,2,28,2,5,4,[],941659.0,0,1208402.0,0,0,0,0,3044804.716884,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'corentin-ryr']","Hey,
Thanks for the review.

The tests in `tests/trainer/test_trainer.py` run without issues but I'm having trouble running the deepspeed tests (due to NCCL it seems). 

 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34642). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM ! Thanks for fixing this ! Just a nit. Could you try to see if the optuna tests from trainer are passing ? Would you like to have a look @sywangyi as you contributed to a lot of optuna ?  LGTM except for a reservation around the `free_memory`call Thanks for this solution!",LGTM ! Thanks for fixing this ! Just a nit. Could you try to see if the optuna tests from trainer are passing ? Would you like to have a look @sywangyi as you contributed to a lot of optuna ?  LGTM except for a reservation around the `free_memory`call Thanks for this solution!,"## Problem Overview

In a distributed DeepSpeed training setting, using Optuna for hyperparameter optimization (HPO) causes hyperparameter inconsistencies across processes. The root of the problem is that `trainer._hp_search_setup`, which applies the hyperparameters for each trial, only executes on the main process. This causes discrepancies in settings such as `gradient_accumulation_steps` across processes, leading to timeout errors due to process misalignment during training.

The current code broadcasts the trainer arguments and applies them to the auxiliary processes but doesn't execute the code to update the deepspeed/accelerate configuration. I also think that the current code would not account for hyperparameter used in the `model_init` on the auxiliary processes.

## Solution

To ensure consistent hyperparameter settings across all processes, the following changes were implemented:

1. **Broadcast a FixedTrial object**: After sampling a trial on the main process, we create a `FixedTrial` object on the main process and broadcast it to the other processes.
   
2. **Update `_report_to_hp_search`**: The `_report_to_hp_search` method was modified to not update the study in case we are on a process that got a FixedTrial (the FixedTrial doesn't have a reference to the study object which is necessary to report to Optuna).

## Implementation Details

**In `integration_utils.py`**:

After initializing the trial on the main process, we create a `FixedTrial` object with the same hyperparameters, we serialize it and broadcast it to all other processes. The train method is called with the trial argument.

On the main process we sample a trial (using the `hp_space` function) and create the `FixedTrial` object (lines [240](https://github.com/huggingface/transformers/blob/33868a057c02f0368ba63bd1edb746be38fe3d90/src/transformers/integrations/integration_utils.py#L240)):
```python
if trainer.args.world_size > 1:
    if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:
        raise RuntimeError(""only support DDP optuna HPO for ParallelMode.DISTRIBUTED currently."")
    trainer.hp_space(trial)
    fixed_trial = optuna.trial.FixedTrial(trial.params, trial.number)
    trial_main_rank_list = [fixed_trial]
    torch.distributed.broadcast_object_list(trial_main_rank_list, src=0)
    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)
```

On the auxiliary processes we retrieve the `FixedTrial` and call `train` by passing the trial as an argument (lines [269](https://github.com/huggingface/transformers/blob/33868a057c02f0368ba63bd1edb746be38fe3d90/src/transformers/integrations/integration_utils.py#L269)):
```python
for i in range(n_trials):
    trainer.objective = None
    trial_main_rank_list = [None]
    if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:
        raise RuntimeError(""only support DDP optuna HPO for ParallelMode.DISTRIBUTED currently."")
    torch.distributed.broadcast_object_list(trial_main_rank_list, src=0)
    trainer.train(resume_from_checkpoint=None, trial=trial_main_rank_list[0])
    # If there hasn't been any evaluation during the training loop.
    if getattr(trainer, ""objective"", None) is None:
        metrics = trainer.evaluate()
        trainer.objective = trainer.compute_objective(metrics)
```

**Other changes**:

The `_hp_search_setup` method now checks if it receives a dictionary of hyperparameters directly, allowing each process to configure itself independently (line [1751](https://github.com/huggingface/transformers/blob/33868a057c02f0368ba63bd1edb746be38fe3d90/src/transformers/trainer.py#L1750)):

```diff
- if not trial.study._is_multi_objective():
+ if hasattr(trial, ""study"") and not trial.study._is_multi_objective():
```

In the `hp_params` the check for the trial object class is broaden to `BaseTrial` (line [211](https://github.com/huggingface/transformers/blob/33868a057c02f0368ba63bd1edb746be38fe3d90/src/transformers/integrations/integration_utils.py#L211)):
```diff
- if isinstance(trial, optuna.Trial):
+ if isinstance(trial, optuna.trial.BaseTrial):
```

## Additional Notes

This change should ensures consistent hyperparameter application and prevents deadlocks during distributed training.

Let me know whether there might be a more efficient or cleaner way to handle the broadcast and reinitialization across processes. I think Marc Sun worked on the Optuna integration before.


","why have you added that here ?  I had an issue where the memory allocated by the Deepspeed engine was not properly released. I found that calling `engine.destroy()` fixed the problem partially (`accelerator.free_memory()` calls `engine.destroy()`).

At the moment, we still have memory leakage but I believe the issue is on Deepspeed side. I added some `del` statements in the `destroy` function of the deepspeed engine which solved the issue and I will try to propose a PR there.

I put the call to `free_memory` here because the engine is reset just after that (in `self.create_accelerator_and_postprocess()`). @muellerzr is this statement here safe? Yep it's safe and being used how it should. "
34805,2024-11-19T12:42:59Z,2024-11-21T13:46:35Z,jmamou,2,1,7,3,1,2,2,[],75556.0,0,178126.0,0,0,0,0,3045679.090329,,0,7,0,False,"['HuggingFaceDocBuilderDev', 'jmamou']","> @jmamou thanks for fixing! Quick clarification, why is there a `-1`, shouldn't `scores` reflect the length of generated tokens?

@zucchini-nlp 

`scores` contains also the additional token produced by the target model independently of the draft generation process as  explained [here](https://github.com/huggingface/transformers/blob/9470d6532436e9db2951a196effd6f8841befb76/src/transformers/generation/utils.py#L4253-L4255)  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34805). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.@jmamou thanks for fixing! Quick clarification, why is there a `-1`, shouldn't `scores` reflect the length of generated tokens? Ah, I see, thanks for explaining. Can be merged after core maintainer's review :) Cool! Thanks for fixing!","@jmamou thanks for fixing! Quick clarification, why is there a `-1`, shouldn't `scores` reflect the length of generated tokens? Ah, I see, thanks for explaining. Can be merged after core maintainer's review :) Cool! Thanks for fixing!","# What does this PR do?
Fix #34803

The number of matching tokens according to the draft vocabulary is missing in [candidate_generator.py](https://github.com/huggingface/transformers/blob/main/src/transformers/generation/candidate_generator.py). To fix the [issue](https://github.com/huggingface/transformers/issues/34803), we propose to modify [update_candidate_strategy](https://github.com/huggingface/transformers/blob/main/src/transformers/generation/candidate_generator.py#L254) by comparing `num_matches` (the number of matched tokens according to the **target** vocabulary) with `len(scores[0]) - 1` that represents the number of tokens generated by the **draft** model according to the **target** vocabulary.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
 @gante @ArthurZucker @zucchini-nlp ",nit: we can put comment on line above so it doesn't get formatted like this
34015,2024-10-07T16:58:39Z,2024-11-21T13:52:39Z,Cyrilvallez,1,3,9,709,3,1,2,[],1462.0,0,3876842.0,0,0,0,0,3046823.773419,,0,9,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34015). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Super nice! Just need clarification about magic comment! Thanks for removing the `# no unravel` a lot better IMO ,Super nice! Just need clarification about magic comment! Thanks for removing the `# no unravel` a lot better IMO ,"# What does this PR do?

Refactor StarCoder2 to remove all the copied-from and use modular instead.",what's a magic comment ? Let's use the nn.Module example as docstring example rather than x y z!  nice
32840,2024-08-16T08:02:05Z,2024-08-22T16:08:03Z,andimarafioti,2,0,1,3,1,1,1,['run-slow'],1149.0,0,8401672.0,0,0,0,0,3046988.705997,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'amyeroberts']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32840). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @andimarafioti Do you have permission to merge in `transformers`? If not I can merge for you Thanks for fixing! multi-gpu runner finally ran and all passing so we're good to merge :),Thanks for fixing! multi-gpu runner finally ran and all passing so we're good to merge :),"# What does this PR do?

Fixes #32288


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Models:

- vision models: @amyeroberts
",
34444,2024-10-27T08:59:05Z,2024-11-21T12:52:22Z,eljandoubi,9,2,14,26,1,3,2,[],206931.0,0,2173998.0,0,0,0,0,3050444.171168,,1,14,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'eljandoubi']","@SunMarc @muellerzr Do you have any feedback? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34444). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. pinging @muellerzr ! Hey @SunMarc. When using NVMe offloading, recursively calling `deepspeed.zero.Init` causes a pinning error.  Because Transformers and accelerate do not support multiple models, `deepspeed.zero.Init` should be called at most once per process. Thanks for the additional context ! I think that we do support multiple models with deepspeed https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed_multiple_model . cc @muellerzr  Anyway, we might use a context manager solution to prevent embedded calls to DeepSpeed.zero.Init. @ArthurZucker @SunMarc @muellerzr Is there any update on the pull request? @SunMarc can you underscore them?  Just missing `make style`Hey @eljandoubi , thanks for the contribution. Could you explain why this is needed ? Thanks !  LGTM ! I think you pushed some unrelated changes. Could you revert those ?  Thanks, let's revert notebook changes, and add a one liner about what we are adding!  Thanks! Same note as Arthur, otherwise LG2M! Thanks 🤗 ","Hey @eljandoubi , thanks for the contribution. Could you explain why this is needed ? Thanks !  LGTM ! I think you pushed some unrelated changes. Could you revert those ?  Thanks, let's revert notebook changes, and add a one liner about what we are adding!  Thanks! Same note as Arthur, otherwise LG2M! Thanks 🤗 ","# What does this PR do?

Fixes #34429 

## Who can review?

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
",this and all ipynb are unrelated can we have a small comment documentation about that (explaning the issue we fixed with this basically) 😉 
32473,2024-08-06T15:30:35Z,2024-09-25T19:28:49Z,andimarafioti,30,30,53,4484,27,5,0,['run-slow'],1184.0,1,9238761.0,0,0,0,0,3046991.79686,,0,53,0,False,"['amyeroberts', 'HuggingFaceDocBuilderDev', 'efenocchi', 'andimarafioti', 'Quinn-Meyer-Sustainment', 'joris-sense', 'woodfrog']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32473). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. updated main @andimarafioti I can see that you re-requested review, but there's still some debugging commits being pushed so will hold of reviewing until this has been resolved. I'm going to unsubscribe to prevent getting notifications for every push - as soon as you have a question or want to let me know it's ready, just ping me with `@amyeroberts` and I'll get a notification :)  @amyeroberts ready to review! There is still the multi-gpu tests that is queued but if those fail I would skip them. They run OOM in the CI on single GPU and there is an issue open for the same on idefics2 https://github.com/huggingface/transformers/issues/32288. If my fix here works, I already opened a PR for that fix as well: https://github.com/huggingface/transformers/pull/32840  Talked to @molbap and he said that there is an issue with the multi-gpu workers getting stuck in the queue. But it's not related to this PR.  @andimarafioti Yes, unfortunately we're having issues at the moment with single GPU and multi-GPU runners taking a long time to run / never running cc @ydshieh. 

It looks like the multi GPU tests did eventually run and there's at least one test which is currently failing to be addressed  rebased on main Hi, I am trying to run this together with structured generation with outlines (which supports Vision Transformers in a package not yet in PyPI) and am having trouble with that. Crossposting this as an issue in Outlines [here](https://github.com/outlines-dev/outlines/issues/1123) as I am not sure who is supposed to change the code. I try the code below and get the error below that. With some print statements, I found that the ""<"" in ""Got <."" in the error message is actually the first character of the prompt ""<image> detailed description:"", which means that each character in the prompt list gets misinterpreted as an image URL.

Roughly following the instructions in the doc [here](https://outlines-dev.github.io/outlines/reference/models/transformers_vision/) (using `load_image` rather than the `image_from_url` as defined here), I am trying the following code:

```
import outlines
from transformers import Idefics3ForConditionalGeneration

model = outlines.models.transformers_vision(
    ""HuggingFaceM4/Idefics3-8B-Llama3"",
    model_class=Idefics3ForConditionalGeneration,
    device=""cuda"",
)

from transformers.image_utils import load_image

description_generator = outlines.generate.text(model)
description_generator(
    [""<image> detailed description:""],
    [[load_image(""https://upload.wikimedia.org/wikipedia/commons/2/25/Siam_lilacpoint.jpg"")]]
)
```
I get the following error:
```
---------------------------------------------------------------------------
UnidentifiedImageError                    Traceback (most recent call last)
File /opt/conda/lib/python3.10/site-packages/transformers/image_utils.py:372, in load_image(image, timeout)
    371     b64 = base64.decodebytes(image.encode())
--> 372     image = PIL.Image.open(BytesIO(b64))
    373 except Exception as e:

File /opt/conda/lib/python3.10/site-packages/PIL/Image.py:3309, in open(fp, mode, formats)
   3308 msg = ""cannot identify image file %r"" % (filename if filename else fp)
-> 3309 raise UnidentifiedImageError(msg)

UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f2d007367f0>

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
Cell In[9], line 4
      1 from transformers.image_utils import load_image
      3 description_generator = outlines.generate.text(model)
----> 4 description_generator(
      5     [""<image> detailed description:""],
      6     [[load_image(""https://upload.wikimedia.org/wikipedia/commons/2/25/Siam_lilacpoint.jpg"")]]
      7 )

File /opt/conda/lib/python3.10/site-packages/outlines/generate/api.py:555, in VisionSequenceGeneratorAdapter.__call__(self, prompts, media, max_tokens, stop_at, seed, **model_specific_params)
    549 prompts, media = self._validate_prompt_media_types(prompts, media)
    551 generation_params = self.prepare_generation_parameters(
    552     max_tokens, stop_at, seed
    553 )
--> 555 completions = self.model.generate(
    556     prompts,
    557     media,
    558     generation_params,
    559     self.logits_processor,
    560     self.sampling_params,
    561     **model_specific_params,
    562 )
    564 return self._format(completions)

File /opt/conda/lib/python3.10/site-packages/outlines/models/transformers_vision.py:46, in TransformersVision.generate(self, prompts, media, generation_parameters, logits_processor, sampling_parameters)
     15 def generate(  # type: ignore
     16     self,
     17     prompts: Union[str, List[str]],
   (...)
     21     sampling_parameters: SamplingParameters,
     22 ) -> Union[str, List[str], List[List[str]]]:
     23     """"""Generate text using `transformers`.
     24 
     25     Arguments
   (...)
     44     The generated text
     45     """"""
---> 46     inputs = self.processor(prompts, media, padding=True, return_tensors=""pt"").to(
     47         self.model.device
     48     )
     50     generation_kwargs = self._get_generation_kwargs(
     51         prompts,
     52         generation_parameters,
     53         logits_processor,
     54         sampling_parameters,
     55     )
     56     generated_ids = self._generate_output_seq(prompts, inputs, **generation_kwargs)

File /opt/conda/lib/python3.10/site-packages/transformers/models/idefics3/processing_idefics3.py:288, in Idefics3Processor.__call__(self, images, text, audio, videos, **kwargs)
    286             new_images[-1].append(im)  # already loaded
    287         elif isinstance(im, str):
--> 288             new_images[-1].append(load_image(im))
    290 images = new_images
    291 del new_images

File /opt/conda/lib/python3.10/site-packages/transformers/image_utils.py:374, in load_image(image, timeout)
    372             image = PIL.Image.open(BytesIO(b64))
    373         except Exception as e:
--> 374             raise ValueError(
    375                 f""Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got {image}. Failed with {e}""
    376             )
    377 elif isinstance(image, PIL.Image.Image):
    378     image = image

ValueError: Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got <. Failed with cannot identify image file <_io.BytesIO object at 0x7f2d007367f0>
``` I tracked down that my issue is due to swapping the order of images and text, see [here](https://github.com/huggingface/transformers/pull/32473/files#r1734779753). Swapping them back in the code makes it work for me, at least when passing a list of prompts and a list of lists of images. @amyeroberts I'm back from your review :)  @amyeroberts ready for re-review  Hi @andimarafioti @amyeroberts,
When I load the processor with `processor.image_processor.do_image_splitting = False`, I encounter an error in the `pixel_shuffle` function located in `transformers/src/transformers/models/idefics3/modeling_idefics3.py`. 
The error message states: ""shape '[1, 90, 90, 1152]' is invalid for input of size 9345024"" (line 581).

If I use a square image it breaks in `def input_merger()` in `modeling_idefics3.py` with a `shape mismatch` error.

Could you please advise on how to resolve this bug?
 I get the same error as @efenocchi when I use `image_splitting=False` in the Idefics3 processor.

When I set `image_splitting=True` I get the following error during accelerated fine-tuning:

```  
File ""/home/ec2-user/SageMaker/LLMs/VLM_fine-tuning/Idefics3/finetune.py"", line 135, in train
    self.trainer.train()
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py"", line 1955, in train
    return inner_training_loop(
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py"", line 2296, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py"", line 3380, in training_step
    loss = self.compute_loss(model, inputs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py"", line 3427, in compute_loss
    outputs = model(**inputs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/distributed.py"", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/distributed.py"", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/utils/operations.py"", line 820, in forward
    return model_forward(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/utils/operations.py"", line 808, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/amp/autocast_mode.py"", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/hooks.py"", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/idefics3/modeling_idefics3.py"", line 1180, in forward
    outputs = self.model(
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/hooks.py"", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/idefics3/modeling_idefics3.py"", line 1004, in forward
    inputs_embeds = self.inputs_merger(
  File ""/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/idefics3/modeling_idefics3.py"", line 896, in inputs_merger
    inputs_embeds[special_image_token_mask] = reshaped_image_hidden_states
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
```

Which could be related to #31373  Edited for formatting & context. The second one is probably why there was that input_embeds.clone 💡 I was asked and wasn't sure why that was there so I removed it, but the error certainly points towards that @Quinn-Meyer-Sustainment , the error you showed should be fixed with my last commit 🙏  Thank you @andimarafioti for the super quick fix! Commenting to confirm that I can now finetune without error😊 @andimarafioti any suggestions for the error I reported? Hi @efenocchi , can you provide more information on your bug? A reproducible snippet would be ideal.
There is a test for not splitting the images which passes, so I'm wondering if the test is not catching your situation or if something else is happening.  @amyeroberts back :)  @andimarafioti  I can reproduce the same error reported by @efenocchi by setting:

```
self.processor = AutoProcessor.from_pretrained(""HuggingFaceM4/Idefics3-8B-Llama3"", size= {""longest_edge"": 2*364}, do_image_splitting = False)` 
```
and then run the pretrained model. I guess this is not the right way to disable image splitting when running the pretrained model? 

Since my inputs contain multiple images, I basically want to disable the image splitting and also reduce the size of the image (not using the default 4x364). What is the right way to do here? Thanks! I rebased on main and a test from the chat_template is failing. I'm investigating it.  There's a PR already addressing the test that's failing here: https://github.com/huggingface/transformers/pull/33462
ie, it's not related to my code Investigating @woodfrog s report now I can't reproduce your error @woodfrog . Can you provide more details? This is my snippet that works:

```python
from io import BytesIO

import requests
from PIL import Image

from transformers.models.auto.processing_auto import AutoProcessor


processor = AutoProcessor.from_pretrained(
    ""HuggingFaceM4/Idefics3-8B-Llama3"", size={""longest_edge"": 2 * 364}, do_image_splitting=False
)

image1 = Image.open(
    BytesIO(
        requests.get(
            ""https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg""
        ).content
    )
)
inputs = processor(images=image1)

print(len(inputs[""pixel_values""]))
# 1
print(len(inputs[""pixel_values""][0]))
# 1
print(inputs[""pixel_values""][0][0].shape)
# (3, 486, 728)
``` @andimarafioti  Thank you for investigating!

I followed the docs [here](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) while changing the `size` and `do_image_splitting` of the processor.

```python
import torch

from transformers.image_utils import load_image
from transformers import AutoProcessor, AutoModelForVision2Seq


DEVICE = ""cuda:0""

processor = AutoProcessor.from_pretrained(
    ""HuggingFaceM4/Idefics3-8B-Llama3"", size={""longest_edge"": 2 * 364}, do_image_splitting=False
)

model = AutoModelForVision2Seq.from_pretrained(
    ""HuggingFaceM4/Idefics3-8B-Llama3"", torch_dtype=torch.bfloat16
).to(DEVICE)

image1 = load_image(""https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"")
image2 = load_image(""https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg"")
image3 = load_image(""https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg"")

messages = [
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image""},
            {""type"": ""text"", ""text"": ""What do we see in this image?""},
        ]
    },
    {
        ""role"": ""assistant"",
        ""content"": [
            {""type"": ""text"", ""text"": ""In this image, we can see the city of New York, and more specifically the Statue of Liberty.""},
        ]
    },
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image""},
            {""type"": ""text"", ""text"": ""And how about this image?""},
        ]
    },       
]
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, images=[image1, image2], return_tensors=""pt"")
inputs = {k: v.to(DEVICE) for k, v in inputs.items()}


# Generate
generated_ids = model.generate(**inputs, max_new_tokens=500)
generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)


```

And I got:

```
transformers/models/idefics3/modeling_idefics3.py"", line 581, in pixel_shuffle
    x = x.view(bsz, height, width, embed_dim)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: shape '[2, 45, 45, 1152]' is invalid for input of size 4792320
```

(Maybe this is not the correct way to change the image resolution and the splitting setting when running the pretrained model. Could you advise on the right solution?) Okay, now I see the issue. The vision encoder expects square images, and when you don't do the image splitting, the images are not squared, leading to reshaping issues. I'm pushing a change that will fix that and I adapted the tests.

Beware that this will reshape your images to 364x364, since that is the `max_image_size`. If you want to have the images be 728x728, you will need to make more changes.

The amount of tokens that you pass to the language model is determined by the size of the output of the vision encoder. If you feed the vision encoder larger images, you get more tokens. That relationship is determined by the formula:
`int(((image_size // patch_size) ** 2) / (scale_factor**2))`
where `scale_factor` is by default `2` and the `patch_size` for this vision encoder is `14`. Since you are passing `image_size=2*364`, the formula gives: `image_seq_len=676`.

Your code would then be:

```python
import torch

from transformers.image_utils import load_image
from transformers import AutoProcessor, AutoModelForVision2Seq


DEVICE = ""cuda:0""

processor = AutoProcessor.from_pretrained(
    ""HuggingFaceM4/Idefics3-8B-Llama3"", size={""longest_edge"": 2 * 364}, max_image_size={""longest_edge"": 2 * 364}, image_seq_len=676, do_image_splitting=False
)

model = AutoModelForVision2Seq.from_pretrained(
    ""HuggingFaceM4/Idefics3-8B-Llama3"", torch_dtype=torch.bfloat16
).to(DEVICE)

image1 = load_image(""https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"")
image2 = load_image(""https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg"")
image3 = load_image(""https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg"")

messages = [
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image""},
            {""type"": ""text"", ""text"": ""What do we see in this image?""},
        ]
    },
    {
        ""role"": ""assistant"",
        ""content"": [
            {""type"": ""text"", ""text"": ""In this image, we can see the city of New York, and more specifically the Statue of Liberty.""},
        ]
    },
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image""},
            {""type"": ""text"", ""text"": ""And how about this image?""},
        ]
    },       
]

prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, images=[image1, image2], return_tensors=""pt"")
inputs = {k: v.to(DEVICE) for k, v in inputs.items()}


# Generate
generated_ids = model.generate(**inputs, max_new_tokens=500)
generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)
```
 It works now :) Appreciate the formula. Thank you for fixing it so quickly! 

Btw, I found that I have to specify CUDA_VISIBLE_DEVICES=X to use a single GPU. Otherwise, there will be a device error. I roughly went through the discussions above and suppose you have been aware of this. One question on the formula: Both // and ``int()`` round down, not up, is this intended? E.g. when image_size <
patch_size, the formula implies that 0 tokens are being passed. @joris-sense , yes if `image_size < patch_size` then it would be zero. The patches for our vision encoder are `14x14`, so an image smaller than that would be rare. But in practice, it would probably still output one latent value, so a `max(formula, 1)` would make sense there. The rest of the rounding is intended and related to how the vision encoder processes the image and how the `pixel_shuffle` works. 

@woodfrog , I wasn't aware of that, but @merveenoyan showed me some errors she had when trying to do multi-gpu training on a computer with too many gpus. I'm testing this with a single GPU and it works well, or with several GPUs but using all of them. Do you only have one GPU? A bit more info would also help here (your setup, a reproducible script).

You're welcome for the quick fix. Thank you for flagging it! I don't think I would have noticed this as I would always split the images (the model was trained this way, so it works better if you keep it) @andimarafioti  I'm using an 8-GPU machine. I have a complex query pipeline and sometimes have ~10 images (so it's not easy to put a concise snippet here). It works well if I run it with CUDA_VISIBLE_DEVICES=0, but got the error below if not:

```
...
...
transformers/models/idefics3/modeling_idefics3.py"", line 957, in forward
    inputs_embeds = self.text_model.get_input_embeddings()(input_ids).to(self.device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
...
...
...
torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)
```

Thank you for sharing the training details! I have to do this for some multi-image queries; otherwise, the number of image tokens explodes after the splitting. Do you have recommendations for multi-image use cases (say we can have 1-10 images)?  I believe the current processing pipeline doesn't support dynamic image processing, so the safe way for me right now is to disable the image splitting.
just have a few comments regarding consistency with other VLMs like Chameleon + nits Yaaay, Idefics3 is here! 

I just left a few comments to make the ongoing work on unifying a bit VLMs easier for us, but didn't really review the whole PR Thanks for all the work adding this model! 

The main comment is for the tests to be properly aligned with the new model behaviours, in particular the processor and image processor. 

Some general nits for the modeling file - the main one being all classes and method which come from idefics2 should have `# Copied from` comments. 

Before the PR can be merged, all slow tests will need to be run & pass. These should be triggered in subsequent commits (I might need to approve the workflow for them to run) ","just have a few comments regarding consistency with other VLMs like Chameleon + nits Yaaay, Idefics3 is here! 

I just left a few comments to make the ongoing work on unifying a bit VLMs easier for us, but didn't really review the whole PR Thanks for all the work adding this model! 

The main comment is for the tests to be properly aligned with the new model behaviours, in particular the processor and image processor. 

Some general nits for the modeling file - the main one being all classes and method which come from idefics2 should have `# Copied from` comments. 

Before the PR can be merged, all slow tests will need to be run & pass. These should be triggered in subsequent commits (I might need to approve the workflow for them to run) ","# What does this PR do?
Adding the Idefics 3 model.

There are still a few things to do before merging this PR. The results are not exactly the same as with our codebase and the tests are not done. We are opening it to unblock our release. 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Models:

- vision models: @amyeroberts
","We're currently working on uniformizing processor kwargs. We should do that here too. See: https://github.com/huggingface/transformers/issues/31911 [nit] I'm just curious: why not add this to the tokenizer/processor's config's `.chat_template` field instead? tests missing Chameleon uses `resolution` instead of this. ~We should stick to one of them (i.e. either rename this or rename the one in Chameleon) cc @zucchini-nlp~ nvm, it seems like all other VLMs use `image_size` lol. @zucchini-nlp what do you think of renaming `resolution` to `image_size` in Chameleon? IMO, it'd be better to reuse some of the reserved tokens instead like how we do it with Chameleon

that way, (1) we could add the token ids to the configs, making it easier for external devs to know which tokens are used for which, (2) it'd be easier to implement the VLMTokenizerMixin later on cc @zucchini-nlp , and (3) we won't have to add special tokens, expand the embeddings, etc might be better to move these to the conversion script (so they'd be saved to the tokenizer/processor configs) btw, in Chameleon, we use `image_start_token` & `image_end_token` (and `boi_token_id` & `eoi_token_id` for the token ids) This needs to be filled in  Why specify 4*364 here and not just the number? 

This value is also very large - it would be better to make this smaller for lighter tests  Same here - this value is large for tests AFAICT this is used to configure the vision tower, so should be in the vision config This isn't right as it means the config arguments are inconsistently applied. In fact, if vocab_size, max_position_embeddings, pad_token_id are just used for the language model, then they should be hard-coded here as defaults, and otherwise passed directly by the user through `text_config`  This isn't an input arg nor is this  Same here I realise this is an overhang from my PR - this docstring isn't quite right - it's not necessarily into 4 subimages  Can we use copied from here? This should include the standard caveat note about note being used by all models This isn't a great demo showing the repeated phrase for the first sample :/  
 ```suggestion
``` Ironically this is coming from idefics2 :) Most of the methods here are the same - so the class should have a `copied from` header  At inference time, I'm encountering an issue here when height and width are not the same. Can those parameters be passed in as parameters to forward? Is it possible to rely on `text_config.vocab_size`? We deprecated `vocab_size` in all VLMs and are trying to rely on `text_config`? We are also in the process of refactoring `attn_impl` for composite models. Would be nice to add the Vision and Perceiver models as a PretrainedModel with their own attr for `supports_sdpa/flash_attn`. That way we can dispatch attn on each PretrainedModel, with different attn (text=sdpa, vision=flash) this should be handled in the general modeling, iirc we added VLM support after deprecating `config.vocab_size` We don't have to support old-style cache for new models, can go directly with DynamicCache. Finally deprecated tuple cache in all decoder-only models, yay :)

And btw, past-cache-length should be obtained from `cache_position`, afaik we'll stop using the `get_seq_length()` some time in  the future Hmm, can go llama-style and use `cache_position`? should not be needed if past-key-values are always initialized with a `Cache` object Also can be handled via general modeling, let me know if it doesn't. I want VLMs to follow similar format and add an extra check in general scripts, rather than overriding in each modeling file Yeah, I guess we can try to unify these special tokens after adding a MultiModalTokenizer wrapper :)"
34515,2024-10-30T16:33:12Z,2024-11-21T12:40:49Z,VladOS95-cyber,7,10,7,336,1,4,3,[],196.0,0,1886857.0,0,0,0,0,3051139.161426,,0,7,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'VladOS95-cyber']","Hey @SunMarc! I made some refactoring changes regarding tensor GGUF processing. Please, take a look. What do you think? Hey @SunMarc! Just a kind ping, does it make sense to have these changes? Hey @SunMarc , @Isotr0py! I made some refactoring based on your comments, please, take a look again. Hey @SunMarc! Do you have any further comments or it looks good to you? Hey @SunMarc! Just a kind reminder in order to not loose this PR. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34515). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Feel free to merge when happy @SunMarc Thanks for making tensor processing cleaner. I think it makes sense to do that as it will be easier to read the code. Left a few minor comments. Can you have a second look @Isotr0py ?  Overall LGTM! Thanks for improving this! Thanks for the PR @VladOS95-cyber as always ! We were away for a week, sorry for the delay. LGTM, just a nit I very much like this split! Thanks @VladOS95-cyber!","Thanks for making tensor processing cleaner. I think it makes sense to do that as it will be easier to read the code. Left a few minor comments. Can you have a second look @Isotr0py ?  Overall LGTM! Thanks for improving this! Thanks for the PR @VladOS95-cyber as always ! We were away for a week, sorry for the delay. LGTM, just a nit I very much like this split! Thanks @VladOS95-cyber!","# What does this PR do?

Adding more and more architectures with custom weights conversion is making tensor processing system bigger, less flexible and readable. 
Improve GGUF tensor processing system in modeling_gguf_pytorch_utils.py by adding TensorProcessor class for general cases without custom weights changes and all necessary separated tensor processor classes based on architecture with custom process logic.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Link: https://github.com/huggingface/transformers/issues/33260
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
Regarding the task @SunMarc @LysandreJik @ArthurZucker .

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","metadata doesn't always contain `bid`
```suggestion
            bid = result[""metadata""].get(""bid"", None)
``` `parsed_parameters` is also modified in some processors. Since we are returning weights, name and bid, let's also do it for parsed_parameters hey, we don't have to explicitly specify None as get() method from dictionary returns None by default if key does not exist Indeed, I forgot that it was a dict !  Can we initialize `TensorProcessor` class with config? So that we can focus on weights processing without considering config inputs. Otherwise we need to parse config to get `num_heads` etc when processing each tensor. How about wrapping `{""weights"": weights, ""name"": name, ""metadata"": {}}` with a namedtuple `GGUFTensor`?
```python
class GGUFTensor(NamedTuple):
    weights: torch.Tensor
    name: str
    metadata: dict
``` Agree, good suggestion yes, sure, fixed I suppose we do not need to return parsed_parameters because we just modify existing dictionary and if we do it during processing, we skip iteration anyway by the line 
```
if result[""name""] is None:
                continue
```
in order to avoid double dictionary calling and value changing. So, i think we do not need to return it as we basically do not plan to use it later or modify somehow.  maybe add a comment above to say that you modified name to None in order to skip the processing done below as we already did it above with `processor.process`.  Or we can refactor a bit to also include what we have below to be included in `processor.process`"
34725,2024-11-14T04:26:47Z,2024-11-21T10:37:34Z,farrosalferro,2,6,4,68,3,4,3,[],329743.0,0,627047.0,0,0,0,0,3058535.499366,,0,4,0,False,"['farrosalferro', 'HuggingFaceDocBuilderDev']","Could you please take a look at this PR? @SunMarc @lyaronskaya @ArthurZucker .
Thank you! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34725). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice PR ! Thanks for adding this @farrosalferro !  Awesome, thank you @farrosalferro!","Nice PR ! Thanks for adding this @farrosalferro !  Awesome, thank you @farrosalferro!","# What does this PR do?
Add Nemotron GGUF loading support

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Link: [Community contribution: Adding GGUF support for more architectures #33260](https://github.com/huggingface/transformers/issues/33260)
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [X] Did you write any new necessary tests?


## Who can review?
@SunMarc @LysandreJik @ArthurZucker 
","Why you explicitly assign architecture to updated one if it is the same? I'm sorry if my answer seems obvious, but isn't it for addressing cases where the ""architecture"" does not only contain ""nemotron""? I took reference on what you did for the qwen2moe, so I think It's better to also do it for nemotron. But I tested it without these lines and it passes through. What do you think? And thank you for reviewing! As this is my first time contributing, please let me know if anything seems odds or is there any better implementation. Thank you! No, for qwen2moe, I explicitly assigned another architecture name, because gguf file contains qwen2moe, but later, execution chain expects to get **qwen2_moe** for config, model processing and so on. You provided the same name ""nemotron"". So, there is no reason to explicitly assign updated architecture to **the same name** and even to mention nemotron, because gguf processing takes it from config by default. I'm sorry for misreading the code and thank you for pointing it out! I have deleted the unnecessary lines in the new commit. Please let me know if there is something needs to be fixed. No worries, other changes is fine from my perspective Thank you for reviewing!"
34810,2024-11-19T14:41:45Z,2024-11-21T10:37:03Z,qgallouedec,2,1,2,2,1,3,3,[],1638.0,0,158119.0,0,0,0,0,3058565.866631,,0,2,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34810). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Unrelated errors, mergingLGTM ! cc @muellerzr for a second look  Agree with Marc, this change seems fine. cc @LysandreJik for final :) Sounds good to me, thanks all!","LGTM ! cc @muellerzr for a second look  Agree with Marc, this change seems fine. cc @LysandreJik for final :) Sounds good to me, thanks all!","# What does this PR do?

Passing `max_step` is a supported feature, and therefore should not raise a warning. I suggest lowering the level to info.

Alternatively, we could have an additional parameter like `ignore_max_step_warning` but this seems overengineered.

Considering that the use of `max_steps` is already documented:

https://github.com/huggingface/transformers/blob/ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee/src/transformers/training_args.py#L307

lowering the level to info seems to be the best solution.


Technically, we can ignore this warnings with 

```python
TrainingArguments(
    ...,
    max_steps=100,
    num_train_epochs=-1
)
```

But it's not intuitive at all IMO.

---

As for warnings, I'd say that a good practice is that they should always be removable by the user:

- either by modifying their code, in the event of a feature being misused, deprecated, etc. E.g example:
    https://github.com/huggingface/transformers/blob/ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee/src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py#L235-L238

- or by acknowledging, in the case of very important information upgraded to warning; we add a parameter for acknowledging. E.g:
example: `allow_missing_key` in `load_pytorch_checkpoint_in_tf2_model`

Related: https://github.com/huggingface/trl/pull/2350

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review?

@muellerzr and @SunMarc

",I don't mind switching to info since we already have documented that setting `max_steps` will overwrite `num_train_epochs`. Another compromise would be to indicate to the user to pass `num_train_epochs = -1` in order to remove the warning but then the code becomes bloated for nothing. 
34484,2024-10-29T08:44:29Z,2024-11-21T10:00:22Z,zucchini-nlp,2,0,5,138,6,2,2,[],2311.0,0,1991753.0,0,0,0,0,3060768.216699,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'gante']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34484). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Plus BLIP doesn't seem to be commonly used anymore when we have better VLMs so prob it doesn't worth spending time refactoring the code

Yeah, I have this impression as well. Let's hold on the test refactor for BLIP, unless we start getting issues :)LGTM, thank you for fixing 🙌  Thanks 🤗 ","LGTM, thank you for fixing 🙌  Thanks 🤗 ","# What does this PR do?

Last 3 VLMs that had no generation tests now support it. After this PR and https://github.com/huggingface/transformers/pull/34174/files is merged we'll have only one model (BLIP) which has no generation tests.

I see only one way of making it, by overwriting all tests for generation in BLIP, The reason is because BLIP adds 'eos' token while tokenizing and then in `self.generate()` removes the last token manually by cropping input ids. Thus we never can check the length of output sequences correctly. 

We could make tokenizer remove 'eos' by post processing outputs in processor code, but that seems too much BC breaking so I am not sure yet if we want it. Plus BLIP doesn't seem to be commonly used anymore when we have better VLMs so prob it doesn't worth spending time refactoring the code",
34833,2024-11-20T17:41:52Z,2024-11-20T20:36:14Z,ydshieh,1,2,5,30,2,2,1,[],2006.0,0,10463.0,0,0,0,0,3109016.774193,,0,5,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34833). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.TY! ,TY! ,"# What does this PR do?

#34812 causes the slack report fail to be sent.

This PR fixes this issue (a bit hacky)","currently, what is shown in the (final) `short test summary info` is something like

> /transformers/src/transformers/testing_utils.py:2420: ==== test session starts ====

which is not very informative. 

And for the same test, we get two `FAILED ` in the log: one from the `subprocess` and another one is the original pytest process. This causes slack CI report script fails. ah got it thanks for explaining "
34832,2024-11-20T16:48:41Z,2024-11-20T19:28:51Z,SunMarc,1,0,2,20,2,1,1,[],1664.0,0,9612.0,0,0,0,0,3113059.006099,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34832). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?
Fixes the torchao tests so that we don't get an error on `tests_hub` CI. The issue was that we were running `TorchAoConfig` when importing the test class. However, since we don't have torchao installed, it raised an error. 

Also, I added more check in torchaoconfig to help the user. ",
33965,2024-10-04T20:55:05Z,2024-10-07T12:45:57Z,ArthurZucker,4,0,5,84,9,0,0,[],1508.0,0,4051423.0,0,0,0,0,3117264.26956,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'gante', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33965). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > I think I am a bit tired 😅 

🫂  cc @xenova as you broke them 💞  Related to #33958",,"# What does this PR do?
I think I am a bit tired 😅 fixes  #33929 again after #33226 was merged.
```python
 RUN_SLOW=1 pytest tests/models/siglip -k test_inference_interpolate_pos_encoding                                              20s py312 22:53:02
=============================================================================== test session starts ===============================================================================
platform darwin -- Python 3.12.3, pytest-8.2.2, pluggy-1.5.0
rootdir: /Users/arthurzucker/Work/transformers
configfile: pyproject.toml
collected 606 items / 605 deselected / 1 selected                                                                                                                                 

tests/models/siglip/test_modeling_siglip.py .                                                                                                                               [100%]

================================================================================ warnings summary =================================================================================
src/transformers/deepspeed.py:24
  /Users/arthurzucker/Work/transformers/src/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
    warnings.warn(

../../.pyenv/versions/3.12.3/envs/py312/lib/python3.12/site-packages/google/protobuf/internal/well_known_types.py:91
  /Users/arthurzucker/.pyenv/versions/3.12.3/envs/py312/lib/python3.12/site-packages/google/protobuf/internal/well_known_types.py:91: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).
    _EPOCH_DATETIME_NAIVE = datetime.datetime.utcfromtimestamp(0)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================================================== 1 passed, 605 deselected, 2 warnings in 5.02s
``` ",
34813,2024-11-19T16:34:02Z,2024-11-20T10:25:38Z,kjohew,0,0,1,2,1,2,2,[],,0,64296.0,0,0,0,0,3145656.212611,,0,1,0,False,[],"Thanks! Failing tests are not related, re-triggered them in case it was flaky Thanks 🤗 ","Thanks! Failing tests are not related, re-triggered them in case it was flaky Thanks 🤗 ","# What does this PR do?

As discussed in [#34796](https://github.com/huggingface/transformers/issues/34796), logits should be sliced before being copied to avoid unnecessary memory usage, especially during the first call to generate.

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
@zucchini-nlp
",
34103,2024-10-12T00:35:02Z,2024-11-20T10:31:21Z,philkuz,15,4,7,92,5,4,3,"['Vision', 'run-slow', 'ExecuTorch']",1550208.0,0,3405398.0,0,0,0,0,3145294.368127,,0,7,0,False,"['qubvel', 'guangy10', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'philkuz']","> Very nice, thanks for unlocking more models for torch export, this is very valuable!
> 
> The same comment as for Mask2Former PR, would be great to have this PR tested, and please push run-slow commit to trigger all tests at the end!

I've added Depth-anything to this PR, I'm not entirely sure if I've triggered the run_slow test for it and DPT correctly. Happy to split it off into a separate PR.

Also ran into an issue with zoedepth not working because of the beit backend. I suspect that will take more time to properly addres, I added a skipped test to zoedepth, but I can also remove that test entirely and add it in a WIP PR.
 > Thanks for the update! Regarding ZoeDepth I suggest either fixing the model export or excluding this model from the PR. A skipped test is not the best solution, cause it might stuck in this state for a very long time 😄
> 
> To trigger multiple models' slow tests you can list them as follows `[run_slow] depth_anything, dpt, zoedepth`

I have to add some of the model changes because of the copy-consistency check, but I'll remove the Relu change and the torch.export test!

Thanks for the heads up on slow tests. @qubvel could you approve the slow workflow? > I have to add some of the model changes because of the copy-consistency check
Can you provide a bit more details on this? Can I somehow help to enable torch export for ZoeDepth? > > I have to add some of the model changes because of the copy-consistency check
> Can you provide a bit more details on this? Can I somehow help to enable torch export for ZoeDepth?

I'm not 100% sure that this is part of the CI, but the contributing guide asks you to run repo-consistency 
https://github.com/huggingface/transformers/blob/bc0633a82cbfe8d828fa2d3b432dfde4fbd2f0e5/CONTRIBUTING.md#L217

which throws an error in  `python utils/check_copies.py` if you don't update ZoeDepth to match DPT. (ZoeDepth copied many layers from DPT
https://github.com/philkuz/transformers/blob/bc0633a82cbfe8d828fa2d3b432dfde4fbd2f0e5/src/transformers/models/zoedepth/modeling_zoedepth.py#L175
)

So basically I have to include those shared changes: https://github.com/huggingface/transformers/pull/34103/files#diff-02337c86e3fba49173cf2cb6fa1595ed168db19726938aec925b8b010a3b6a8c

The current crux of ZoeDepth is that the BEIT model, the backbone of all the HF hub models for ZoeDepth, isn't compatible. So you have to address that issue, which I have not had time to address yet.
 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34103). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. The `slow_tests` are failing, but I think they're broken on main as well. Here's a repro:
```
git checkout main
# DPT Failures
CUDA_VISIBLE_DEVICES="""" RUN_SLOW=true pytest tests/models/dpt/test_modeling_dpt_auto_backbone.py -v  -k=test_inference_depth_estimation_dinov2
# Depth-anything failures
CUDA_VISIBLE_DEVICES="""" RUN_SLOW=true pytest tests/models/depth_anything/test_modeling_depth_anything.py -v  -k test_inference
```

The following also has checks on the output of slices and those checks seem to work.
```
CUDA_VISIBLE_DEVICES="""" RUN_SLOW=true pytest tests/models/dpt/test_modeling_dpt.py -v  -k=test_inference
```

I went ahead and made a PR to try and address this issue: https://github.com/huggingface/transformers/pull/34518
 > I'm not 100% sure that this is part of the CI, but the contributing guide asks you to run repo-consistency

Ahh, ok, it's because model modules contain ""Copied from"" statements and these parts are synced across models. No worries then! Extend the script in https://github.com/pytorch/executorch/pull/6509 to support lowering (with the simplest recipe) the `DepthEstimation` and `SemanticSegmentation` models enabled in this PR. 

The `dpt` model works as expected. 
However, the `depth-anything` model fails due to an unsupported dim order. ExecuTorch supports `dim_order: (0, 1, 2, 3)` but got `dim_order: (0, 2, 3, 1)` for a placeholder node `aten_clone_default`. There seems to be a way to insert a compiler pass to fix it w/o requiring changing the source code. I will give a try. > Extend the script in [pytorch/executorch#6509](https://github.com/pytorch/executorch/pull/6509) to support lowering (with the simplest recipe) the `DepthEstimation` and `SemanticSegmentation` models enabled in this PR.
> 
> The `dpt` model works as expected. However, the `depth-anything` model fails due to an unsupported dim order. ExecuTorch supports `dim_order: (0, 1, 2, 3)` but got `dim_order: (0, 2, 3, 1)` for a placeholder node `aten_clone_default`. There seems to be a way to insert a compiler pass to fix it w/o requiring changing the source code. I will give a try.

Any luck on the compiler pass? 

Also do you think that this gates the support for `torch.export`? Seems like this is `Executorch` specific. Maybe we can scope this PR down to be more for `torch.export` generally and focus on adding support for Executorch in another PR? Happy to help with that > > Extend the script in [pytorch/executorch#6509](https://github.com/pytorch/executorch/pull/6509) to support lowering (with the simplest recipe) the `DepthEstimation` and `SemanticSegmentation` models enabled in this PR.
> > The `dpt` model works as expected. However, the `depth-anything` model fails due to an unsupported dim order. ExecuTorch supports `dim_order: (0, 1, 2, 3)` but got `dim_order: (0, 2, 3, 1)` for a placeholder node `aten_clone_default`. There seems to be a way to insert a compiler pass to fix it w/o requiring changing the source code. I will give a try.
> 
> Any luck on the compiler pass?

@philkuz Sorry, haven't got a chance to write the pass yet.

> 
> Also do you think that this gates the support for `torch.export`? Seems like this is `Executorch` specific. Maybe we can scope this PR down to be more for `torch.export` generally and focus on adding support for Executorch in another PR? Happy to help with that

Right, it's ExecuTorch specific, i.e. all tensors need to be contiguous. BTW, do you happen to know where the channel_last tensor may come from the eager, we can fix it here, otherwise, having a separate PR for ExecuTorch is fine. Please note that unlike compiled artifact the exported program is just an intermediate representation, typically should only being used as the entry for further optimizations, i.e. ExecuTorch.  > > > Extend the script in [pytorch/executorch#6509](https://github.com/pytorch/executorch/pull/6509) to support lowering (with the simplest recipe) the `DepthEstimation` and `SemanticSegmentation` models enabled in this PR.
> > > The `dpt` model works as expected. However, the `depth-anything` model fails due to an unsupported dim order. ExecuTorch supports `dim_order: (0, 1, 2, 3)` but got `dim_order: (0, 2, 3, 1)` for a placeholder node `aten_clone_default`. There seems to be a way to insert a compiler pass to fix it w/o requiring changing the source code. I will give a try.
> > 
> > 
> > Any luck on the compiler pass?
> 
> @philkuz Sorry, haven't got a chance to write the pass yet.
> 
> > Also do you think that this gates the support for `torch.export`? Seems like this is `Executorch` specific. Maybe we can scope this PR down to be more for `torch.export` generally and focus on adding support for Executorch in another PR? Happy to help with that
> 
> Right, it's ExecuTorch specific, i.e. all tensors need to be contiguous. BTW, do you happen to know where the channel_last tensor may come from the eager, we can fix it here, otherwise, having a separate PR for ExecuTorch is fine. Please note that unlike compiled artifact the exported program is just an intermediate representation, typically should only being used as the entry for further optimizations, i.e. ExecuTorch.

I did a very quick scan for the channel_last tensor, and I believe it's in DINOv2 (the backbone) which is not part of this particular modeling code. I think we should move it to another PR IMO. Any other block for merging this PR? @guangy10 no blocks IMO, waiting for @ArthurZucker's review, he has quite a few in line  Sorry for the delay @guangy10 we were on a company wide offsite! 🌴 Very nice, thanks for unlocking more models for torch export, this is very valuable!

The same comment as for Mask2Former PR, would be great to have this PR tested, and please push run-slow commit to trigger all tests at the end! Thanks for the update! Regarding ZoeDepth I suggest either fixing the model export or excluding this model from the PR. A skipped test is not the best solution, cause it might stuck in this state for a very long time 😄 

To trigger multiple models' slow tests you can list them as follows `[run_slow] depth_anything, dpt, zoedepth` Thanks for updating it!

cc @guangy10 LGTM! Thanks for the contribution. Great contribution! Thanks all for iterating 🤗 
Super good in general as slicing does not go well with compile either!","Very nice, thanks for unlocking more models for torch export, this is very valuable!

The same comment as for Mask2Former PR, would be great to have this PR tested, and please push run-slow commit to trigger all tests at the end! Thanks for the update! Regarding ZoeDepth I suggest either fixing the model export or excluding this model from the PR. A skipped test is not the best solution, cause it might stuck in this state for a very long time 😄 

To trigger multiple models' slow tests you can list them as follows `[run_slow] depth_anything, dpt, zoedepth` Thanks for updating it!

cc @guangy10 LGTM! Thanks for the contribution. Great contribution! Thanks all for iterating 🤗 
Super good in general as slicing does not go well with compile either!","# What does this PR do?
Small modification of the DPT modeling code to remove a new object creation in a `forward()` method of a Module. This object creation makes the model incompatible with `torch.export`, which is a key part of preparing a model to run on a variety of hardware backends through projects such as [ExecuTorch](https://pytorch.org/executorch/main/intro-overview.html) (related issue: https://github.com/huggingface/transformers/issues/32253)

## Motivation
[torch.export](https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#) allows you to export PyTorch models into standardized model representations, intended to be optimized and run efficiently using frameworks such as TensorRT or ExecuTorch.

## The Bug 
They key issue was the slice on `self.layers`:
https://github.com/huggingface/transformers/blob/617b21273a349bd3a94e2b3bfb83f8089f45749b/src/transformers/models/dpt/modeling_dpt.py#L696

`self.layers[1:]` creates a new `ModuleList()` each time this line is executed.

https://github.com/pytorch/pytorch/blob/69bcf1035e7f06f2eefd8986d000cc980e9ebd37/torch/nn/modules/container.py#L330

The model tracer in `torch.export` monkey-patches nn.Module constructors during evaluation of the `forward()` pass, so the original DPT modeling code raises the following error:
```
  File ""/home/philkuz/.pyenv/versions/gml311/lib/python3.11/site-packages/torch/nn/modules/container.py"", line 293, in __getitem__
      return self.__class__(list(self._modules.values())[idx])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 TypeError: _ModuleStackTracer.__init__.<locals>.AttrProxy.__init__() missing 1 required positional argument: 'path'

```

## The Solution
Pytorch recommends users update the modeling code. My team and I figured this could be helpful to the broader community, especially a future where Export to Executorch becomes more widely available: https://github.com/huggingface/transformers/issues/32253

This also removes an unnecessary creation of a new module list as a bonus.

### Tests
I ensured that `tests/models/dpt/test_modeling_dpt.py` passes, which appears to test a portion of the outputs. I also verified that the entire output of the model
before and after my changes matched with the following script:
```python
import os
import sys

import numpy as np
import requests
import torch
from PIL import Image
from transformers import pipeline

url = ""http://images.cocodataset.org/val2017/000000039769.jpg""
image = Image.open(requests.get(url, stream=True).raw)


model = pipeline(""depth-estimation"", ""facebook/dpt-dinov2-base-kitti"")
result = model(image)


output_file = ""depth_estimation_output.npy""

if not os.path.exists(output_file):
    # Save the current output
    np.save(output_file, result[""predicted_depth""])
    print(f""Depth estimation output saved to {output_file}"")
    print(""Rerun the script to compare the output"")
    sys.exit(0)
# Load existing output and compare
expected_output = np.load(output_file)
np.testing.assert_allclose(
    result[""predicted_depth""],
    expected_output,
    rtol=1e-5,
    atol=1e-5,
    err_msg=""Depth estimation output has changed"",
)
print(""Depth estimation output matches the saved version."")
```

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@amyeroberts, @qubvel

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","If we have `fused_hidden_state = None` we can probably avoid `if` at all I sees why you might think that, but the way the DPTFeatureFusionLayer expects the first argument to be not `None` while the second argument (`hidden_state`) can be `None`. Thanks for the clarification! missed it Comment for final review:

This change included in the ZoeDepth model because of the ""Copied from"" statement, it doesn't unlock torch export for the model, however will be useful if we decide to enable it"
34355,2024-10-23T17:14:39Z,2024-11-20T16:24:45Z,SunMarc,3,20,5,120,3,5,3,[],1579.0,0,2416208.0,0,0,0,0,3124107.678249,,0,5,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev', 'MekkCyber']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34355). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks for this PR @SunMarc, really helpful ! added a weights_only check in case the user don't want to upgrade torch The title and description of the PR don't seem to match the changes no? 
 Thanks for detailing 🤗  Awesome thanks for taking it into account!","The title and description of the PR don't seem to match the changes no? 
 Thanks for detailing 🤗  Awesome thanks for taking it into account!","# What does this PR do ? 

This PR pins the torch version to 2.5 since torchao needs it in order to load the pre-quantized model with weights_only=True. We also fix a few minor bug. I've added some tests for the serialization logic.

cc @jerryzh168 cc @MekkCyber ","if we do `import torchao`, I think we should get everything here (classes etc. being added to safeglobals)? otherwise we'd need to fix torchao I'm using torchao 0.5.0 and it's not working on my side. I can try with the latest tomorrow !  I see, it's not expected I think, I think it should be fixed in torchao side, I feel 0.5 should have this functionality already actually. if you can have a standalone repro that will be very helpful for us. I remember I have tested in https://huggingface.co/docs/transformers/main/en/quantization/torchao just a nit! I think it would be nicer to have the same vertical layout as the other ones a nit : We could add `device` as a class property too, instead of hardcoding `cuda:0`, and then add a `cpu` test for int8 quantization Is there a specific reason why we specify for the model `cuda:0` as a device, and for the input_ids `torch_device` ? Very good catch ! worth investigating, will try to do that ! Actually, we ran into this issue with @MekkCyber on the example you shared in the docs. 
Here's a the reproducer, let us know if you also have this issue : 

```python
from transformers import TorchAoConfig, AutoTokenizer, AutoModelForCausalLM
import torch

model_name = ""TinyLlama/TinyLlama-1.1B-Chat-v1.0""
quant_config = TorchAoConfig(""int4_weight_only"", group_size=32)
quantized_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map=""cuda:0"",
    quantization_config=quant_config,
)
output_dir = ""llama3-8b-int4wo-128""
quantized_model.save_pretrained(output_dir, safe_serialization=False)

loaded_quantized_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=""cuda:0"")
``` fixed !  copy paste issue ;) I will do something better done !  yeah, i'm not sure where this comes from but I don't think this is a huge issue. Maybe check the perplexity for both models just to have an overview ?  Yes great idea ! I was thinking about comparing the weights, and activations using `lovely_tensors` but it's better to start with the perplexity Whatever you think works the best, thanks ! After fixing this bug, we should be good to share the spaces you created !  OK will test and report back It works for torchao >= 0.5.0 and torch >= 2.5.1 @SunMarc can you help double check? what is the torchao and pytorch version you are using ok this might be an issue with the torch version. I tested with 2.5 and it works but fails with 2.4. Can you try with this version ?  If we can't fix it for older version of torch, we can either do a check for torch>=2.5 or merge this PR.  yeah we can't really fix older versions since these are already released I think, I remember there was some missing features in pytorch in 2.4 as well, I feel it's fine to just enable torchao for 2.5 and later Sounds good ! I pinned torch version when using prequnatized model "
34746,2024-11-15T10:14:59Z,2024-11-20T06:46:35Z,zucchini-nlp,1,1,4,15,3,1,1,[],1596.0,0,419497.0,0,0,0,0,3158800.583574,,1,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34746). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Very nice!,Very nice!,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34232 and bring the test back",ooooo what a great catch
30794,2024-05-14T02:26:34Z,2024-05-14T06:36:11Z,jla524,0,1,1,4,1,1,1,[],,1,16413589.0,0,0,0,0,3176813.865433,,0,1,0,False,[],,,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes https://github.com/huggingface/transformers/pull/30686#issuecomment-2107416445

Tested on a 7900 XTX machine

```
% pytest --doctest-modules docs/source/en/model_doc/owlv2.md -sv --doctest-continue-on-failure --doctest-glob=""*.md""
<log redacted>
=========================================================== test session starts ============================================================
platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0 -- /home/jacky/repos/transformers/env/bin/python3
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/home/jacky/repos/transformers/.hypothesis/examples'))
rootdir: /home/jacky/repos/transformers
configfile: pyproject.toml
plugins: timeout-2.3.1, hydra-core-1.3.2, hypothesis-6.102.1, rich-0.1.1, xdist-3.6.1, dash-2.17.0, anyio-4.3.0
collected 1 item                                                                                                                           

docs/source/en/model_doc/owlv2.md::owlv2.md PASSED

<warnings redacted>
====================================================== 1 passed, 2 warnings in 4.26s =======================================================
```

## Who can review?

@ydshieh @amyeroberts 
","Our T4 runners gives the same new value. Thank you for the quick fix @jla524 !
"
30962,2024-05-22T12:27:03Z,2024-08-13T05:14:39Z,zucchini-nlp,17,30,44,2741,37,3,1,"['WIP', 'run-slow']",1240.0,0,15708562.0,0,0,0,0,3154612.238756,,0,44,0,False,"['pspdada', 'molbap', 'HuggingFaceDocBuilderDev', 'zucchini-nlp']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_30962). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Looking forward to see this expanded to other VLMs! Some might be trickier, PaliGemma incorporates causal mask computation in the merge method for instance (thought about that when reading) but it makes sense that most should belong in the processor, not the modeling @amyeroberts I did some clean-up after Arthur's comments. Requesting review, should be ready. If this works I will expand the logic to BLIP and PaliGemma in the next weeks 

What changed:
- Model can generate from both: old inputs and new-expanded inputs. If it's old inputs, warning is raised, asking to upgrade the processor config.
- Processor also can return both types. If it has all the necessary parts to calculate image embedding length, the inputs will be expanded. Otherwise, warning is raised and old behavior retained.
- Old behavior is planned to be totally removed in v4.44 (or better v4.43?)
- Added tests to check that old vs new inputs generation is identical
- To actually have llava-based models work in new style, I'll later update all hf-llava configs in the hub. Other models in the hub will continue to work with old behavior @amyeroberts addressed the comments and added all VLMs to the PR (excluding Idefics, Fuyu and Kosmos as those already have expansion in processing).

- warning text is more clear and it's easy for users to add new attributes to `Processor` class (with `processor.patch_size = patch_size`)
- BLIP-2 needed more modifications as it didn't have special image token, lmk if the way I did works
- Paligemma worked out of the box but needed changes for causal mask. There's also smth weird with ""position_ids"" which will be fixed by @molbap 
-  All models have their ""old-new format equivalence"" tests and are passing locally. I don't know how to make happy the failing doctest, it's red even after I deprecated the unused attribute
 This should be done, addressed the comments. For the failing test, I have no idea how to skip it after deprecating a property from config. Alright cool, taking a look soon! For the config option, a quick&dirty solution could be to do something like `_ = config.ignore_index` in the modeling? I'll run slow tests and check everything is okey, will merge some time next week I encountered the warning: ""Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and `processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47."" 
Could you please guide me on how to choose the `patch_size` and `vision_feature_select_strategy` based on the model? Are there any related documents available? @zucchini-nlp  @pspdada hey! The `patch_size` and `vision_feature_select_strategy` should be taken from model config, so that `vision_feature_select_strategy = model.config.vision_feature_select_strategy` and `patch_size = model.config.vision_config.patch_size`

The message says about deprecation in v4.47 but we encountered a few things to be done so the target version will be moved a few more versions up. After the issues we encountered are fixed, I'll update official checkpoints on the hub. Until then you are welcome to use the old way (and ignore the warning) or try out the new logic by setting necessary params in the processor :) > `vision_feature_select_strategy = model.config.vision_feature_select_strategy` and `patch_size = model.config.vision_config.patch_size`

Thank you for your response, but there is an issue.
I use transformers 4.46.0.dev0 using `pip install --upgrade git+https://github.com/huggingface/transformers.git`, which means this pull request has already taken effect (because it has been merged into the main branch).

When using the following code to load the llava 1.5 model and generate with it:
```
def _create_v_1_5(self) -> tuple[LlavaForConditionalGeneration, LlavaProcessor]:
    model_name = f""llava-hf/llava-1.5-{self.model_size}-hf""

    model: LlavaForConditionalGeneration = LlavaForConditionalGeneration.from_pretrained(
        model_name,
        cache_dir=self.model_dir,
        torch_dtype=self.torch_dtype,
        device_map=""auto"",
        low_cpu_mem_usage=True,
        attn_implementation=attn_implementation,
    ).to(self.device).eval()

    processor: LlavaProcessor = LlavaProcessor.from_pretrained(
        model_name,
        cache_dir=self.model_dir,
        padding_side=""left"",
        vision_feature_select_strategy=model.config.vision_feature_select_strategy,
        patch_size=model.config.vision_config.patch_size,
    )
    print(model.config.vision_feature_select_strategy, model.config.vision_config.patch_size)

    return model, processor

def _gen(
    self,
    images: list[Image.Image],
    prompts: list[str],
    max_token: int,
    do_sample: list,
    temp: float,
    eos_token_id: list[int] | None = None,
) -> list:
    with torch.inference_mode():
        inputs = self.processor(
            images=images,
            text=prompts,
            return_tensors=""pt"",
            return_token_type_ids=False,
            padding=True,
        ).to(self.device, torch.float16)

        generated_ids = self.model.generate(
            **inputs,
            max_new_tokens=max_token,
            temperature=temp if do_sample else None,
            do_sample=do_sample,
            use_cache=True,
            eos_token_id=eos_token_id,
        )
        decoded_outputs: list[str] = self.processor.batch_decode(
            generated_ids,
            skip_special_tokens=False,
            clean_up_tokenization_spaces=False,
        )

    return decoded_outputs
```
The output is: `default 14` and an error occured.
An error occurs:
```
File ""/root/llm-project/LVLM/model/llava.py"", line 202, in _gen
    generated_ids = self.model.generate(
File ""/root/anaconda3/envs/LVLM/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
File ""/root/anaconda3/envs/LVLM/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2220, in generate
    result = self._sample(
File ""/root/anaconda3/envs/LVLM/lib/python3.10/site-packages/transformers/generation/utils.py"", line 3211, in _sample
    outputs = self(**model_inputs, return_dict=True)
File ""/root/anaconda3/envs/LVLM/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
File ""/root/anaconda3/envs/LVLM/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
File ""/root/anaconda3/envs/LVLM/lib/python3.10/site-packages/transformers/models/llava/modeling_llava.py"", line 524, in forward
    raise ValueError(
ValueError: Image features and image tokens do not match: tokens: 325440, features 576
```
If I remove those two lines, everything works fine.
```
vision_feature_select_strategy=model.config.vision_feature_select_strategy,
patch_size=model.config.vision_config.patch_size,
```
Is this a phenomenon that under control? or should I open a new issue about it and provide full infomation? I ues the following code to decode the output from the model.generate:
```
new_sentence = decoded_output.strip('<pad>').strip('<s>').strip()[len(prompt) + 1:].strip('</s>').strip()
```
An interesting phenomenon is that the original output was:
```
The image features a man and a woman standing close to each other, posing for a picture.
```
However, after adding these two lines:
```
vision_feature_select_strategy=model.config.vision_feature_select_strategy,
patch_size=model.config.vision_config.patch_size,
```
the output becomes particularly strange:
```
...ge><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image> 
Please describe this image in detail and tell me what you see. ASSISTANT: The image features a man and a woman standing close to each other, posing for a picture.
``` @pspdada I just opened a PR for that (https://github.com/huggingface/transformers/pull/34332). It is a totally unrelated issue and should be solved soon > @pspdada I just opened a PR for that (#34332). It is a totally unrelated issue and should be solved soon

Thank you for your attention, and I wish for a good outcome. > @pspdada I just opened a PR for that (#34332). It is a totally unrelated issue and should be solved soon

I've discovered a new issue after the merge of #34332. In the latest version of `transformers==4.46.2`, problems still occur when I set:
```python
vision_feature_select_strategy=model.config.vision_feature_select_strategy,
patch_size=model.config.vision_config.patch_size,
```
After setting these parameters, the result of batch inference (without truncation, directly using `batch_decode` output) is as follows:
```
['<s> USER: <image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image> \nPlease describe this image in detail and tell me what you see. ASSISTANT: The image features a man and a woman standing close to each other, posing for a picture. The man is wearing a tie, and the woman is wearing a white shirt. They are both smiling and enjoying the moment.\n\nIn the background, there is a TV mounted on the wall, and a few bottles can be seen placed around the room. There are also two other people in the scene, one on the left side and another on the right side of the image.</s>', 
'<pad><pad><pad><pad><pad><pad><pad><s> USER: <image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image> \nWhat is in the image? ASSISTANT: Theo, the image\n\nWhat\n\n</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']
```

However, when I remove these two lines, everything works as expected, and the result is:
```
['<s> USER: <image> \nPlease describe this image in detail and tell me what you see. ASSISTANT: The image features a man and a woman standing close to each other, posing for a picture. The man is wearing a tie, and the woman is wearing a white shirt. They are both smiling and enjoying the moment.\n\nIn the background, there is a TV mounted on the wall, and a few bottles can be seen placed around the room. There are also two other people in the scene, one on the left side and another on the right side of the image.</s>',
 '<pad><pad><pad><pad><pad><pad><pad><s> USER: <image> \nWhat is in the image? ASSISTANT: The image features a group of birds perched on a tree branch.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']
```
 Shall I open a new issue about it and provide full infomation? @zucchini-nlp @pspdada that is the expected logic for the new processing code, as we expand the text with as many ""image"" tokens as there will be image embeddings. When decoding the text you should pass `skip_special_tokens=False` to remove all `image` tokens > @pspdada that is the expected logic for the new processing code, as we expand the text with as many ""image"" tokens as there will be image embeddings. When decoding the text you should pass `skip_special_tokens=False` to remove all `image` tokens


@zucchini-nlp But the output of the model seems to be not quite right this way. Look at my previous example; if we remove the `<Image>` and `<pad>`, what remains is:
``` python
 \nWhat is in the image? ASSISTANT: Theo, the image\n\nWhat\n\n</s>
```
If I don't set the two line, it will be:
```
\nWhat is in the image? ASSISTANT: The image features a group of birds perched on a tree branch.</s>
```
  @pspdada so the new logic generates gibberish when the inputs are batched? I don't see such behavior with the latest release. Can you verify if the padding side if set to left as it is recommended in [docs](https://huggingface.co/docs/transformers/en/model_doc/llava_onevision) (`processor.tokenizer.padding_side = ""left""`)Thanks for working on this - will be great to have some of this logic unified! 

Main comment is about how we set the required arguments for processing in the processor  Wow - a big piece of work! 

Overall looks good to me, just a few comments here and there. I'd like to have a second review from @molbap and a run on the slow tests for all the models touched here  Will continue reviewing (biig work) but sharing feedback on the 2 first things I tested, Llava and Paligemma It's a biiig piece of work, nicely done, tests and all! I left a few comments on some things I didn't understand well + paligemma masking in particular LGTM! Some minor comments remaining but seems good","Thanks for working on this - will be great to have some of this logic unified! 

Main comment is about how we set the required arguments for processing in the processor  Wow - a big piece of work! 

Overall looks good to me, just a few comments here and there. I'd like to have a second review from @molbap and a run on the slow tests for all the models touched here  Will continue reviewing (biig work) but sharing feedback on the 2 first things I tested, Llava and Paligemma It's a biiig piece of work, nicely done, tests and all! I left a few comments on some things I didn't understand well + paligemma masking in particular LGTM! Some minor comments remaining but seems good","# What does this PR do?

Fixes #30809, This PR moves the `_merge_inputs_with_vision_embeds` to the processing logics, and thus making VLMs more versatile in terms of generation strategies. All models were tested locally with different batch sizes and img resolutions, the generation is same as it was before making changes.

The main idea is to get sequence length for image features inside the processing files, and expand input ids by repeating special image token. Same is already done for IDEFICS in `transformers`.
","Mostly copied from TGI with minor changes in calculations for unpadding, otherwise it won't work for low resolution images As llava can take any image processor, we can't assume `size` will have `shortest_edge`, it should also be a dict of `{""height"": h, ""width"": w}`  Is this consistent for all the tokenizers?  I think this is both going to be annoying to handle and creates leakiness in our abstractions: not everyone who uses the image processor will be using it for llava, and there's no reason for image processors for models which don't have `patch_size` to have this attribute. 

Instead, I'd suggest getting the defaults from the image processor if available, but also allowing them as processor arguments in the init and possibly the forward pass which the user can directly set and control. This way, when we save out the processor, these can be saved alongside, and the user doesn't need to change an image processor or override the processor each time they want to use it  Why index on the 0'th output here ? For tokenizers, yes! The only part from tokenizer is ""image token"" which I believe is established as `""<image>""` for LLaVas.

For all image processors/vision towers, then maybe no. Calculation of image token seq length is done assuming we get CLIP as image tower. I am not sure if other vision backbones are all similar or require different handling. 

In case there are llavas w/o CLIP and it has to be handled differently, I will see if we can unify it without extra code-hacks  Right, that will be easier for users. Will make changes no reason, it was simply copied as is from `test_modeling_common.py` with extra handling of `pixel_values` as input to forward. 

I'll remove it here, as it's not making any difference  since we can't be sure which one is used by image processor and what's the output size, I decided to check `pixel_values` size and use that as `self.image_size`

Also, now we don't assume it's always a square size output and calculate patches for height and width separately, similar to what ViT does in its modeliing file. For existing possible backbones (CLIP, SigLIP, Dino, ViT) the current code will work as expected. Same for tokenizers, and in case any model decided to change image token, the user can set it by `processr.image_token=""{ANY_NEW_TOKEN}""` ❤️  Should.be added to the docstring  Logic check here also isn't consistent with the warning - it doesn't catch if both are None Why have these set as private?  Is the 576 the sequence length? We should take this from the config, rather than hard-code here  We should make sure the official checkpoints have been updated this way  Both can be `None`, that happens when we are in decoding stage and get only the last token for generation. We only can't have both set to a tensor, because that means we don't know where are `image_token` so we can't merge images into embeddings oh, forgot to get it back, was part of an older logic Yes, it's the length for one image, and can be added as image_seq_length to all configs. I will start updating llava (and maybe others by opening a PR) configs today so the default behavior becomes the ""expand in processor"" General comment:
- nice simplification!
- currently generation seems to be broken on PaliGemma. When doing 

```python
from PIL import Image
from transformers import PaliGemmaForConditionalGeneration, PaliGemmaProcessor

model = PaliGemmaForConditionalGeneration.from_pretrained('google/paligemma-3b-ft-docvqa-896')
processor = PaliGemmaProcessor.from_pretrained('google/paligemma-3b-ft-docvqa-896')

img = Image.open(""/home/pablo/docvqa_example.png"").convert(""RGB"")
text = ""answer en what is the document type""

inputs = processor(images=[img], text=text, return_tensors=""pt"")
outputs = model.generate(**inputs, max_new_tokens=30)
print(processor.decode(outputs))
```
`model.generate(...)` works on main, but fails in `modeling_gemma` when setting the `max_past_length` attribute which requires the check
```python
if past_key_values.get_max_length() is not None
```
and it seems that `past_key_value` is a tuple, raising `AttributeError: 'tuple' object has no attribute 'get_max_length'`. This happens after the first token is generated, ie when past_key_values is not None Same comment as for `modeling_paligemma`, wondering if it's an issue on my end? after the first token generated, `modeling_llama` will throw because of 
```python
AttributeError: 'tuple' object has no attribute 'get_seq_length'
``` 
(this does not happen on `main`) couldn't we also have a very tiny image embeddings sequence length? Hmm, weird since we should be doing Cache class inside generate before going to the modeling part. I will check out tomorrow on this For LLaVa models the usual backbone is CLIP (i don't remember which ckpt exactly), which always give 576 length. In case some LLaVas have another vision backbone, I guess user have to switch the `image_seq_length` parameter in model's config. 

I am thinking if we can catch that and give user an informative error msg? Will explore more this issue Found the bug, PaliGemma didn't have the cache flags on. Resolved!  Maybe this, or another version number as we're in 4.44 dev version
```suggestion
                ""Using processors without these attributes in the config is deprecated and will throw an error in a later version""
``` typo to change in a few places
```suggestion
        # otherwise we expand manually by concatenating
``` Is this equivalent to
```python
text_encoding[k] = [img_encoding + txt_encoding for img_encoding, txt_encoding in zip(image_token_encoding[k], _text_encoding[k])]
```
? slightly more readable could also `pixel_values.shape[2:]` access image size? Even if it's a private method, a docstring would help code inspectors here"
30842,2024-05-15T21:49:04Z,2024-05-17T12:57:47Z,jla524,0,0,3,2,1,1,1,[],,0,16257438.0,0,0,0,0,3176816.839944,,0,3,0,False,[],Thanks! ,Thanks! ,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #30060 (issue)

Tested on an M1 Mac and a 7900 XTX system

## Who can review?

@ArthurZucker @amyeroberts 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
30630,2024-05-03T06:21:16Z,2024-05-09T10:23:39Z,jla524,0,12,19,95,2,2,1,[],,0,17349907.0,0,0,0,0,3176815.909655,,0,19,0,False,[],"Thanks for working on this! 

Just a few nit comments  Thanks for improving the models and iterating on this! 

Just a few small nits. For the quality checks, running `make fixup` and pushing the changes should resolve them  Thanks for working on this and improving the library's models! ","Thanks for working on this! 

Just a few nit comments  Thanks for improving the models and iterating on this! 

Just a few small nits. For the quality checks, running `make fixup` and pushing the changes should resolve them  Thanks for working on this and improving the library's models! ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Addresses #30579 (issue)

## Who can review?

@amyeroberts 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Should not have print statement here.  ```suggestion
        if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):
            raise ValueError(
                f""Image image size ({height}*{width}) doesn't match model""
                f"" ({self.image_size[0]}*{self.image_size[1]}).""
            )
``` nit - I realise this comes from vit. The convention in the library is not to have assert statements in the modeling files 

```suggestion
``` ```suggestion
``` No need to test the logits unless they're taken from the original model for this image size 

```suggestion
``` * We should use the image processor already created
* image processor's behaviour can be configured when they're called 
* size should be a dict 
* I believe the value we want to modify for this is also `crop_size` 

```suggestion
        image_processor = VivitImageProcessor.from_pretrained(""google/vivit-b-16x2"")
        video = prepare_video()
        inputs = image_processor(video, size={""shortest_edge"": 480}, crop_size={""height"": 480, ""width"": 480}, return_tensors=""pt"")
        pixel_values = inputs.pixel_values.to(torch_device)
``` ```suggestion
        # verify the logits shape
``` ```suggestion
    def forward(self, pixel_values, interpolate_pos_encoding: bool = False):
``` It's not possible to define a default like this unless interpolate_pos_encoding is a global

```suggestion
    def forward(self, pixel_values, interpolate_pos_encoding: bool = False):
``` I realise this comes from vit - but let's default to `False` here. It's easier than having to deal with null/true/false 

```suggestion
        interpolate_pos_encoding (`bool`, *optional*, `False`):
            Whether to interpolate the pre-trained position encodings.
``` ```suggestion
        interpolate_pos_encoding: bool = False,
``` ```suggestion
        interpolate_pos_encoding: bool = False,
```"
31646,2024-06-26T18:49:53Z,2024-06-28T11:18:01Z,jla524,2,4,5,45,2,3,1,[],51979.0,0,12639388.0,0,0,0,0,3176818.452397,,0,5,0,False,"['kamilakesbi', 'HuggingFaceDocBuilderDev']","Thanks for iterating on this @jla524! 

@amyeroberts this should be ready for final review/merge :)  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_31646). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks @jla524 for iterating on this! 

I left a comment for small suggested changes :) 
 Thanks for fixing! 

Just a q on the change to the recursive check  Thanks for fixing ad iterating on this! ","Thanks @jla524 for iterating on this! 

I left a comment for small suggested changes :) 
 Thanks for fixing! 

Just a q on the change to the recursive check  Thanks for fixing ad iterating on this! ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #31642 (issue)

With this PR, `return_dict=False` returns a tuple, and the unit test compares tuple vs dict values.

```
% pytest tests/models/encodec/test_modeling_encodec.py -k test_model_outputs_equivalence
=========================================================== test session starts ============================================================
platform darwin -- Python 3.12.3, pytest-7.4.4, pluggy-1.4.0
rootdir: /Users/jacky/repos/transformers
configfile: pyproject.toml
plugins: xdist-3.5.0, timeout-2.3.1, rich-0.1.1
collected 116 items / 115 deselected / 1 selected                                                                                          

tests/models/encodec/test_modeling_encodec.py .                                                                                      [100%]

<warnings redacted>
============================================== 1 passed, 115 deselected, 9 warnings in 1.56s ===============================================
```

## Who can review?

@kamilakesbi

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc 

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Here we could do instead: 

```suggestion
        return_dict = return_dict if return_dict is not None else self.config.return_dict
```
so that `return_dict` is still set to self.config.return_dict by default.  
 It would be also nice to do the same modification in the `decode` method (at this [line](https://github.com/huggingface/transformers/blob/f183580d0541f895d941dd4ce2393c237c25e243/src/transformers/models/encodec/modeling_encodec.py#L732)).  Even if we assert that `tuple_object` is a tuple on L377, as this is a recursive function, isn't is still possible that the values in tuple_object i.e. `tuple_iterable_value` are a dict or None? The values in the tuple_object should be `tensor.Tensor` only.

To check the types, I added a print statement in the recursive function:

```py
def recursive_check(tuple_object, dict_object):
    print(f""[DEBUG]: {type(tuple_object)=}, {type(dict_object)=}"")
    ...
```

and I got this:

```py
[DEBUG]: type(tuple_object)=<class 'tuple'>, type(dict_object)=<class 'transformers.models.encodec.modeling_encodec.EncodecOutput'>
[DEBUG]: type(tuple_object)=<class 'torch.Tensor'>, type(dict_object)=<class 'torch.Tensor'>
[DEBUG]: type(tuple_object)=<class 'torch.Tensor'>, type(dict_object)=<class 'torch.Tensor'>
```

edit: it's probably more intuitive to just iterate over the items and compare it"
34800,2024-11-19T08:43:57Z,2024-11-19T08:54:11Z,ArthurZucker,1,3,1,16,2,3,1,[],1635.0,0,53075.0,0,0,0,0,3185089.017575,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34800). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.🔥  Thanks for the fix!,🔥  Thanks for the fix!,"# What does this PR do?
Fixes #34795","Hi @ArthurZucker - this mismatch between the torch 2.4 check and 2.5 requirement means that torch 2.4 still hits this issue (where torch 2.3 is now working properly)  Actually, `Replicate` is in `torch.distributed.tensor` >= torch 2.5. 
Between 2.0-2.4 (included), it is in `torch.distributed._tensor`.

Thus it seems there are two options:
Option 1:
```
try:
    from torch.distributed.tensor import Replicate
except ImportError:
    from torch.distributed._tensor import Replicate
```
Option 2:
bump the requirement to 2.5.

`ColwiseParallel` and `RowwiseParallel` well exists since 2.0.  Hi @kwen2501 - no preference from my side.  I have a PR that does the first option here: https://github.com/huggingface/transformers/pull/34816

But happy to abandon or switch in favor of any you have to add support."
34716,2024-11-13T13:51:36Z,2024-11-19T18:03:12Z,wwwbai,7,5,12,39,2,4,1,[],190348.0,0,533496.0,1,0,0,0,3204609.721145,,0,12,0,False,"['wwwbai', 'stevhliu', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34716). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. 
Oh, I'm sorry, I'm a first-time contributor to the open source community, so I may not know where to find someone who can read Chinese.Or can I find someone who can read Chinese under the comments of other issues that translate documents into Chinese?

> Thanks for your translation! Do you know someone who can read Chinese and would be interested in reviewing your translation? I can only review the high-level things because I can't read Chinese 😅

 Maybe @Isotr0py would you like to preview my PR? I found you under other issues related to Chinese translation. I submitted a new PR(#34781 ), could you please take a look at it?Thank you very much!

> To be honest, this translation is quite in a style of machine translation, with many terminologies and code structures translated incorrectly...
> 
> I think you have better take a look at the dictionary in Chinese README to see how to deal with terminology:
> 
> https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/i18n/README_zh-hans.md?plain=1#L22-L39
> 
> For some terminologies not listed in README, you can refer to [Google Developers Documentation](https://developers.google.cn/machine-learning/glossary?hl=zh-cn#Transformer), they have provided a dictionary for machine learning terminology.

 Hey, instead of creating a new PR, you can keep working on this current one to avoid cluttering the repo :)

@statelesshz or @jiaqiw09, based on your great work on previous Chinese translations, would you be interested in quickly reviewing? 🤗  > Hey, instead of creating a new PR, you can keep working on this current one to avoid cluttering the repo :)
> 
> @statelesshz or @jiaqiw09, based on your great work on previous Chinese translations, would you be interested in quickly reviewing? 🤗

Okay, I understand, thank you very much for your guidance. I have submitted the latest revision 'translated2'. Can it be reviewed now? > Only a small nit

Thank you very much for your valuable suggestions. I have submitted the revised changes accordingly.Thanks for your translation! Do you know someone who can read Chinese and would be interested in reviewing your translation? I can only review the high-level things because I can't read Chinese 😅  To be honest, this translation is quite in a style of machine translation, with many terminologies and code structures translated incorrectly...

I think you have better take a look at the dictionary in Chinese README to see how to deal with terminology: https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/i18n/README_zh-hans.md?plain=1#L22-L39

For some terminologies not listed in README, you can refer to [Google Developers Documentation](https://developers.google.cn/machine-learning/glossary?hl=zh-cn#Transformer), they have provided a dictionary for machine learning terminology. Only a small nit LGTM and thanks for the review @statelesshz!","Thanks for your translation! Do you know someone who can read Chinese and would be interested in reviewing your translation? I can only review the high-level things because I can't read Chinese 😅  To be honest, this translation is quite in a style of machine translation, with many terminologies and code structures translated incorrectly...

I think you have better take a look at the dictionary in Chinese README to see how to deal with terminology: https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/i18n/README_zh-hans.md?plain=1#L22-L39

For some terminologies not listed in README, you can refer to [Google Developers Documentation](https://developers.google.cn/machine-learning/glossary?hl=zh-cn#Transformer), they have provided a dictionary for machine learning terminology. Only a small nit LGTM and thanks for the review @statelesshz!","# What does this PR do?

Hello, I am a beginner of open source project. This PR translates attention.md into Chinese.Please let me know if there are any omissions, and I will be happy to revise them.

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
大多数 transformer 模型使用完全注意力机制，该机制采用正方形的注意力矩阵。当输入很长的文本时，这将导致巨大的计算瓶颈。Longformer 和 Reformer 是提高注意力机制效率的改进模型，它们使用稀疏化的注意力矩阵来加速训练。
``` Why `\times` command used in latex was translated to ""\乘""? Thank you for your advice! I'm sorry that I didn't fully understand this concept, so I translated it like this. I'll revise my translation right away. I'm so sorry I didn't realize it was a latex text at first. I really lack this knowledge. I'm going to study it carefully now. ```suggestion
[Reformer](model_doc/reformer)模型使用轴向位置编码：在传统的transformer模型中，位置编码矩阵E的大小是\\(l\\)乘以\\(d\\)，其中\\(l\\)是序列长度，\\(d\\)是隐藏状态的维度。如果你有非常长的文本，这个矩阵可能会非常大，将会占用大量的GPU显存。为了缓解这个问题，轴向位置编码将这个大矩阵E分解成两个较小的矩阵E1和E2，它们的维度分别是\\(l_{1} \times d_{1}\\) 和\\(l_{2} \times d_{2}\\)，满足\\(l_{1} \times l_{2} = l\\)和\\(d_{1} + d_{2} = d\\)（通过长度的乘积，最终得到的矩阵要小得多）。在E中，对于时间步\\(j\\) 的嵌入是通过连接E1中时间步 \\(j \% l1\\) 的嵌入和E2中时间步\\(j // l1\\)的嵌入来获得的。
```"
34393,2024-10-24T19:35:11Z,2024-11-19T15:44:53Z,philkuz,13,26,9,88,2,4,2,"['Vision', 'run-slow', 'ExecuTorch']",15737.0,0,2248761.0,0,0,0,0,3196730.370911,,0,9,0,False,"['qubvel', 'yonigozlan', 'guangy10', 'HuggingFaceDocBuilderDev', 'philkuz']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34393). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hey @yonigozlan if you have a chance, I would love another review of this PR after addressing your comments!
Thanks Looks great to me thanks again for fixing this! I think we should add a short comment just above the definition of `level_start_index_list` explaining why we do this (iterating over a tensor breaks torch.compile/export).
Please rebase on main and then ask a core maintainer for a final review :). Thanks Yoni!

Pinging @amyeroberts and @qubvel for a core maintainer review! > Thanks for working on this!
> 
> It would be also nice to have a test that export works, maybe smth similar to [this](https://github.com/huggingface/transformers/commit/5392f12e1614383270ae8df524415a1f6b555773).

Added an export test and addressed your other comments. Please let me know if you would like me to modify anything else! > Thanks for a quick iteration! A few more comments.
> 
> Please, push empty commit with message`[run_slow] mask2former` at the end to trigger all slow tests run for mask2former model

You got it! I think I need maintainer approval to run the slow workflows soft ping @ArthurZucker, regarding your last comment, I'm not sure it can be done without breaking changes, but as soon as it is the internal module of the model I suppose it is fine to change its signature. 🚨 can be added to PR message if it is still required > soft ping @ArthurZucker, regarding your last comment, I'm not sure it can be done without breaking changes, but as soon as it is the internal module of the model I suppose it is fine to change its signature. 🚨 can be added to PR message if it is still required

Hi @ArthurZucker @qubvel, 

Hope you're doing well, could you find some time today or tomorrow to provide guidance on this PR? Would love to check it off of my list! > > soft ping @ArthurZucker, regarding your last comment, I'm not sure it can be done without breaking changes, but as soon as it is the internal module of the model I suppose it is fine to change its signature. 🚨 can be added to PR message if it is still required
> 
> Hi @ArthurZucker @qubvel,
> 
> Hope you're doing well, could you find some time today or tomorrow to provide guidance on this PR? Would love to check it off of my list!

Hi @ArthurZucker @qubvel any chance we can move forward with this PR this week? Happy to do whatever you would like, just need to get guidance on what you would like to do here. Hi @philkuz, sorry for the delay, the team was on the offsite this week. I will ping Arthur to get it reviewed and merged. Thanks for the patience  > Hi @philkuz, sorry for the delay, the team was on the offsite this week. I will ping Arthur to get it reviewed and merged. Thanks for the patience

Thank you, Pavel! cc @guangy10  @philkuz Awesome! Thanks for expanding the export coverage to more models! 🚀 🚀 🚀 Looks good to me, thanks for working on this! I just have a question regarding one of your changes Thanks for working on this!

It would be also nice to have a test that export works, maybe smth similar to [this](https://github.com/huggingface/transformers/commit/5392f12e1614383270ae8df524415a1f6b555773). Thanks for a quick iteration! A few more comments.

Please, push empty commit with message`[run_slow] mask2former` at the end to trigger all slow tests run for mask2former model LGTM, thanks! I am not 100% we have to break not, do you think we can keep supporting the old behaviour / at least have a deprecation cycle?  Thanks for this contribution! 🤗 Looks good will just update the PR name!","Looks good to me, thanks for working on this! I just have a question regarding one of your changes Thanks for working on this!

It would be also nice to have a test that export works, maybe smth similar to [this](https://github.com/huggingface/transformers/commit/5392f12e1614383270ae8df524415a1f6b555773). Thanks for a quick iteration! A few more comments.

Please, push empty commit with message`[run_slow] mask2former` at the end to trigger all slow tests run for mask2former model LGTM, thanks! I am not 100% we have to break not, do you think we can keep supporting the old behaviour / at least have a deprecation cycle?  Thanks for this contribution! 🤗 Looks good will just update the PR name!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #34390 (issue)

Mask2Former modeling had a set of issues that prevented `torch.export` from working. This PR addresses ~3 individual problems that prevented the model from working. I took direction from a few PRs
that made similar changes:
1. [fix(Wav2Vec2ForCTC): torch export #34023](https://github.com/huggingface/transformers/pull/34023) for the attention mask modification to prevent a RuntimeError during torch.export
2. [Optim deformable detr #33600](https://github.com/huggingface/transformers/pull/33600) for passing around [spatial_shapes as a list](https://github.com/huggingface/transformers/pull/33600/files#diff-137bda311c71a6ad1765c311a04366dc1b547213c95795752ddb11e39466bbabR701) and as a tensor [and for the if statement fix](https://github.com/huggingface/transformers/pull/33600/files#diff-137bda311c71a6ad1765c311a04366dc1b547213c95795752ddb11e39466bbabR711)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@amyeroberts, @qubvel, @ylacombe (tagging because you looked at #34023)

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
 ## Testing:
 Run this once in `main` and then once with this branch. Ensure the testing.all_close() works.
 ```python
 import os

import numpy as np
import requests
import torch
from PIL import Image
from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation


def get_test_image():
    url = ""http://images.cocodataset.org/val2017/000000039769.jpg""
    image = Image.open(requests.get(url, stream=True).raw)
    return image


processor = AutoImageProcessor.from_pretrained(
    ""facebook/mask2former-swin-base-coco-panoptic""
)
model = Mask2FormerForUniversalSegmentation.from_pretrained(
    ""facebook/mask2former-swin-base-coco-panoptic""
)


results = model(**processor(images=get_test_image(), return_tensors=""pt""))

results = processor.post_process_panoptic_segmentation(results)

results_file = ""segmentation_results.pt""
if os.path.exists(results_file):
    old_segmentation = torch.load(results_file)
    np.testing.assert_array_almost_equal(results[0][""segmentation""], old_segmentation)
else:
    torch.save(results[0][""segmentation""], results_file)

scripted_model = torch.export.export(model, args=(torch.randn(1, 3, 800, 1280),))

```

I also visually compared the output masks and they look the same","Nice! Is this change necessary for torch compile/export? Does it impact performance? Just curious as this is also present in other models where a spatial_shapes_list is added, but it did not seem to break the graph or cause any problems with torch compile When I undo this line change, I start getting this error during torch.export
```
from user code:
   File ""/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py"", line 2512, in forward
    outputs = self.model(
  File ""/home/philkuz/.pyenv/versions/gml311/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py"", line 2283, in forward
    pixel_level_module_output = self.pixel_level_module(
  File ""/home/philkuz/.pyenv/versions/gml311/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py"", line 1406, in forward
    decoder_output = self.decoder(backbone_features, output_hidden_states=output_hidden_states)
  File ""/home/philkuz/.pyenv/versions/gml311/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py"", line 1355, in forward
    outputs = [
  File ""/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py"", line 1356, in <listcomp>
    x.transpose(1, 2).view(batch_size, -1, spatial_shapes_list[i][0], spatial_shapes_list[i][1])
 ```
 
 which is connected through this:
 1. `level_start_index` fills in the `split_sizes` in L1347-1352
 2. `split_sizes` is an input to `torch.split` which outputs `encoder_output` in L1345 
 3. `encoder_output` is iterated over in the list comprehension of L1357-1359 which is where we get the bottom of the stack trace.

Let me find the other uses of `spatial_shapes_list` and see what's different. Tangent: `level_start_index` isn't used at all in the attention module and I can't find any other uses than to inform `split_sizes` Some updates:
Some models that have `level_start_index` don't have the same `torch.split` call that we have above.
Just FYI this split is to do the mutli-level feature input to the TransformerDecoder (here's from the Mask2Former paper)
<img width=""412"" alt=""image"" src=""https://github.com/user-attachments/assets/99bfc2b6-06fd-4993-9c6f-f7e8e0292ef9"">

The ones that do have the torch.split seem to fail export, likely for similar reasons to mask2former.

I've found the following models use a `level_start_index`:
1. deformable_detr: no `torch.split`  call, torch.export runs
2. oneformer: has `torch.split` call, `torch.export` fails
3. rt_detr: no `torch.split` call, torch.export runs
this is not a full list, but I think helped me get an idea of what's going on.

But my conclusion is that `level_start_index` does need to change to be compatible for our specific use-cases. We could simply rename `level_start_index` in this case to better match that we are using it for split rather than for `level_start_index` parameter. Kinda simliar to how you added the `spatial_shapes_list`. I updated the PR to separate level_start_index into level_start_index and level_start_index_list

Let me know if there's anything else that you need from me! Thanks for diving deeply in this issue and for the detailed explanation! You're right it looks like `level_start_index` is actually not used in this model, apart for building `split_sizes`, so we definitely shouldn't let it block `torch.export` :). Looking at other models using `level_start_index`, it seems that it is used only for the custom kernel `MultiScaleDeformableAttentionFunction`, which is not present in mask2former (don't know why, but that can be the object of another PR).
I agree with what you've done, I think we can leave the tensor version of `level_start_index` even if it's not used now, and open a PR in the future to add the custom kernel `MultiScaleDeformableAttentionFunction` that will use it.
As you explained, only oneformer should have a similar problem with `level_start_index`, so feel free to open a PR for that too! ```suggestion
        for height, width in spatial_shapes_list[:-1]:
            level_start_index_list.append(level_start_index_list[-1] + height * width)
``` Should we pass both the list and the tensor? Can't we convert it inside or are there any performance issues in that case?
cc @yonigozlan we could probably convert the list to the tensor inside this forward call without affecting performance. 

the spatial_shapes tensor is directly passed into multiple `encoder_layer`s of this module so this feels like the lowest we can go without changing the performance. Although I wonder how bad the performance impact will be if we never create a tensor and instead read the spatial_shapes_list here: https://github.com/philkuz/transformers/blob/78f384808a70d346cd8f1e8d314411510edec11d/src/transformers/models/mask2former/modeling_mask2former.py#L962  Wrote a quick torch benchmark based on where this tensor is actually used
```
import time
import torch

# Create sample data - list of 1000 tuples with 3 elements each
# https://github.com/philkuz/transformers/blob/78f384808a70d346cd8f1e8d314411510edec11d/src/transformers/models/mask2former/modeling_mask2former.py#L1219
num_feature_levels = 3  # hardcoded in the modeling code

# https://huggingface.co/facebook/mask2former-swin-small-coco-instance/blob/main/config.json#L2118
# https://huggingface.co/facebook/mask2former-swin-large-coco-instance/blob/main/config.json#L2118
config_encoder_layers = (
    6  # Set in the config, seems to be set at 6 across small and large models
)
data = [(i, i + 1) for i in range(num_feature_levels)]

# Run benchmark
num_iterations = 1000
total_time = 0

for _ in range(num_iterations):
    start = time.perf_counter()
    tensor = torch.tensor(data)
    for i in range(config_encoder_layers):
        offset_normalizer_tensor = torch.stack([tensor[..., 1], tensor[..., 0]], -1)
    end = time.perf_counter()
    total_time += end - start

avg_time = total_time / num_iterations
print(
    f""Average time from spatial_shapes tensor {num_iterations} iterations: {avg_time:.6f} seconds""
)


num_iterations = 1000
total_time = 0
for _ in range(num_iterations):
    start = time.perf_counter()
    for i in range(config_encoder_layers):
        offset_normalizer_tensor = torch.tensor([[d[1], d[0]] for d in data])
    end = time.perf_counter()
    total_time += end - start

avg_time = total_time / num_iterations
print(
    f""Average time from original list of tuples {num_iterations} iterations: {avg_time:.6f} seconds""
)
```

results:
```
Average time from spatial_shapes tensor 1000 iterations: 0.000064 seconds
Average time from original list of tuples 1000 iterations: 0.000033 seconds
```

I think we should just drop the tensor entirely, given that we really are dealing with a 3x2 sized list, seems like tensors just add cognitive and performance overhead. I ended up dropping the spatial_shapes tensor that's passed around. I still have the original because `level_start_index` still exists, but I think we should just remove `level_start_index` in a separate PR. I address that change here: https://github.com/huggingface/transformers/pull/34498 Can you please update 

```python
from transformers.pytorch_utils import is_torch_greater_or_equal_2_4
```
instead of doing comparison with packaging it should be slow test -  decorated with `@slow` Ok, great! Thanks a lot for benchmarking it! yep the testcase (the class) is already marked as slow: https://github.com/philkuz/transformers/blob/d08855c2e7e49878f8f80af9c590778f77dbe651/tests/models/mask2former/test_modeling_mask2former.py#L364 Without setting the RUN_SLOW env: 
```
tests/models/mask2former/test_modeling_mask2former.py::Mask2FormerModelIntegrationTest::test_export SKIPPED (test is slow)
``` no problem changing the name is a breaking change, we should probably have a deprecation cycle no?  if not, we can add 🔴 as I think the motivation is strong enough.  Hmm, I suppose it is an internal module of the model, not sure if it is intended to be used elsewhere, let me know if I'm wrong Isn't changing the logic, but keeping both `spatial_shapes` and `spatial_shapes_list` already a breaking change?

Sure there's some question of whether users can rely on internal Modules of transformers models, but also if a user doesn't pass a value for `spatial_shapes_list` this code will fail as [L939](https://github.com/huggingface/transformers/pull/34393/files#diff-1fee617cdf6a827988f42ee7a5cde812e08a4867257e03684df091e03a4141deR939) will try to iterate over a None object.

BTW seems like the changes in #33600 already violate this contract([see this line](https://github.com/huggingface/transformers/pull/33600/files#diff-137bda311c71a6ad1765c311a04366dc1b547213c95795752ddb11e39466bbabR711))? I followed that PR as a guide on what I should change here.

It seems like the proper way forward is to add a 🔴  here.
 What's the path forward @ArthurZucker ? Bumping this again after the weekend. I'm not familiar with Huggingface's policies of 
1. What qualifies as a breaking change?
2. What the release process is?

Could you provide a recommendation on where we should take this PR? Usually even if a module is `Internal` people still end up using it 😅 
This IS a breaking change, but acceptable IMO. Let's use  some  🚨 on the PR title to make sure we communicate about it on the release!"
34783,2024-11-18T15:13:10Z,2024-11-19T17:49:10Z,merveenoyan,2,4,7,40,1,1,1,[],1202.0,0,95762.0,0,0,0,0,3205451.248763,,0,7,0,False,"['HuggingFaceDocBuilderDev', 'yonigozlan']",Thanks a lot for taking care of that! Looks great 🤗 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34783). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for updating!,Thanks for updating!,Documented the pipeline made by @yonigozlan cc @stevhliu ,"```suggestion
The fastest way to get started is to use the [`Pipeline`] API. Specify the `""image-text-to-text""` task and the model you want to use.
``` ```suggestion
The example below uses chat templates to format the text inputs.
``` ```suggestion
Pass the chat template formatted text and image to [`Pipeline`] and set `return_full_text=False` to remove the input from the generated output.
``` You can just merge these two code blocks. Simpler to just show instead of tell :)

```suggestion
```"
34806,2024-11-19T12:51:35Z,2024-11-19T16:48:34Z,ydshieh,1,0,1,4,1,1,1,[],1656.0,0,14220.0,0,0,0,0,3209088.646381,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34806). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks 🧼 ,Thanks 🧼 ,"# What does this PR do?

Currently inour CI runners' environment, there is no `pytest-subtests` installed. In this case, using `subTest` with `skipTest` in it will skip the whole test even if our goal is to just skip that `subTest` and continue to the next step in the for loop. In `check_training_gradient_checkpointing`, the first `model_class` is the base model and it calls `skipTest` and no other `model_class` being checked.

However, `pytest-subtests` has some issues regarding the reporting. A PR is opened https://github.com/pytest-dev/pytest-subtests/pull/169.

In the meantime, let's use `continue` for now instead of `skipTest` so we can really test gradient checkpointing.",
34812,2024-11-19T15:22:31Z,2024-11-19T16:32:10Z,ydshieh,1,0,3,3,1,1,1,[],2142.0,0,4181.0,0,0,0,0,3210072.349646,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34812). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice thanks!,Nice thanks!,"# What does this PR do?

This (pipeline) test 

> test_medium_seamless_m4t_pt

is always failing all the time. 

Previously, the pipeline test is running on CPU, but after #34026, it's running on GPU if available.

And with the issue #34811, this failed test affect many subsequent pipeline tests because of

> RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. ",
34617,2024-11-05T15:15:03Z,2024-11-18T20:37:51Z,ydshieh,9,2,3,15,2,2,1,[],1643.0,0,1213800.0,0,0,0,0,3210502.462647,,0,3,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'gante', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34617). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > tests/models/rt_detr/test_image_processing_rt_detr.py::RtDetrImageProcessingTest::test_fast_is_faster_than_slow

it's flaky Should this be merged? > Should this be merged?

Yes, but I am waiting for an approval. Currently, `whisper` has ~170 failed tests without this PR. Hey guys 👋 @ydshieh @ArthurZucker 

The generate changes in this PR are incorrect (changing `max_length`) :) I'm reverting them as part of https://github.com/huggingface/transformers/pull/34807, but I'd recommend rolling back this PR.

If Whisper's tests are failing, then it is probably something wrong with either whisper or whisper's tests


 Hi @gante This PR is only adding

> if generation_config.max_length == GenerationConfig().max_length

to avoid the failure caused in #34377 (which was an attempt fix for #34026)

> The generate changes in this PR are incorrect

Is the code changes in the previous PR #34377  (and/or #34026) are already wrong, or it's wrong because of the `if` condition added in this PR? Yeah I think it's from the older pipeline PR 😭 (`generation_config.max_length = generation_config.max_length + input_ids_seq_length` shouldn't exist) But the check for `max_position_embeddings` should exist either -- some models can generate beyond this value (e.g. llama) So if we want to rollback, it's more involved (i.e. not revert this PR but some previous changes too).
In any case, ping me before merging a new change on this PR so I can see if the Whisper CI is still OK 🙏 .
(or if you want to check that part on your side 😄 )Thanks! 🤗 ",Thanks! 🤗 ,Finally CI is green,"Diverge a bit from torch one, but let's try to match them in a follow-up when we feel it's time to do it.
Here is just to make CI green thanks for putting this back!"
34353,2024-10-23T16:50:33Z,2024-11-19T16:18:58Z,yonigozlan,3,2,10,1655,10,2,1,"['Vision', 'optimization', 'Processing']",1603.0,0,2330905.0,0,0,0,0,3210867.803314,,0,10,0,False,"['HuggingFaceDocBuilderDev', 'yonigozlan']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34353). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Will make the modifications once this PR https://github.com/huggingface/transformers/pull/34354 is merged, as most of them will be copied from :) > One thing I don't understand: literally everything is copied from. Why not directy map to use the detr class?

All pre-processing functions are copied from `image_processing_detr_fast`, but the post-processing function are copied from `image_processing_deformable_detr`. I guess the post-processing functions are also the reasons why there is a base `DeformableDetrImageProcessorFast` in the first place, as all the base pre-processing functions are also copied from `image_processing_detr`Thanks, same comment as for the other PR mostly! 🤗  One thing I don't understand: literally everything is copied from. Why not directy map to use the detr class?  Got it, thanks! Let's work to make it simpler to add these, with maybe a bit of abstraction on the FastImageProcessor class!","Thanks, same comment as for the other PR mostly! 🤗  One thing I don't understand: literally everything is copied from. Why not directy map to use the detr class?  Got it, thanks! Let's work to make it simpler to add these, with maybe a bit of abstraction on the FastImageProcessor class!","# What does this PR do?
Adds a fast image processor for Deformable DETR. Follows issue https://github.com/huggingface/transformers/issues/33810.
This image processor is a result of [this work](https://www.notion.so/huggingface2/OptimVision-Optimize-preprocessing-time-10f1384ebcac8091a12debb87fe5f591) on comparing different image processing method.

The diffs look bad but this PR is almost exclusively made up of `# Copied from` based on the fast image processor for DETR!

## Implementation
See https://github.com/huggingface/transformers/pull/34063

## Usage
Except for the fact that it only returns torch tensors, this fast processor is fully compatible with the current one.
It can be instantiated through AutoImageProcessor with use_fast=True, or through the Class directly:
```python
from transformers import AutoImageProcessor

processor = AutoImageProcessor.from_pretrained(""SenseTime/deformable-detr"", use_fast=True)
```
```python
from transformers import DeformableDetrImageProcessorFast

processor = DeformableDetrImageProcessorFast.from_pretrained(""SenseTime/deformable-detr"")
```

Usage is the same as the current processor, except for the `device` kwarg:
```python
from torchvision.io import read_image
images = torchvision.io.read_image(image_path)
processor = DeformableDetrImageProcessorFast.from_pretrained(""SenseTime/deformable-detr"")
images_processed = processor(images , return_tensors=""pt"", device=""cuda"")
```
If `device` is not specified:
- If the input images are tensors, the processing will be done on the device of the images.
- If the inputs are PIL or Numpy images, the processing is done on CPU.

## Performance gains

- Average over 100 runs on the same 480x640 image. No padding needed, as ""all"" the images have the same size.

![benchmark_results_full_pipeline_deformable_detr_fast_single](https://github.com/user-attachments/assets/f16b78ef-849a-44a8-a2b0-94b236fd3ce2)

---

- Average over 10% of the COCO 2017 validation dataset, with `batch_size=8`. Forcing padding to 1333x1333 (=""longest_edge""), as otherwise torch.compile needs to recompile if the different batches have different max sizes.

![benchmark_results_full_pipeline_deformable_detr_fast_batched_compiled](https://github.com/user-attachments/assets/ac201f68-0fbf-459c-87be-50909f7dc2cd)


---

- Average over 10% of the COCO 2017 validation dataset, with `batch_size=1`. Forcing padding to 1333x1333.
![benchmark_results_full_pipeline_deformable_detr_fast_padded](https://github.com/user-attachments/assets/066de4b1-0742-484d-bdc9-8a24b21a1154)


---

## Tests
- The new image processor is tested on all the tests of the current processor.
- I have also added two consistency tests (panoptic and detection) for processing on GPU vs CPU.


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

## Who can review?

@ArthurZucker Pinging you directly as there is almost no ""new"" code here.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Code is very similar to #34354 (of course!), comments over there are valid here too :) 
```suggestion
``` identation seems wrong no? "
34562,2024-11-01T13:02:35Z,2024-11-19T16:08:38Z,yonigozlan,9,4,8,80,2,3,2,[],1740.0,0,1566363.0,0,0,0,0,3211488.819041,,0,8,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1', 'yonigozlan', 'zucchini-nlp']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34562). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks for the review! Agreed that the Transformers chat template looks cleaner and easier to use, and nice to see it's getting even better with your PR. 
However it does look like the pipelines are the one place where we need support for both transformers and TGI chat templates format, so I can improve this PR to do so. @zucchini-nlp I think modifications inside `apply_chat_template` to do image loading are a good idea! We should ensure that the input to Jinja doesn't change though.

For the formats of content keys with PIL images vs. image paths etc. my preference is always to be close to the OpenAI API, and then add extra features on top of that if we need. Thanks, i think in that case we can remove the extra nesting in dicts which will still be close to OpenAI format. And yeah, we can consider passing load time params through the template dict, currently we dont have any options except for number of frames in loading videos or sampling rate in audios, which are usually same for all video/audio in the conversation

@yonigozlan wdyt? we can update the PR with these in mind > WDYT if we remove extra nesting but leave the keys to define ""url"" or ""path""? That is what I had in mind before simplifying it further in linked PR
> 
> Otherwise the format looks a bit weird to me as follows:
> 
> URL: content: [""type"": ""image_url"", ""image_url"": {""url"": 'https:// or ""data:base64-image""'}]
> PIL Image: content: [""type"": ""image"", ""image"": {""image"": PIL.Image}]
> Path but not here yet?: content: [""type"": ""image_path"", ""image_path"": {""path"": ""my-local-path.png""}]

Looking again at the OpenAI API doc, it seems that the only way to pass in images is through ""image_url"", even if the image is local, as shown [here ](https://platform.openai.com/docs/guides/vision#uploading-base64-encoded-images).
Same for TGI and API inference.

I also agree that the simplified format of transformers using only ""image"", whether we have an url, PIL image or path makes more sense and is more user friendly. So I don't think we need to add support for anything else other than the openai/TGI nested ""image_url"" format in this pipeline. It seems to me that we should keep the nesting in that specific case, because the goal is to be fully compatible with the openai/tgi format. Just to sync after the offsite so we have same formats with https://github.com/huggingface/transformers/pull/34275, do I remember correctly that we will:
1. use unnested dict but with typing so each image might be a url, path, base64, or just PIL/array/tensor
2. can support OpenAI format which is only for URLs with a bit preprocessing, but only as a possible format for pipelines/endpoints while still keeping the general chat template format with unnested dicts 

I agree OpenAI format is kinda standard for most users and we use it in TGI/endpoints etc., but sadly it is very inconvenient and is made to support only URLs Sounds good to me @zucchini-nlp!
Just to make sure about 1. , we would still have:
```
 {
     ""role"": ""user"",
     ""content"": [
         {
             ""type"": ""image"",
             ""image"": image, # where image can be a url, path, base64, or PIL/array/tensor
         },
         {""type"": ""text"", ""text"": ""Describe this image.""},
     ],
 },
```

Or are we explicitly typing `""image""`? @yonigozlan no, let's keep the explicit typing only and OpenAI format (which will be available only through pipelines for now) > @yonigozlan no, let's keep the explicit typing only and OpenAI format (which will be available only through pipelines for now)

Ok I see, I made some changes to reflect this in this PR then :) Hey, thanks for changing the format to be consistent with the API inference endpoint.

Actually I have a PR for updating chat templates to laod images/videos directly when calling the `apply_chat_template` (https://github.com/huggingface/transformers/pull/34275/). Yes, my inital idea was to use the format from TGI but I realized since we use the same function to load from path/url/PIL it would be easier to simplify the format.

Indeed that would make things easier if we use the same format and since the pipeline will be used by the API endpoint. But I find the format a bit complicated myself for url, with unnecessary nesting. Also the `audio` format from OpenAI is completely different according to [this post](https://simonwillison.net/2024/Oct/18/openai-audio/).

From what I see the serving libraries dont have a uniform format yet except for the current url format in some cases. That is a good chance for us to set the new standard and encourage developers/users to user our templates when they upload models on the hub. 

WDYT if we remove extra nesting but leave the keys to define ""url"" or ""path""? That is what I had in mind before simplifying it further in linked PR

Otherwise the format looks a bit weird to me as follows:
- URL: `content: [""type"": ""image_url"", ""image_url"": {""url"": 'https:// or ""data:base64-image""'}]`
- PIL Image: `content: [""type"": ""image"", ""image"": {""image"": PIL.Image}]`
- Path but not here yet?: `content: [""type"": ""image_path"", ""image_path"": {""path"": ""my-local-path.png""}]`

also cc @Rocketknight1 in the loop to get your opinion on templates Thanks, LGTM! I'll change the chat template later to support the same format, so we can call `apply_template` internally in pipelines LGTM, I think we need to mention this in the documentation of the pipeline as well! Quite a big win IMO","Hey, thanks for changing the format to be consistent with the API inference endpoint.

Actually I have a PR for updating chat templates to laod images/videos directly when calling the `apply_chat_template` (https://github.com/huggingface/transformers/pull/34275/). Yes, my inital idea was to use the format from TGI but I realized since we use the same function to load from path/url/PIL it would be easier to simplify the format.

Indeed that would make things easier if we use the same format and since the pipeline will be used by the API endpoint. But I find the format a bit complicated myself for url, with unnecessary nesting. Also the `audio` format from OpenAI is completely different according to [this post](https://simonwillison.net/2024/Oct/18/openai-audio/).

From what I see the serving libraries dont have a uniform format yet except for the current url format in some cases. That is a good chance for us to set the new standard and encourage developers/users to user our templates when they upload models on the hub. 

WDYT if we remove extra nesting but leave the keys to define ""url"" or ""path""? That is what I had in mind before simplifying it further in linked PR

Otherwise the format looks a bit weird to me as follows:
- URL: `content: [""type"": ""image_url"", ""image_url"": {""url"": 'https:// or ""data:base64-image""'}]`
- PIL Image: `content: [""type"": ""image"", ""image"": {""image"": PIL.Image}]`
- Path but not here yet?: `content: [""type"": ""image_path"", ""image_path"": {""path"": ""my-local-path.png""}]`

also cc @Rocketknight1 in the loop to get your opinion on templates Thanks, LGTM! I'll change the chat template later to support the same format, so we can call `apply_template` internally in pipelines LGTM, I think we need to mention this in the documentation of the pipeline as well! Quite a big win IMO","# What does this PR do?
The huggingface inference API uses an OpenAI like chat template for image_url input: https://huggingface.co/docs/api-inference/tasks/image-text-to-text#using-the-api

This adds support for such image inputs in chats in the image-text-to-text pipeline.
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->


## Who can review?

Cc @zucchini-nlp @NielsRogge 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","let's also have one more type here - `base64` Done! this can be more efficient, a single for loop should be enough!  Just learned about the for...else python structure, makes it cleaner indeed, thanks!"
34792,2024-11-18T21:23:30Z,2024-11-19T16:08:07Z,dependabot[bot],1,0,1,2,1,1,1,"['dependencies', 'python']",1708.0,0,67479.0,0,0,0,0,3211518.468963,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34792). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"Bumps [aiohttp](https://github.com/aio-libs/aiohttp) from 3.10.2 to 3.10.11.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/releases"">aiohttp's releases</a>.</em></p>
<blockquote>
<h2>3.10.11</h2>
<h2>Bug fixes</h2>
<ul>
<li>
<p>Authentication provided by a redirect now takes precedence over provided <code>auth</code> when making requests with the client -- by :user:<code>PLPeeters</code>.</p>
<p><em>Related issues and pull requests on GitHub:</em>
<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9436"">#9436</a>.</p>
</li>
<li>
<p>Fixed :py:meth:<code>WebSocketResponse.close() &lt;aiohttp.web.WebSocketResponse.close&gt;</code> to discard non-close messages within its timeout window after sending close -- by :user:<code>lenard-mosys</code>.</p>
<p><em>Related issues and pull requests on GitHub:</em>
<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9506"">#9506</a>.</p>
</li>
<li>
<p>Fixed a deadlock that could occur while attempting to get a new connection slot after a timeout -- by :user:<code>bdraco</code>.</p>
<p>The connector was not cancellation-safe.</p>
<p><em>Related issues and pull requests on GitHub:</em>
<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9670"">#9670</a>, <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9671"">#9671</a>.</p>
</li>
<li>
<p>Fixed the WebSocket flow control calculation undercounting with multi-byte data -- by :user:<code>bdraco</code>.</p>
<p><em>Related issues and pull requests on GitHub:</em>
<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9686"">#9686</a>.</p>
</li>
<li>
<p>Fixed incorrect parsing of chunk extensions with the pure Python parser -- by :user:<code>bdraco</code>.</p>
<p><em>Related issues and pull requests on GitHub:</em>
<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9851"">#9851</a>.</p>
</li>
<li>
<p>Fixed system routes polluting the middleware cache -- by :user:<code>bdraco</code>.</p>
<p><em>Related issues and pull requests on GitHub:</em></p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>
<blockquote>
<h1>3.10.11 (2024-11-13)</h1>
<h2>Bug fixes</h2>
<ul>
<li>
<p>Authentication provided by a redirect now takes precedence over provided <code>auth</code> when making requests with the client -- by :user:<code>PLPeeters</code>.</p>
<p><em>Related issues and pull requests on GitHub:</em>
:issue:<code>9436</code>.</p>
</li>
<li>
<p>Fixed :py:meth:<code>WebSocketResponse.close() &lt;aiohttp.web.WebSocketResponse.close&gt;</code> to discard non-close messages within its timeout window after sending close -- by :user:<code>lenard-mosys</code>.</p>
<p><em>Related issues and pull requests on GitHub:</em>
:issue:<code>9506</code>.</p>
</li>
<li>
<p>Fixed a deadlock that could occur while attempting to get a new connection slot after a timeout -- by :user:<code>bdraco</code>.</p>
<p>The connector was not cancellation-safe.</p>
<p><em>Related issues and pull requests on GitHub:</em>
:issue:<code>9670</code>, :issue:<code>9671</code>.</p>
</li>
<li>
<p>Fixed the WebSocket flow control calculation undercounting with multi-byte data -- by :user:<code>bdraco</code>.</p>
<p><em>Related issues and pull requests on GitHub:</em>
:issue:<code>9686</code>.</p>
</li>
<li>
<p>Fixed incorrect parsing of chunk extensions with the pure Python parser -- by :user:<code>bdraco</code>.</p>
<p><em>Related issues and pull requests on GitHub:</em>
:issue:<code>9851</code>.</p>
</li>
<li>
<p>Fixed system routes polluting the middleware cache -- by :user:<code>bdraco</code>.</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/aio-libs/aiohttp/commit/3e09325e4839117df13fbac301f360edf8d3a0ee""><code>3e09325</code></a> Remove 3.10.11rc0 from 3.10 changelog (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9858"">#9858</a>)</li>
<li><a href=""https://github.com/aio-libs/aiohttp/commit/beb7b740533b81d75706e6615f07d92fcbf1c325""><code>beb7b74</code></a> Release 3.10.11 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9857"">#9857</a>)</li>
<li><a href=""https://github.com/aio-libs/aiohttp/commit/259edc369075de63e6f3a4eaade058c62af0df71""><code>259edc3</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9851"">#9851</a>/541d86d backport][3.10] Fix incorrect parsing of chunk extensions w...</li>
<li><a href=""https://github.com/aio-libs/aiohttp/commit/bc15db61615079d1b6327ba42c682f758fa96936""><code>bc15db6</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9852"">#9852</a>/249855a backport][3.10] Fix system routes polluting the middleware ...</li>
<li><a href=""https://github.com/aio-libs/aiohttp/commit/158bf304bdd8047eec192540fa5bf7fe3862bffd""><code>158bf30</code></a> Release 3.10.11rc0 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9848"">#9848</a>)</li>
<li><a href=""https://github.com/aio-libs/aiohttp/commit/e5917cd3480b01e7527b6524f9bec954325e1d5f""><code>e5917cd</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9844"">#9844</a>/fabf3884 backport][3.10] Fix compressed get request benchmark paylo...</li>
<li><a href=""https://github.com/aio-libs/aiohttp/commit/68a1f42af90a5beae28c8617e0dfc15c3bd5153c""><code>68a1f42</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9840"">#9840</a>/cc5fa316 backport][3.10] Add benchmark for sending compressed paylo...</li>
<li><a href=""https://github.com/aio-libs/aiohttp/commit/4f4b90fef082fbb37395c394d68ee0ab3fcbc7e6""><code>4f4b90f</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9835"">#9835</a>/32ccfc9a backport][3.10] Adjust client payload benchmarks to better...</li>
<li><a href=""https://github.com/aio-libs/aiohttp/commit/f3dd0f9fece79dc3cd9d00e2ffddd49c36598361""><code>f3dd0f9</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9832"">#9832</a>/006f4070 backport][3.10] Increase allowed import time for Python 3....</li>
<li><a href=""https://github.com/aio-libs/aiohttp/commit/f2aab2e40336848d6a53ea03dc6d072a38c5e7f9""><code>f2aab2e</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/9827"">#9827</a>/14fcfd4c backport][3.10] Adjust client GET read benchmarks to inclu...</li>
<li>Additional commits viewable in <a href=""https://github.com/aio-libs/aiohttp/compare/v3.10.2...v3.10.11"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=aiohttp&package-manager=pip&previous-version=3.10.2&new-version=3.10.11)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/huggingface/transformers/network/alerts).

</details>",
34728,2024-11-14T07:32:48Z,2024-11-19T15:51:32Z,sywangyi,3,3,1,8,1,4,2,"['Vision', 'Multimodal']",142669.0,0,461924.0,0,0,0,0,3212516.169644,,0,1,0,False,"['qubvel', 'HuggingFaceDocBuilderDev', 'sywangyi']","Hi @sywangyi, thanks for submitting a PR! Can you please provide your environment setup and minimum example, so we can reproduce the error on our side too? Adding a test also would be much appreciated! Thank you!

cc @zucchini-nlp for vlms I use the latest transformers code 
and here's my example

```
from transformers import LlavaNextForConditionalGeneration, LlavaNextProcessor
from PIL import Image
import requests
import torch

processor = LlavaNextProcessor.from_pretrained(""tiiuae/falcon-11B-vlm"", tokenizer_class='PreTrainedTokenizerFast')
model = LlavaNextForConditionalGeneration.from_pretrained(""tiiuae/falcon-11B-vlm"", torch_dtype=torch.bfloat16)


url = ""https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true""
cats_image = Image.open(requests.get(url, stream=True).raw)

prompt = ""User:<image>\nWhat is shown in this image?\nAssistant:""
inputs = processor(text=prompt, images=cats_image, return_tensors=""pt"").to('cuda:0')
model.to('cuda:0')
output = model.generate(**inputs, max_new_tokens=100, do_sample=False)


generated_captions = processor.decode(output[0], skip_special_tokens=True).strip()

print(generated_captions)
```


 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34728). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Copilot reviewed 2 out of 2 changed files in this pull request and generated no suggestions.

<details>
<summary>Comments skipped due to low confidence (2)</summary>

**src/transformers/models/llava_next/modeling_llava_next.py:840**
* Ensure that self.config.image_token_index is defined and used correctly to avoid runtime errors.
```
input_ids_mask[input_ids == self.config.image_token_index] = 0
```
**src/transformers/models/falcon/modeling_falcon.py:1280**
* The new parameter 'num_logits_to_keep' should have test cases to ensure the slicing behavior of 'hidden_states' is correct.
```
num_logits_to_keep: int = 0,
```
</details>

 perfect, LGTM! Thanks! ","Copilot reviewed 2 out of 2 changed files in this pull request and generated no suggestions.

<details>
<summary>Comments skipped due to low confidence (2)</summary>

**src/transformers/models/llava_next/modeling_llava_next.py:840**
* Ensure that self.config.image_token_index is defined and used correctly to avoid runtime errors.
```
input_ids_mask[input_ids == self.config.image_token_index] = 0
```
**src/transformers/models/falcon/modeling_falcon.py:1280**
* The new parameter 'num_logits_to_keep' should have test cases to ensure the slicing behavior of 'hidden_states' is correct.
```
num_logits_to_keep: int = 0,
```
</details>

 perfect, LGTM! Thanks! ","# What does this PR do?

fix crash when using tiiuae/falcon-11B-vlm for image-to-text generation task

fix two crashes
1. one is in inputs_embeds calculation, ""image_token_index"": 65024 while ""vocab_size"": 65024, so embedding operation out of bounds
2. language_modeling forward in llava_next. the parameters contains num_logits_to_keep, while forward of FalconForCausalLM does not contain the params. crash in forward API calling 



- vision models: @amyeroberts, @qubvel

","we also need to add it in the docstring :) Hmm, makes sense indeed, but I am not sure if this would break compile compatibility for LLaVA models due to `clone()` op. Because if we accept this change, it has to be propagated to all LLaVA models. 

Currently they are not all compatible with `torch.compile(fullgraph=True)` as we have to remove legacy path, but nice to make sure it will work. I am more inclined to asking the authors on the hub to resize their embedding layer than adding a workaround in code though
 ok, maybe I could do model.resize_token_embeddings(processor.tokenizer.vocab_size+1) in example to WA it"
34588,2024-11-04T02:54:21Z,2024-11-19T15:49:25Z,OnTheThirdDay,3,4,6,40,8,4,4,[],37726.0,0,1342505.0,0,0,0,0,3212643.438906,,0,6,0,False,"['OnTheThirdDay', 'Rocketknight1']","This looks like a good fix to me, but cc @nielsrogge to confirm! I just made another change to the doc of post_process_instance_segmentation of both maskformer and mask2former image processors.

I added this line, as current default behavior of the method will not deal with the cases where instances could overlap, but the descriptions related to `return_coco_annotation ` and `return_binary_maps`, though clearly understandable, are not very intuitive for users to realize they need to use them.

```
If instances could overlap, set either return_coco_annotation or return_binary_maps to `True` to get the correct segmentation result.
```

Additionally, I changed this line to make it more precise for the description of the returned result covering the case where `return_binary_maps` is set to `True`.

```
`List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:
            - **segmentation** -- A tensor of shape `(height, width)` where each pixel represents a `segment_id`, or
              `List[List]` run-length encoding (RLE) of the segmentation map if return_coco_annotation is set to
              `True`, or a tensor of shape `(num_instances, height, width)` if return_binary_maps is set to `True`.
              Set to `None` if no mask if found above `threshold`.
``` the pr now has included above mentioned changes on doc.
@NielsRogge @qubvel @ArthurZucker Thanks for fixing! cc @qubvel  Thank you! 🤗 thanks",Thanks for fixing! cc @qubvel  Thank you! 🤗 thanks,"# What does this PR do?

- change doc of mask2former example of post_process_instance_segmentation to the right function call.
- change doc of maskformer and mask2former on post_process_instance_segmentation for clarity on working with overlapping instances.
- change examples in doc of segmentation models from image[::-1] to (image.height, image.width) for clarity.



<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","It would be also nice to include:
```suggestion
        ...     outputs, target_sizes=[(image.height, image.width)]
``` Should we change this line in the examples for all three segmentation tasks? Yeah, sure! Thanks for improving it, it's much better now!"
34523,2024-10-31T01:30:06Z,2024-11-19T15:48:06Z,jp1924,3,3,8,26,2,2,2,"['Vision', 'Processing']",601958.0,0,1693080.0,0,0,0,0,3212723.648051,,0,8,0,False,"['jp1924', 'HuggingFaceDocBuilderDev']","Thanks for the advice @qubvel! That worked out well.

I wrote that type_hint with reference to siglip, should I modify siglip as well? @qubvel 
Oh..... I thought I added `do_convert_rgb` to the fast processor, but I only added it to the arg??? haha😂
Thanks for letting me know!
 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34523). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Hi @jp1924, thanks for adding this feature! To make it complete we also have to update `VitImageProcessorFast` to be compatible with this image processor. Please have a look if there is a required transform in torchvision or use Lambda transform. Thanks! Hi @jp1924, thanks for the update! For fast image processor we also should add a transform that do RGB conversion, not only an argument. Thanks for the update! Looks good to me! LGTM but as this was not requested before I am not entirely sure it will be used! 🤗 ","Hi @jp1924, thanks for adding this feature! To make it complete we also have to update `VitImageProcessorFast` to be compatible with this image processor. Please have a look if there is a required transform in torchvision or use Lambda transform. Thanks! Hi @jp1924, thanks for the update! For fast image processor we also should add a transform that do RGB conversion, not only an argument. Thanks for the update! Looks good to me! LGTM but as this was not requested before I am not entirely sure it will be used! 🤗 ","# What does this PR do?

While resolving issue #34301, discovered that ViT lacks do_rgb_convert.

Therefore, added do_rgb_convert to ViTImageProcessor.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@amyeroberts, @qubvel
","```suggestion
        do_convert_rgb: Optional[bool] = None,
``` ```suggestion
        do_convert_rgb (`bool`, *optional*):
``` ```suggestion
        do_convert_rgb: Optional[bool] = None,
```"
34507,2024-10-30T12:33:42Z,2024-11-19T15:46:04Z,tibor-reiss,6,4,4,25,1,3,2,[],14349.0,0,1739543.0,0,0,0,0,3212845.529721,,1,4,0,False,"['tibor-reiss', 'HuggingFaceDocBuilderDev', 'SunMarc', 'LysandreJik']","I saw #31501 and these two seem related to some extent. Proposal: remove `include_tokens_per_second` and use `num_tokens=self.state.num_input_tokens_seen` instead of `num_tokens=num_train_tokens` when reporting metrics for the full run. What do you think @muellerzr? > We already have the option to set `include_num_input_tokens_seen`. Could you try it and tell us if it fits your use case. One thing that could be nice it to compare the results we get with `include_num_input_tokens_seen` and `include_tokens_per_second` cc @muellerzr

Hi @SunMarc, I think we are talking about the same. Someone requested in #34471 the option to be able to see the number of tokens inside the training loop - currently only the total is logged at the end of the training. `num_input_tokens_seen` is already tracked if `include_num_input_tokens_seen`, so I just added one line for the logging. This is the scope of the current PR.

My comment (https://github.com/huggingface/transformers/pull/34507#issuecomment-2447742561) is about simplification, because the current logic poses anyways issues with large datasets. This is not yet covered by this PR, but I could make another one if you agree with my proposal. > My comment (https://github.com/huggingface/transformers/pull/34507#issuecomment-2447742561) is about simplification, because the current logic poses anyways issues with large datasets.

Before doing that, could you verify that these two metrics returns actually the same thing. if yes, then we should definitely do as you suggested !  > > My comment ([#34507 (comment)](https://github.com/huggingface/transformers/pull/34507#issuecomment-2447742561)) is about simplification, because the current logic poses anyways issues with large datasets.
> 
> Before doing that, could you verify that these two metrics returns actually the same thing. if yes, then we should definitely do as you suggested !

Results from some basic tests (for speed, I commented out the call to `self.training_step` because it does not influence the result) with the following args:

case 1: `--log_level info     --model_name_or_path=distilbert/distilgpt2    --dataset_name=Salesforce/wikitext     --dataset_config_name=wikitext-2-raw-v1     --block_size=1024     --per_device_train_batch_size=8     --do_train     --output_dir=Llama-3.2-1B-wikitext-2-raw-v1     --overwrite_output_dir     --seed=42     --logging_steps=10     --lr_scheduler_type=cosine     --num_train_epochs=3     --learning_rate=5e-05     --warmup_ratio=0.03     --dataloader_drop_last  --include_tokens_per_second --include_num_input_tokens_seen`

case 2: using different model, i.e. `model_name_or_path=meta-llama/Llama-3.2-1B`

case 3: using `--max_step=50`

In all 3 cases, the values matched:
- case 1: 7'102'464
- case 2: 7'348'224
- case 3: 409'600

The one test missing is when dataloader does not have a size. Not sure yet how to create that. However, looking at the formula, it is likely that it will also match. cc @SunMarc @muellerzr :) The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34507). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.We already have the option to set `include_num_input_tokens_seen`. Could you try it and tell us if it fits your use case. One thing that could be nice it to compare the results we get with `include_num_input_tokens_seen` and `include_tokens_per_second` cc @muellerzr  LGMT ! Thanks for fixing this. A few nits. Can you have a second look @muellerzr ? 
Also @tibor-reiss, it can be a nice follow-up PR to deprecate `include_tokens_per_second`.  🚀 niceeee super long due haha thanks for adding this","We already have the option to set `include_num_input_tokens_seen`. Could you try it and tell us if it fits your use case. One thing that could be nice it to compare the results we get with `include_num_input_tokens_seen` and `include_tokens_per_second` cc @muellerzr  LGMT ! Thanks for fixing this. A few nits. Can you have a second look @muellerzr ? 
Also @tibor-reiss, it can be a nice follow-up PR to deprecate `include_tokens_per_second`.  🚀 niceeee super long due haha thanks for adding this","Fixes #34471 

If `include_num_input_tokens_seen` was part of `args`, the requested metrics can be printed from `self.state.num_input_tokens_seen`.

@muellerzr and @SunMarc
","Maybe we can put that in `_maybe_log_save_evaluate`  function instead as it is called at the end of each batch Thanks for the hint, I was not aware of this function. Moved the logic now. Maybe use `speed_metrics` here and we can also update the description of `include_num_input_tokens_seen` since we also including the speed metrics for tokens per second Corrected and rebased, thanks!"
34326,2024-10-23T01:29:21Z,2024-11-19T15:37:39Z,huismiling,5,0,25,26,1,1,1,[],14779.0,0,2383698.0,0,0,0,0,3213352.401176,,0,25,0,False,"['huismiling', 'HuggingFaceDocBuilderDev']","@ArthurZucker 
Hi, Could you help to merge this PR ? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34326). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Hey is there any doc regarding `PYTORCH_CNDEV_BASED_MLU_CHECK` anywhere? Or are we just using / creating is for this?

@ArthurZucker 
Hi, Just using / creating `PYTORCH_CNDEV_BASED_MLU_CHECK` is OK.
Like nvml, CNDEV is a cambricon library for MLUs, . It will not affect pytorch but torch_mlu.

`nvml-based` and `cndev-based` check has been used in accelerate. 
https://github.com/huggingface/accelerate/pull/3187

https://github.com/huggingface/accelerate/blob/ba7ab93f5e688466ea56908ea3b056fae2f9a023/src/accelerate/utils/imports.py#L116
```
def is_cuda_available():
    """"""
    Checks if `cuda` is available via an `nvml-based` check which won't trigger the drivers and leave cuda
    uninitialized.
    """"""
    with patch_environment(PYTORCH_NVML_BASED_CUDA_CHECK=""1""):
        available = torch.cuda.is_available()

    return available
```

https://github.com/huggingface/accelerate/blob/ba7ab93f5e688466ea56908ea3b056fae2f9a023/src/accelerate/utils/imports.py#L322

```
def is_mlu_available(check_device=False):
    """"""
    Checks if `mlu` is available via an `cndev-based` check which won't trigger the drivers and leave mlu
    uninitialized.
    """"""
    if importlib.util.find_spec(""torch_mlu"") is None:
        return False

    import torch_mlu  # noqa: F401

    with patch_environment(PYTORCH_CNDEV_BASED_MLU_CHECK=""1""):
        available = torch.mlu.is_available()

    return available
```
 @ArthurZucker 
Hi, is this PR okay to merge ? Is there anything else I can help ? @ArthurZucker 
Hey, Just remind this . Hey is there any doc regarding `PYTORCH_CNDEV_BASED_MLU_CHECK` anywhere? Or are we just using / creating is for this? Cool! Thanks for updating! 🤗 
Sorry we were on a company wide offsite, had no time to work 🌴 ","Hey is there any doc regarding `PYTORCH_CNDEV_BASED_MLU_CHECK` anywhere? Or are we just using / creating is for this? Cool! Thanks for updating! 🤗 
Sorry we were on a company wide offsite, had no time to work 🌴 ","# What does this PR do?

MLU devices : Checks if mlu is available via an cndev-based check which won't trigger the drivers and leave mlu

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34802,2024-11-19T11:35:55Z,2024-11-19T15:08:58Z,Cyrilvallez,1,0,4,178,6,1,1,[],2718.0,0,12784.0,0,0,0,0,3215072.640777,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34802). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks 🤗 ,Thanks 🤗 ,"# What does this PR do?

As per the title. This is basically the changes already present in https://github.com/huggingface/transformers/pull/34282 + a fix for protected imports duplication.
",
34750,2024-11-15T14:32:54Z,2024-11-19T13:16:34Z,SunMarc,1,2,6,22,3,4,3,[],1879.0,0,341022.0,0,0,0,0,3221816.601864,,0,6,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34750). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Only a small nit LGTM, thanks! Sounds good!","Only a small nit LGTM, thanks! Sounds good!","# What does this PR do?
Supersedes https://github.com/huggingface/transformers/pull/34606 and updated a bit the docs

Quanto kv cache tests are passing ",I think you can remove that print statement. oh yes my bad 
34282,2024-10-21T13:03:53Z,2024-11-19T12:52:38Z,ArthurZucker,2,22,58,962,8,4,1,[],63127.0,0,2504928.0,0,0,0,0,3223253.125533,,0,58,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker']","Okay @Cyrilvallez good point regarding cleaning! Makes more sense indeed, will update to fix 😉  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34282). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.There are a lot of edge cases in imports which are very hard to deal with with the proposed approach. I think a simpler and more general approach is to do it the other way around: 
- dump all imports from the `modular_xxx.py` as is
- dump all imports from the dependency files as is (this is currently the case)
- Then, in the `PostModularConverterCleaner`, clean the imports (may even only clean the protected imports, and let `ruff` remove the other unused, non-protected imports)

This approach is much easier and versatile because in the Cleaner, we have access to the final source code, which is not the case when visiting the `modular_xxx.py` file (we only see the modular + the dependencies, and it is hard to check imports relative to only the part of the dependency files that we copy in the final file). Thus, it would ensure that all needed imports are present (i.e. we will never reach a weird edge-case when trying to match the imports as we do currently), and we can correctly remove imports that were wrongly added from the dependency files (i.e. see duplicate import in Glm due to Phi3 dependency).  
This would greatly simplify the code complexity as well in my opinion. Very nice approach! Much simpler IMO 🤗 just added some nits for clarity LGTM, I actually love it, I think it's much better to use different attention functions instead of different attention classes (clearer, less duplicated code, and we can easily switch between implementations even after the model has been instantiated)","There are a lot of edge cases in imports which are very hard to deal with with the proposed approach. I think a simpler and more general approach is to do it the other way around: 
- dump all imports from the `modular_xxx.py` as is
- dump all imports from the dependency files as is (this is currently the case)
- Then, in the `PostModularConverterCleaner`, clean the imports (may even only clean the protected imports, and let `ruff` remove the other unused, non-protected imports)

This approach is much easier and versatile because in the Cleaner, we have access to the final source code, which is not the case when visiting the `modular_xxx.py` file (we only see the modular + the dependencies, and it is hard to check imports relative to only the part of the dependency files that we copy in the final file). Thus, it would ensure that all needed imports are present (i.e. we will never reach a weird edge-case when trying to match the imports as we do currently), and we can correctly remove imports that were wrongly added from the dependency files (i.e. see duplicate import in Glm due to Phi3 dependency).  
This would greatly simplify the code complexity as well in my opinion. Very nice approach! Much simpler IMO 🤗 just added some nits for clarity LGTM, I actually love it, I think it's much better to use different attention functions instead of different attention classes (clearer, less duplicated code, and we can easily switch between implementations even after the model has been instantiated)","# What does this PR do?
Adds capping for gemma2, fixes #32877","API has no `dropout_p` or `is_causal`:

```suggestion
        attn_output = flex_attention(
            query_states,
            key_states,
            value_states,
            # attn_mask=causal_mask,
            block_mask=causal_mask if is_causal else None,
            score_mod=tanh_softcap,
            scale=self.scaling,
        )
``` We're adding class names to something called `self.function_call_class_mapping` here. Maybe we should rename it to `self.callable_dependency_mapping`? This should be more general. Currently will match only Callable dependencies inside classes. But it could be inside functions, or even may not be a Call node (i.e. `from typing import xxx`, `xxx` is never a Call but still needs to be added.  This is hard to read. I would use more explicit variable names. Also, I think we should avoid using `node` for variables that are not of type `cst.Node` (here it is a set of strings), very confusing otherwise.  

Moreover, the check  `if len(node) == 1` is wrong in general -> if we have more than 1 class or function relying on the dependency, then it will not be added. It should instead iterate over the set and add the import to all files where at least one dependency is found. (I think the easiest for that is to call this function after the call to `self._recursively_add_all_new_needed_functions_in_files()` in `leave_Module` and check the bodies at this point (since dependencies may appear from new functions, as well as classes -> `self.class_to_file_type` cannot be used) Here imports from the dependency files are just dumped as is, without being filtered. In Glm, this causes a duplicated import because one import from Phi3 is protected, and the same import is unprotected in other files. Isn't it a bit misleading to use flex attn when we have `attn_implementation=""sdpa""`? My concerns would be 
- People that previously used sdpa (forced or not) will suddenly have different torch requirements
- Sdpa != Flexattn imo, it's a different API, name, and potentially slightly different behaviour
- Are the slow tests still passing? We should ensure that it's still behaving the same ish in comparison to eager

Wdyt about making another attn implementation option for flex attn specifically? Not sure if this goes over the goal but control over the specific implementation is always appreciated.

Overall excited to see this, great work!  SDPA version of `gemma` never ""worked"" TBH! 
I'll probably add a new class for flex attention, this was simpler for testing Wrong type here, should be `Dict[str, Set[Union[cst.Import, cst.ImportFrom]]]` if I understand correctly Should we keep those prints? In the end this name does not need to be modified! No need to iterate on `k` and use a dict: simpler to add to the whole node in a set as before imo (`code_for_node` is called in `leave_Module` anyway) Same here Not needed anymore! Here should be added only if this is an import statement Not needed anymore either, can be reversed ```suggestion
        self.config._attn_implementation = ""flash_attention_2""
        logger.warning_once(
            ""The `Gemma2FlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`""
            ""attribute of the `GemmaAttention` class! It will be removed in v4.48""
``` ```suggestion
        self.config._attn_implementation = ""sdpa""
        logger.warning_once(
            ""The `Gemma2FlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`""
            ""attribute of the `GemmaAttention` class! It will be removed in v4.48""
``` ```suggestion
    ""flash_attention_2"": flash_attention_forward,
```
I think we should use `flash_attention_2` for BC ```suggestion
            attention_type = ""flex_attention""
``` yes indeed! this breaks a few tests for now + is not BC indeed"
34240,2024-10-18T09:31:19Z,2024-11-19T12:20:07Z,ArthurZucker,8,30,16,229,15,4,3,[],265575.0,0,2774930.0,0,0,0,0,3225205.532193,,0,16,0,False,"['mostafaelhoushi', 'HuggingFaceDocBuilderDev', 'gante']","status: code runs, output is gibberish. Numerical debugging after lunch to figure out what's wrong > status: code runs, output is gibberish. Numerical debugging after lunch to figure out what's wrong

<s>Can you show a sample of how the output looks like? It is expected to be of lower quality but interested to see how gibberish it would be.

Also, maybe try a later layer like layer 14 or 13 to see if it's still gibberish?</s>

EDIT: Please ignore my comment above. I thought the output of early exit was gibberish but I think it was the output of self-speculative decoding was gibberish. Yes, self-speculative decoding should have same quality as last layer. Hi Mostafa (@mostafaelhoushi) 👋  Glad to see you here!

My utmost goal for this PR is to get Layer Skip to the hands of our users with a) good throughput numbers b) a simple interface. Self-speculative decoding is indeed the best of both worlds for low batch sizes 💪 

I appreciate the extra suggestions, but they add significant complexity -- e.g. if we accept callable for self-speculative decoding, we might want to apply the callable in different positions. Keeping things somewhat simple means others can just fork what we have and implement their idea quickly on top! It also makes our maintenance job doable 🤗 Naturally, if a given technique shows clear advantages and can be applied on pre-trained weights without too much complexity, like layer skip, we'll jump straight to implementation.

(For instance, a few years ago we implemented a complex constrained decoding method, before json generation became popular. However, because the implementation was complex and it was somewhat niche, it quickly became unmaintained -- we got the additional code bloat with no relevant benefits)

Sorry to be a turn off -- I really appreciate the ideas coming in! Update: I've removed cache sharing between the early exit and the full model -- it would require significant changes in `forward` to skip the computation of the cached values, which we don't want to commit for now. The outputs are now correct 👍 

Tomorrow I'll work on updating the interface as per comments above, and update tests. > Hi Mostafa (@mostafaelhoushi) 👋 Glad to see you here!
> 
> My utmost goal for this PR is to get Layer Skip to the hands of our users with a) good throughput numbers b) a simple interface. Self-speculative decoding is indeed the best of both worlds for low batch sizes 💪
> 
> I appreciate the extra suggestions, but they add significant complexity -- e.g. if we accept callable for self-speculative decoding, we might want to apply the callable in different positions. Keeping things somewhat simple means others can just fork what we have and implement their idea quickly on top! It also makes our maintenance job doable 🤗 Naturally, if a given technique shows clear advantages and can be applied on pre-trained weights without too much complexity, like layer skip, we'll jump straight to implementation.
> 
> (For instance, a few years ago we implemented a complex constrained decoding method, before json generation became popular. However, because the implementation was complex and it was somewhat niche, it quickly became unmaintained -- we got the additional code bloat with no relevant benefits)
> 
> Sorry to be a turn off -- I really appreciate the ideas coming in!

Thanks @gante ! Everything you said makes sense and I agree that this is the wise way forward.

> Update: I've removed cache sharing between the early exit and the full model -- it would require significant changes in `forward` to skip the computation of the cached values, which we don't want to commit for now. The outputs are now correct 👍
> 
> Tomorrow I'll work on updating the interface as per comments above, and update tests.

OK. Sounds good. According to our experiments, cache sharing leads to an additional 10% speedup, but without cache sharing we should still get significant speedup. Hi everyone. In my opinion, the PR is ready to go, except for one feedback. My only feedback is to modify the name of the argument from `early_exit` to `assistant_early_exit` for a couple of reasons:
- to be consistent with the `assistant_model` argument for speculative decoding. Hence `assistant_model` is when the draft stage corresponds to a different model, and `assistant_early_exit` is when the draft stage is an exit at an earlier layer.
- to avoid confusion with the idea of early exit of autoregressive decoding that the feature doesn't implement.

Thanks! @ArthurZucker @mostafaelhoushi have a look at the state of the PR and, if we all agree, let's merge 🤗  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34240). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Hi. My name is Mostafa and I am one of the main authors of the LayerSkip paper!
Thanks for working on this PR so quickly! I have provided some comments.

Also, for the future, I have some suggestions to consider:
- `early_exit` arg in generation could be come a callable function for researchers to experiment with dynamic early exit, i.e., a different condition or heurestic to exit for each token (e.g., cosing similarity between a layers input and output above a certain threshold). This is done in papers like [CALM](https://arxiv.org/abs/2207.07061).
- adapter modules for early exit. Rather than just exiting by jumping to the model's LM head, users may opt to add their own separate LM head or even add their own adapter layers when exiting. This is done in a paper like [Kangaroo](https://github.com/Equationliu/Kangaroo).
- Different types of self-speculative decoding, e.g.,
   - Draft stage uses a subset of KV cache. This is done in [MagicDec](https://github.com/Infini-AI-Lab/MagicDec/).

I am happy to discuss online or offline how we can add more features along this direction to enable researchers to unlock a lot of early exit ideas. I have added my final suggested edits to rename `early_exit` argument to `assistant_early_exit` (double-checked the changes, most of them done by me, LGTM :D) Very nice! 🔥  Thanks @gante ! All my feedback is addressed! Approving the PR from my side. It's a smart trick! 🤗 ","Hi. My name is Mostafa and I am one of the main authors of the LayerSkip paper!
Thanks for working on this PR so quickly! I have provided some comments.

Also, for the future, I have some suggestions to consider:
- `early_exit` arg in generation could be come a callable function for researchers to experiment with dynamic early exit, i.e., a different condition or heurestic to exit for each token (e.g., cosing similarity between a layers input and output above a certain threshold). This is done in papers like [CALM](https://arxiv.org/abs/2207.07061).
- adapter modules for early exit. Rather than just exiting by jumping to the model's LM head, users may opt to add their own separate LM head or even add their own adapter layers when exiting. This is done in a paper like [Kangaroo](https://github.com/Equationliu/Kangaroo).
- Different types of self-speculative decoding, e.g.,
   - Draft stage uses a subset of KV cache. This is done in [MagicDec](https://github.com/Infini-AI-Lab/MagicDec/).

I am happy to discuss online or offline how we can add more features along this direction to enable researchers to unlock a lot of early exit ideas. I have added my final suggested edits to rename `early_exit` argument to `assistant_early_exit` (double-checked the changes, most of them done by me, LGTM :D) Very nice! 🔥  Thanks @gante ! All my feedback is addressed! Approving the PR from my side. It's a smart trick! 🤗 ","# What does this PR do?

Adds self-speculation -- support for Meta Llama 3.2 Layer-Skip model

____________________

Test script:
```py
from transformers import AutoTokenizer, AutoModelForCausalLM
import time

expected_output = [""""]

prompt = ""Alice and Bob""
checkpoint = ""facebook/layerskip-llama3.2-1B""

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors=""pt"").to(""cuda"")

model = AutoModelForCausalLM.from_pretrained(checkpoint).to(""cuda"")
original_outputs = model.generate(**inputs, do_sample=False, max_new_tokens=20)  #warmup

start = time.time()
original_outputs = model.generate(**inputs, do_sample=False, max_new_tokens=20)
end = time.time()
print(f""Original: {end-start}"")
print(f""Output text"", tokenizer.batch_decode(original_outputs, skip_special_tokens=True))

start = time.time()
early_exit_outputs = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)
end = time.time()
print(f""Early Exit: {end-start}"")
print(f""Early Exit text"", tokenizer.batch_decode(early_exit_outputs, skip_special_tokens=True))

```","```suggestion
        elif generation_config.prompt_lookup_num_tokens is not None:
```

Maybe? ```suggestion
        base_model = getattr(self.assistant_model, ""base_model"")
``` Should it be the assistant model instead? This causes a loop as `get_candidates` will call `generate` which will obtain another instance of `EarlyExitCandidateGeneration` through [utils.py#L2162](https://github.com/huggingface/transformers/blob/42f0df691e1515b2c9ea2b7ef81b25cedcf964bd/src/transformers/generation/utils.py#L2162) (yes, present in the latest commit) I would like to mention that ""Early Exit"" is orthogonal to ""Self-Speculative Decoding"". I mean that there are other self-speculative decoding approaches that use methods other than early-exit, e.g., 
- [Draft & Verify](https://github.com/dilab-zju/self-speculative-decoding): Draft stage is skipping intermediate FFNs and attention layers of the model
- [MagicDec](https://github.com/Infini-AI-Lab/MagicDec/): Draft stage is attending to a window of the KV cache rather than the ful KV-cache  Here `torch.cat` will only be correct if `min(new_positions) == previous_length + 1`? If that's correct, should we also add an `assert` statement for that? FYI, I recently also added `stopping_criteria` as well to support integration with Eleuther LM Eval Harness:
https://github.com/facebookresearch/LayerSkip/pull/10/commits/e38784d6ece628681056ef5b43ba3b925052471a Smart! I like that simple change that enables flexibility. I suggest adding an assertion check to ensure the output of a model prunted to `early_exit` layers has the identical output as the same model with `early_exit` arg in `generation`

```suggestion
        # Remove layers manually
        model = model.model.layers[:4]
        del model.model.layers[4:]
        model.num_hidden_layers = 4
        manual_early_exit_outputs = model.generate(**inputs, do_sample=False, max_new_tokens=20)
        manual_early_exit_decoded = tokenizer.batch_decode(manual_early_exit_outputs, skip_special_tokens=True)
        self.assertEqual(early_exit_decoded, manual_early_exit_decoded)
``` I might have misunderstood the code, does `model.generate(**inputs, early_exit=4, do_sample=False, max_new_tokens=20)` perform static early exit, or does it perform self-speculative early-exit decoding?

Personally, I would suggest to separate them some how:
- Static early exit: `model.generate(**inputs, early_exit=4)`
- Self-speculative decoding, early exit: `model.generate(**inputs, assisstant_model={""early_exit"": 4})` or something like that Good point! I'm going to rename it to self-speculative decoding, and mention that we support self-speculative decoding by specifying an exit layer yes, that is correct!

I'm not going to add any check for now, though, and rely on internal tests to detect issues: adding a check here would hurt throughput in the forward pass, and a test can immediately detect issues :) The interface is indeed confusing -- the demo above was meant to run self-speculative early-exit decoding. 

I see two options:
1. `model.generate(**inputs, assistant_early_exit=4)` -- make the name of the argument more precise
2. `model.generate(**inputs, assistant_model=model, early_exit=4)` -- with `assistant_model` set we know we are doing speculative decoding, so the use of `early_exit` becomes more self-evident. 

I was thinking of going with option 2, since we could then do `model.generate(**inputs, early_exit=4)` to run static early exit. WDYT?

(btw, in the long run, we will mode ALL assisted generation/speculative decoding args into a `assistant_kwags` dictionary, otherwise things will get messy soon) I am worried that option 2. might be confusing because `assistant_model=model, early_exit=4` could potentially be the main model exiting at layer 4 and draft model be the main model.

On the short-term, I feel option 1 is better as `assistant_early_exit=4` clearly means we exit early for speculative decoding.

On the long-term, `assistant_kwags` is a great idea, or even having something like`assisstant_model=SelfSpeculative(early_exit=4)` I think you can ignore my comment about supporting `StoppingCriteria`. I checked out the PR and integrated with LM Eval Harness and found out that we don't need it.
I think I needed it in my custom implementation, but the native HF implementation doesn't. ```suggestion
`assistant_early_exit` (integer). In this case, the assistant model will be the model itself with early exit, hence
``` ```suggestion
>>> outputs = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)
``` ```suggestion
        self.assistant_early_exit = self.generation_config.assistant_early_exit
        self.generation_config.assistant_early_exit = None
``` ```suggestion
        base_model.num_hidden_layers = self.assistant_early_exit
``` ```suggestion
        assistant_early_exit(`int`, *optional*):
``` ```suggestion
        self.assistant_early_exit = kwargs.pop(""assistant_early_exit"", None)
``` ```suggestion
        if assistant_model is not None or self.prompt_lookup_num_tokens is not None or self.assistant_early_exit is not None:
``` ```suggestion
        if generation_config.assistant_early_exit is not None:
``` ```suggestion
        outputs_assisted = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)
        decoded_assisted = tokenizer.batch_decode(outputs_assisted, skip_special_tokens=True)
        self.assertEqual(decoded_assisted, [expected_output])
``` ✅  ✅  ✅  (nesting was not fully right -- normal ""speculative decoding"" examples were under ""Universal Assisted Decoding"". Moved a few things around) ```suggestion
An LLM can be trained to also use its language modeling head with earlier hidden states as input, effectively
```"
34301,2024-10-22T01:33:27Z,2024-11-19T11:40:37Z,jp1924,4,0,1,6,1,3,3,[],173141.0,0,2455630.0,0,0,0,0,3227577.617285,,1,1,0,False,"['molbap', 'jp1924']","@molbap 
Oh, thank you for reviewing.
Separately, I noticed that `do_convert_rgb` is not implemented in [ViTImageProcessor](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/image_processing_vit.py#L233-L239). Was this intentional? There's no particular reason why - I have no opposition to introducing this in another PR :)  @molbap 
Oh yeah? So I can just open the PR and add it? @jp1924, sure, no opposition to it - the main thing we are careful about is not breaking backwards compatibility. In this case, no previous behaviour can be broken since you're adding a functionality, so feel free to open a PR :) Good catch @jp1924 ! - I checked, it seems Siglip was the only exception with the wrong order of operations. Thanks for the fix, indeed convert_rgb is not supported for numpy arrays! I suppose we can merge this fix!

Also would be great to update `convert_rgb` to avoid silently skipping numpy arrays. thanks 🤗 ","Good catch @jp1924 ! - I checked, it seems Siglip was the only exception with the wrong order of operations. Thanks for the fix, indeed convert_rgb is not supported for numpy arrays! I suppose we can merge this fix!

Also would be great to update `convert_rgb` to avoid silently skipping numpy arrays. thanks 🤗 ","# What does this PR do?

```python
        # All transformations expect numpy arrays.
        images = [to_numpy_array(image) for image in images]

        if do_convert_rgb:
            images = [convert_to_rgb(image) for image in images]
```
The convert_to_rgb function is not being applied because ndarray values are being input 
(convert_to_rgb simply returns the input value if it's not a PIL.Image).

If an image with RGBA or similar format is input, the conversion doesn't work properly, 
causing the following error in the infer_channel_dimension_format method:

```python
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/transformers/image_utils.py"", line 254, in infer_channel_dimension_format
    raise ValueError(""Unable to infer channel dimension format"")
ValueError: Unable to infer channel dimension format
```

`transformers` version: 4.46.0.dev0


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@amyeroberts

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34729,2024-11-14T08:38:44Z,2024-11-19T10:26:35Z,cgebbe,2,0,1,5,1,2,2,['Vision'],108184.0,0,438471.0,0,0,0,0,3232021.14622,,0,1,0,False,"['LysandreJik', 'rwightman']","In addition to pinging @qubvel, pinging @rwightman for knowledge @LysandreJik @cgebbe I assume author of PR has verified that this fix allows the desired use case to work so I'm not looking at that. A quick look I believe it should allow the use case for hf-hub timm models which definitely should be supported.

I also believe the removal of that check is appropriate in any case. If the requested model isn't in timm it will error anyways.  e.g. `timm.create_model('crumpet')` -->  `RuntimeError: Unknown model (crumpet)`

If ValueError is preferred for uniform exception handling I can change RuntimeError to slightly more specific ValueError.Hi @debbie, thanks for the contribution! Agree with the messages above, it makes sense to remove this check and enable loading timm-compatible weight from the hub 👍  Great! Thanks @cgebbe. @qubvel, feel free to merge when happy with the state of the PR","Hi @debbie, thanks for the contribution! Agree with the messages above, it makes sense to remove this check and enable loading timm-compatible weight from the hub 👍  Great! Thanks @cgebbe. @qubvel, feel free to merge when happy with the state of the PR","# What does this PR do?

Problem: This doesn't currently work, but should in my opinion? 

```python
        import transformers
        model = transformers.AutoBackbone.from_pretrained(
            ""hf-hub:bioptimus/H-optimus-0"",
            use_timm_backbone=True,
            use_pretrained_backbone=True,
        )
```

Solution approach: 

Just remove the two lines which explicitly check whether the string is in `timm.list_models()`. Alternatively, we could introduce more e.g. `backbone_kwargs` arguments.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Models:

- vision models: @amyeroberts, @qubvel
",
34459,2024-10-28T07:00:46Z,2024-11-19T10:23:04Z,GuillemGSubies,6,0,3,19,1,1,1,[],68.0,0,1912938.0,0,0,0,0,3232232.552603,,0,3,0,False,"['LysandreJik', 'stevhliu', 'HuggingFaceDocBuilderDev', 'GuillemGSubies']","@stevhliu The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34459). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks for your patience and for trying out all these potential fixes! Unfortunately, it looks like it didn't work :( I'm not sure why the other CI tests aren't running for you. @ydshieh do you happen to know why the other tests aren't running? @LysandreJik @ArthurZucker, not sure why the two tests just hang without ever starting. This happened in https://github.com/huggingface/transformers/pull/34311 as well, and opening a new PR didn't help. Do you think its ok to just merge this? Trying to push this branch on another to see if it triggers it I pushed the branch to another and it now works. Unfortunately couldn't get to the root of the issue as the last commit was too old; @stevhliu, let me know if you see this issue happening again. In the meantime, yes, let's merge it as soon as it's green.",,"# What does this PR do?

superseding of #34311 

Fixes #34239 ",
34687,2024-11-11T16:19:33Z,2024-11-18T19:15:35Z,aymeric-roucher,2,0,4,101,1,1,1,['Agents'],2713.0,0,615385.0,0,0,0,0,3286658.591042,,0,4,0,False,"['aymeric-roucher', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34687). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks @LysandreJik !Sounds good, thanks @aymeric-roucher ","Sounds good, thanks @aymeric-roucher ","# What does this PR do?

Allow sending files as args for a tool created with `Tool.from_space()`.

Leverages `gradio_client`'s `handle_file` under the hood to prepare files before sending them to the API.",
34757,2024-11-16T03:26:07Z,2024-11-18T18:42:28Z,ecyht2,0,0,1,4,1,1,1,['Documentation'],,0,227781.0,0,0,0,0,3288670.215819,,0,1,0,False,[],Thanks for the fix!,Thanks for the fix!,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes https://github.com/huggingface/hub-docs/issues/1423.


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34762,2024-11-17T09:43:01Z,2024-11-18T18:41:13Z,jung-hunsoo,0,0,1,2,1,1,1,"['Documentation', 'trainer']",,0,118692.0,0,0,0,0,3288745.475417,,0,1,0,False,[],Thanks for the fix!,Thanks for the fix!,"Fixes typo.

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34782,2024-11-18T13:50:52Z,2024-11-18T18:40:01Z,thisisiron,1,0,1,4,1,1,1,[],3004.0,0,17350.0,0,0,0,0,3288816.486022,,0,1,0,False,['thisisiron'],"@qubvel Thank you for your quick response. As you mentioned, while pixel_values and original_pixel_values are identical, the code changes were made to maintain naming consistency.

The following is a portion of the code for passing input to the model appropriately:
```python
 with torch.no_grad():
    if ""opt"" in model_name:
        original_logits = original_model({""image"": original_pixel_values, ""text_input"": [""""]}).logits
        logits = hf_model(pixel_values, input_ids).logits
    else:
        original_logits = original_model(
            {""image"": original_pixel_values, ""text_input"": [""\n""], ""text_output"": [""\n""]}
        ).logits
        labels = input_ids.masked_fill(input_ids == tokenizer.pad_token_id, -100)
        logits = hf_model(pixel_values, input_ids, labels=labels).logits
```

```python
original_outputs = original_model.generate(
    {""image"": original_pixel_values, ""prompt"": prompt}, use_nucleus_sampling=True, max_length=50
)
outputs = hf_model.generate(
    pixel_values,
    input_ids,
    do_sample=True,
    num_beams=5,
    max_length=30,
    min_length=1,
    top_p=0.9,
    repetition_penalty=1.0,
    length_penalty=1.0,
    temperature=1,
)
```

If this isn't necessary, I'll close the request.Hi @thisisiron, thanks for the update! I'm not sure we have to fix this, because it's verified earlier in the code that `pixel_values` are equal to `original_pixel_values` with

```python
assert torch.allclose(pixel_values, original_pixel_values.to(pixel_values.device))
``` You are right, the code is a bit inconsistent, thanks for improving it!
","Hi @thisisiron, thanks for the update! I'm not sure we have to fix this, because it's verified earlier in the code that `pixel_values` are equal to `original_pixel_values` with

```python
assert torch.allclose(pixel_values, original_pixel_values.to(pixel_values.device))
``` You are right, the code is a bit inconsistent, thanks for improving it!
","# What does this PR do?

## Changes
- Modified hf_model input parameter from original_pixel_values to pixel_values to use correct variable name
- The code shows that original_model takes original_pixel_values as input while hf_model uses pixel_values.
```python
logits = hf_model(
-       pixel_values=original_pixel_values,
+       pixel_values=pixel_values, 
    input_ids=input_ids,
    attention_mask=attention_mask,
    use_image_text_matching_head=True,
)
```

## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@amyeroberts, @qubvel",
34778,2024-11-18T11:19:20Z,2024-11-18T17:59:11Z,faaany,1,0,1,6,1,1,1,[],23507.0,0,23991.0,0,0,0,0,3291269.134296,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34778). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for all the XPU related updates everywhere :),Thanks for all the XPU related updates everywhere :),"## What does this PR do?
Add XPU environment variable `ZE_AFFINITY_MASK` besides `CUDA_VISIBLE_DEVICES`.

@stevhliu",
34777,2024-11-18T11:09:02Z,2024-11-18T17:58:50Z,faaany,1,0,1,2,1,1,1,[],24122.0,0,24588.0,0,0,0,0,3291290.399868,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34777). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for updating!,Thanks for updating!,"## What does this PR do?
We should have XPU as one option in device-agnostic. 

Documentation: @stevhliu

",
34774,2024-11-18T10:33:51Z,2024-11-18T17:58:26Z,faaany,1,0,2,6,1,1,1,[],26026.0,0,26675.0,0,0,0,0,3291314.625924,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34774). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Very nice!,Very nice!,"## What does this PR do?
This PR uses the device-agnostic implementation from accelerate to empty cache and check device availability. 

Thanks for the review! @stevhliu
",
32943,2024-08-22T15:33:10Z,2024-11-18T15:49:37Z,winglian,3,6,1,5,1,3,3,[],441.0,0,7604188.0,0,0,0,0,3299044.273975,,0,1,0,False,"['winglian', 'HuggingFaceDocBuilderDev', 'muellerzr']","cc @ArthurZucker for final :)  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32943). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker @muellerzr I rebased this against latest transformers and tested on my endNice discovery! LGTM other than that!  IF that is the only fix we can do so be it, but it's fairly weird !  Thanks @winglian 🤗 ","Nice discovery! LGTM other than that!  IF that is the only fix we can do so be it, but it's fairly weird !  Thanks @winglian 🤗 ","# What does this PR do?

When doing 8bit lora's with deepspeed zero3, the Int8Params should have requires_grad=False otherwise the following error occurs:

```
[rank0]:   File ""/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/bitsandbytes/nn/modules.py"", line 574, in __new__
[rank0]:     obj = torch.Tensor._make_subclass(cls, data, requires_grad)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: Only Tensors of floating point and complex dtype can require gradients
```
this used to work several months ago


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","since this is only applicable to `Int8Param` which I suppose is specific to a certain lib, why is this not going in the `create_quantized_param` ? (if it's bnb's int8 then in bnb) `create_quantized_param` already attempts to set requires_grad=False  (https://github.com/huggingface/transformers/blob/main/src/transformers/quantizers/quantizer_bnb_8bit.py#L215), but that attribute isn't part of the `__dict__` when we're trying to move it to cpu for for fsdp/deepspeed mmmm then is there no other way to just preserve the actual tensor (here we are acessing value.data so that's were we lose the info from the create quantize param no?  Yeah, I'm assuming we can't simply move the tensor and that's why the current code has to recreate the tensor on meta/cpu and it loses the requires_grad info ```suggestion
                 value = torch.tensor(value.data.to(""cpu""), **value.__dict__)
```
I am guessing you tried, but something as simple as this doesn't work?  I gave that a shot and that doesn't work. `TypeError: tensor() got an unexpected keyword argument 'CB'`"
34723,2024-11-14T00:09:35Z,2024-11-18T14:45:40Z,dvrogozh,18,0,1,36,1,2,2,[],1512.0,1,398166.0,0,0,0,0,3302881.445062,,0,1,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'dvrogozh', 'ydshieh']","@amyeroberts : test failures seem unrelated. Please, advice when I should rebase/try again?
```
_ ERROR at setup of JambaModelIntegrationTest.test_simple_batched_generate_with_padding _
....
/usr/local/lib/python3.10/ssl.py:1163: Failed
``` @amyeroberts : rerun tests, no side fails now. Please, help review. Thanks a lot for your PR @dvrogozh! Pinging @qubvel and @ydshieh  Hi @dvrogozh, thank you for the PR.

When I run things like 

> python -m pytest -v tests\models\llama\test_modeling_llama.py -k ""test_training_gradient_checkpointing""

I see 

```
tests/models/llama/test_modeling_llama.py::LlamaModelTest::test_training_gradient_checkpointing SKIPPED (`supports_gradient_checkpointing` is False for LlamaModel.)                                  [ 33%] tests/models/llama/test_modeling_llama.py::LlamaModelTest::test_training_gradient_checkpointing_use_reentrant SKIPPED (`supports_gradient_checkpointing` is False for LlamaModel.)                    [ 66%] tests/models/llama/test_modeling_llama.py::LlamaModelTest::test_training_gradient_checkpointing_use_reentrant_false SKIPPED (`supports_gradient_checkpointing` is False for LlamaModel.)
```

and I also check the body after the `subTest` block which is not run.

Could you provide us in which cases it will enter that body while `self.skipTest` is executed. Could you set breakpoint(s) to check which `model_class` is not skipped.

Also, is `pytorch/pytorch@1a8752b` necessary to produce the issue? I am running with `2.6.0.dev20241112+cu121` and it works well. > Could you provide us in which cases it will enter that body while self.skipTest is executed.

Shortly: if `pytest-subtests` package is installed.

@ydshieh : this seems interesting. I was wondering why HF ci does not see this issue. Your comment above also suggests that you don't see the issue on your side. It seems I've found the reason. I have 2 systems, one with XPU, another with CUDA. Initially I saw issue on both systems. Yesterday, I fully cleaned and reinstall environment for XPU and issue was gone. **So, the issue which I observe is triggered by environment difference. In particular, it shows up if the following package is installed: `pytest-subtests` (I have version 0.13.1).** I am not sure when I got this package in my environment. I did check that if I install it on XPU I again can reproduce the issue. Most likely I got `pytest-subtests` installed from the HF Accelerate. It's in dependencies:
https://github.com/huggingface/accelerate/blob/c0552c9012a9bae7f125e1df89cf9ee0b0d250fd/setup.py#L25 > Also, is pytorch/pytorch@1a8752b necessary to produce the issue? I am running with 2.6.0.dev20241112+cu121 and it works well.

Most likely no. I am just building pytorch from sources on my side since I on purposely look for XPU backend issues in the most recent code. I did not try to check other pytorch versions, but the issue I see does not seem to be related to pytorch. Hi thanks for the information. Indeed, our CI env. doesn't have `pytest-subtests`.

I will check it and see how a fix would be.

Currently, without `pytest-subtests`, although we don't see the mentioned issue, the logic is not good: when a `model_class` is identified to be skipped and run `self.skipTest`, it will skip the whole test case (not just that single `subTest`) and the other `model_class` won't be checked if to run at all (i.e. no more for loop).

 > Currently, without pytest-subtests, although we don't see the mentioned issue, the logic is not good: <...> it will skip the whole test case (not just that single subTest) <...>

This sounds like we need to add `pytest-subtests` to the HF Transformers dependencies list. Let me know if you want me to do that in this PR. Although I think installing `pytest-subtests` is the way to go, I find something quite strange like:

```
import unittest


class T(unittest.TestCase):
    def test_foo(self):
        for i in range(7):
            with self.subTest(""custom message""):
                if i < 3:
                    self.skipTest(f""bon {i}"")
                # self.assertLess(i, 3)
                assert i < 3
```
fails with 
```
[custom message] SUBFAIL test_foo.py::T::test_foo - AssertionError: assert 6 < 3
====================================================================================== 1 failed, 1 passed, 3 skipped in 0.67s =======================================================================================
```

while

change ` for i in range(7):` to ` for i in range(6):` gives

```
1 passed, 3 skipped in 0.43s
```

I am not sure I am doing stupid things 😭  without `with self.subTest(f""custom message {i}"")`, it works with

```
[custom message 3] SUBFAIL test_foo.py::T::test_foo - AssertionError: assert 3 < 3
[custom message 4] SUBFAIL test_foo.py::T::test_foo - AssertionError: assert 4 < 3
[custom message 5] SUBFAIL test_foo.py::T::test_foo - AssertionError: assert 5 < 3
[custom message 6] SUBFAIL test_foo.py::T::test_foo - AssertionError: assert 6 < 3

``` with `range(7)` adding `print()` right after the range:
```
>>> 0
>>> 1
>>> 2
>>> 3
>>> 4
>>> 5
>>> 6

T
 » foo
 » foo
 » foo
 ✗ foo
 ✓ foo
[custom message] SUBFAIL d.py::T::test_foo - AssertionError: assert 6 < 3
============================== 1 failed, 1 passed, 3 skipped in 0.11s ==============================
```

So, it loops thru all 6 cases. It correctly skips first 3, but after that I don't understand what's going on. It should have reported 4 failures, but it reported 1 failure, 1 pass and ate up 2 others. And in the end only printed assertion failure for the last iteration :). Yeah. Maybe it's a bug. I will open an issue.
In the meantime, for `test_training_gradient_checkpointing`, I will talk to @ArthurZucker . > without with self.subTest(f""custom message {i}""), it works with

You mean without `if + skip`  condition? It works reasonably on my side without that. It behaves strange with `if + skip` though. I am checking this now.
```
import unittest
class T(unittest.TestCase):
    def test_foo(self):
        for i in range(7):
            print(f"">>> {i}"")
            with self.subTest(i=i):
                assert i < 3
```
output:
```
$ python3 -m pytest --capture=no d.py
======================================= test session starts ========================================
platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0
rootdir: /home/dvrogozh/tmp
plugins: pspec-0.0.4, timeout-2.3.1, hypothesis-6.118.8, subtests-0.13.1, dash-2.18.2, xdist-3.6.1, rich-0.1.1
collected 1 item

d.py >>> 0
>>> 1
>>> 2
>>> 3
>>> 4
>>> 5
>>> 6
uuuu.

============================================= FAILURES =============================================
_________________________________________ T.test_foo (i=3) _________________________________________

self = <d.T testMethod=test_foo>

    def test_foo(self):
        for i in range(7):
            print(f"">>> {i}"")
            with self.subTest(i=i):
                #if i < 3:
                #    self.skipTest(f""bon {i}"")
                #self.assertLess(i, 3)
>               assert i < 3
E               AssertionError: assert 3 < 3

d.py:12: AssertionError
_________________________________________ T.test_foo (i=4) _________________________________________

self = <d.T testMethod=test_foo>

    def test_foo(self):
        for i in range(7):
            print(f"">>> {i}"")
            with self.subTest(i=i):
                #if i < 3:
                #    self.skipTest(f""bon {i}"")
                #self.assertLess(i, 3)
>               assert i < 3
E               AssertionError: assert 4 < 3

d.py:12: AssertionError
_________________________________________ T.test_foo (i=5) _________________________________________

self = <d.T testMethod=test_foo>

    def test_foo(self):
        for i in range(7):
            print(f"">>> {i}"")
            with self.subTest(i=i):
                #if i < 3:
                #    self.skipTest(f""bon {i}"")
                #self.assertLess(i, 3)
>               assert i < 3
E               AssertionError: assert 5 < 3

d.py:12: AssertionError
_________________________________________ T.test_foo (i=6) _________________________________________

self = <d.T testMethod=test_foo>

    def test_foo(self):
        for i in range(7):
            print(f"">>> {i}"")
            with self.subTest(i=i):
                #if i < 3:
                #    self.skipTest(f""bon {i}"")
                #self.assertLess(i, 3)
>               assert i < 3
E               AssertionError: assert 6 < 3

d.py:12: AssertionError
===================================== short test summary info ======================================
(i=3) SUBFAIL d.py::T::test_foo - AssertionError: assert 3 < 3
(i=4) SUBFAIL d.py::T::test_foo - AssertionError: assert 4 < 3
(i=5) SUBFAIL d.py::T::test_foo - AssertionError: assert 5 < 3
(i=6) SUBFAIL d.py::T::test_foo - AssertionError: assert 6 < 3
=================================== 4 failed, 1 passed in 0.11s ====================================
```
 yes, sorry. I mean `without if ... skipTest` it works (as you noted). But with it, things goes crazy .. @ydshieh : I filed #34755 to specifically consider what to do with subtests story. See breakdown on how it works in different cases. Really confusing...

As for the `test_training_gradient_checkpointing` story I think there are 2 possible ways to handle it separate from subtests story:
1. Either merge in PR 34723 as is, i.e. `with self.subTest()` remains, but we move the actual test body under the clause
2. Remove `with self.subTest()` entirely, i.e. revert to previous test version when skipped cases were handled with just `continue` in the loop without explict markup of what passed or failed  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34723). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Leaving this up to you @ydshieh 🤗 I think this one makes sense! Despite the weired issue from the `pytest-subtests` with `subTest + skipTest`, this PR itself makes sense.

Furthermore, it doesn't change the current behavior when `pytest-subtests` is not installed (although that behavior is not desirable).

Therefore LGTM to merge.","Leaving this up to you @ydshieh 🤗 I think this one makes sense! Despite the weired issue from the `pytest-subtests` with `subTest + skipTest`, this PR itself makes sense.

Furthermore, it doesn't change the current behavior when `pytest-subtests` is not installed (although that behavior is not desirable).

Therefore LGTM to merge.","19d58d31f has introduced a context manager to manage subtests of test_training_gradient_checkpointing. However, test body was not moved under the ""with"" statement. Thus, while tests are correctly marked as skipped, test bodies were still executed. In some cases, as with llama this caused attribute errors.

Fixes: #34722
Fixes: 19d58d31f (""Add MLLama (#33703)"")

CC: @amyeroberts, @ArthurZucker ",
34637,2024-11-07T09:13:07Z,2024-11-18T13:41:49Z,ZuoChenFttS,0,0,1,2,1,1,1,[],,0,966522.0,0,0,0,0,3306713.600043,,0,1,0,False,[],"Nice catch, thank you!","Nice catch, thank you!","fix a typo bug in examples/pytorch/image-classification/run_image_classification_no_trainer.py where 'id2label' was incorrectly written as 'i2label' when reading pretrained config

# What does this PR do?
Fix a  typo bug in examples/pytorch/image-classification/run_image_classification_no_trainer.py
Where 'id2label' was incorrectly written as 'i2label' when reading pretrained config; 
This bug will cause the saved config to not properly reflect the relationship between tags and ids.

## Before submitting
- [ √] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).



## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:
- vision models: @amyeroberts, @qubvel

Maintained examples (not research project or legacy):
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
",
34618,2024-11-05T18:18:48Z,2024-11-18T13:13:27Z,ofek,0,0,1,2,1,1,1,[],,0,1104879.0,0,0,0,0,3308417.266303,,0,1,0,False,[],Thank you!,Thank you!,"Page: https://huggingface.co/docs/transformers/main/en/tasks/image_classification
Current link: https://huggingface.co/docs/transformers/main/en/tasks/model_doc/vit
Correct link: https://huggingface.co/docs/transformers/main/en/model_doc/vit",
34551,2024-10-31T23:40:39Z,2024-11-18T09:43:10Z,2015aroras,8,10,26,2641,17,2,2,[],474.0,0,1504951.0,0,0,0,0,3321035.07729,,0,26,0,False,"['2015aroras', 'HuggingFaceDocBuilderDev', 'ArthurZucker']","I tested this before we locked in the `Olmo1124` naming conventions. Will update once tested. Tests are passing, including slow ones (except for `Olmo1124ModelTest::test_generate_compile_1_end_to_end`, but this appears to be broken for base OLMo too so I'm considering it an existing problem).. I've used a test HF hub repo ([shanearora/OLMo-7B-1124-hf](https://huggingface.co/shanearora/OLMo-7B-1124-hf)) since the official final model is not ready yet. PR checks were passing before I merged main again, and PR check failures relate to other models. @ArthurZucker Gentle ping The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34551). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Something has gone wrong with the rebasing it seems 😓  We can merge once this is fixed! I'm just going to add something for the model card (better than the blank state it is, we can change it later).Looks marvellous thanks for your hard work, let's get this merged asap! 🤗 Left very small comments it's great. 
apologies again for the delay Merging! Thanks for the clean work 🤗 ","Looks marvellous thanks for your hard work, let's get this merged asap! 🤗 Left very small comments it's great. 
apologies again for the delay Merging! Thanks for the clean work 🤗 ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


An updated OLMo model will be released in November. The new model has a few small architecture changes compared to the existing model in transformers:

- RMSNorm is used instead of standard layer norm.
- Norm is applied to attention queries and keys.
- Norm is applied after attention/feedforward rather than before.

The original PR https://github.com/huggingface/transformers/pull/34497 updated the OLMo implementation in transformers to support the November release. This PR instead adds a new model using the modular approach.

@ArthurZucker 

Fixes #34496 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","these would need to be filled new init should look more like thisL https://github.com/huggingface/transformers/blob/aca9120a6e71b77edd86ff63a6cb0e3a998cb4af/src/transformers/models/albert/__init__.py#L4 
Should make it simpler 🤗  Very very well down, the modular is super simple, makes it easy to identify differences! is this the final checkpoint? 🤗  let's create it outside the call! No, I just grabbed an intermediate checkpoint to use for the implementation. It's from pretty close to end of training.

We will upload the official final and intermediate checkpoints in an official HF Hub repo under the `allenai` org. Now that this PR is approved, I think we can start the uploading. [5b7cad9](https://github.com/huggingface/transformers/pull/34551/commits/5b7cad9536f1156a62235a4978f97a68fd3737bf) This was auto-generated from `transformers-cli add-new-model-like`, but fixed anyways. [f29dc50](https://github.com/huggingface/transformers/pull/34551/commits/f29dc5024c1feca23f1507f1f5a869c5a8a05c09) Done, though it took me a while to debug why it wasn't working. The simplified init requires `__all__` to be explicitly set: [3960e35](https://github.com/huggingface/transformers/pull/34551/commits/3960e3540087094ba1745178fe80b391a40c6f0b). Other than some difficulties getting started/finished and a bit less docs, modular was a nice experience! 💘 super clear, love this!"
33424,2024-09-11T06:48:19Z,2024-11-18T12:21:08Z,zucchini-nlp,12,18,9,146,17,4,3,[],1205.0,0,5895169.0,0,0,0,0,3311557.122326,,0,9,0,False,"['qubvel', 'HuggingFaceDocBuilderDev', 'zucchini-nlp', 'amyeroberts']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33424). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @amyeroberts 

Yes, you're right that we are trying to make this work for current existing vision models and there may be some other models that do not have hard requirement for having a square image and thus square patches.

The problem with CLS is that num of image tokens is now computed as `patches_height * patches_width` for all cases and we add `+1` by default to account for the CLS. But not all existing vision models add a cls token.

Since we have all VLMs on CLIP-like vision tower, I think we'll be safe in general either way we choose to implement. But apart from that, my main idea was that if some fancy architecture comes out where we won't be able to infer number of tokens from image/patch size. Then we can safely delegate setting correct number of image tokens for whatever vision model is used to users. I see this option as safer (imo) and more aligned with models like BLIP or Paligemma where we don't do special calculations to get how many image tokens are needed.

What do you think if we leave the patch_size as kwargs only for llava-next given that it is more quirky and requires to know how many patches each image is divided into? For all others we go with `num_image_tokens` to be on the safer side  > But apart from that, my main idea was that if some fancy architecture comes out where we won't be able to infer number of tokens from image/patch size. Then we can safely delegate setting correct number of image tokens for whatever vision model is used to users. I see this option as safer (imo) and more aligned with models like BLIP or Paligemma where we don't do special calculations to get how many image tokens are needed.

This seems to me like we're engineering for a problem which doesn't yet exist. 

> For all others we go with num_image_tokens to be on the safer side

My main reservation with this is that yes, we can safely delegate the number of tokens, but we can no longer safely calculate patch_size. 

Would it be possible to have both arguments? Where if both are set we verify they're compatible? 

As I'm off I'll delegate this decision to @qubvel. I'll be happy with whatever you both decide!  Hi @qubvel !

Can you look at the last discussion we had with Amy when you have time :) I tried to summarize the pros and cons of existing options, feel free to add if I missed anything

### 1. `patch_size` and default `+1` for CLS token

 \+ Support for dynamic image size
 \- Not all architectures have CLS token added, we cannot reliably infer `num_image_tokens` one from `patch_size`
 \- [Potential] There might be an architecture where we can't infer `num_image_tokes` based on `patch_size` and CLS token (examples?)
 
### 2. `num_image_tokens`
\+ Do not depend on CLS token and can support custom `num_image_tokens`
\- Still need `patch_size` for some processors, we have the same problem - we cannot reliably infer one from another
\- From user's point of view it might be not obvious how to get `num_image_tokens` while patch size can be taken directly from the config

So, both of them are not ideal 🥲 Are there any other options? Maybe we can add to configs smth like `num_special_tokens`, wdyt? @qubvel right, `patch_size` gave us flexibility to work with non-square images but we can't reliably get number of image tokens in some cases. I did a small overview of what types of VLMs are out there and

we can categorize VLMs image backbones into three types: 
- perceiver-like models have a fixes image seq length (Ideifcs, BLIP) so we are good here
- models that use simple ViT architecture and optionally anyres splitting/pixel shuffle. We can be sure abuot image seq length in all cases if position interpolation is not on. AFAIK none of existing VLM papers use interpolation, but they rather try anyres/multiple encoders etc. for high resolutions. We can also use `patch_size` with same level of certainty and might need extra kwargs for specific models (convnext, internViT) to account for donwsampling 
- flexible image size models where the image is resized smartly like in Qwen2-VL. These models resize so that it fits in one of the possible shapes and preserved max of the original image. We can't use image seq length for them, and we should infer number of tokens from each image's size and other params like downsample ratio or merge size


This means we can't use one version of processing for all VLMs to get number of image tokens. And I am still more inclined to `num_image_tokens` because
- we already use `image_seq_length` in other VLMs like Paligemma, Idefics, Chameleon and BLIP instead of trying to get number of tokens using heuristics
- regarding the above concerns with non-square images, it seems like we try to accoun for smth that the model checkpoints don't support. In other words, non-square can be the case for llava only when positions in ViT are interpolated which is not used in any of the models
- we still will have one model that needs special treatment. Qwen2-VL has dynamic seq length for each image depending on its size, so we leave it as is
- yeah, from user perspective seems easier to pass in `patch_size` and hope everything works as magic. We are actually going to add `self._get_image_features` for all VLMs to make it more modular. Maybe users who want to tinker can rely on that method later if they decide to change ViT patch sizes?

At the end we'll do `num_image_tokens` or `image_seq_length` (to be aligned with existing code) for models where pixel shapes are fixed based on crop-size/resize. And for those with dynamic resize, we'll need to also dynamically calculate image seq length . WDYT?

Unfortunately I can't come up with a better solution that will work out of the box for all possible VLMs in all cases, but imo we don't have to. As long as the current approach works with the current model, and is consistent library-wise
 Thanks for the very detailed clarification! Indeed it looks like processors' logic can vary significantly across models, the questions here are
1) how to uniformize processors logic (would be easier for users and pipelines integration)
2) make a reliable method to compute image_tokens/patch_size

My suggestion was to use `num_special_tokens` or `num_additinal_tokens`, which could be used instead of `+1`. In that case, we have to provide two arguments `patch_size` and `num_special_tokens` (it could be 0/1/2/... and can be a property of a model config, to be computed dynamically based on enabled cls token for example):
```py
# for square images
num_image_tokens = (image_size / patch_size) ** 2 + num_special_tokens
```
For such a case we might dynamically compute the number of image tokens based on the resulting image size and patch_size to support interpolation. TBH, I'm not sure how it's scaling up to all architectures, it's just an idea to take into consideration 😄 

I'm happy to go with your option too! And maybe just more examples/docs will be enough to make users comfortable! > how to uniformize processors logic (would be easier for users and pipelines integration)

Unfortunately relying on only one way to ave uniform/same kwargs is impossible at the moment since models have different requirements for vision backbone. Though we can use the `patch_size` + `num_additional_tokens` for VLMs with ViT backbone and `image_seq_length` for others where the length is fixed

Let me then add the `num_additional_tokens`, looks good to me and maybe less breaking (even though we didn't update model checkpoints yet) Slow tests for expansion are passing all locally yea, docs will be much needed given we already had some users asking about how to suppress the warning. I don't know if there is a place we can add info about VLMs in more details so I am thinking to add a WARNING section in model doc page for each touched model. Glad there are only ~5 models Done, added the docs. LMK if it looks good to you and I'll request review from core maintainer. I hope after this PR we can update hub files and remove extra logic by v4.50 (deprecation target will be modified in https://github.com/huggingface/transformers/pull/34245) Cool, merging soon and I'll update hub configs this week for llava-hf models + open PRs for BLIP models. We've had the deprecation warning for 3-4 releases and should be fine with updating configsThanks for opening this PR @zucchini-nlp! 

I know this has been discussed elsewhere - but I can't remember exactly where - if you find it, could you link to it in this PRs description? 

I still don't think `num_image_tokens` is a great solution -- I would have assumed the number of image tokens could be variable based on the input image size, which `patch_size` accounts for but `num_image_tokens` does not. 

Assuming this is safe because of some of the backbones used with current checkpoints is an unreliable assumption. Part of the feature of a lot of these VLMs architectures is that different vision towers can swapped out easily and so it's very difficult to have strong assumptions about their properties. One option is to try and enforce these properties through validation. 

I know part of the motivation is because patch_size calculations are also not reliable, although I don't think I've understood why adding the CLS token affects this? I would help to understand with an example to show where the issue is.  Thanks, looks good to me! 
It'd be great to add some docs regarding where should users look for `num_additional_image_tokens` param. Should we add it as a config property/param? LGTM, thanks! I am a bit out of loop, so needs some clairiication! just 1 place to fix","Thanks for opening this PR @zucchini-nlp! 

I know this has been discussed elsewhere - but I can't remember exactly where - if you find it, could you link to it in this PRs description? 

I still don't think `num_image_tokens` is a great solution -- I would have assumed the number of image tokens could be variable based on the input image size, which `patch_size` accounts for but `num_image_tokens` does not. 

Assuming this is safe because of some of the backbones used with current checkpoints is an unreliable assumption. Part of the feature of a lot of these VLMs architectures is that different vision towers can swapped out easily and so it's very difficult to have strong assumptions about their properties. One option is to try and enforce these properties through validation. 

I know part of the motivation is because patch_size calculations are also not reliable, although I don't think I've understood why adding the CLS token affects this? I would help to understand with an example to show where the issue is.  Thanks, looks good to me! 
It'd be great to add some docs regarding where should users look for `num_additional_image_tokens` param. Should we add it as a config property/param? LGTM, thanks! I am a bit out of loop, so needs some clairiication! just 1 place to fix","# What does this PR do?

Following https://github.com/huggingface/transformers/issues/33374, we'll use `num_image_tokens` instead of `patch_size` VLM in processors. The reason is because some image backbones add a CLS token while other do not, and we cannot infer the number of tokens needed for an image from number of patches reliably.

Yes, as noted in some prev discussions, we'll have to assume for llava-next style models that the image is a square and obtain patch-width/patch-height from `num_image_tokens`. If I'm not mistaken, the backbones we have in the library and are used by VLMs are all square sized, and since llava-next class if not used with other backbones apart from CLIP afaik, we should be good to assume it that way.

Modified tests are passing locally for me
","repeated in the next lines  For backwards compatibility, we can't just remove accepting patch_size immediately. We need to deprecate it and not allow both `patch_size` and `num_image_tokens` to be specified. 

You can use the @deprecate_kwarg decorator to handle this 

 Same here - we need to deprecate this property  I don't think the numbers quite match here. Even if we assume the patches are square, this also assumes the image is square, which might not be the case. 

Let's say the size of the image is 22x18 and a patch size is 4. 

In the first case: 

    patches_height = 22 // 4 = 5
    patches_width = 18 // 4 = 4

    base_features = patches_height * patches_width + 1
                  = 21 

If we then work backwards

num_image_tokens = 21 
patches_height = patches_width = int(math.sqrt(self.num_image_tokens)) = 4

So this conversion isn't backwards compatible and I think it's also less correct  Yes, right. but also I don't think this is possible in available llava-next checkpoints this was released a long time ago we can probably ignore it now no?  we are in 4.47 same here (unless this was not released!) should probably be placed at the end for BC as well 😉  also it was set to `1` (i mean `num_image_tokens -= 1` now it will use 0 this will be in 4.47 they are added where (to which image embedding and why) also I am not even sure I understand the usage of this myself. As it will be in all processors, can you explain a bit more why someone should set this?  There is a PR from @gante  (https://github.com/huggingface/transformers/pull/34245) that moves deprecation target a few more releases up, so we have time to update configs on the hub and for users to get used to new format

Imo we need at least 1 release to remove it totally Yes, the very first reason for this PR is that some ViT backbones can add a CLS token to image patches, while other do not. Therefore our current processors can't work with SigLIP because we hardcoded the CLS token addition in code.

We had two options to fix it:
- what I proposed initially was to simply use `num_image_tokens` arg and let users specify any amount so we don't have to infer how many patches there will be from `patch_size` and `image_size`. Some processor already do that like Paligemma
- it had drawbacks though that we cannot work with non-square images in that case, because in case of llava-next the padding/unpadding is needed. That depends on `patch-size` and `image-size`. I realize we have no models that accept non-square images, but that was the strongest objection from core maintainer against the `num-image-tokens`

So after discussions with Pavel, we decided to make as less changes as possible and still support SigLIP by adding `num_additional_tokens`. It will be 1 if the vision tower adds CLS, and otherwise 0. It can be more than 1 in case there is any vision tower with extra tokens added on top of image patches  ok thanks for explaning! ok this one is important oke, will fix it and merge after making sure tests pass"
34245,2024-10-18T10:40:04Z,2024-11-15T22:07:24Z,gante,7,2,7,217,17,5,4,[],1659.0,0,2460440.0,1,0,0,0,3535582.620232,,0,7,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev', 'gante', 'zucchini-nlp']","> since the changes will invalidate many models on the hub that are not owned by us.

We should then support it for as long as possible :) If it is critical to any popular model, I would not even set a deprecation version target at all. Leave the exception without target, and set a target when it blocks you from doing something OR if it is causing many issues. An example of this is supporting `model.config` modifications to parameterize `generate` -- it is super inconvenient for maintenance, but many examples/models rely on it. 

If it is only used in some niche models, and applying the deprecation (raising exception) is a significant win for transformers, then I would set at least 6 minor versions.

LMK which one is it, and I will update the warning. I hope you meant `warning`, not `exception`. I don't think we should be raising exception unless at least the official checkpoint is updated accordingly. 

I would say this particular one is a niche model, given download stats from the hub. And most older llava and BLIP models may be outdated fast as we get newer and better releases each month. But I think we still can't enforce users to do something by elevating this to an Exception. My preference is to keep it for 6 minor versions as a warning, and try to update official models on the hub until that. We have write access to all of them afaik except for Video-LLaVA so updating will be straightforward > I hope you meant `warning`, not `exception`

@zucchini-nlp haha yes, `warning`! In essence, allowing the behavior. @zucchini-nlp @SunMarc PR comments addressed, LMK if they look good to you :) The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34245). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @zucchini-nlp done ✅  To pass the CI, you need to rebase the PR @gante  ;)Thanks for fixing but the processing logic refactor wen longer than I expected. We are still doing the last bits in https://github.com/huggingface/transformers/pull/33424/ and the checkpoints on the hub are not updated yet (tracker https://github.com/huggingface/transformers/issues/33374). So we can't be raising ValueError yet

Maybe we can change the deprecation version to be after a few more minor releases. We have also same warnings in all llava and blip models afair. Also now I think we can even do more than just a few releases, since the changes will invalidate many models on the hub that are not owned by us. WDYT, which version we can put in the message? Thanks for the update ! I left a comment. Also, I check that we need to enforce huggingface-hub to 0.24.0 because of this [commit](https://github.com/huggingface/huggingface_hub/releases?page=2#:~:text=Support%20max_shard_size%20as%20string%20in%20split_state_dict_into_shards_factory%20by%20%40SunMarc%20in%20%232286).  we accept `max_shard_size` with string format only from this version.  Thanks, looks good to me! Can we propagate the same thing to all files with this deprecation warning? I guess it is all LLaVA and some of BLIP models (processing and modeling files) Nice ! Thanks for iterating !  Ok, sounds good! Thanks for the PR and cleanup @gante 🧼 thanks","Thanks for fixing but the processing logic refactor wen longer than I expected. We are still doing the last bits in https://github.com/huggingface/transformers/pull/33424/ and the checkpoints on the hub are not updated yet (tracker https://github.com/huggingface/transformers/issues/33374). So we can't be raising ValueError yet

Maybe we can change the deprecation version to be after a few more minor releases. We have also same warnings in all llava and blip models afair. Also now I think we can even do more than just a few releases, since the changes will invalidate many models on the hub that are not owned by us. WDYT, which version we can put in the message? Thanks for the update ! I left a comment. Also, I check that we need to enforce huggingface-hub to 0.24.0 because of this [commit](https://github.com/huggingface/huggingface_hub/releases?page=2#:~:text=Support%20max_shard_size%20as%20string%20in%20split_state_dict_into_shards_factory%20by%20%40SunMarc%20in%20%232286).  we accept `max_shard_size` with string format only from this version.  Thanks, looks good to me! Can we propagate the same thing to all files with this deprecation warning? I guess it is all LLaVA and some of BLIP models (processing and modeling files) Nice ! Thanks for iterating !  Ok, sounds good! Thanks for the PR and cleanup @gante 🧼 thanks","# What does this PR do?

See title :)

@zucchini-nlp -> video llava changes
@SunMarc -> all other changes","(followed this instruction whenever `shard_checkpoint` was used) Unfortunately, this is not the exact function. We can't just replace it directly. You need to build the shard and the index again using the output of this function. Check the example here: https://huggingface.co/docs/huggingface_hub/main/en/package_reference/serialization#huggingface_hub.split_torch_state_dict_into_shards.example"
34645,2024-11-07T17:29:38Z,2024-11-15T21:28:06Z,winglian,3,0,2,14,2,4,4,[],7999.0,0,705508.0,0,0,0,0,3537941.126392,,0,2,0,False,"['winglian', 'HuggingFaceDocBuilderDev']","@ArthurZucker @muellerzr for review is it possible to also trigger the slow tests for this PR too to make sure the new test passes? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34645). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for catching this! Make sense ! Thanks for fixing !  Thank you.

Newly added tests are passing (in this PR) and failed on `main`. So it does its job.

The change in `trainer` seems reasonable to me too. Sorry for being late on this, thanks 🤗 will patch","Thanks for catching this! Make sense ! Thanks for fixing !  Thank you.

Newly added tests are passing (in this PR) and failed on `main`. So it does its job.

The change in `trainer` seems reasonable to me too. Sorry for being late on this, thanks 🤗 will patch","# What does this PR do? 

https://github.com/huggingface/transformers/commit/8c62a92b3c3fea13652ef5d292a7863138410c59#diff-ed55888e6665791fe92cc8fc0c499da54f4ace6738551cd9a2591881cda076deL2473-R2480 seems to have broken FSDP training as we need to wrap the train step in no_sync on every step except the last step in the gradient accumulation batch. Without this, during the optimizer.step(), I get the error: 
```
RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
```

this wasn't caught by the fsdp tests since gradient accumulation wasn't part of those tests, so I also added tests for that as well.

see https://muellerzr.github.io/blog/gradient_accumulation.html#what-is-the-right-way-then

<img width=""836"" alt=""Screenshot 2024-11-07 at 12 17 36 PM"" src=""https://github.com/user-attachments/assets/ceaef77e-5f87-45bb-8b72-c112fae3c2ee"">

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34452,2024-10-28T00:32:33Z,2024-11-15T22:00:03Z,eljandoubi,6,4,6,36,2,4,3,[],150847.0,0,1632450.0,0,0,0,0,3536024.270426,,0,6,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'eljandoubi']","@SunMarc @muellerzr Did you get a different result than I did? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34452). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @SunMarc I migrated to `unwrap_model`. Please rebase this PR on main in order to pass the CI @eljandoubi !  @SunMarc @LysandreJik @muellerzr Is there any update on the pull request? We were on a company wide offsite! merging as they all approved 🤗 Thnaks for fixing the issue @eljandoubi ! Do you think there is a simpler way to handle this edge case @muellerzr ?  Thanks for the fix, can you add a test in `tests/test_trainer.py` for this? Thanks ! Left an suggestion for `unwrap_model` Let's merge it if you're both ok with it @SunMarc @muellerzr ","Thnaks for fixing the issue @eljandoubi ! Do you think there is a simpler way to handle this edge case @muellerzr ?  Thanks for the fix, can you add a test in `tests/test_trainer.py` for this? Thanks ! Left an suggestion for `unwrap_model` Let's merge it if you're both ok with it @SunMarc @muellerzr ","# What does this PR do?

Fixes #34113 

## Who can review?

Library:

- trainer: @muellerzr and @SunMarc
","You can use `unwarp_model` function in transformers instead. Also, why do we need to set `recursive` to `True` ? Also, please leave a comment above as this specific path is only to make it functional with `auto_find_batch_size `.   `unwrap_model` does not provide access to the `recursive` argument. Auto-wrap policies wrap submodules with FSDP, and `unwrap_model` is unable to remove them. You can test this on the toy example from [the PyTorch FSDP tutorial](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html) for `rank=0` and `world_size=1`, then experiment with the line I provided in a notebook.

```
my_auto_wrap_policy = functools.partial(
        size_based_auto_wrap_policy, min_num_params=20000
    )
torch.cuda.set_device(rank)
model = Net().to(rank)
print(model)
fsdp_model = FSDP(model,
    auto_wrap_policy=my_auto_wrap_policy)
print(fsdp_model)
unwrap_model = unwarp_model(fsdp_model)
print(unwrap_model)
```
VS
You need to reinstantiates `model` and `fsdp_model`:
```
model = Net().to(rank)

fsdp_model = FSDP(model,
    auto_wrap_policy=my_auto_wrap_policy)

extract_model = extract_model_from_parallel(fsdp_model, recursive=True)
print(extract_model)
``` I'm talking about this [function](https://github.com/huggingface/transformers/blob/ef976a7e181b78abf2f1ba7ea02e506ea1cb111e/src/transformers/modeling_utils.py#L5464) in transformers. It uses `extract_model_from_parallel` under the hood so it should be comparable.  Euh I see."
34720,2024-11-13T20:02:58Z,2024-11-15T14:44:03Z,CezaPasc,3,0,1,2,1,1,1,[],138525.0,0,153665.0,0,0,0,0,3562185.58578,,0,1,0,False,"['LysandreJik', 'CezaPasc']","Hello @CezaPasc, I believe this should have been fixed with https://github.com/huggingface/transformers/pull/34455 Ah, my bad, I see where this one is different. There are a few files which are changed which shouldn't be; do you mind keeping the diff only to the fix here? Thank you! @LysandreJik 
It seems like I got the other diffs from rebasing with the upstream.
I reverted them so that the diff should now only be about the changes in this PR again.Cool, thank you!","Cool, thank you!","# What does this PR do?
This PR passes a fake evaluation dataset for the initialisation of the `fake_trainer` of WandB.
This is necessary, since the init of the `Trainer` checks if an `eval_strategy` is set and raises an error if the `eval_dataset` is missing.
This issue is documented in #34701.
The fix here is similar to the one in #34455.


Fixes # (issue)
- Pass `[""fake""]` as `eval_dataset` during initialisation of `fake_trainer` for saving purposes of WandB.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@SunMarc

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34749,2024-11-15T13:28:51Z,2024-11-15T14:39:57Z,LysandreJik,3,0,1,4,1,1,1,[],28.0,0,4268.0,0,0,0,0,3562430.353634,,0,1,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev']","cc @zucchini-nlp  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34749). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. No worries, thanks for the review!Oh, cool, didn't know we had typo in the docs. Thanks for fixing it ❤️ ","Oh, cool, didn't know we had typo in the docs. Thanks for fixing it ❤️ ",Closes https://github.com/huggingface/transformers/issues/34680,
34253,2024-10-18T21:09:24Z,2024-11-15T14:27:04Z,lewtun,10,7,13,40,2,3,2,[],439.0,0,2395062.0,0,0,0,0,3563203.890424,,0,13,0,False,"['lewtun', 'HuggingFaceDocBuilderDev', 'Rocketknight1']","CI seems to be failing for reasons unrelated to this PR Wow, this issue was only introduced extremely recently, in #34214 ! I completely didn't foresee this being an issue when I merged that one.

The fundamental problem is that some templates `strip()` input text, and others don't. This means that the string-matching we do in `continue_final_message` isn't guaranteed to find a match if we don't also strip the search string. However, as you've noted, this can cause other problems.

Maybe the right solution is to first try to match the input with trailing whitespace, and then fallback to searching for the stripped version if that fails? > Maybe the right solution is to first try to match the input with trailing whitespace, and then fallback to searching for the stripped version if that fails?

I like this approach - let me amend my PR. Do we also need to retain the `rstrip()` that's applied to `rendered_chat`?
 No, I don't think so! I think it would break your task if we did, and I really just included it because I didn't realize the trailing whitespace could be meaningful. cc @schoennenbeck, who wrote #34214 - you don't need to do anything here, just making you aware that we might change the behaviour again! @Rocketknight1 I've added a try/except in https://github.com/huggingface/transformers/pull/34253/commits/0fd98e1a05bb987b9ee0cdcb2246de67d3665dd7

I've also tested that the Llama code still works

```py
# From: https://github.com/huggingface/transformers/pull/34214
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(""hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"")
messages=[
    {""role"": ""user"", ""content"": ""What is the capital of France?""},
    {""role"": ""assistant"", ""content"": ""Great question! The capital of France is ""}
]
s = tokenizer.apply_chat_template(
    messages, add_generation_prompt=False, 
    continue_final_message=True,
    tokenize=False
)
print(s)
```

Would you like me to include this as a regression test? I'm also not sure if we need something for the Qwen model, but for that we'd want to run it on GPU The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34253). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @lewtun yeah, this feels like an area where other changes could easily break things again. A regression test would definitely be appreciated! @Rocketknight1 I've included your suggestions and the CI seems to be failing for reasons unrelated to this PR 😢 . What do you recommend doing in this case? @lewtun CI looks green now, so I'm happy! cc @ArthurZucker / @LysandreJik for core maintainer review and we should be good to go.This looks like a great PR to me, and the regression test is nice! I left two nits, but both are just code style rather than anything that actually affects the output. Thanks @lewtun for the PR and @Rocketknight1 for the review!","This looks like a great PR to me, and the regression test is nice! I left two nits, but both are just code style rather than anything that actually affects the output. Thanks @lewtun for the PR and @Rocketknight1 for the review!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR addresses an issue with chat templates producing incorrect inputs when `continue_final_message=True` and we wish to terminate generation on newlines like `\n` (as is common in [process supervision](https://arxiv.org/abs/2211.14275) with math reward models like [this one](https://huggingface.co/peiyi9979/math-shepherd-mistral-7b-prm)). 

In this case, the prefills had their newlines stripped at each step of generation and produced results that disagree with pure greedy decoding to the EOS token. Here's an example:

```py
import copy
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = ""Qwen/Qwen2.5-Math-1.5B-Instruct""
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id).to(""cuda"")

### Inputs
system_prompt = ""Please reason step by step, and put your final answer within \\boxed{}.""
query = ""Which number is larger, 9.8 or 9.11?""
messages = [{""role"": ""system"", ""content"": system_prompt}, {""role"": ""user"", ""content"": query}]

### Reference - greedy decoding to EOS
inputs = tokenizer.apply_chat_template(messages, return_tensors=""pt"", return_dict=True, add_generation_prompt=True).to(""cuda"")
output = model.generate(**inputs, max_new_tokens=256, do_sample=False)
ref_output = tokenizer.decode(output[0][inputs.input_ids.shape[-1]:])
print(f""\n=== Ref completion ===\n{ref_output}"")

### Iterative decoding with prefill
current_messages = copy.deepcopy(messages)

for i in range(5):
    if current_messages[-1][""role""] == ""user"":
        inputs = tokenizer.apply_chat_template(current_messages, return_tensors=""pt"", return_dict=True, add_generation_prompt=True).to(""cuda"")
    else:
        inputs = tokenizer.apply_chat_template(current_messages, return_tensors=""pt"", return_dict=True, continue_final_message=True).to(""cuda"")
    output = model.generate(**inputs, max_new_tokens=256, do_sample=False, stop_strings=[""\n""], tokenizer=tokenizer)
    output_decoded = tokenizer.decode(output[0][inputs.input_ids.shape[-1]:])
    print(f""\n=== Completion {i+1} ===\n{repr(output_decoded)}"")
    if current_messages[-1][""role""] == ""user"":
        current_messages.append({""role"": ""assistant"", ""content"": output_decoded})
    else:
        current_messages[-1][""content""] += output_decoded

# we strip the final output to remove any trailing newlines produced by terminating on `\n`
final_output = current_messages[-1][""content""].rstrip()
print(f""\n=== Final output ===\n{final_output}"")
assert ref_output == final_output
```

Here's the reference completion from greedy decoding until EOS (note the new lines):

```
To determine which number is larger between 9.8 and 9.11, we can compare them digit by digit starting from the leftmost digit.

1. Both numbers have the same digit in the units place, which is 9.
2. Next, we compare the digits in the tenths place. The digit in the tenths place of 9.8 is 8, and the digit in the tenths place of 9.11 is 1. Since 8 is greater than 1, we can conclude that 9.8 is greater than 9.11.

Therefore, the larger number is \boxed{9.8}.<|im_end|>
```

Without the fix we get bad generations because the prefill is continually stripping the trailing new lines per iteration:

```
=== Completion 1 ===
'To determine which number is larger between 9.8 and 9.11, we can compare them digit by digit starting from the leftmost digit.\n\n'

=== Completion 2 ===
"" Here's the step-by-step process:\n\n""

=== Completion 3 ===
' \n\n'

=== Completion 4 ===
' \n\n'

=== Completion 5 ===
' \n\n'

=== Final output ===
To determine which number is larger between 9.8 and 9.11, we can compare them digit by digit starting from the leftmost digit.

 Here's the step-by-step process:
```

With the proposed fix, order is restored and the resulting completion is sane:

```
=== Completion 1 ===
'To determine which number is larger between 9.8 and 9.11, we can compare them digit by digit starting from the leftmost digit.\n\n'

=== Completion 2 ===
'1. Both numbers have the same digit in the units place, which is 9.\n'

=== Completion 3 ===
'2. Next, we compare the digits in the tenths place. The digit in the tenths place of 9.8 is 8, and the digit in the tenths place of 9.11 is 1. Since 8 is greater than 1, we can conclude that 9.8 is greater than 9.11.\n\n'

=== Completion 4 ===
'Therefore, the larger number is \\boxed{9.8}.<|im_end|>'

=== Completion 5 ===
'\n'

=== Final output ===
To determine which number is larger between 9.8 and 9.11, we can compare them digit by digit starting from the leftmost digit.

1. Both numbers have the same digit in the units place, which is 9.
2. Next, we compare the digits in the tenths place. The digit in the tenths place of 9.8 is 8, and the digit in the tenths place of 9.11 is 1. Since 8 is greater than 1, we can conclude that 9.8 is greater than 9.11.

Therefore, the larger number is \boxed{9.8}.<|im_end|>
```

**Questions** 

* I am not sure why stripping was included in the original implementation. If it's important to retain, could we instead expose a `strip_prefill` arg in `apply_chat_template` that is `True` by default but can be disabled in cases like process supervision?
* I wasn't sure how to write a regression test for this behaviour since the tiny-gpt2 models aren't prone to emitting newlines and running a 1.5B Qwen model in the CI is presumably discouraged.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","FYI @Rocketknight1 I kept the `rstrip()` here for consistency with the prior implementation. Let me know if you want it removed. Added a regression test here @Rocketknight1  It should make no difference, because we're slicing the string to the end of `final_message.strip()`, which must end with a non-whitespace character because it's been stripped. Therefore, there'll be nothing to `rstrip()` regardless! As such, we can remove it for cleanliness, but it won't break anything if you leave it either. ```suggestion
```
Redundant line Nice! This is a clean test. ok let's nuke it! ```suggestion
                    rendered_chat = rendered_chat[: rendered_chat.rindex(final_message) + len(final_message)]
```"
34460,2024-10-28T07:05:39Z,2024-10-29T06:54:51Z,zucchini-nlp,3,1,4,424,6,1,1,[],3478.0,0,1572707.0,0,0,0,0,3572184.617221,,0,4,0,False,"['jiqing-feng', 'HuggingFaceDocBuilderDev', 'zucchini-nlp']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34460). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @zucchini-nlp @ArthurZucker . This PR could lead the issue #34696 . Do you mind take a look? Thanks. @jiqing-feng I'll be updating all model configs starting next week to enable the new logic for processing. So we stop seeing bugs related to the old logic maintenance. For the linked issue, will take a look soon , thanksThanks, noting that this is needed in a patch 🤗 ","Thanks, noting that this is needed in a patch 🤗 ","# What does this PR do?

Fixes latency issues in some llava models caused by the recent processing logic refactoring. It was reported by one of the users that the new release is twice slower than the prev release. The problem was in indentation where images were embedded in every run instead of only the pre-fill stage

I believe it is worth fixing and merging until https://github.com/huggingface/transformers/pull/33424 is merged to unblock e from updating hub configs and enabling the new logic","glad that this is removed, never saw this being added (was not pinged on the PR!) "
33703,2024-09-25T17:29:48Z,2024-09-25T17:56:25Z,ArthurZucker,3,0,399,6281,31,1,1,[],2800.0,0,4309078.0,0,0,0,0,3649566.07302,,0,399,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker', 'SachinVarghese']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33703). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Nice work. Is the `MllamaVisionModel` introduced here completely running on CPU device? Yes, it can run on CPULet's get it in!",Let's get it in!,"# What does this PR do?
Adds model",
33196,2024-08-29T12:01:33Z,2024-09-06T12:09:49Z,AmirMohammadFakhimi,5,22,24,64,2,3,2,[],91760.0,0,6733410.0,0,0,0,0,3577729.122192,,0,24,0,False,"['jugal-sheth', 'LysandreJik', 'HuggingFaceDocBuilderDev', 'ylacombe', 'AmirMohammadFakhimi']","cc @ylacombe and @eustlb when back from leave! Hey @AmirMohammadFakhimi, thanks for providing the code snippet!

So, according to [Whisper configuration](https://github.com/huggingface/transformers/blob/ecd61c62862f925a18b4f063dc17fcaf01826e25/src/transformers/models/whisper/configuration_whisper.py#L118-L122), there's two hard limits on tokens and spectrograms lengths: `max_source_positions` and  `max_target_positions`. The latter is where you can find the hard 448 token limits.

If you wanted to modify that limit, you'd have to modify the architecture, and thus to re-train the model from scratch.

Your PR is justified, I'd just modify the mention to 448 to `config.max_target_positions`, e.g:
```diff
-            if labels.shape[1] > 448:
+           if labels.shape[1] > config.max_target_positions:
```
 > Could you also add a test [here](https://github.com/huggingface/transformers/blob/ecd61c62862f925a18b4f063dc17fcaf01826e25/tests/models/whisper/test_modeling_whisper.py#L375) that makes sure the model works with labels sequence length smaller than the limit, and that makes sure it throws an error if not ?

I added three tests to the file. I also [merged branch 'huggingface:main' into patch-1](https://github.com/huggingface/transformers/pull/33196/commits/03db15fb832bcffe6edb86c0e84be0c31d5ecdec).
For testing, I made a hardcode [here](https://github.com/AmirMohammadFakhimi/transformers/blob/8bf582f176ffc7a367bfb3dca02f5a68f478fec7/tests/models/whisper/test_modeling_whisper.py#L237) because I think it is hardcoded in the whisper model, too. Do you have any better idea?

And finally, just for info, there are these lines in [test_modeling_whisper.py](https://github.com/AmirMohammadFakhimi/transformers/blob/patch-1/tests/models/whisper/test_modeling_whisper.py):
```python
config, inputs_dict = self.prepare_config_and_inputs()
```
```python
config, input_dict = self.prepare_config_and_inputs()
```
```python
config, inputs = self.model_tester.prepare_config_and_inputs()
```
I used the second format. I think having a convention for it will help. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33196). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. try below 
```
# Initialize parameters and tokenizer
transcripts_folder = ""path_to_transcripts""
max_target_length = 448
show_target_length = 400
tokenizer = InitializeTokenizer(""model_name"", language=""en"", task=""transcribe"")

# Define the function to check and delete faulty transcripts
function check_and_delete_transcripts():
    files = ListAllFiles(transcripts_folder, extension="".txt"")  # Get all .txt files

    for file in ProgressBar(files, description=""Checking transcripts""):
        content = ReadFile(file)  # Read the transcript content
        tokenized = TokenizeContent(tokenizer, content)  # Tokenize the content
        token_length = GetTokenLength(tokenized)  # Get the length of tokens

        if token_length > max_target_length:
            Print(f""Faulty file: {file}, Tokens: {token_length}"")
            DeleteFile(file)  # Delete if too long
        elif token_length > show_target_length:
            Print(f""File exceeds show limit: {file}"")
            PrintContent(content)  # Optionally delete or inspect further

# Execute the function
check_and_delete_transcripts()

```

this will help you get rid of faulty data then finetune the modelHey @AmirMohammadFakhimi, many thanks for opening this PR!

Do you think you could give a minimum code snippet to reproduce the issue you have? It doesn't have to be a training code, I think a forward pass with handmade labels should do the trick!


Thanks for your help


cc @eustlb for visibility

 Could you also add a test [here](https://github.com/huggingface/transformers/blob/ecd61c62862f925a18b4f063dc17fcaf01826e25/tests/models/whisper/test_modeling_whisper.py#L375) that makes sure the model works with labels sequence length smaller than the limit, and that makes sure it throws an error if not ? Thanks for iterating ! Let's not hardcode the max length ! Thanks for iterating!
This is a bit complicate, you can do it like propose in this review! Thanks for iterating quickly on this !

cc @LysandreJik for review!","Hey @AmirMohammadFakhimi, many thanks for opening this PR!

Do you think you could give a minimum code snippet to reproduce the issue you have? It doesn't have to be a training code, I think a forward pass with handmade labels should do the trick!


Thanks for your help


cc @eustlb for visibility

 Could you also add a test [here](https://github.com/huggingface/transformers/blob/ecd61c62862f925a18b4f063dc17fcaf01826e25/tests/models/whisper/test_modeling_whisper.py#L375) that makes sure the model works with labels sequence length smaller than the limit, and that makes sure it throws an error if not ? Thanks for iterating ! Let's not hardcode the max length ! Thanks for iterating!
This is a bit complicate, you can do it like propose in this review! Thanks for iterating quickly on this !

cc @LysandreJik for review!","# What does this PR do?

Added a validation check to ensure that the sequence length of labels does not exceed the maximum allowed length of 448 tokens. If the sequence length exceeds this limit, a `ValueError` is raised with a descriptive error message.

This change prevents the model from encountering errors or unexpected behavior due to excessively long sequences during training or fine-tuning, ensuring consistent input dimensions and improving overall robustness.

While training, I encountered the following error multiple times:
```
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1167,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && ""index out of bounds""` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1167,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && ""index out of bounds""` failed.
```

Eventually, this led to the following error message:
```
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging, consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
```
It took me almost a week to diagnose and understand the root cause of this problem. This simple validation can save others from facing similar debugging challenges and wasted time.


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

speech models: @sanchit-gandhi","448 is hardcoded, which is something we would probably like to avoid. Do you think you could get this value from the config?

I think [config.max_length](https://huggingface.co/openai/whisper-large-v3/blob/main/config.json#L36) should do the trick Hi @ylacombe

This the code snippet to reproduce the issue:
```python
import torch
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v3')

# max_length don't help
# model.config.max_length = 500
# model.generation_config.max_length = 500

input_features = torch.ones(1, 128, 3000, dtype=torch.float32)
# You can put any number greater than 448 to reproduce the error
labels = torch.ones(1, 449, dtype=torch.int64)
loss = model(input_features=input_features, labels=labels).loss
print(loss)
```
Also you can refer to [this issue](https://github.com/openai/whisper/discussions/2313) which I faced to it while I was fine tuning.

Because the 448 was hardcoded in the whisper model, itself, as said [here](https://github.com/huggingface/transformers/issues/27445#issuecomment-1818970859), changing `model.config.max_length` and `model.generation_config.max_length` doesn't have any effect. How about defining a global variable in [src/transformers/models/whisper/modeling_whisper.py](https://github.com/huggingface/transformers/pull/33196/files/3056095a45d2fbbd8ab818a68dd1a00828c45158#diff-6ec07359a4b7f7c8c986c86549f01867ea071bef35cac64e0bd05302fef2e615) like `WHISPER_MAX_LENGTH = 448` and then using it in my code? ```suggestion
            Labels for computing the language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`
            or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored (masked), the loss is
            only computed for the tokens with labels in `[0, ..., config.vocab_size]`. `sequence_length` should be smaller than `config.max_target_positions`.
```

? ```suggestion
                    f""Labels' sequence length {labels.shape[1]} cannot exceed the maximum allowed length of {config.max_target_positions} tokens.""
``` ```suggestion
            if labels.shape[1] > config.max_target_positions:
``` ```suggestion
            Labels for computing the language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`
            or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored (masked), the loss is
            only computed for the tokens with labels in `[0, ..., config.vocab_size]`. `sequence_length` should be smaller than or equal to `config.max_target_positions`.
```

I added `or equal to` to the last sentence. Is it better? yes Let's revert to a smaller `max_target_positions` and use `config.max_target_positions` in the tests! For example here it would be `labels_length = config.max_target_positions` And you wouldn't need these assert anymore 
```suggestion
``` What's `config` variable here? Is it defined in the code? it's the config that you're using [in the code commented here](https://github.com/huggingface/transformers/pull/33196#discussion_r1744092462) I understood  :) I applied these changes, too :) ```suggestion
        max_target_positions=40,
        bos_token_id=98,
``` ```suggestion
        self.max_target_positions = max_target_positions
        self.eos_token_id = eos_token_id
``` ```suggestion
``` Let's add it back Thanks for your feedbacks.
I applied your suggestions.
I also changed [modeling_whisper.py](https://github.com/huggingface/transformers/pull/33196/files/3056095a45d2fbbd8ab818a68dd1a00828c45158#diff-6ec07359a4b7f7c8c986c86549f01867ea071bef35cac64e0bd05302fef2e615), because after creating the model, the user can't change the `max_target_positions` so I saved it in the model.
And finally I added some tests to [test_modeling_whisper.py](https://github.com/huggingface/transformers/pull/33196/files/8bf582f176ffc7a367bfb3dca02f5a68f478fec7#diff-cf6c12f8da48db4d91bcc6db32ecb7c1609a76e30719b5d47cccf595d326d235) in order to test this new scenario. This seems like something that could slow down the forward pass during training; it seems to me like that could/should be handled from the processor side of things, but I understand `labels` aren't managed by the processor. 

WDYT @ylacombe? I'm not aware of this kind of verification being done by the processor or the tokenizer tbh! 
Are the costs of computing the shape and doing an `if` statement high enough to actually slow down training ? I'm not that sure

Happy to take other opinions here, maybe @eustlb or @ArthurZucker ?  @ArthurZucker mentions that it's unlikely to slow down, so let's merge like this and eventually revert if we see a slowdown."
33487,2024-09-14T10:13:02Z,2024-10-29T14:27:34Z,simonJJJ,6,0,1,3,1,1,1,[],2494877.0,0,5244589.0,0,0,0,0,3690661.608982,,0,1,0,False,"['simonJJJ', 'zhangfaen', 'Betty-J', 'zytx121']","I am trying to finetune Qwen2-VL for Object detection on coco dataset ( https://github.com/zhangfaen/finetune-Qwen2-VL  )

Without this PR, my finetuned model performance is bad.
With this PR (by cherry picking), my finetuned model performance is really good.

Any progress for this PR to be merged?  

Thank you  > I am trying to finetune Qwen2-VL for Object detection on coco dataset ( https://github.com/zhangfaen/finetune-Qwen2-VL )
> 
> Without this PR, my finetuned model performance is bad. With this PR (by cherry picking), my finetuned model performance is really good.
> 
> Any progress for this PR to be merged?
> 
> Thank you

@zhangfaen Strange, after I modified this code, the fine-tuning performance still did not improve. Is there anything else that needs to be modified? > > I am trying to finetune Qwen2-VL for Object detection on coco dataset ( https://github.com/zhangfaen/finetune-Qwen2-VL )
> > Without this PR, my finetuned model performance is bad. With this PR (by cherry picking), my finetuned model performance is really good.
> > Any progress for this PR to be merged?
> > Thank you
> 
> @zhangfaen Strange, after I modified this code, the fine-tuning performance still did not improve. Is there anything else that needs to be modified?

for what task and against what dataset are you finetuning qwen2-vl ? hi @ArthurZucker, would u mind taking a look at merging this PR? The community is waiting for it. > hi @ArthurZucker, would u mind taking a look at merging this PR? The community is waiting for it.

+1 @ArthurZucker  > Yep sorry I approved I don't know why it was lost in tracks!

Hi there, when I used `pip install transformers`, I installed version 4.46.2, which still does not include the updates mentioned here https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L1723.  Makes sense! Do you want to add a small test for training? 🤗  Yep sorry I approved I don't know why it was lost in tracks!",Makes sense! Do you want to add a small test for training? 🤗  Yep sorry I approved I don't know why it was lost in tracks!,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # [(issue)](https://github.com/QwenLM/Qwen2-VL/issues/105) [(issue)](https://github.com/QwenLM/Qwen2-VL/issues/112)

We encountered some issues with `position_ids==None` when fine-tuning the qwen2vl model, because the model_inputs do not pass the `position_ids` arg.


## Before submitting
- [] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34653,2024-11-08T07:42:10Z,2024-11-13T21:55:59Z,yuanx749,1,0,1,9,1,1,1,[],482803.0,0,496454.0,0,0,0,0,3695849.933862,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34653). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Awesome, thanks for the fix!","Awesome, thanks for the fix!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Added missing newlines before `>>>`. Moreover, to make this code work without errors, I corrected the typos and added necessary config.


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@stevhliu, could you please review this PR?

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34684,2024-11-11T11:20:26Z,2024-11-13T22:16:59Z,faaany,3,1,5,52,12,1,1,[],93091.0,0,212193.0,0,0,0,0,3707815.360208,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'faaany']","> Thanks for the suggestion!
> 
> I think using `get_backend` may be the easiest option for users, and this will also include other devices such as `mps`.

thanks! code update. Is this okay for you? Then I can change to other places as well. > Very nice, thank you! 👍

I only made one change: I removed GPU from the comment because GPU is a general term including CUDA, XPU, NPU. And I made the same change at other places as well. Pls have a review, thx a lot!  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34684). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the suggestion!

I think using `get_backend` may be the easiest option for users, and this will also include other devices such as `mps`. Very nice, thank you! 👍  Fantastic, works for me!","Thanks for the suggestion!

I think using `get_backend` may be the easiest option for users, and this will also include other devices such as `mps`. Very nice, thank you! 👍  Fantastic, works for me!","## What does this PR do?
There are many places in the task guides that only check the cuda device availability. This PR adds xpu besides cuda, but I am not sure whether my current change is the best one, because there are other ways to add xpu as, e.g. 
- add xpu in the inline-comment 
- use accelerate's `get_backend` API to get the underlying device 

Just let me what is your favorite, @stevhliu Once agreed by you, I can submit further commits to replicate the change in other places. ","Maybe add a comment here that explains what's happening. Perhaps something like:

```suggestion
# automatically detects the underlying device type (GPU, CUDA, CPU, XPU, MPS, etc.)
>>> device, _, _ = get_backend()
```"
34698,2024-11-12T07:27:20Z,2024-11-13T21:14:24Z,maximizemaxwell,2,0,3,221,2,1,1,[],28542.0,0,139055.0,0,0,0,0,3708539.740163,,0,3,0,False,"['HuggingFaceDocBuilderDev', '4N3MONE']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34698). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. LGTM! 번역 고생하셨습니다. 😊LGTM thank you! We can merge once a PseudoLab team member has reviewed :),LGTM thank you! We can merge once a PseudoLab team member has reviewed :),"# What does this PR do?

Translated the marian.md file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179
## Before reviewing

  - [x]  Check for missing / redundant translations (번역 누락/중복 검사)
  - [x] Grammar Check (맞춤법 검사)
  - [x] Review or Add new terms to glossary (용어 확인 및 추가)
  - [x]  Check Inline TOC (e.g. [[lowercased-header]])
  - [ ]  Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

@junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang
## Before submitting

   - [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
   - [x]  Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
    Pull Request section?
   - [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
    to it if that's the case.
   - [x]  Did you make sure to update the documentation with your changes? Here are the
    [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
    [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
   - [ ]  Did you write any new necessary tests?

## Who can review? (Final)

@stevhliu May you please review this PR?",
34717,2024-11-13T16:37:56Z,2024-11-13T21:44:42Z,pcuenca,1,0,1,2,1,1,1,[],1741.0,0,18407.0,0,0,0,0,3709752.823235,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34717). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks Pedro! 🤗 ,Thanks Pedro! 🤗 ,"## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@stevhliu, cc @gante 
",
34549,2024-10-31T22:21:03Z,2024-11-11T19:52:10Z,aymeric-roucher,2,0,5,179,5,1,1,['Agents'],327932.0,0,941468.0,0,0,0,0,3889305.282428,,0,5,0,False,"['aymeric-roucher', 'HuggingFaceDocBuilderDev']","Note to self: my attempt to unify a `stream_run` function that `yield`s results as they come and the `direct_run` function was misled: inserting even the tyniest `yield` in a function turns it into a generator, which I don't want for my `run` function.
Since this PR has nice monitoring tests and improvements, I'll still keep it with only these bits! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34549). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks @aymeric-roucher!",Thanks @aymeric-roucher!,"# What does this PR do?

Merge `stream_run` and `direct_run` in one more elegant function to streamline the code.",
34682,2024-11-11T10:34:28Z,2024-11-11T15:09:31Z,faaany,1,0,1,16,8,1,1,[],16367.0,0,16503.0,0,0,0,0,3906265.866918,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34682). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the fix! 🤗 ,Thanks for the fix! 🤗 ,"## What does this PR do?
Using the existing example code in doc `Share a Model`, I got the following error: 
```bash
OSError: v2.0.1 is not a valid git identifier (branch name, tag name or commit id) that exists for this model name. Check the model page at 'https://huggingface.co/julien-c/EsperBERTo-small' for available revisions.
```
I think it is not intended to make this example code, so I assume that the revision was deleted after the doc was published. This PR updates the revision using commit hash.  

@stevhliu
",
33332,2024-09-05T21:14:00Z,2024-11-10T11:19:20Z,louisbrulenaudet,7,0,11,73,1,1,1,[],68511.0,0,5668207.0,0,0,0,0,4004990.820322,,0,11,0,False,"['aymeric-roucher', 'LysandreJik', 'louisbrulenaudet', 'HuggingFaceDocBuilderDev']","cc @aymeric-roucher for review @louisbrulenaudet you'll just have to do a few style fixes (trailing whitespaces) before being able to merge. Hi @aymeric-roucher, normally I've made the necessary corrections and removed the whitespaces, but I'm still getting a validation error that I can't explain.

Could you tell me what you think is wrong? @louisbrulenaudet the error in both `check_repository_consistency`  and `check_code_quality` is the following:
```
The following objects docstrings do not match their signature. Run `make fix-copies` to fix this. In some cases, this error may be raised incorrectly by the docstring checker. If you think this is the case, you can manually check the docstrings and then add the object name to `OBJECTS_TO_IGNORE` in `utils/check_docstrings.py`.
- HfApiEngine
```
I believe it might be because 3 of the 4 arguments are defined as `(str, optional)` in the docstrings but are only `str` instead of `Optional[str]` in their signature, so should be `str`in the docstrings (yes the verification is picky 😅). You can run `make fix-copies` to check this.

 Hello @aymeric-roucher,

I have the impression that all the tests have been passed, thank you again for your information and your help, I hope that everything will go well from now on and wish you a good day. It's merged! Thanks a lot for this PR @louisbrulenaudet 🤗 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33332). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Great changes, thanks a lot @louisbrulenaudet!","Great changes, thanks a lot @louisbrulenaudet!","# What does this PR do?
- Added support for optional token and max_tokens parameters in the constructor.
- Provided usage examples and detailed documentation for each method.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?",
34561,2024-11-01T12:59:46Z,2024-11-10T11:22:40Z,aymeric-roucher,2,0,3,189,3,1,1,['Agents'],1607.0,0,771776.0,0,0,0,0,4006276.301166,,0,3,0,False,"['julien-c', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34561). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. cc @nsarrazin @gary149 have you seen this?Great, thanks Aymeric!","Great, thanks Aymeric!","# What does this PR do?

Allows to create tools from Spaces.",
33080,2024-08-22T20:24:49Z,2024-11-09T19:26:28Z,AhmedAlmaghz,6,30,53,910,4,3,2,[],6136005.0,0,6826564.0,1,0,0,0,4059186.783751,,0,53,0,False,"['HuggingFaceDocBuilderDev', 'AhmedAlmaghz']","Thank you @abodacs  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33080). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. The following pull requests have been added to the current pull request to be merged once

- [x]  Run training on Amazon SageMaker #33071
- [x]  Export to ONNX #33072

@abodacs  , @stevhliu  أخي عبدالله @abodacs  هذا ما تم إنجازه في هذا القسم 
إلى الامام إن شاء الله



# DEVELOPER GUIDES

- [x]  Use fast tokenizers from 🤗 Tokenizers #33034
- [x]  Run inference with multilingual models #33048
- [x]  Use model-specific APIs #33030
- [x]  Share a custom model #33031
- [x]  Templates for chat models #33026
- [x]  Trainer #33080
- [x]  Run training on Amazon SageMaker #33071
- [x]  Export to ONNX #33072
- [x]  Export to TFLite #33077
- [ ]  Export to TorchScript #33079
- [ ]  Benchmarks #33023
- [ ]  Notebooks with examples #33049
- [ ]  Community resources #33027
- [ ]  Troubleshoot #33017
- [x]  Interoperability with GGUF files #33037
 Please, merge this PR.
@stevhliu  Thank you @stevhliu السﻻم عليكم
تمت المراجعة
@AhmedAlmaghz 
جزاك الله خيرا Cool, thanks for the translation! Few more formatting issues, then we should be ready 👍  Thanks!","السﻻم عليكم
تمت المراجعة
@AhmedAlmaghz 
جزاك الله خيرا Cool, thanks for the translation! Few more formatting issues, then we should be ready 👍  Thanks!","
## What does this PR do?
Translated the `docs/source/ar/trainer.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ","```suggestion
تُتيح وحدة [`Trainer`] حلقة تدريب وتقييم متكاملة لنماذج PyTorch المطبقة في مكتبة Transformers. تحتاج فقط إلى تمرير المكونات الضرورية للتدريب (النموذج، والمجزىء النصى، ومجموعة البيانات، دالة التقييم، معلمات التدريب الفائقة، إلخ)، وستتولى فئة [`Trainer`] الباقي. هذا يُسهّل بدء التدريب بشكل أسرع دون كتابة حلقة التدريب الخاصة بك يدويًا. ولكن في الوقت نفسه، فإن [`Trainer`] قابل للتخصيص بدرجة كبيرة ويوفر العديد من خيارات التدريب حتى تتمكن من تخصيصه وفقًا لاحتياجات التدريب الخاصة بك بدقة.
```

 ```suggestion

بالإضافة إلى فئة [`Trainer`], توفر مكتبة Transformers أيضًا فئة [`Seq2SeqTrainer`] للمهام التسلسلية مثل الترجمة أو التلخيص. هناك أيضًا فئة [`~trl.SFTTrainer`] من مكتبة [TRL](https://hf.co/docs/trl) التي تغلّف فئة [`Trainer`] وهي مُحُسَّنة لتدريب نماذج اللغة مثل Llama-2 وMistral باستخدام تقنيات التوليد اللغوي. كما يدعم [`~trl.SFTTrainer`] ميزات مثل حزم التسلسلات، وLoRA، والقياس الكمي، وDeepSpeed مما يُمكّن من التدريب بكفاءة على نماذج ضخمة الحجم.

<br>

لا تتردد في الاطلاع على [مرجع API](./main_classes/trainer) لهذه الفئات الأخرى من النوع [`Trainer`] لمعرفة المزيد حول متى يتم استخدام كل منها. بشكل عام، [`Trainer`] هو الخيار الأكثر تنوعًا ومناسبًا لمجموعة واسعة من المهام. تم تصميم [`Seq2SeqTrainer`] للمهام التسلسلية ، و [`~trl.SFTTrainer`] مُصمم لتدريب نماذج اللغة الكبيرة.

```  ```suggestion
قبل البدء، تأكد من تثبيت مكتبة [Accelerate](https://hf.co/docs/accelerate) - وهي مكتبة تُمكّن تشغيل تدريب PyTorch في بيئات مُوزعة.
```
  ```suggestion
يتضمن [`Trainer`] جميع التعليمات البرمجية التي ستجدها في حلقة التدريب الأساسية:

1. قم بتنفيذ خطوة تدريب لحساب الخسارة
2. احسب المشتقات باستخدام طريقة [`~accelerate.Accelerator.backward`]
3. تحديث الأوزان بناءً على المشتقات
4. كرر هذه العملية حتى تصل إلى عدد محدد مسبقًا من الدورات (epochs).
```

 ```suggestion
تُجرد فئة [`Trainer`] كل هذه التعليمات البرمجية حتى لا تضطر إلى القلق بشأن كتابة حلقة تدريب يدويًا في كل مرة أما إذا كنت بدأت للتو في PyTorch والتدريب. كل ما عليك فعله هو توفير المكونات الأساسية اللازمة للتدريب، مثل النموذج ومجموعة بيانات، وتتعامل فئة [`Trainer`] مع كل شيء آخر.

إذا كنت تُريد تحديد أي خيارات تدريب أو معلمات فائقة، فيمكنك العثور عليها في فئة [`TrainingArguments`]. على سبيل المثال، دعنا نحدد أين يتم حفظ النموذج في `output_dir` ورفع النموذج إلى Hub بعد التدريب باستخدام `push_to_hub=True`.

```

  ```suggestion
مرر `training_args` إلى [`Trainer`] جنبًا إلى جنب مع النموذج، ومجموعة بيانات، وشئ لمعالجة مجموعة البيانات مسبقًا (حسب نوع البيانات، فقد يكون محللًا رمزيًا أو مستخرج ميزات أو معالج صور)، وجامع بيانات، ودالة لحساب المقاييس التي تُريد تتبعها أثناء التدريب.

أخيرًا، استدعِ [`~Trainer.train`] لبدء التدريب!
```
  ```suggestion
### نقاط الحفظ
``` ```suggestion
تحفظ فئة [`Trainer`] نقاط الحفظ النموذج في الدليل المحدد في معامل `output_dir` من [`TrainingArguments`]. ستجد نقاط الحفظ في مجلد فرعي يسمى `checkpoint-000` حيث تتوافق الأرقام في النهاية مع خطوة التدريب. إن حفظ نقاط الحفظ مفيد لاستئناف التدريب لاحقًا.
```
 ```suggestion
# استأنف من أحدث نقطة حفظ
``` ```suggestion
# استأنف من نقطة حفظ محددة محفوظة في دليل الإخراج
``` ```suggestion
يمكنك حفظ نقاط الحفظ الخاصة بك (لا يتم حفظ حالة المُجزىء اللغوى تقائيًا)  إلى Hub عن طريق تعيين `push_to_hub=True` في [`TrainingArguments`] لرفعها. الخيارات الأخرى لاتخاذ القرار بشأن كيفية حفظ هذة النقاط الخاصة بك هي الإعداد في معامل [`hub_strategy`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.hub_strategy):
```
 

 ```suggestion
* `hub_strategy=""checkpoint""` يدفع أحدث نقطة حفظ إلى مجلد فرعي يسمى ""last-checkpoint"" يمكنك استئناف التدريب منه
* `hub_strategy=""all_checkpoints""` يدفع جميع نقاط الحفظ إلى الدليل المحدد في `output_dir` (سترى نقطة حفظ واحدة لكل مجلد في مستودع النموذج الخاص بك)
```

  ```suggestion
عند استئناف التدريب من نقطة حفظ، تُحاول [`Trainer`] الحفاظ على حالات RNG Python وNumPy وPyTorch كما كانت عندما تم حفظ نقطة الحفظ. ولكن لأن PyTorch لديها العديد من الإعدادات الافتراضية غير الحتمية مُتنوعة، فإن حالات RNG ليست مضمونة لتكون هي نفسها. إذا كنت تريد تمكين الحتمية الكاملة، فراجع دليل [التحكم في مصادر العشوائية](https://pytorch.org/docs/stable/notes/randomness#controlling-sources-of-randomness) لمعرفة ما يُمكنك تمكينه لجعل تدريبك حتميًا تمامًا. ضع في اعتبارك أنه من خلال جعل إعدادات معينة حتمية، فقد يكون التدريب أبطأ.
```


  ```suggestion
في حين أن فئة [`Trainer`] مُصممة لتكون سهلة الوصول وسهلة الاستخدام، فإنها توفر أيضًا الكثير من قابلية التخصيص للمستخدمين المغامرين.  يُمكن إنشاء فئات فرعية من العديد من أساليب [`Trainer`] وتجاوزها لدعم الوظائف التي تُريدها، دون الحاجة إلى إعادة كتابة حلقة التدريب بأكملها من البداية لاستيعابها. تتضمن هذه الأساليب:
```
 
 ```suggestion
* [`~Trainer.get_train_dataloader`] ينشئ DataLoader للتدريب
* [`~Trainer.get_eval_dataloader`] ينشئ DataLoader للتقييم
* [`~Trainer.get_test_dataloader`] ينشئ DataLoader للاختبار
* [`~Trainer.log`] يسجل معلومات حول مختلف الكائنات التي تراقب التدريب
* [`~Trainer.create_optimizer_and_scheduler`] ينشئ محسنًا ومخططًا لمُعدل التعلم إذا لم يتم تمريرهما في `__init__`؛ يمكن أيضًا تخصيص هذه الوظائف بشكل منفصل باستخدام [`~Trainer.create_optimizer`] و [`~Trainer.create_scheduler`] على التوالي
* [`~Trainer.compute_loss`] يحسب دالة الخسارة على دفعة من مُدخلات التدريب
* [`~Trainer.training_step`] يُنفذ خطوة التدريب
* [`~Trainer.prediction_step`] يُنفذ خطوة التنبؤ والاختبار
* [`~Trainer.evaluate`] يُقيّم النموذج ويعيد مقاييس التقييم
* [`~Trainer.predict`] يُجري التنبؤات (مع المقاييس إذا كانت العلامات متاحة) على مجموعة الاختبار
```
  ```suggestion
على سبيل المثال، إذا كنت تريد تخصيص طريقة [`~Trainer.compute_loss`] لاستخدام دالة خسارة ذات ترجيح بدلاً من ذلك.
```

  ```suggestion
``` ```suggestion
### دوال الاستدعاء Callbacks
``` ```suggestion
خيار آخر لتخصيص [`Trainer`] هو استخدام [دوال الاستدعاء](callbacks). لا *تغير* دوال الاستدعاء أي شيء في حلقة التدريب. إنهم تفحص حالة حلقة التدريب ثم تُنفذ بعض الإجراءات (مثل الإيقاف المبكر أو تسجيل النتائج، إلخ) اعتمادًا على الحالة. وبعبارة أخرى، لا يمكن استخدام دالة الاستدعاء لتنفيذ شيء مثل دالة خسارة مخصصة، ويجب عليك تجاوز دالة [`~Trainer.compute_loss`] لذلك.

على سبيل المثال، إذا كنت تريد إضافة دالة استدعاء إيقاف مبكر إلى حلقة التدريب بعد 10 خطوات.
```
  ```suggestion
ثم مرره إلى معامل `callback` في [`Trainer`].
``` ```suggestion
## تسجيل الأحداث (Logging)
``` ```suggestion
راجع مرجع [API](./main_classes/logging) للتسجيل للحصول على مزيد من المعلومات حول مستويات التسجيل المختلفة للأحداث.
```

  ```suggestion
يتم تعيين [`Trainer`] إلى `logging.INFO` افتراضيًا والذي يُبلغ عن الأخطاء والتحذيرات ومعلومات أساسية أخرى. يتم تعيين نسخة [`Trainer`] - في البيئات الموزعة - إلى `logging.WARNING` والتي يُبلغ فقط عن الأخطاء والتحذيرات. يمكنك تغيير مستوى تسجيل الأحداث باستخدام معاملي [`log_level`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.log_level) و [`log_level_replica`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.log_level_replica) في [`TrainingArguments`].
```
  ```suggestion
لتهيئة إعداد مُستوى تسجيل  اﻷحداث لكل عقدة، استخدم معامل [`log_on_each_node`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments.log_on_each_node) لتحديد ما إذا كان سيتم استخدام مُستوى السجل على كل عقدة أو فقط على العقدة الرئيسية.
```
  ```suggestion
يحدد [`Trainer`] مُستوى التسجيل بشكل مُنفصل لكل عقدة في طريقة [`Trainer.__init__`]، لذا فقد ترغب في التفكير في تعيين هذا الإعداد في وقت سابق إذا كنت تستخدم وظائف Transformers الأخرى قبل إنشاء كائن [`Trainer`].
```

  ```suggestion
على سبيل المثال، لتعيين التعليمات البرمجية والوحدات النمطية الرئيسية الخاصة بك لاستخدام نفس مُستوى التسجيل وفقًا لكل عقدة:
```
  ```suggestion
``` ```suggestion
[NEFTune](https://hf.co/papers/2310.05914) هي تقنية يمكن أن تحسن الأداء عن طريق إضافة ضوضاء إلى مُتجهات التعلم أثناء التدريب. لتمكينه في [`Trainer`], قم بتعيين معامل `neftune_noise_alpha` في [`TrainingArguments`] للتحكم في مقدار الضوضاء المُضافة.
```

  
## نواة Liger

[Liger-Kernel](https://github.com/linkedin/Liger-Kernel) Kernel هي مجموعة من نوى Triton التي طورتها Linkedin مُصممة خصيصًا لتدريب نماذج اللغة الكبيرة (LLM). لقد قمنا بتنفيذ RMSNorm و RoPE و SwiGLU و CrossEntropy و FusedLinearCrossEntropy مُتوافقة مع Hugging Face، والمزيد قادم. يُمكنها زيادة إنتاجية التدريب متعدد وحدات معالجة الرسومات (GPU) بنسبة 20٪ وتقليل استخدام الذاكرة بنسبة 60٪. تعمل النواة بشكل تلقائي مع flash attention و PyTorch FSDP و Microsoft DeepSpeed.

<Tip>
احصل على زيادة في الإنتاجية بنسبة 20٪ وتقليل استخدام الذاكرة بنسبة 60٪ على تدريب نماذج LLaMA 3-8B. حقق أطوال سياق أكبر وأحجام دفعات أكبر. كما أنها مُفيدة إذا كنت تُريد زيادة حجم نموذجك إلى تدريب بنماذج متعددة الرؤوس أو أحجام مُفردات ضخمة. أطلق العنان للتدريب بنماذج متعددة الرؤوس (medusa) والمزيد. راجع التفاصيل والأمثلة في [Liger](https://github.com/linkedin/Liger-Kernel/tree/main/examples)
</Tip>

تأكد أولاً من تثبيت مستودع Liger الرسمي:

```bash
pip install liger-kernel
```

يجب عليك تمرير `use_liger_kernel=True` لتطبيق نواة liger على نموذجك، على سبيل المثال:

```py
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir=""your-model"",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    eval_strategy=""epoch"",
    save_strategy=""epoch"",
    load_best_model_at_end=True,
    push_to_hub=True,
    use_liger_kernel=True
)
```

تدعم النواة معماريات نماذج Llama و Gemma و Mistral و Mixtral. يُمكن العثور على أحدث قائمة بالنمائج المدعومة [هنا](https://github.com/linkedin/Liger-Kernel). عندما يتم تعيين `use_liger_kernel` إلى `True`، سيتم تصحيح الطبقات المُقابلة في النموذج الأصلي باستخدام تطبيق Liger الفعال، لذلك لا تحتاج إلى فعل أي شيء إضافي بخلاف تعيين قيمة المعامل.
 ## المُحسِّنات

يمكنك اختيار مُحسِّن مدمج للتدريب باستخدام:

```python
from transformers import TrainingArguments
training_args = TrainingArguments(..., optim=""adamw_torch"")
```

اطلع على [`OptimizerNames`](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py) للاطلاع على القائمة الكاملة للخيارات. نُدرج أمثلة مُتقدمة في الأقسام أدناه.

يمكنك أيضًا استخدام مُحسِّن PyTorch عشوائي عبر:

```python
import torch

optimizer_cls = torch.optim.AdamW
optimizer_kwargs = {
    ""lr"": 4e-3,
    ""betas"": (0.9, 0.999),
    ""weight_decay"": 0.05,
}

from transformers import Trainer
trainer = Trainer(..., optimizer_cls_and_kwargs=(optimizer_cls, optimizer_kwargs))"
34450,2024-10-27T14:46:47Z,2024-11-05T17:26:20Z,Isotr0py,8,0,3,19,2,3,3,[],88545.0,0,1039913.0,0,0,0,0,4163719.928636,,0,3,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev', 'VladOS95-cyber', 'Isotr0py']","Hey @Isotr0py! These changes are good to me. Thank you! Please, fix test_stablelm_weights_conversion_fp16 and test_stablelm_fp16 in test_ggml.py as well to get rid of config=original_model.config as we do not need it anymore The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34450). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Can you try to rebase your PR in order to pass the CI @Isotr0py ?  @SunMarc I have merged the main branch but seems that the test CI is still failing in unrelated tests:
https://app.circleci.com/pipelines/github/huggingface/transformers/110104/workflows/8e77c517-3ae4-4cad-ac75-c4ba5616e9d6/jobs/1464043/tests > @SunMarc I have merged the main branch but seems that the test CI is still failing in unrelated tests: https://app.circleci.com/pipelines/github/huggingface/transformers/110104/workflows/8e77c517-3ae4-4cad-ac75-c4ba5616e9d6/jobs/1464043/tests

Hey, try to do rebase. First of all, in command line, `git fetch upstream`
`git rebase upstream/main` and then `git push -u origin gguf-fix-stablelm --force` The problematic test should have been fixed by https://github.com/huggingface/transformers/pull/34605. Can you try to rebase on that ?  @SunMarc All tests passed now! Niceee ! Thanks for fixing this @Isotr0py !  Thank you!",Thanks for fixing this @Isotr0py !  Thank you!,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #34426 (issue)
- #34426


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@VladOS95-cyber @SunMarc 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34627,2024-11-06T15:38:47Z,2024-11-08T02:56:09Z,maximizemaxwell,3,4,7,344,2,2,1,[],28618.0,0,127106.0,0,0,0,0,4209407.473664,,0,7,0,False,"['maximizemaxwell', 'HuggingFaceDocBuilderDev', '4N3MONE']","사소한 수정사항 제안 남겨두었습니다.

번역 고생하셨습니다 😊 Thanks for suggestions!!
Those are updated now.
Any other things to change? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34627). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks! Just a few more things :)",Thanks! Just a few more things :),"# What does this PR do?

Translated the bert.md file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179
## Before reviewing

  - [x]  Check for missing / redundant translations (번역 누락/중복 검사)
  - [x] Grammar Check (맞춤법 검사)
  - [x] Review or Add new terms to glossary (용어 확인 및 추가)
  - [x]  Check Inline TOC (e.g. [[lowercased-header]])
  - [ ]  Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

@junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang
## Before submitting

   - [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
   - [x]  Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
    Pull Request section?
   - [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
    to it if that's the case.
   - [x]  Did you make sure to update the documentation with your changes? Here are the
    [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
    [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
   - [ ]  Did you write any new necessary tests?

## Who can review? (Final)

@stevhliu May you please review this PR?","```suggestion
#### 학습
``` ```suggestion
#### 추론
``` ```suggestion
#### 학습 [[Training]]
``` ```suggestion
#### 추론 [[Inference]]
```"
32371,2024-08-01T08:45:38Z,2024-08-01T11:57:42Z,itazap,2,0,1,54,2,1,1,[],1186.0,0,8502193.0,0,0,0,0,4239910.889242,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'Tataaa-cans']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32371). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. clean_up_tokenization_spacescool thanks for separating ! 
","cool thanks for separating ! 
","ref #31884 

start of deprecating `clean_up_tokenization_spaces`. Right now it defaults to True, will be updated in the future to default to False.

future pr: https://github.com/huggingface/transformers/pull/31938

@ArthurZucker ",
33972,2024-10-05T06:22:04Z,2024-11-07T19:04:27Z,mreraser,4,4,7,55,2,2,1,[],281890.0,0,2896943.0,0,0,0,0,4237775.073925,,0,7,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'cjfghk5697', 'Rocketknight1']","Hello! @yijun-lee, @jungnerd, @cjfghk5697
May I kindly ask for your review? Thank you for your consistent reviews 🤗 
 LGTM! cc @stehvliu! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33972). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice, thank you!","Nice, thank you!","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `timesformer.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [x] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@yijun-lee, @mreraser, @jungnerd, @cjfghk5697
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

Hello @stevhliu !  May you please review this PR? Thank you! 
Have a nice day :)","I think your alignment may be off here. Should look like:

```yml
- isExpanded: false
  sections:
  - local: model_doc/timesformer
    title: TimeSFormer
``` Thank you! I might have missed that. 😅  Oops looks like the toctree still isn't properly aligned! The line [here](https://github.com/mreraser/transformers/blob/9ad85130c6e2c3636642418fca7c80d9aa339035/docs/source/ko/_toctree.yml#L658) should be aligned with the one [here](https://github.com/mreraser/transformers/blob/9ad85130c6e2c3636642418fca7c80d9aa339035/docs/source/ko/_toctree.yml#L663) @stevhliu I have completed modifying the toctree, thank you!"
34455,2024-10-28T03:49:09Z,2024-11-07T14:57:34Z,shcheklein,3,0,1,7,1,2,2,[],130768.0,0,904105.0,0,0,0,0,4252588.576951,,1,1,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1', 'shcheklein']","cc @SunMarc @jackyjinjing for the original PR! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34455). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @SunMarc done, rebasedThanks for fixing ! To fix the CI, please rebase the PR on main  Thank you!","Thanks for fixing ! To fix the CI, please rebase the PR on main  Thank you!","Fixes these failing tests https://github.com/iterative/dvclive/actions/runs/11545815176/job/32134865977?pr=846 downstream.

The cause is this change: https://github.com/huggingface/transformers/pull/33743 which made `trainer` ctr a bit more strict.

I see that `WandbCallback` is also using the same technique (initializing fake trainer). @parambharat could you check if we need to add a similar fix there.",
31095,2024-05-28T21:30:20Z,2024-05-29T14:20:59Z,dhruvbpai,6,0,4,16,3,3,3,[],41750.0,0,14029484.0,0,0,0,0,4282740.678834,,1,4,0,False,"['Arunprakash-A', 'HuggingFaceDocBuilderDev', 'YeLuoSuiYou', 'colmon46', 'Gaoyg', 'PangziZhang523']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_31095). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hello, thank you for your contribution. I used on_optimizer_step to print the gradient, and all the printed values ​​were None, but the final grad_norm had a value. Why is that? > Hello, thank you for your contribution. I used on_optimizer_step to print the gradient, and all the printed values ​​were None, but the final grad_norm had a value. Why is that?

Same question with @PangziZhang523 . Do you know how to fix this problem plz? It is working fine for me.  > > Hello, thank you for your contribution. I used on_optimizer_step to print the gradient, and all the printed values ​​were None, but the final grad_norm had a value. Why is that?
> 
> Same question with @PangziZhang523 . Do you know how to fix this problem plz?

Hi @colmon46, if we use deepspeed or FSDP, we should use self.model_wrapped to get gradient of every layers, but do you know how can we get self.model_wrap in callbacks, 3ku > > Hello, thank you for your contribution. I used on_optimizer_step to print the gradient, and all the printed values ​​were None, but the final grad_norm had a value. Why is that?
> 
> Same question with @PangziZhang523 . Do you know how to fix this problem plz?

Same question. Is there a solution already? thx~This looks great to me thanks !  Makes sense to me as well! Nice job. cc @amyeroberts for final review  Awesome, LGTM! Thanks a lot :hugs: ","This looks great to me thanks !  Makes sense to me as well! Nice job. cc @amyeroberts for final review  Awesome, LGTM! Thanks a lot :hugs: ","# Add on_optimizer_step callback option in TrainerCallbacks

Aside: This is my first open source pull request, so any feedback would be much appreciated!

The test `tests/trainer/test_trainer_callback.py` has been modified appropriately to invoke the new callback method.
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #31033  (issue)


## Reviewers
As tagged in initial issue - @muellerzr @younesbelkada

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34531,2024-10-31T11:58:04Z,2024-11-05T15:10:15Z,yonigottesman,3,3,4,130,8,3,2,"['bug', 'Chat Template']",95824.0,0,490856.0,0,0,0,0,4377304.835599,,0,4,0,False,"['yonigottesman', 'HuggingFaceDocBuilderDev', 'Rocketknight1']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34531). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. rebased and ready for merge... Merged!Yes, this makes sense to me now, and the tests are helpful! Thank you!

cc @LysandreJik for core maintainer review Nice, impressive test! Thanks @yonigottesman ","Yes, this makes sense to me now, and the tests are helpful! Thank you!

cc @LysandreJik for core maintainer review Nice, impressive test! Thanks @yonigottesman ","This pr fixes a bug that caused  #34494.
when using `apply_chat_template` with `return_assistant_tokens_mask` and token truncation, the assistant mask was not correct.

@Rocketknight1 ","Will this line work for both batched and unbatched cases? yes, i also added both cases in the new test i added You're right, sorry!"
29261,2024-02-23T19:37:53Z,2024-08-27T17:50:28Z,jpizarrom,30,30,38,1679,17,3,0,['run-slow'],297921.0,0,22260172.0,1,0,0,0,4266800.485808,,0,38,0,False,"['KilianKW', 'amyeroberts', 'NielsRogge', 'HuggingFaceDocBuilderDev', 'jpizarrom', 'ArthurZucker', 'ydshieh', 'github-actions[bot]', 'atomwalk12']","cc @NielsRogge and @younesbelkada if one of you want to review on @jpizarrom makes the CIs go green! > cc @NielsRogge and @younesbelkada if one of you want to review on @jpizarrom makes the CIs go green!

Hi, what could I do to makes the CIs go green! shall I just merge to upstream/main, or rebase to it? @jpizarrom It's preferable for you to rebase onto main. To see how to make the CIs green, you'll need to click on `details` and look at the output error logs from circleci. I'd suggest doing this after rebasing so see which errors are coming from this branch.  This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.

Please note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored. @jpizarrom once the CI is green I can assign a core maintainer for a final approval > @jpizarrom once the CI is green I can assign a core maintainer for a final approval

I believe the CI errors are not related to this branch, i see modeling_mra and other non related error logs, don't know how to make CI green, maybe rebase to a more recent commit of main branch? Could you rebase on main and push? (`git fetch upstream` followed by `git merge upstream/main`, assuming your upstream is set) > Could you rebase on main and push? (`git fetch upstream` followed by `git merge upstream/main`, assuming your upstream is set)

Thanks,
i merged with upstream, but the CI is still not green, other errors occurs, i believe not related to this branch.
`RUN_SLOW=1 python -m pytest tests/models/blip_2/test_modeling_blip_2.py` pass locally Ok, pinging @ydshieh here > Ok, pinging @ydshieh here

Hi, i merged main again, there are some errors in `tests/utils/test_offline.py` > > Ok, pinging @ydshieh here
> 
> Hi, i merged main again, there are some errors in `tests/utils/test_offline.py`

Those could be ignored. But we could probably get them fixed in another PR soon. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_29261). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Thanks for working on this - exciting to have this feature finally added!
> 
> Mostly a few nits. Main comment is about identifying the cause of the change in integration tests values and possibly rectifying given it indicates a degradation in performance.

Hi, thanks for the feedback, I am making the suggested changes.

I don't know what values from the integration tests you are referring to that indicate performance degradation, could you give more context? > Thanks for all the work adding this!
> 
> Only thing left to do is to update the checkpoint references to point to ones under the salesforce org

Shall I do it? can i publish a model under salesforce org? > Shall I do it? can i publish a model under salesforce org?

I'll make sure the checkpoints get transferred Hi @jpizarrom could you push the checkpoints to your HF profile so that I can transfer them to the Salesforce org? > Hi @jpizarrom could you push the checkpoints to your HF profile so that I can transfer them to the Salesforce org?

Hi, I just ran the conversion script, so the checkpoints are updated in.

https://huggingface.co/jpizarrom/blip2-itm-vit-g
https://huggingface.co/jpizarrom/blip2-itm-vit-g-coco @NielsRogge Have the checkpoints been moved?  Hi @jpizarrom , how far is the pull request from being completed? Any chance it could be merged into main in the following weeks? Hi the repos are now available here: https://huggingface.co/models?search=/blip2-itm-vit-g. Great! @jpizarrom Could you update the paths to the checkpoints to the salesforce ones? After that, we just need a run of the slow tests to confirm they all pass. These can be triggered by pushing a commit with the message `[run_slow] blip_2` Hi @amyeroberts, now the slow tests are passing. please let me know if I need to make any more changes. Please excuse me as I am not OP. However, I have an inquiry about this feature as I would really like to be able to use it. Generally, how much time does it take once a feature has been merged into the main branch to be made available in the next release? @jpizarrom Was an issue created to track the failing test c.f. this comment: https://github.com/huggingface/transformers/pull/29261#discussion_r1719797912

 > @jpizarrom Was an issue created to track the failing test c.f. this comment: [#29261 (comment)](https://github.com/huggingface/transformers/pull/29261#discussion_r1719797912)

Not yet, I can do it, but I don't have my computer with me until the first week of September. @amyeroberts @jpizarrom Thanks a lot for adding this feature! I've noticed that the relevant model weights hosted [here](https://huggingface.co/Salesforce/blip2-itm-vit-g) are still missing some license information. Do you need a dedicated ticket for that or is this post enough? > @amyeroberts @jpizarrom Thanks a lot for adding this feature! I've noticed that the relevant model weights hosted [here](https://huggingface.co/Salesforce/blip2-itm-vit-g) are still missing some license information. Do you need a dedicated ticket for that or is this post enough?

Hi, i am not sure, other blip2 models like [Salesforce/blip2-opt-2.7b-coco](https://huggingface.co/Salesforce/blip2-opt-2.7b-coco) show MIT, but in the repo [LAVIS](https://github.com/salesforce/LAVIS/tree/main?tab=readme-ov-file#license) there is [BSD 3-Clause License](https://github.com/salesforce/LAVIS/blob/main/LICENSE.txt)
 Is there any update on the license? Hi, thanks for the ping, I'll reach out to the authors to ask for adding a license tag Do I understand correctly that the license for https://huggingface.co/Salesforce/blip2-itm-vit-g has been set to MIT now?Thanks for adding this! Overall looks great, just a few small comments

Once they're addressed we can move the checkpoints to be under the salesforce org Thanks for your work! Would request some changes however in order to be able to make BLIP-2 compatible with the zero-shot image classification pipeline.","Thanks for adding this! Overall looks great, just a few small comments

Once they're addressed we can move the checkpoints to be under the salesforce org Thanks for your work! Would request some changes however in order to be able to make BLIP-2 compatible with the zero-shot image classification pipeline.","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Add `Blip2ForImageTextRetrieval`, `Blip2TextModelWithProjection`, `Blip2VisionModelWithProjection` models to be able to get Image Text Matching scores, and extract text,image,multimodal features.

Fixes part of #25300 #25245

This is continuation of https://github.com/huggingface/transformers/pull/25612, I tried to apply most of the feedback received in that PR.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker @amyeroberts 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Hi @amyeroberts , I found that in the original model the scale was applied to q before the q*k product [2], and it looks like that was the reason that the outputs of the new HF models [1] were not close enough to the original implementation, you asked in the other PR [1] why tolerance i used was large.

What should i do about this change, as it could affect all the models that are using `Blip2Attention` e.g. blip_2, instructblip share the same code base, should a add config param or so?

[1] https://github.com/huggingface/transformers/pull/25612/files#r1406487831
[2] https://github.com/salesforce/LAVIS/blob/ac8fc98c93c02e2dfb727e24a361c4c309c8dbbc/lavis/models/eva_vit.py#L128 Hmmm, this is a tricky one. Changing would break backwards compatibility, however, I'd be in favour if updating as it's effectively fixing a bug: we want our model to be as close as possible to the original implementation. In this case, can you update the PR name to include a 🚨 prefix? This will enable us to quickly spot the breaking change when preparing the next release an warn users  I don't think it's necessary to add a separate method here. We can just make `text_config` optional in `from_vision_qformer_text_config`  Description here doesn't match what the class returns  Should be filled in here Autocasting and typing should be handled outside of the model definition

```suggestion
        vision_outputs = self.vision_model(
                pixel_values=pixel_values,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
        )
``` 256 value here should be taken from the config, rather than hardcoded ultra nit - one line and remove unnecessary space

```suggestion
            expected_arg_names = [""input_ids"", ""attention_mask"", ""position_ids""]
``` nit 

```suggestion
        inputs_dict = {""pixel_values"": pixel_values}
``` nit 

```suggestion
            expected_arg_names = [""pixel_values""]
``` ```suggestion
            expected_arg_names = [""pixel_values"", ""input_ids"", ""attention_mask""]
``` Following library patterns - `output_xxx` and `return_dict` arguments should go last. I'd suggest putting these after `query_embeds`.  Instead of using this config argument to conditionally call and create this layer, I'd suggest instead call  `self.embeddings` if `input_ids` is not None

```suggestion
        self.embeddings = Blip2TextEmbeddings(config)
``` ```suggestion
        if input_ids is not None:
``` Is this even necessary? when this layer is created_always_, I got this type of errors, don't know how to fix them.
Some Blip2 models do not use this bert based embeddings, they use _opt_ or _flan-t5_ to create the query_embeds. Maybe I could try to refactor the code to move the `Blip2TextEmbeddings` outside of `Blip2QFormerModel` and pass always query_embeds. what do you think?

```
FAILED tests/models/blip_2/test_modeling_blip_2.py::Blip2ForConditionalGenerationDecoderOnlyTest::test_training_gradient_checkpointing - AssertionError: False is not true : qformer.embeddings.word_embeddings.weight in Blip2ForConditionalGeneration has no gradient!
```

```
FAILED tests/models/blip_2/test_modeling_blip_2.py::Blip2ModelTest::test_training_gradient_checkpointing - AssertionError: False is not true : qformer.embeddings.word_embeddings.weight in Blip2ForConditionalGeneration has no gradient!
``` this was done because in the original model the autocast was applied only to the vision layers, don't know yet how to do this in a different way.

https://github.com/salesforce/LAVIS/blob/ac8fc98c93c02e2dfb727e24a361c4c309c8dbbc/lavis/models/blip2_models/blip2_qformer.py#L423-L424 cc @amyeroberts  updated I did a refactor, embeddings were removed from `Blip2QFormerModel`, and place them into `Blip2ForImageTextRetrieval` and `Blip2TextModelWithProjection`, but to do so i needed to add `query_length` param to `Blip2QFormerModel.forward`. Does this affect the slow tests?

i.e. does `RUN_SLOW=yes pytest tests/models/blip_2/test_modeling_blip2.py` still pass? this is outdated, because embeddings were removed from `Blip2QFormerModel` As far as I can tell we don't add `torch.cuda.amp.autocast` code to modeling files, they are just in float32 by default. This was discussed on the original BLIP-2 model addition PR from what I remember. It's up to users to call something like `torch.cuda.amp.autocast` themselves if they wish to load the model in a different precision than the default one (cc @younesbelkada).

Hence in the conversion script I casted both the original weights and my BLIP-2 implementation to float32 in order to verify the conversion. Not sure if feasible, but it'd be nice to match the output class of CLIP, which is also an image-text matching model. It consists of the following keys:
* loss
* logits_per_image (this I assume is the itm_score)
* logits_per_text (this I assume is the itm_score transposed)
* and some other keys which are CLIP-specific.

Making sure that Blip2ForImageTextRetrieval matches this would allow it to be added to the zero-shot image classification pipeline, which relies on this output key: https://github.com/huggingface/transformers/blob/bbaa8ceff696c479aecdb4575b2deb1349efd3aa/src/transformers/pipelines/zero_shot_image_classification.py#L143 Otherwise we will have a hard time adding BLIP-2 support to the zero-shot image classification pipeline. Should not be necessary indeed given that modeling code is by default in torch.float32 ```suggestion
        self.vision_projection = nn.Linear(config.qformer_config.hidden_size, config.image_text_hidden_size)

        # text projection layer
        self.text_projection = nn.Linear(config.qformer_config.hidden_size, config.image_text_hidden_size)
``` ```suggestion
        self.text_projection = nn.Linear(config.qformer_config.hidden_size, config.image_text_hidden_size)
``` ```suggestion
```
past_key_values are not used I assume yes, slow modeling_blip2 tests pass"
32385,2024-08-01T20:06:54Z,2024-10-02T13:08:47Z,amyeroberts,5,6,4,1011,99,4,3,[],1226.0,0,8311096.0,0,0,0,0,4390137.395046,,1,4,0,False,"['SangbumChoi', 'goea-shuhei', 'HuggingFaceDocBuilderDev', 'amyeroberts']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32385). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Love this (Always confused about writing `tokenizer=image_processor`) @molbap If you have capacity - do you think you could do a first review of this? Arthur and Zack are super busy so would be good to have someone who knows the processors well to review in their stead  Asking for review from @SunMarc for Trainer as Zach is off. As this is a very public API I'd like to get a few eyes on it, especially from people who know it well, to make sure it's OK!  _pad_tensors_to_max_len in Seq2SeqTrainer still uses tokenizer instead of  processing_classChecked out the rework, nice - saw that now you can't pass both `tokenizer` and `processing_class`, it will rightfully raise an error, much better. lgtm!  Clean, nothing to say! Thanks for ths!  Thanks for the PR @amyeroberts ! LGTM ! ","Checked out the rework, nice - saw that now you can't pass both `tokenizer` and `processing_class`, it will rightfully raise an error, much better. lgtm!  Clean, nothing to say! Thanks for ths!  Thanks for the PR @amyeroberts ! LGTM ! ","For reviewers: most files touched here are just updates to the documentation. Unfortunately, a lot of the diff is just stripping of unnecessary whitespaces (which my editor does automatically). 

The most important changes are in: 
* src/transformers/trainer.py
* src/transformers/trainer_callback.py
* src/transformers/trainer_seq2seq.py

-------------------------------------------

# What does this PR do?

At the moment, if we wish for a processing class e.g. an image processor, to be saved alongside the model e.g. when pushing to the hub, we have to do the following: 

```py
trainer = Trainer(
    model, 
    args, 
    tokenizer=image_processor, 
    ...
)
```

This causes a lot of confusion for users (an image processor is not a tokenizer). 

Previous efforts were made to add individual classes to the trainer s.t. one could do: `trainer = Trainer(model, args, image_processor=image_processor, ...)`, however, this has some drawbacks: 
* It creates ambiguity when a model has a processor class: should we be able to do `Trainer(model, args, processor=processor)` or should it be `Trainer(model, args, tokenizer=tokenizer, image_processor=image_processor)`? 
* It adds new arguments to handle for every processing class, meaning more work if future classes are added

This PR therefore just deprecates `tokenizer` in favour of a more general `processing_class` argument, which can be any class with a `from_pretrained` method. 

Relevant PRs and discussions: 
* #29896
* #30129 
* #30102 
* #30864


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","This file contains the most important logic changes for review  I directly just swapped the names here, as the docstring says the object is just an internal class do we want futur warning? Thanks for adding these nice tests !  Maybe say that it will be removed in the v5 ?  I'll move it to a future warning and specify v5 removal :)"
34599,2024-11-04T13:36:30Z,2024-11-05T17:32:17Z,ahnjj,1,0,4,139,2,2,1,[],99664.0,0,100548.0,0,0,0,0,4416109.619669,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34599). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM! 번역 감사합니다 Thanks for your contribution!,LGTM! 번역 감사합니다 Thanks for your contribution!,"<!— PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 —>
# What does this PR do?

Translated the `convbert.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [ ] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->
@ahnjj, @jun048098, @Jwaminju, @jeongiin, @chhaewxn, @yijun-lee

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR? ",
34283,2024-10-21T13:59:20Z,2024-10-23T15:24:58Z,muellerzr,6,12,15,112,25,5,1,"['Core: Modeling', 'trainer']",858.0,0,1341696.0,0,0,0,0,4383191.624733,,0,15,0,False,"['JaheimLee', 'man-shar', 'HuggingFaceDocBuilderDev', 'muellerzr', 'ArthurZucker']","@ArthurZucker `LlamaForCausalLM` isn't there bc you did it in your PR 😉 Awesome work by all of you on this. Insane dev speed over the past few days 🙏 🔥  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34283). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Rebasing should probably fix quality!  I wonder whether encoder models need to be fixed as well. @ArthurZucker  @muellerzr  Yep! Fill free to open a PR 🤗 Nice, I don't see Llama being modified, that's probably because it now has `FlashAttentionKwargs` type dict as kwargs. We can create ExtraKwargs, a nested dict with both flash kwargs and loss kwargs and default loss kwargs can be type dict? 
🤗  I only reviewed the PEFT-related code in `trainer.py` and it LGTM. Thanks Zach.","Nice, I don't see Llama being modified, that's probably because it now has `FlashAttentionKwargs` type dict as kwargs. We can create ExtraKwargs, a nested dict with both flash kwargs and loss kwargs and default loss kwargs can be type dict? 
🤗  I only reviewed the PEFT-related code in `trainer.py` and it LGTM. Thanks Zach.","# What does this PR do?

Since most users still want OOTB, this trickles the loss kwargs to the rest of the models so that causal loss can be calculated properly 

Fixes # (issue)

Fully fixes https://github.com/huggingface/transformers/issues/34263 / finishes https://github.com/huggingface/transformers/pull/34191 & https://github.com/huggingface/transformers/pull/34198 & fixes https://github.com/huggingface/transformers/issues/34242


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker 
","This if condition doesn't seem to work for `PeftModel` class (it only has `kwargs` not `loss_kwargs` 🫠 )

I tried just changing that condition to `if True` and ran some tests, and the loss calculation worked perfectly for a LORA on a Llama 3 1B.

I'm wondering if there's a safe/non-breaking way to support peft models here as well? Thanks for the flag, that may be why I couldn't recreate https://github.com/huggingface/transformers/issues/34263#issuecomment-2427711753 @man-shar can you try giving peft another go? Should have fixed it Can confirm `self.model_accepts_loss_kwargs` is the correct value now for a peft model!

But seems like the `compute_loss` function inside trainer.py isn't getting the `num_items_in_batch` argument passed to it from `training_step`.

I notice that argument was removed in 4f3f86d2517e766a5773afb0e58c8bf95df10138 and the commit still says ""Experimental"". So I assume it will be reverted once you're done experimenting! It should work after that! Yes indeed :) Why `num_items_in_batch` is removed here? I think making it an argument `loss_args` rather than kwargs would be better? They are explicitly `kwargs`, not positional so kwargs is more accurate Leftover artifact while I was experimenting I wasn’t referring to the variable name. My suggestion is to use loss_kwargs instead of **loss_kwargs. The current design prevents forward functions from accepting additional keyword arguments, which could be inconvenient. We haven't accepted that at all before This is prone to evolve either way, with #33932 as an example!"
34539,2024-10-31T15:31:54Z,2024-11-05T16:02:15Z,yonigozlan,1,0,1,21,2,1,1,[],1610.0,0,433821.0,0,0,0,0,4421514.752531,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34539). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks!,Thanks!,"# What does this PR do?
Fix error with make fixup when torchvision is not present in the environment.
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

## Who can review?

Cc @ArthurZucker
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33034,2024-08-22T20:20:27Z,2024-10-28T17:54:38Z,AhmedAlmaghz,3,11,15,59,2,2,1,[],4173918.0,0,6468634.0,1,0,0,0,4417388.765201,,0,15,0,False,"['HuggingFaceDocBuilderDev', 'AhmedAlmaghz']","من هنا نبداء القسم الجديد
 @abodacs 




# DEVELOPER GUIDES

- [x]  Use fast tokenizers from 🤗 Tokenizers #33034
- [x]  Run inference with multilingual models #33048
- [x]  Use model-specific APIs #33030
- [x]  Share a custom model #33031
- [x]  Templates for chat models #33026
- [x]  Trainer #33080
- [x]  Run training on Amazon SageMaker #33071
- [x]  Export to ONNX #33072
- [x]  Export to TFLite #33077
- [ ]  Export to TorchScript #33079
- [ ]  Benchmarks #33023
- [ ]  Notebooks with examples #33049
- [ ]  Community resources #33027
- [ ]  Troubleshoot #33017
- [x]  Interoperability with GGUF files #33037
 Thank you @abodacs  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33034). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.سﻻم عليكم
@AhmedAlmaghz 

تمت المراجعة جزاء الله خيرا Thanks for the translation and thanks @abodacs for the review! 👏 ","سﻻم عليكم
@AhmedAlmaghz 

تمت المراجعة جزاء الله خيرا Thanks for the translation and thanks @abodacs for the review! 👏 ","
## What does this PR do?
Translated the `docs/source/ar/fast_tokenizers.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ","```suggestion
    title: استخدم مجزئيات النصوص السريعة من 🤗 Tokenizers 
``` ```suggestion
# استخدام مجزئيات النصوص من 🤗 Tokenizers
``` ```suggestion
يعتمد [`PreTrainedTokenizerFast`] على مكتبة [🤗 Tokenizers](https://huggingface.co/docs/tokenizers). يمكن تحميل المجزئات اللغويين الذين تم الحصول عليهم من مكتبة 🤗 Tokenizers ببساطة شديدة في 🤗 Transformers.
``` ```suggestion
قبل الدخول في التفاصيل، دعونا نبدأ أولاً بإنشاء مُجزىء لغوي تجريبي في بضع سطور:
``` ```suggestion
الآن لدينا مُجزىء لغوي مدرب على الملفات التي حددناها. يمكننا إما الاستمرار في استخدامه في وقت التشغيل هذا، أو حفظه في ملف JSON لإعادة استخدامه لاحقًا.
``` ```suggestion
## تحميل مُجزئ  النّصوص  مُباشرةً
``` ```suggestion
دعونا نرى كيف يمكننا الاستفادة من كائن (مُجزئ النصوص) في مكتبة 🤗 Transformers. تسمح فئة [`PreTrainedTokenizerFast`] سهولة إنشاء *tokenizer*، من خلال قبول كائن *المُجزئ النصوص*  مُهيّأ مُسبقًا كمعامل:
``` ```suggestion
يمكن الآن استخدام هذا الكائن مع جميع الطرق المُشتركة بين مُجزّئي النّصوص  لـ 🤗 Transformers! انتقل إلى [صفحة مُجزّئ  النّصوص](main_classes/tokenizer) لمزيد من المعلومات.
``` ```suggestion
لتحميل مُجزّئ النص من ملف JSON، دعونا نبدأ أولاً بحفظ مُجزّئ النّصوص:
``` ```suggestion
يمكن تمرير المسار الذي حفظنا به هذا الملف إلى طريقة تهيئة [`PreTrainedTokenizerFast`] باستخدام المُعامل  `tokenizer_file`:
``` ```suggestion
يمكن الآن استخدام هذا الكائن مع جميع الطرق التي تشترك فيها مُجزّئي  النّصوص لـ 🤗 Transformers! انتقل إلى [صفحة مُجزّئ النص](main_classes/tokenizer) لمزيد من المعلومات.
```"
33048,2024-08-22T20:21:43Z,2024-10-31T23:10:09Z,AhmedAlmaghz,7,22,31,1907,7,2,1,[],5617916.0,0,6467027.0,1,0,0,0,4418920.181209,,0,31,0,False,"['HuggingFaceDocBuilderDev', 'AhmedAlmaghz']","Thank you @abodacs  Done @stevhliu 
Please, merging The following pull requests have been added to the current pull request to be merged once
- [x]  Run inference with multilingual models #33048
- [x]  Use model-specific APIs #33030
- [x]  Share a custom model #33031
- [x]  Templates for chat models #33026
- [x]  Interoperability with GGUF files #33037
- [x]  Export to TFLite #33077

@abodacs  @stevhliu  Done 
@stevhliu  Thank you @stevhliu 
Done The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33048). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thank you:
@abodacs 
@stevhliu السﻻم عليكم
 @AhmedAlmaghz 
 تمت المراجعة جزاء الله خيرا Cool, thanks for the translation! Remember to add it to the toctree 🙂  The tflite and gguf docs are also missing from the toctree ;) A few more comments  Awesome, thanks!","السﻻم عليكم
 @AhmedAlmaghz 
 تمت المراجعة جزاء الله خيرا Cool, thanks for the translation! Remember to add it to the toctree 🙂  The tflite and gguf docs are also missing from the toctree ;) A few more comments  Awesome, thanks!","
## What does this PR do?
Translated the `docs/source/ar/multilingual.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ","```suggestion
# النماذج متعددة اللغات للاستدلال
``` ```suggestion
هناك العديد من النماذج متعددة اللغات في مكتبة 🤗 Transformers، وتختلف طريقة استخدامها للاستدلال عن النماذج أحادية اللغة. ولكن ليس كل استخدام النماذج متعددة اللغات مختلف. فبعض النماذج، مثل [google-bert/bert-base-multilingual-uncased](https://huggingface.co/google-bert/bert-base-multilingual-uncased)، يمكن استخدامها تمامًا مثل النموذج أحادي اللغة. سيوضح لك هذا الدليل كيفية استخدام النماذج متعددة اللغات التي تختلف طريقة استخدامها للاستدلال.
``` ```suggestion
يحتوي XLM على عشر نسخ مختلفة، واحدة منها فقط أحادية اللغة. ويمكن تقسيم نسخ النماذج التسع المتبقية إلى فئتين: نسخ التي تستخدم تضمينات اللغة (language embeddings)  وتلك التي لا تستخدمها.
``` ```suggestion
تستخدم النماذج التالية من XLM تضمينات اللغة لتحديد اللغة المستخدمة أثناء الاستدلال:
``` ```suggestion
تُمثل تضمينات اللغة على شكل مصفوفة بنفس شكل  `input_ids` التي يتم تمريره إلى النموذج. وتعتمد القيم في هذه المصفوفات على اللغة المستخدمة ويتم تحديدها بواسطة معاملى المجزىء `lang2id` و `id2lang`.
``` ```suggestion
في هذا المثال، قم بتحميل نسخة `FacebookAI/xlm-clm-enfr-1024` ( نمذجة اللغة السببية، الإنجليزية-الفرنسية):
``` ```suggestion
تُظهر خاصية `lang2id` في المجزىء اللغات وأرقام تعريفها في هذا النموذج:
``` ```suggestion
تُستخدم هذه النماذج لتمثيل الجمل العامة، على عكس نسح XLM السابقة.
``` ```suggestion
لا تتطلب هذه النماذج تضمينات اللغة أثناء الاستدلال. يجب أن تُحدّد اللغة من السياق وتستنتج وفقاً لذلك.
``` ```suggestion
في هذا المثال، قم بتحميل نسحة  `facebook/m2m100_418M` لترجمة النص من الصينية إلى الإنجليزية. يمكنك تعيين اللغة المصدر في المجزىء اللغوى:
``` ```suggestion
تقسيم النّص إلى رموز:
``` ```suggestion
يجبر M2M100 معرف اللغة الهدف كأول رمز مولد للترجمة إلى اللغة الهدف. قم بتعيين `forced_bos_token_id` إلى `en` في طريقة `generate` للترجمة إلى الإنجليزية:
``` ```suggestion
في هذا المثال، قم بتحميل نسخة `facebook/mbart-large-50-many-to-many-mmt` لترجمة النص من الفنلندية إلى الإنجليزية. يمكنك تعيين اللغة المصدر في المجزىء:
``` ```suggestion
تقسيم النّص إلى رموز:
``` ```suggestion
يجبر MBart معرف لغة الهدف كأول رمز مولد للترجمة إلى اللغة الهدف. قم بتعيين `forced_bos_token_id` إلى `en` في طريقة `generate` للترجمة إلى الإنجليزية:
``` ```suggestion
إذا كنت تستخدم نسخة `facebook/mbart-large-50-many-to-one-mmt`، فلا تحتاج إلى إجبار معرف لغة الهدف كأول رمز مولد، وإلا فإن الاستخدام هو نفسه.
``` ```suggestion
``` This sentence is untranslated Need to also close this section with `</hfoptions>`

```suggestion
<hfoption id=""timm backbone"">
``` Place the tip inside on a new line from the `<Tip>` tag itself This special syntax doesn't have to be translated Also doesn't need to be translated"
34202,2024-10-16T22:19:18Z,2024-11-05T15:11:03Z,MekkCyber,3,9,8,36,1,3,2,[],138432.0,0,1702305.0,0,0,0,0,4424589.207751,,1,8,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker', 'MekkCyber']","cc @SunMarc for review ! Thank you ! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34202). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. We can merge in the mean time 🤗 Thanks for figuring out the issue @MekkCyber ! Left a few comments LGTM ! Thanks for fixing this ! Just a nit. Also rebase the PR to fix the CI.  LGTM, quick q on perf! 
","Thanks for figuring out the issue @MekkCyber ! Left a few comments LGTM ! Thanks for fixing this ! Just a nit. Also rebase the PR to fix the CI.  LGTM, quick q on perf! 
","# What does this PR do?

When a model is quantized using TorchAO and then loaded, the representation of its Linear layers is expected to be different compared to the standard representation. This pull request (PR) modifies the representation of these Linear layers to match the format used in TorchAO's implementation : https://github.com/pytorch/ao/blob/main/torchao/quantization/quant_api.py 

Before : 
`
Linear(in_features=4096, out_features=4096, bias=False)
`
After : 
```
Linear(in_features=4096, out_features=4096, weight=AffineQuantizedTensor(shape=torch.Size([4096, 4096]), block_size=(1, 128), device=cuda:0, layout_type=TensorCoreTiledLayoutType(inner_k_tiles=8), layout_tensor_dtype=torch.int32, quant_min=0, quant_max=15))
```
## Who can review?
cc @SunMarc ","Is there an easier way to get the right representation ? What is done in `quantize_` that gives us the right representation ? cc @jerryzh168  Let's only change the representation we have a quantized weight. I'm a bit skeptical with this since we need to check for `AffineQuantizedTensor` and if we add new classes, we will have to update this.  Looked at the source code of _quantize, it's the same logic Just checked that this is how it is done in `_quantize` and since the function is private, it's better to copy paste and update if needed. Still, I prefer to remove the ""not quantized"" and ""not recognized"" cases.  small nit that I didn't check last time but can we move these imports inside the function where we are using them ? This way, transformers import will be faster Check if `_linear_extra_repr` returns something also before setting module.extra_repr you can also move this !  do we want to put this on lru cache? Or is it smart enough to be fast ?  I would think it's smart enough to be fast, but I will try to do a benchmark to test that"
34605,2024-11-04T21:04:57Z,2024-11-05T14:12:47Z,ydshieh,1,0,1,9,1,1,1,[],1667.0,0,61672.0,0,0,0,0,4428083.215053,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34605). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.We can probably ignore flax issues 🙄 but okay thanks,We can probably ignore flax issues 🙄 but okay thanks,"Reverts huggingface/transformers#34541

My bad, there are some torch / flax equivalent issues. Revert for now",
34558,2024-11-01T09:06:54Z,2024-11-05T15:10:42Z,ydshieh,2,17,11,919,21,4,3,[],2660.0,0,367430.0,0,0,0,0,4424608.576049,,0,11,0,False,"['HuggingFaceDocBuilderDev', 'gante']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34558). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Tagging @zucchini-nlp to double-check VLM test changes :)Yay fewer slow tests 🙌 

Added a few questions/suggestions to see if we can remove a few more overwritten cases 😈  Thanks, LGTM! Left a few questions and things we can clean up more thanks 🤗 ","Yay fewer slow tests 🙌 

Added a few questions/suggestions to see if we can remove a few more overwritten cases 😈  Thanks, LGTM! Left a few questions and things we can clean up more thanks 🤗 ","# What does this PR do?

And make it less flaky

- Use smaller (short) inputs
- Use larger epsilon in norm layers
","make it more general.

`vision_start_token_id` is required for `qwen2_vl` On skips like this, on Albert and other models: the test pulls the main input and the attention mask to manipulate them, finally sending them to the model. We could `pop` these items from `inputs_dict`, and then pass `**inputs_dict` to the model (e.g. `model_eager(**prepared_inputs, **inputs_dict)`) -- I think then we wouldn't need to skip tests due to missing inputs 🤗  (same comment as in Albert) Can we add a comment at the top explaining why we need to overwrite this test in mimi? 🤗 

If the test is overwritten only to skip the case `torch_dtype == ""float16"" and torch_device == ""cpu""`, can we delete the overwritten test and skip only that case? Can't we just delete the test? (it has `# Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_inference` and it inherits the mixin, so it should run the original test!) Same comment as in musicgen good catch! Wondering how lucky we were that models never failed for generation hehe no, it fails. I don't dive into why it's failing (input issues) though. Maybe let's merge and do this in a follow-up PR. It's never worked before. Interesting -- in that case, how does `# Copied from ` work? 👀  `# Copied from` is only applied to files under `src` I believe :-) but people sometimes use it in the `tests/` 😆  i think this test dont need skip anymore since we dont check if model has SDPA layers within this test anymore. But it can be skipped due to the same flakiness not very related to this PR but lets clean up also, maybe we dont need `encoder-seq-length` in llavas anymore because it was a hack to account for image tokens previously for my understanding, any reason why last token has to be a pad? cam you add a comment about why we do this?  It still has input preparation issues, where I added below to another model test class

> ""Idefics requires both text and image inputs which is currently not done in this test.""

As mentioned in a reply to Joao's comment, let's try to do it in a follow up PR to avoid index error 

(modeling code)
> vision_tokens = input_ids[vision_start_indices + 1]"
34516,2024-10-30T17:00:08Z,2024-11-05T14:01:25Z,thisisiron,1,0,2,35,1,2,2,[],2211.0,0,507678.0,0,0,0,0,4428768.141584,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34516). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.At first I was going to object, but you're completely right, those args aren't actually used anywhere in the example, good spot.

cc @ArthurZucker @LysandreJik for core maintainer review! Thanks! Cool to clean this up, thanks @thisisiron ","At first I was going to object, but you're completely right, those args aren't actually used anywhere in the example, good spot.

cc @ArthurZucker @LysandreJik for core maintainer review! Thanks! Cool to clean this up, thanks @thisisiron ","# What does this PR do?

This PR removes unused code related to the test dataset, improving code clarity.

Fixes # (issue)

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@amyeroberts, @qubvel
",
34031,2024-10-08T22:05:08Z,2024-10-17T15:39:05Z,chanind,0,0,1,2,1,1,1,[],,0,2381753.0,0,0,0,0,4437193.628484,,0,1,0,False,[],Thanks for deep-diving into this loophole! FYI @muellerzr 🤗 ,Thanks for deep-diving into this loophole! FYI @muellerzr 🤗 ,"# What does this PR do?
There's a bug on M1 macs with transformer >= 4.43.0 and torch >= 2.1.0, where if a model has tied embeddings, then the fast loading from #31771 causes a bus error when the model is actually run. This can be solved by disabling `_supports_param_buffer_assignment` for these models.

More info in comments in #33357


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34475,2024-10-29T00:03:00Z,2024-11-05T12:41:48Z,guangy10,1,2,3,44,1,2,2,"['run-slow', 'ExecuTorch']",115791.0,0,650328.0,0,0,0,0,4433546.702691,,0,3,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34475). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks, just a few nits!
Can you please also push empty commit`[run_slow] distilbert` to trigger slow test run, thanks! Thanks! LGTM!@ ","Thanks, just a few nits!
Can you please also push empty commit`[run_slow] distilbert` to trigger slow test run, thanks! Thanks! LGTM!@ ","# What does this PR do?

DistilBert is ExecuTorch compatible.

Unit Test:
`RUN_SLOW=1 pytest tests/models/distilbert/test_modeling_distilbert.py -k test_export -v`
```
tests/models/distilbert/test_modeling_distilbert.py::DistilBertModelIntergrationTest::test_export PASSED                                                                                [100%]
```

E2E test in ExecuTorch:
Patch https://github.com/pytorch/executorch/pull/6509 
`python -m extension.export_util.export_hf_model -hfm=""distilbert-base-uncased"" -lm masked_lm`
```
Saved exported program to ./distilbert.pte
```
`./cmake-out/backends/xnnpack/xnn_executor_runner --model_path distilbert.pte`
```
I 00:00:00.080326 executorch:executor_runner.cpp:82] Model file distilbert.pte is loaded.
I 00:00:00.080359 executorch:executor_runner.cpp:91] Using method forward
I 00:00:00.080361 executorch:executor_runner.cpp:138] Setting up planned buffer 0, size 12286720.
I 00:00:00.115094 executorch:executor_runner.cpp:161] Method loaded.
I 00:00:00.115124 executorch:executor_runner.cpp:171] Inputs prepared.
I 00:00:00.179285 executorch:executor_runner.cpp:180] Model executed successfully.
I 00:00:00.179301 executorch:executor_runner.cpp:184] 1 outputs:
Output 0: tensor(sizes=[1, 64, 30522], [
  -4.47825, -4.55548, -4.59359, -4.61276, -4.71701, -4.22803, -4.54525, -4.30736, -4.532, -4.9645,
  -4.19537, -4.51069, -4.34262, -4.96867, -4.38696, -5.06627, -5.01279, -4.89841, -4.42651, -4.47658,
  -4.70912, -4.49927, -4.48796, -4.67513, -4.3218, -4.54809, -4.59159, -4.65592, -4.54133, -4.50207,
  -4.24141, -4.65805, -4.49932, -4.36075, -4.38477, -4.69771, -4.76032, -5.06464, -4.57687, -4.54149,
  -4.54834, -4.80815, -4.47513, -4.61154, -4.69458, -4.09497, -4.42706, -4.48752, -4.84431, -4.40653,
  -4.6515, -4.60421, -4.39167, -4.9955, -4.65156, -4.57042, -4.58516, -4.46815, -4.43985, -4.83551,
  -4.20381, -4.59275, -4.94262, -4.32183, -4.44933, -4.59167, -4.66095, -4.85241, -4.83965, -4.37491,
  -4.82371, -4.34802, -4.26705, -4.79766, -4.47379, -4.7745, -4.59805, -4.6717, -4.2979, -4.65086,
  -4.88208, -4.84994, -4.24183, -4.73356, -4.97729, -5.18642, -4.64655, -4.64227, -4.46517, -4.6624,
  -4.50896, -4.75761, -4.26062, -4.75898, -4.7547, -4.54612, -4.43117, -4.4847, -4.28017, -4.33875,
  ...,
  -2.56383, -0.124811, -1.62058, -0.539149, -2.0116, -2.13068, 0.614868, -1.62362, -2.73875, -0.295115,
  -2.33206, 0.223186, -3.19978, -2.81419, -0.764227, 0.385865, -3.02447, -4.4802, -3.33432, -1.58703,
  -1.79603, -2.96534, -1.06687, -3.17183, -1.81405, 0.0236263, -0.992222, -3.71788, 0.761198, 0.089091,
  -2.99735, -2.04351, -2.40324, -2.86246, -1.24337, -2.34749, -2.01503, -2.45599, -4.6185, 1.14074,
  -3.04769, -1.78048, -1.09878, -3.30111, -2.08858, -1.64816, -2.03306, -1.94704, -0.205174, -1.90752,
  -2.6837, -1.25019, -0.415001, -3.73985, -1.53322, -0.605044, -3.7232, -0.258519, -1.85742, -1.55172,
  -4.25782, -3.31136, -1.23, -1.60789, -2.16738, -2.58743, 0.324617, 0.266767, -2.14392, -2.59203,
  -1.90562, -3.10258, -1.81314, 1.15056, -3.81185, -2.48559, -2.03798, -2.57377, -2.39025, -1.43463,
  -0.672718, -1.97253, -3.45209, -1.31699, -0.362099, -2.69917, -3.11479, -3.16947, -0.0704084, 0.330248,
  -3.50465, -3.19989, -4.00352, -3.97841, -2.49317, -4.99941, -4.31784, -3.77685, -4.15103, 3.47488,
])
```


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. #33835
- [x] Did you write any new necessary tests?


## Who can review?

@ArthurZucker
@qubvel
","We already have `is_torch_greater_or_equal_than_2_4` variable
```python
from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4
``` Variable names look a bit similar, let's name them more explicitly to avoid future errors. Is it intended to be `eager_predicted_mask` and `exported_predicted_mask`? "
34410,2024-10-25T10:44:10Z,2024-11-05T10:34:01Z,zucchini-nlp,2,14,14,1516,78,4,1,[],1366.0,0,949792.0,0,0,0,0,4441214.809465,,0,14,0,False,"['HuggingFaceDocBuilderDev', 'zucchini-nlp']","Modular is trying to overwrite InstructBlip and remove `logger`, even though we do `log.warning` in configuration code, so I have no idea why. Same thing is happening in my other PR for porting Emu3

Other failing tests have no relation to the current PR The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34410). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Very very nice! 🤗 Let's switch to a dict, always test and should be good to go Very very nice! 
Thanks for iterating! ","Very very nice! 🤗 Let's switch to a dict, always test and should be good to go Very very nice! 
Thanks for iterating! ","# What does this PR do?

This PR removed overwritten `from_pretrained` in many configs that are part of a bigger config. Instead it moved the loading logic to the base config class through `base_config_key`. Additionally it adds a few more flags to the config, in a similar fashion to processors where each sub-config's class name is recorded. Currently those are needed only for tests to load the sub-config directly and compare with `base_config's` value

Weird thing is that `config._is_composition` is not an indicator of whether the config is composite or not. Several configs were missing the flag and operating good, because the flag is used only to separate out configs that can be init without params and those that require params. Those that require are flagged as `is_composition`, see more in https://github.com/huggingface/transformers/pull/25237 description and in https://github.com/huggingface/transformers/pull/7943 which introduced the flag.

I personally see only a few models that require input params (other configs) which are indeed composite, and all others don't need the flag so I removed it. But still I think the naming is quite misleading so maybe we should rename that in long-term or even remove that and find a better way to save configs when default params are overwritten.

","I think the subconfigs should be a dict: `{ ""text_config"": AutoConfig, ""qformer_config"": Blip2QFormerConfig, ...}` 
this way the init loop is ""simpler"", you already have the class, you just use it (and use the CONFIG_MAPPING) for AutoConfig On top of that, it scales and makes more sense imo!  Very nice.
One think that is missing is saving: 
I had a lot of issues with Pixtral on this one, when saving composition it was overwriding some stuff and nothing was going as planed. We need from pretrained -> save_pretrained -> from_pretrained  we need to document these as well!
```suggestion
    base_config_key: str = """"
    sub_configs: Dict[str, PreTrainedConfig] = []
``` okay, as long as the attributes here are different than default we are testing good stuff do we have to add this? Can't we add this as part of the run_common? (I mean this should always be on by default without us having to write anything given the CONFIG_MAPPING and model types that are available to us! Hi! Super nice work :fire: I was wondering about this, for composite modular-based _new_ models, could we add something in the converter to enforce this structure? Could be but the issues is in other tests that fail in case of composite models due to various reasons. We could maybe skip those tests if the model has non-empty `sub_configs`, lemme see  Sounds good having it automated, I'll try to enable it through this PR if I can find a way. But I am wondering how would we automatically know which class does each `sub_config` map to if no info is given except for the code. Hmm, should we parse the code body then maybe with AST? we have typed args for the configs that inform of which config goes where, too yes, seems to be doable if you enforce config docstring to contain the sub-config class in type-hinting. Maybe we'll add it as the next PR so we can take time for making modular better. Also I've another PR on improving modular in general, might as well go there as a new feature cc @Cyrilvallez since you are working closely with modular. Do you think this is smth we can add in your PR or maybe subsequent PRs?  Hey, sorry for the late reply! This is definitely do-able - it would require parsing the `__init__` to check subconfig-specific assignments and add them as class attributes in the body.
However, it breaks a bit the ""no magic"" policy of the modular, as it means we automatically add stuff based on simple naming conventions. vey nice!"
34560,2024-11-01T11:12:04Z,2024-11-05T09:26:13Z,BenjaminBossan,6,2,1,10,2,5,4,[],879.0,0,339249.0,0,0,0,0,4445283.853366,,0,1,0,False,"['BenjaminBossan', 'HuggingFaceDocBuilderDev', 'Rocketknight1', 'MekkCyber']","@SunMarc Could you please review or suggest a reviewer?

The failing test is a timeout error and unrelated to this PR. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34560). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > I'm not actually a core maintainer, but as this is obviously a bug and the fix is clean and without side effects elsewhere, I'm happy to merge it!

Not sure what the policy is, I think it's a very straightforward change and might not require a core maintainer to approve. @BenjaminBossan, yes I'm happy for you to merge it once you can rerun/rebase/etc. and get the CI green! > yes I'm happy for you to merge it once you can rerun/rebase/etc. and get the CI green!

Not sure if I should merge into transformers :D Let's wait if one of the cores chimes in, it's not critical. Yes LGTM ! Thanks for the fix @BenjaminBossan I'm not actually a core maintainer, but as this is obviously a bug and the fix is clean and without side effects elsewhere, I'm happy to merge it! LGTM ! Can you have a second look @MekkCyber as you did the PR ?  Thanks all for reviews!","I'm not actually a core maintainer, but as this is obviously a bug and the fix is clean and without side effects elsewhere, I'm happy to merge it! LGTM ! Can you have a second look @MekkCyber as you did the PR ?  Thanks all for reviews!","# What does this PR do?

The `__repr__` method references a non-existent `self.kwargs`. This is now fixed.

There does not appear to be a uniform way of defining `__repr__` for quantization configs. I copied the method as implemented for HQQ:

https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/src/transformers/utils/quantization_config.py#L285-L287

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

-","Also, this is the tiniest nit ever, but do reprs normally end in `\n`? I agree that it looks unnecessary, but I copied this 1:1 from here for consistency:

https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/src/transformers/utils/quantization_config.py#L285-L287"
34395,2024-10-24T20:40:40Z,2024-11-05T09:06:07Z,eljandoubi,2,4,20,16,1,4,3,[],417392.0,0,999620.0,0,0,0,0,4441997.089653,,1,20,0,False,"['HuggingFaceDocBuilderDev', 'eljandoubi']",@SunMarc @muellerzr What do you think of this solution? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34395). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the PR ! Left a few comments  Thanks I think this solution looks quite nice. cc @ArthurZucker  Works for me ! Thanks for iterating and coming up with this solution !  Thank you!,Thanks for the PR ! Left a few comments  Thanks I think this solution looks quite nice. cc @ArthurZucker  Works for me ! Thanks for iterating and coming up with this solution !  Thank you!,"# What does this PR do?

Skip DeepSpeed ZeRO Stage 3 model initialization when it is intended to be quantized.

Fixes #34378

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber","We don't want to have this in the model config. Otherwise, we would have to do it for every model. Also, we shouldn't need to do that as we quantize the model from the top level. Maybe we can propagate the `quantization_config` in `from_pretrained` ? Not a huge fan of checking the `quantization_config` here as we don't really quantize the model with `from_config`. However, I'm not sure if there is an easier solution.Another solution would be to pass an arg in kwargs that we will pop.  `quantization_config` is already passed to the model class via `from_pretrained`, but the sub-models are instantiated using `from_config`, which does not include it. Perhaps we can propagate the quantization information using a context manager. If we pass an argument to flag quantization, it would require changes in every composed model."
34541,2024-10-31T17:02:51Z,2024-11-04T20:35:37Z,ydshieh,3,3,1,9,1,2,1,[],3379.0,0,358368.0,0,0,0,0,4491519.795206,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34541). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Thanks, this fixes quite some tests no? Can we make sure with a tiny pipeline that by default we generate more than 20 tokens?

Do you mean a new test case that will `generate more than 20 tokens`? Any motivation for such a new test? merge it as it will unblock whisper CI (170 -> 20 failures) + (quite) some pipeline testsThanks, this fixes quite some tests no? Can we make sure with a tiny pipeline that by default we generate more than 20 tokens? ","Thanks, this fixes quite some tests no? Can we make sure with a tiny pipeline that by default we generate more than 20 tokens? ","# What does this PR do?

","if `generation_config.max_length` is not the same as the default value (`20`), let's not change it!!!! should we not change `has_default_max_length` directly? 🤗  I would rather avoid changing its original definition in this PR as it is also used in several places."
34590,2024-11-04T03:41:46Z,2024-11-04T17:41:44Z,maximizemaxwell,2,0,3,67,2,1,1,[],32587.0,0,55014.0,0,0,0,0,4497338.965787,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'jungnerd']","LGTM! Thanks for the nice translation 👍🏻 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34590). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Very nice, thanks for your contribution!","Very nice, thanks for your contribution!","# What does this PR do?
Translated the `perf_train_special.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
-  [x] Check for missing / redundant translations (번역 누락/중복 검사)
 - [x] Grammar Check (맞춤법 검사)
 - [x] Review or Add new terms to glossary (용어 확인 및 추가)
 - [x] Check Inline TOC (e.g. [[lowercased-header]])
 - [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)
## Who can review? (Initial)
@junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang

## Before submitting
 - [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
 - [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
[documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
[here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
 - [ ] Did you write any new necessary tests?
## Who can review? (Final)
@stevhliu May you please review this PR?",
34593,2024-11-04T07:24:42Z,2024-11-04T17:42:20Z,techkang,2,0,1,3,1,1,1,[],20324.0,0,37061.0,0,0,0,0,4501916.162129,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']",cc @stevhliu! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34593). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for clarifying!,Thanks for clarifying!,"
# What does this PR do?

Fix https://github.com/huggingface/transformers/issues/34567.
""`include_num_input_tokens_seen` argument is set to `False` by default, similar to `load_best_model_at_end`. However, the documentation for `best_metric` and `best_model_checkpoint` strongly implies that these two arguments will have no effect when the corresponding flag is not set. This PR makes the documentation consistent.""

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33098,2024-08-23T15:37:08Z,2024-11-04T17:42:07Z,J4BEZ,5,2,14,155,1,2,1,[],681494.0,0,6314699.0,0,0,0,0,4501934.05889,,0,14,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1', 'J4BEZ']","@heuristicwave 섬세한 피드백 너무나 감사드립니다🙇‍♂️!!
리뷰하여 주신대로 코드에 나타난 주석을 번역해주신 제안으로 수정하였으며
한국어 README.md에만 집중될 수 있도록 영문 README.md 파일의 링크는 원본으로 되돌렸습니다😆😆

이번 한 주 고생 많으셨습니다🙇‍♂️
토요일 밤, 하루 잘 마무리 하시고 복되고 평안한 주말 되시길 바랍니다🤗 cc @stevhliu for vis The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33098). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @stevhliu Thank you for your kind reply😄
I'll change this PR from `Draft` to `Ready for review` soon, after adjusting some improvements!

Thank you for your hard effort and hope you have a great day😄 @stevhliu Thank you for waiting😆
I finished translation and synchronization🙌🙌

Hope you have a peaceful day and stay healthy!안녕하세요 @J4BEZ 님, 소중한 변역 감사합니다!

이전에 코드의 주석도 번역하라는 가이드가 있어 변경해보았습니다!

edit_README_ko 에서 README.md 파일도 수정한 것은 Update README_ko.md 과는 분리하는게 좋아보입니다!
@junejae @jungnerd @harheem @nuatmochoi 의견 부탁드려요~ LGTM, thanks for updating! Feel free to mark as ready for review when you're done 🤗 ","안녕하세요 @J4BEZ 님, 소중한 변역 감사합니다!

이전에 코드의 주석도 번역하라는 가이드가 있어 변경해보았습니다!

edit_README_ko 에서 README.md 파일도 수정한 것은 Update README_ko.md 과는 분리하는게 좋아보입니다!
@junejae @jungnerd @harheem @nuatmochoi 의견 부탁드려요~ LGTM, thanks for updating! Feel free to mark as ready for review when you're done 🤗 ","Delete the blank paragraph in the language selection button and Edit to synchronize with the English version of README.md

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Delete an empty paragrah under the language selection by removing white space.
And synchronize `README_ko.md` to [README.md](https://github.com/huggingface/transformers/blob/main/README.md).

- [x] Additional edit for keep consistency with main [documentation](https://huggingface.co/docs/transformers/v4.44.2/ko/index). 
(메인 문서와 일관성 유지를 위한 수정)
- [x] Grammar Check (맞춤법 검사)

* Maintain the content of the existing document as much as possible.
* Juxtapose in English of some tasks.

There could be mistranslations or awkwardness.
If you found any of them, I would appreciate it if you could correct the error to make the community shine🙇‍♂️

Thanks for your hard effort that everyone can enjoy thousands of models and learn DL more easily!

I'm sorry to find this PR late #20179
(굉장히 좋은 가이드가 있었군요! 늦게 확인해서 죄송합니다😅)

## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
# 귀여운 고양이가 있는 이미지를 다운로드하세요
``` ```suggestion
# 객체 감지를 위한 파이프라인을 할당하세요
```"
34572,2024-11-02T03:10:58Z,2024-11-04T17:40:30Z,karthik-script,1,1,3,59,2,2,2,[],223149.0,0,224972.0,0,0,0,0,4502031.210012,,0,3,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34572). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for your contribution, the additions LGTM. 

I have made a minor suggestion to revert the log output translation back since it only supports English. Nice, thanks for the translation!","Thanks for your contribution, the additions LGTM. 

I have made a minor suggestion to revert the log output translation back since it only supports English. Nice, thanks for the translation!","# What does this PR do?

Translated accelerate page to Hindi and updated the `_toctree.yml` file

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue) https://github.com/huggingface/transformers/issues/34441


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->

cc:  @stevhliu @MKhalusova @kbdharun","```suggestion
Validating TFLite model...
	-[✓] TFLite model output names match reference model (logits)
	- Validating TFLite Model output ""logits"":
		-[✓] (1, 128, 30522) matches (1, 128, 30522)
		-[x] values not close enough, max diff: 5.817413330078125e-05 (atol: 1e-05)
The TensorFlow Lite export succeeded with the warning: The maximum absolute difference between the output of the reference model and the TFLite exported model is not within the set tolerance 1e-05:
- logits: max diff = 5.817413330078125e-05.
 The exported model was saved at: bert_tflite
```"
34538,2024-10-31T15:23:15Z,2024-11-04T16:18:50Z,gante,3,2,5,12,2,2,2,[],2179.0,0,352380.0,0,0,0,0,4503486.482871,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'gante', 'Swastik-Swarup-Dash']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34538). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Do we need a patch for this? 🤗

I don't think so, it's quite niche 🤔 If we make a patch, yes, let's include it :D  Thanks @gante for using my approach.Do we need a patch for this? 🤗  Thanks 🤗 ",Do we need a patch for this? 🤗  Thanks 🤗 ,"# What does this PR do?

Fixes #33786 

Applies the fix in `isin_mps_friendly` as suggested by @Swastik-Swarup-Dash [here](https://github.com/huggingface/transformers/issues/33786#issuecomment-2382363240)","we ought to open the issue on pytorch / link the tracker ! On torch >=2.4 this is fixed, i.e. `torch.isin` is supported with `mps`

(in other words, this fix only affects older versions of torch) 🤗 "
34418,2024-10-25T18:51:05Z,2024-11-04T16:36:27Z,JacobLinCool,2,4,8,22,2,3,3,[],841463.0,0,855922.0,0,0,0,0,4505876.349832,,0,8,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev']",Feel free to merge when good @SunMarc  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34418). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks a lot ! LGMT  LGTM! Awesome ! Thanks for integrating my suggestion. ,Thanks a lot ! LGMT  LGTM! Awesome ! Thanks for integrating my suggestion. ,"# What does this PR do?

Add text support to `TensorBoardCallback` so users can see the decoded text generated in `compute_metrics` in TensorBoard. Sometimes the string is very long, so a `max_str_len` (defaults to 100) property has also been added to `ProgressCallback` to prevent polluting the progress log.

Can be used to checkout transcription when fine-tuning ASR models:
<img width=""2180"" alt=""截圖 2024-10-26 凌晨2 49 07"" src=""https://github.com/user-attachments/assets/c6f1c114-63e3-44dc-b6af-82cf23928f62"">

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

- trainer: @muellerzr and @SunMarc

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Maybe you can even tell them to modify max_str_len in the callback if they want to increase the length I've added some docstrings and included a hint in the message. Hi @SunMarc, just checking in to see if there's any update on this PR. Please let me know if any additional changes are needed. Thanks! Nice, thanks for your work ! "
34461,2024-10-28T07:28:38Z,2024-11-04T15:37:51Z,zucchini-nlp,2,10,24,581,35,2,2,[],1608.0,0,634154.0,0,0,0,0,4509391.671705,,0,24,0,False,"['HuggingFaceDocBuilderDev', 'zucchini-nlp']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34461). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Should be ready for review @ArthurZucker ! 

I think we'll support simple non-multimodal tokenizers for quite a while in VLMs, no idea yet how/when to make this a new defaultOkay super super good! The only thing I don't like is the `is_multimodal`! 
I think what you added gives a lot of freedom to all tokenizers -> audio_cls_token or anything that ends with token / ends with id will be properly processed! 

Let's remove the is_mulitmodal and should be good!  Perfect Very nice!","Okay super super good! The only thing I don't like is the `is_multimodal`! 
I think what you added gives a lot of freedom to all tokenizers -> audio_cls_token or anything that ends with token / ends with id will be properly processed! 

Let's remove the is_mulitmodal and should be good!  Perfect Very nice!","# What does this PR do?

Part of Major VLM standardization (https://github.com/huggingface/transformers/issues/33948). We will have special tokens that are present in all VLMs to be part if `XXXTokenizer` attributes. This will make our lives easier when doing several processing manipulations and/or formatting the prompt manually, as we can simply call `self.tokenizer.image_token`. 

Currently if we need any of VLM special tokens, those are saved in processor config, but not all models save it since not all models use it when calling the processor. After this PR I'll go over models and clean up the processing code given the changes. But we might still have to support old way, because we can't change stuff if that can break loading configs from the hub","```suggestion
tokenizer = AutoTokenizer.from_pretrained(model_id, extra_special_tokens = [""image_token"", ""boi_token"", ""eoi_token""])
``` ```suggestion
vision_tokenizer.image_token = ""IMAGE""
vision_tokenizer.image_token_id
```
``` let's add a small test for this yes, this is actually not correct anymore hehe, forgot to update the docs. And it has a test for that already so we are good

new way of adding extra special tokens is like
`tokenizer.extra_special_tokens = {""eoi_token"": ""<s>"", ""image_token"": ""<image>""}`. After adding this line and saving the tokenizer, loading back will do the magic and tokenizer will have `self.image_token` attribute we should be able to pass it as input as well instead of forcing people to use the setter! 🤗  yeap, realized later and added that in the docs instead of ""saving-loading back"". Plus extended the test when we do this, we don't add them to the tokenizer vocab  right?  I think you are already checking that these tokens are added to the vocab if not already present right?  if the special token is not present in the vocab, we do add them as new tokens to the tokenizer vocab. Should we prevent users from adding new tokens and allow to use only available tokens?

It happens because the Tokenizer initially is wired to do that, irrespective of current changes

```
# 4. If some of the special tokens are not part of the vocab, we add them, at the end.
# the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`
``` NO it's alright IMO we have not really seen reports about that "
28621,2024-01-21T09:20:13Z,2024-02-07T12:42:01Z,danielkorat,9,1,7,15,3,2,1,[],254619.0,0,24808127.0,0,0,0,0,4607123.739174,,0,7,0,False,"['HuggingFaceDocBuilderDev', 'danielkorat', 'gante', 'ArthurZucker']","Hi @gante 
Can you please have a look?
Not sure why the tests are failing. @danielkorat the test is failing because it is poorly parameterized -- it raises the newly added exception. I'd like to ask you to try to fix the test parameterization (ping me if you get stuck :) ) > @danielkorat the test is failing because it is poorly parameterized -- it raises the newly added exception. I'd like to ask you to try to fix the test parameterization (ping me if you get stuck :) )

Hi @gante @amyeroberts, 
I made the requested changes and fixed the test above as well. 
This test is now failing and it seems to be unrelated to my changes:
```bash
_ FlaxWav2Vec2ModelTest.test_equivalence_pt_to_flax [FlaxWav2Vec2ForPreTraining] _
....
E   AssertionError: 1.1444092e-05 not less than or equal to 1e-05 : outputs.codevector_perplexity: Difference between PyTorch and Flax is 1.1444091796875e-05 (>= 1e-05).
``` It was probably a flaky test 😉  Thanks @ArthurZucker, I changed the title The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_28621). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker can you merge please? Thanks for the PR 😉  @danielkorat thank you for the fix 💛 In addition to the changes in the PR, I would also check `max_new_tokens` in `GenerationConfig.validate()` -- it must be > 0 when it is set, otherwise an exception should be raised LGTM let's make sur the title of the pr has ⚠️ as it's breaking ","In addition to the changes in the PR, I would also check `max_new_tokens` in `GenerationConfig.validate()` -- it must be > 0 when it is set, otherwise an exception should be raised LGTM let's make sur the title of the pr has ⚠️ as it's breaking ","# What does this PR do?

Currently, setting `max_new_tokens=0` produces 1 token instead of 0, and only a warning is produced.
To prevent unexpected patterns of generation, this warning should be changed to an `Exception`.

### Example:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
checkpoint = ""bigcode/tiny_starcoder_py""
tokenizer = AutoTokenizer.from_pretrained(""bigcode/tiny_starcoder_py"")
model = AutoModelForCausalLM.from_pretrained(""bigcode/tiny_starcoder_py"")
inputs = tokenizer(""def print_hello_world():"", return_tensors=""pt"")
outputs = model.generate(**inputs,
                        pad_token_id=tokenizer.eos_token_id,
                        max_new_tokens=0)
print(f""Input length: {len(inputs['input_ids'][0])}"")
print(f""Output length: {len(outputs[0])}"")
```

### Output before fix:
```bash
/home/sdp/fix-zero-max-new-tokens/transformers/src/transformers/generation/utils.py:1136: UserWarning: Input length of input_ids is 7, but `max_length` is set to 7. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
Input length: 7
Output length: 8
```

### Output after fix:
```bash
Traceback (most recent call last):
  File ""/home/sdp/fix-zero-max-new-tokens/test.py"", line 8, in <module>
    outputs = model.generate(**inputs,
  File ""/storage/sdp/anaconda3/envs/fix-zero-max-new-tokens/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/sdp/fix-zero-max-new-tokens/transformers/src/transformers/generation/utils.py"", line 1396, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File ""/home/sdp/fix-zero-max-new-tokens/transformers/src/transformers/generation/utils.py"", line 1136, in _validate_generated_length
    raise ValueError(
ValueError: Input length of input_ids is 7, but `max_length` is set to 7. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
```

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@gante 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
                "" increasing `max_length` or, better yet, setting `max_new_tokens`.""
```"
34540,2024-10-31T15:56:58Z,2024-11-01T08:06:17Z,molbap,1,0,8,48,2,1,1,[],2419.0,0,58161.0,0,0,0,0,4795686.784241,,0,8,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34540). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks, can you add some 🔴 to the PR name! as this is breaking!","Thanks, can you add some 🔴 to the PR name! as this is breaking!","# What does this PR do?

as per title. Previous default config was a nonworking default config, new defaults are aligned with gemma2-2b-it.

@ArthurZucker ",
34443,2024-10-27T04:06:44Z,2024-11-01T15:26:46Z,karthik-script,2,4,5,140,2,2,2,[],134.0,0,472802.0,0,0,0,0,4769259.819406,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'karthik-script']","Upon the successful implementation of this PR, I intend to increase my translation efforts. Thank you for your support. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34443). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM, thanks for your translation!

You'll also need to find someone who can review the translated content since I can't read Hindi! https://github.com/huggingface/transformers/issues/26787 is a good place to see if other community members are interested :) Thanks for your contribution, chiming in from https://github.com/tldr-pages/tldr/pull/14452. As a translator myself, I have a few technical suggestions for this page.

---

>You'll also need to find someone who can review the translated content since I can't read Hindi! https://github.com/huggingface/transformers/issues/26787 is a good place to see if other community members are interested :)

Hi, I have gone through the page. Overall it looks fine, I have made a few technical suggestions after this the PR should be GTG.

> [!NOTE]
> While I can read and understand Hindi, I am not a native speaker so feel free to get a second review if required. LGTM Great job and thanks again!","LGTM, thanks for your translation!

You'll also need to find someone who can review the translated content since I can't read Hindi! https://github.com/huggingface/transformers/issues/26787 is a good place to see if other community members are interested :) Thanks for your contribution, chiming in from https://github.com/tldr-pages/tldr/pull/14452. As a translator myself, I have a few technical suggestions for this page.

---

>You'll also need to find someone who can review the translated content since I can't read Hindi! https://github.com/huggingface/transformers/issues/26787 is a good place to see if other community members are interested :)

Hi, I have gone through the page. Overall it looks fine, I have made a few technical suggestions after this the PR should be GTG.

> [!NOTE]
> While I can read and understand Hindi, I am not a native speaker so feel free to get a second review if required. LGTM Great job and thanks again!","# What does this PR do?

Translated accelerate page to Hindi and updated the `_toctree.yml` file

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue) https://github.com/huggingface/transformers/issues/34441


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->

cc:  @stevhliu  and @MKhalusova ","**Tip**: Around translator circles, we generally suggest not to translate copyright headers and licenses from English (unless it's required and legal professionals are vetting the writing), since any technical words we use as a substitute may take a different meaning, etc.

---

I went through the translation files in this directory and can confirm the notice was left in English without translating (maybe for the above reason) or because it's a comment not displayed to the end user.

```suggestion
<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->
``` When translating technical content, it isn't suggested to transliterate brand/project/package names unless it's an approved/official string.

```suggestion
# वितरित प्रशिक्षण के साथ 🤗 Accelerate

जैसे-जैसे मॉडल बड़े होते हैं, समानांतरता सीमित हार्डवेयर पर बड़े मॉडल को प्रशिक्षित करने और प्रशिक्षण की गति को कई आदेशों के आकार में तेज करने के लिए एक रणनीति के रूप में उभरी है। हगिंग फेस में, हमने उपयोगकर्ताओं को किसी भी प्रकार के वितरित सेटअप पर 🤗 ट्रांसफार्मर्स मॉडल को आसानी से प्रशिक्षित करने में मदद करने के लिए [🤗 Accelerate](https://huggingface.co/docs/accelerate) पुस्तकालय बनाया है, चाहे वह एक मशीन पर कई GPU हों या कई मशीनों में कई GPU। इस ट्यूटोरियल में, जानें कि अपने मूल PyTorch प्रशिक्षण लूप को कैसे अनुकूलित किया जाए ताकि वितरित वातावरण में प्रशिक्षण सक्षम हो सके।

## सेटअप

🤗 Accelerate स्थापित करके शुरू करें:
``` ```suggestion
अंतिम जोड़ यह है कि आपके प्रशिक्षण लूप में सामान्य `loss.backward()` को 🤗 Accelerate के [`~accelerate.Accelerator.backward`] विधि से बदलें:
``` ````suggestion
🤗 Accelerate एक नोटबुक में भी चल सकता है यदि आप Colaboratory के TPU का उपयोग करने की योजना बना रहे हैं। प्रशिक्षण के लिए जिम्मेदार सभी कोड को एक फ़ंक्शन में लपेटें, और इसे [`~accelerate.notebook_launcher`] में पास करें:

```py
>>> from accelerate import notebook_launcher

>>> notebook_launcher(training_function)
```

🤗 Accelerate और इसकी समृद्ध सुविधाओं के बारे में अधिक जानकारी के लिए, [दस्तावेज़ीकरण](https://huggingface.co/docs/accelerate) देखें।
````"
34487,2024-10-29T10:59:52Z,2024-11-01T09:13:51Z,Cyrilvallez,8,30,40,4268,19,2,1,[],1597.0,0,253002.0,0,0,0,0,4791472.514374,,0,40,0,False,"['HuggingFaceDocBuilderDev', 'Cyrilvallez', 'yonigozlan', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34487). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hey, so nice to see modular getting better and better! Just a heads up that I have this relatively short PR out on modular as well https://github.com/huggingface/transformers/pull/34477 , which adds some functionalities needed for the ColPali PR, so I was wondering if it will be compatible with this refactoring!
Also made a comment there on adding the modular examples to the check_modular_conversion script to traced when some issues are introduced, I think it could be useful to have here also :) Hey @yonigozlan thanks for the heads-up! I added the new `TYPE_TO_FILE_TYPE` and slightly tweaked how I was handling `Annotations`, your `modular_new_kwargs_model.py` now behaves correctly also in this PR.
If you have any issue let me know. @Cyrilvallez Nice! Thank you! Hey again @Cyrilvallez @ArthurZucker! 
It seems like with this new modular converter, several examples that used to work in examples/modular have now some issues. Some of these examples have functionalities that we would need for the ColPali PR, so it would be great if we could try to keep supporting them.
Also should we add the modular examples to the default call of `check_modular_conversion`? As otherwise, the modular examples aren't automatically updated when changes are made to modular_model_converter, which makes it hard to trace when issues are introduced Hey @yonigozlan! Thanks for the feedback! I added **assignment dependency tracking** to correctly follow everything (I previously thought it would never be needed, but it turns out it is in some niche usecases! And good to have it in general anyway).  
Everything is now behaving nicely, both in the actual models definitions and examples (the examples were broken already btw). Could you double-check just in case? This highlights that we really need those **tests** soon!  
I'll let @ArthurZucker decide if we want to add the examples in the `check_conversion`, but I added them to the `--files_to_parse all` anyway. - Added support for a new important use-case (see https://github.com/huggingface/transformers/pull/33770): **correct dispatch of fully new class**. Consider the following `modular.py`:
```
    from ..llama.modeling_llama import LlamaModel

    class NewNameTextConfig(PretrainedConfig):
        ...

    class NewNameConfig(PretrainedConfig):
        ...

    class NewNameModel(LlamaModel):
        config = NewNameConfig()
        text_config = NewNameTextConfig()
        ...
```
we previously had no way of correctly dispatching `NewNameTextConfig` to `configuration_newname.py`, without importing it in `modeling_newname.py` as well as part of the dependencies (because `modeling_llama.py` only tell us that `NewNameConfig` will be imported, but nothing about `TextConfig`).
This is now solved, every fully new class (without exact name match e.g. `LlamaConfig` <-> `NewNameConfig`) which does not belong to the correct file type will not be added, and an import will be created instead. 🥳 Okay! Very nice idea to have a general ModelMapper, and have one for Modular files and one for ""other"" files. 

Let's keep splitting the functionalities, I think creating the module should go outside the visiter, don't necessarily need a class ! 

Looks good in general! Great work 🔥  Very very nice! 
The only things missing now: 
- unittests /  small examples of the capabilities
- update the documentation a little bit to further explain how we do this but mostly 
To be honest we can merge this will unblock me as well 🤗 ","Okay! Very nice idea to have a general ModelMapper, and have one for Modular files and one for ""other"" files. 

Let's keep splitting the functionalities, I think creating the module should go outside the visiter, don't necessarily need a class ! 

Looks good in general! Great work 🔥  Very very nice! 
The only things missing now: 
- unittests /  small examples of the capabilities
- update the documentation a little bit to further explain how we do this but mostly 
To be honest we can merge this will unblock me as well 🤗 ","# What does this PR do?

This PR largely rework the logic we use in the modular converter. It is (hopefully) clearer and maintainable. Instead of going in all directions, adding stuff, then deleting it if not needed, we now do the following:

- visit all the modular file (record imports/functions/classes/assignments nodes)
  - create function dependency mapping
- for each import coming from another model:
  - visit the corresponding file
  - create function dependency mapping
  - update mapping with function/assignment from the modular (updated/new functions)
  - create the class dependency graph based on merged dependencies
- update dependency graph of the modular with the functions and assignments imported from the other files
- for each class recorded in the modular:
  - if inherithing from class in another file:
    - replace call to super
    - find the dependencies after the node was replaced
    - follow (updated with modular defs) dependency mapping to add all nodes
  - else:
    - only add needed imported functions (and their dependencies)
- determine the needed imports and add them

Note that we now only visit each files once, instead of potentially revisiting them multiple times due to renaming or deleting nodes at the end.
cc @ArthurZucker for the logic design

Still yet to come if the design looks good:
- Unit-tests!!!
","let's just have two steps (easier to debug)  in the spirit of being a bit general, this is kind of a processor, we can have a separate function!  this output is neither a set of all dependencies, nor a list containing parents as well. 
(cf the return docstring) ```suggestion
    """"""A visitor which is designed to analyze a single class node to get all its dependencies that are shared with the set of `global_names`. 
    This class is used through the 2 convenient class methods.
```
We might also want to rename this  If these are only used for the Modular File Mapper, let's maybe move them around as I don't really see the need to have them here! 
 what are common visiting patterns? ```suggestion
        Global Assigns like `GEMMA_INPUT_DOCSTRING = 'THIS IS THE INPUT'` and all import statements
``` does top level mean module scoped functions? (top level harder to understand IMO) I am not sure I understand what 1st level is, let's use scope names instead ```suggestion
            # We need to check if they are present in self.functions to avoid built-in functions
            dependencies = find_all_dependencies(self.function_call_dependency_mapping, start_entity=function_name)
            all_dependencies = {dep for dep in dependencies if dep in self.functions.keys()}
``` Are these dependencies nodes, (as in cst.Nodes) or str (as strings mapping to the node) etc?  (we can have typing to help)  ```suggestion
        """"""For each visited class, find its dependencies based on visiting the current file + potential merged dependencies.
``` I only see a `compute_class_dependencies` and not a `compute_function_dependencies`, we can say something like ""computes class dependencies via `compute_...` and the function dependencies are stored in `function_call_dependency_mapping` wondering if `cst.Call` is the only way to ""use"" a dependency? 
You can have attributes as well no? (like 
```def __init__():
    self.name = CONFIG_MAPPING[xxxx]
```
I don't think it's a call ! ```suggestion
    """"""A mapper designed to parse modeling files (like `modeling_llama.py`). When encountering an import to such a file
``` ```suggestion
    in the `modular_xxx.py` file, we need to correctly visit it and merge the dependencies of the modular and current file.
    For example, if you define `GemmaDecoderLayer`, which inherits from `LlamaDecoderLayer`, but you add `GemmaFlashAttention` as an attribute, we want to make sure `GemmaDecoderLayer` has the dependencies of `LlamaDecoderLayer` and `GemmaFlashAttention`.
    For this reason, this class should only be instantiated from the class method `visit_and_merge_dependencies`, which takes
    care of correctly merging dependencies, then finalizes all dependency graph computations.""""""
```
 i don't really understand this sentence! ```suggestion
    Replace a class node which inherits from another class. This function works in the following way:
``` ```suggestion
    - start from the base class node of the inherited class (a cst.Node)
``` ```suggestion
    - replace all methods of the base node with the methods defined in the child class.
``` ```suggestion
    - append all new methods defined in the child class
``` ```suggestion
    """"""Based on a class name, find the file type corresponding to the class.
    If the class name is `LlamaConfig` it will return `configuration`. 
    The list of suffixes is in `TYPE_TO_FILE_TYPE`. If there are no match, we match by default to `modeling`
    """"""
``` ```suggestion
# These top-level variables will always appear at the very beginning of the file, in the order they are defined in
``` I don't know if recursion is easier to understand / debug but okay !  I think ruff does this for us, but otherwise okay What is body? (str, dict of str?)
Would be nice to mention when this is used! ```suggestion
    Note: we need to use `isinstance` on scope assignements, m.matches apparently does not work here yet!
``` only starting python 3.9 I think, we can import OrderedDict otherwise ```suggestion
        if m.matches(node, m.If()): # handle safe imports
```"
34174,2024-10-15T11:51:02Z,2024-11-01T07:54:48Z,zucchini-nlp,5,5,16,787,8,2,1,[],3088.0,0,1454627.0,0,0,0,0,4796379.25077,,1,16,0,False,"['HuggingFaceDocBuilderDev', 'gante', 'zucchini-nlp']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34174). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. >  changed the generation tests to use modelTest.input_name as BLIP is the only model that uses pixel values as main input and thus checking generated text length's will always fail.

I'd like very much to avoid this change -- extra logic for all tests to handle a niche corner case. Let's brainstorm alternatives! `main_input` in the generative tests is used to check the shapes. Perhaps we always want to look for `input_ids` or `inputs_embeds` in the input dictionary? 🤔  @gante I tried to force `input_ids` always and I found another corner case with ~~Whisper which expects `input_features` to be the main input for shape checking~~. I would very much love to make BLIP standard and maybe I'll make so in v5, because it will break a whole lot of things.

OMG, I found an option while writing this reply, whisper and the other audio model are encoder-decoder so we can make it work by getting main input in decoder-only models. Just before the check happens, in the same indent block  :)
If you agree, I'll make the change hehe
 > OMG, I found an option while writing this reply, whisper and the other audio model are encoder-decoder so we can make it work by getting main input in decoder-only models. Just before the check happens, in the same indent block :)
If you agree, I'll make the change hehe

if it works, sounds good! (make sure to leave a comment)  @gante requesting re-review, since the `input-name` was merged as a separate PR. I rebased `main` and ran tests againAH actually we might need / want to force `return_dict` to TRUE, to avoid all the if elses Thanks for cleaning up","AH actually we might need / want to force `return_dict` to TRUE, to avoid all the if elses Thanks for cleaning up","# What does this PR do?


Enables generation tests for BLIP models, except BLIP-1 (turned out to be a bit harder). I changed the generation tests to use `modelTest.input_name` as BLIP is the only model that uses pixel values as main input and thus checking generated text length's will always fail. 

I tried to get rid of custom generate for these models, but that opened a Pandora box so I think better not waste time on an old model and maintain it for a while, until the model gets deprecated. But still I did some changes so we don't need to add extra `bos` at the beginning and now the decoder-based BLIP models return full text at output. Encoder-decoder based models return only generated text, which is consistent with what an LLM should do","outputs will have loss and logits twice there  (this was probably already a bug)  when we overwrite we don't need that no? yeah, will remove unnecessary part yep, in general i don't like that we return it as this and would better return unwrapped lm outputs. But we can't prob just delete it for BC reasons"
34152,2024-10-14T09:27:52Z,2024-11-01T07:39:39Z,zucchini-nlp,3,9,13,320,8,2,3,[],12722.0,0,1548708.0,0,0,0,0,4797288.277777,,0,13,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34152). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Also :  we might need / want to force return_dict to TRUE, to avoid all the if else would make it simpler!🤗 thanks  🤗  Let's go! ",🤗 thanks  🤗  Let's go! ,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34109 and adds `get_input_embeddings` method to the retrieval model. Also fixes the same methods in BLIP model where we should be working with text embeddings. Returning vision embeddings will not be able to resize the vocab size

Added tests as those were all skipped and thus we never knew there was an issue","I think if there is a text_config, we could automatically deduce this from the `key` which would be here `text_model` which to call? (thinking about general api-wise!)  hmm, i see that in PreTrainedModel we try to get the method from `base_model` and prob we can fallback to that by indicating the `base_model_prefix`

I am not very sure yet how the prefix is used when loading the model, so lemme quick check that state dict is still correctly loaded  Update: yes the idea works and loading happens same way as without the `base_model_prefix`. But some of the tests will fail because of the composite nature of `BlipConfig` (`test_correct_missing_keys`)

I will take this noted, and will add it to my TODO list. But I believe it would force us to refactor `from_pretrained` to work well with composite models  Okay Sorry 😐 realized this would break torch.script or fx export compatibility so maybe False by default ? (I might be wrong tho, but I don't think it's suported)  yeah, torchscript is not supported for BLIP afaik and the tests are disabled therefore. I guess in that case we don't need it to be `False` No but you could script only the LM model and not the full model no? I added torchscript tests and they are passing currently. FX test cannot be added because the model architecture is not in supported list

I don't think we should do `False` be default, as that would add more complexity than before when we passed the actual `return_dict`. We'd have to wrap outputs from tuple into the correct `ModelOutputClass` manually if `return_dict`. If you think we should still not set `True` by default let's get to the very first solution I proposed Okay sounds good!"
28887,2024-02-06T09:11:12Z,2024-02-08T16:00:53Z,zucchini-nlp,5,7,15,53,3,3,2,[],1119.0,0,23226765.0,0,0,0,0,4806632.119856,,0,15,0,False,"['HuggingFaceDocBuilderDev', 'gante', 'zucchini-nlp', 'amyeroberts']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_28887). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @amyeroberts or @ArthurZucker Hi, PR is ready to review @amyeroberts it should be ready to merge -- with a `list` as input (as opposed to a tensor), there are no serialization issues 🤗  @amyeroberts can we merge this PR? The failing test is unrelated (`test_save_load_fast_init_from_base`, i.e. model init), and I've opened [this PR](https://github.com/huggingface/transformers/pull/28930) to tag it as flaky. Its flakiness is discussed on our internal slack [here](https://huggingface.slack.com/archives/C01NE71C4F7/p1707407250079089). @gante Yep! I can merge LGTM, only two minor nits. Thanks for fixing! Thanks for adding this! 

You'll need to add tests for the batched vs unbatched behaviour, as well as serializing the config.  Looks great - thanks for iterating and adding this! ","LGTM, only two minor nits. Thanks for fixing! Thanks for adding this! 

You'll need to add tests for the batched vs unbatched behaviour, as well as serializing the config.  Looks great - thanks for iterating and adding this! ","# What does this PR do?

This PR addresses [issue #28763 ](https://github.com/huggingface/transformers/issues/28763). The requested feature already work out of the box, I just made it explicit and added one line in the docs. 

The changes were tested with `pytest -k generate_input tests/generation/test_utils.py`

## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).


## Who can review?
@gante 

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
","I cannot think of cases when start_ids have length > 1  ```suggestion
        decoder_start_token_id (`Union[int, torch.Tensor]`, *optional*):
``` Was this changed from `elif` to `if` for a particular reason? If not, let's revert to `elif` accidentally changed that, now it's back What happens when you create a config with a tensor, save it and load it again? 

I don't think tensors are json serializable, so you'll need to conversion before saving and loading.  Oh yeah, that's a good point @amyeroberts ! Perhaps it being a `list` is wiser got it, thanks! Changed to `list` and checked I can save and load again"
34419,2024-10-25T19:21:59Z,2024-10-31T21:15:04Z,NielsRogge,4,4,6,26,2,2,1,[],2750.0,0,525186.0,0,0,0,0,4834765.764986,,0,6,0,False,"['manuelsh', 'NielsRogge', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34419). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Please see my comment https://github.com/huggingface/transformers/issues/34415#issuecomment-2438972495 before merging. I've updated my PR description based on your comments I just reverted to the interpolation which is used by all other models, so the logits diff remains (edit: it looks like it has to do with my local setup)LGTM thanks, not sure why one of the integration test is gone, congrats on getting the old logits back! 🥳 ","LGTM thanks, not sure why one of the integration test is gone, congrats on getting the old logits back! 🥳 ","# What does this PR do?

This PR fixes #34415 by making `interpolate_pos_encoding` default to `True`.","Why not just revert this? I think we should keep what we had: script support and etc! I would prefer to keep the old way of interpolation to match the logits THis is another kind of breaking change were we added support for torch script, and now we are removing it + we have less standardization. 

Can you investigate in the logit difference?  Yes it was just minor difference in my local setup!"
34295,2024-10-21T19:31:24Z,2024-10-31T17:51:15Z,fpgaminer,0,0,1,3,1,2,2,[],,0,857992.0,0,0,0,0,4846995.286658,,0,1,0,False,[],"Thanks for the update!

Comment for core maintainers
Update SigLIP similar to CLIP:
https://github.com/huggingface/transformers/blob/32590b5ecb50f1c56be32cb0e686196be9427f2f/src/transformers/models/clip/modeling_clip.py#L247-L248 Thanks 🤗 ","Thanks for the update!

Comment for core maintainers
Update SigLIP similar to CLIP:
https://github.com/huggingface/transformers/blob/32590b5ecb50f1c56be32cb0e686196be9427f2f/src/transformers/models/clip/modeling_clip.py#L247-L248 Thanks 🤗 ","Update SiglipVisionEmbeddings.forward to cast input to correct dtype before embedding it.

Fixes #34294


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@amyeroberts @qubvel
",
34170,2024-10-15T08:51:56Z,2024-10-31T19:48:11Z,yonigozlan,21,30,45,1021,47,4,0,[],2469.0,0,1426470.0,0,0,0,0,4835285.601427,,1,45,0,False,"['yonigozlan', 'HuggingFaceDocBuilderDev', 'Wauplin', 'Rocketknight1', 'knkski', 'ArthurZucker', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34170). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Will it be possible to use this PR for just text generation with a image-capable model? I'm trying to use this PR (at commit 4ac2d1fce81a00d251ae9af75f32b1f821d56296) with `meta-llama/Llama-3.2-90B-Vision-Instruct` so that I can compare the language capabilities vs Llama 3.1 70B, and I don't need to use the image support.

I tried calling it like this:

```python
pipe = pipeline(
    ""image-text-to-text"",
    model=""meta-llama/Llama-3.2-90B-Vision-Instruct"", 
    device_map=""auto"",
)
messages = [
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""text"", ""text"": ""What is 1+1?""},
        ],
    }
]
outputs = pipe(text=messages, max_new_tokens=20, return_full_text=False)
print(outputs[0][""generated_text""])
```

That resulted in this error:

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/pipelines/image_text_to_text.py:393, in ImageTextToTextPipeline.preprocess(self, inputs, truncation, padding, max_length, timeout, continue_final_message)
    392 try:
--> 393     model_inputs = self.processor(images=images, text=text, return_tensors=self.framework, **kwargs).to(
    394         dtype=self.torch_dtype
    395     )
    396 except TypeError:

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/models/mllama/processing_mllama.py:285, in MllamaProcessor.__call__(self, images, text, audio, videos, **kwargs)
    284 _ = text_kwargs.pop(""padding_side"", None)  # hack until padding-side is an accepted kwarg by tokenizers
--> 285 encoding = self.tokenizer(text, **text_kwargs)
    286 data.update(encoding)

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3020, in PreTrainedTokenizerBase.__call__(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   3019         self._switch_to_input_mode()
-> 3020     encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
   3021 if text_target is not None:

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3108, in PreTrainedTokenizerBase._call_one(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)
   3107     batch_text_or_text_pairs = list(zip(text, text_pair)) if text_pair is not None else text
-> 3108     return self.batch_encode_plus(
   3109         batch_text_or_text_pairs=batch_text_or_text_pairs,
   3110         add_special_tokens=add_special_tokens,
   3111         padding=padding,
   3112         truncation=truncation,
   3113         max_length=max_length,
   3114         stride=stride,
   3115         is_split_into_words=is_split_into_words,
   3116         pad_to_multiple_of=pad_to_multiple_of,
   3117         padding_side=padding_side,
   3118         return_tensors=return_tensors,
   3119         return_token_type_ids=return_token_type_ids,
   3120         return_attention_mask=return_attention_mask,
   3121         return_overflowing_tokens=return_overflowing_tokens,
   3122         return_special_tokens_mask=return_special_tokens_mask,
   3123         return_offsets_mapping=return_offsets_mapping,
   3124         return_length=return_length,
   3125         verbose=verbose,
   3126         split_special_tokens=split_special_tokens,
   3127         **kwargs,
   3128     )
   3129 else:

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3310, in PreTrainedTokenizerBase.batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)
   3301 padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
   3302     padding=padding,
   3303     truncation=truncation,
   (...)
   3307     **kwargs,
   3308 )
-> 3310 return self._batch_encode_plus(
   3311     batch_text_or_text_pairs=batch_text_or_text_pairs,
   3312     add_special_tokens=add_special_tokens,
   3313     padding_strategy=padding_strategy,
   3314     truncation_strategy=truncation_strategy,
   3315     max_length=max_length,
   3316     stride=stride,
   3317     is_split_into_words=is_split_into_words,
   3318     pad_to_multiple_of=pad_to_multiple_of,
   3319     padding_side=padding_side,
   3320     return_tensors=return_tensors,
   3321     return_token_type_ids=return_token_type_ids,
   3322     return_attention_mask=return_attention_mask,
   3323     return_overflowing_tokens=return_overflowing_tokens,
   3324     return_special_tokens_mask=return_special_tokens_mask,
   3325     return_offsets_mapping=return_offsets_mapping,
   3326     return_length=return_length,
   3327     verbose=verbose,
   3328     split_special_tokens=split_special_tokens,
   3329     **kwargs,
   3330 )

TypeError: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'legacy'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
Cell In[5], line 9
      1 messages = [
      2     {
      3         ""role"": ""user"",
   (...)
      7     }
      8 ]
----> 9 outputs = pipe(text=messages, max_new_tokens=20, return_full_text=False)
     10 print(outputs[0][""generated_text""])

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/pipelines/image_text_to_text.py:291, in ImageTextToTextPipeline.__call__(self, images, text, **kwargs)
    285 if isinstance(text, (list, tuple, KeyDataset) if is_torch_available() else (list, tuple)) and isinstance(
    286     text[0], (list, tuple, dict)
    287 ):
    288     # We have one or more prompts in list-of-dicts format, so this is chat mode
    290     if isinstance(text[0], dict):
--> 291         return super().__call__(Chat(text, images), **kwargs)
    292     else:
    293         if images is None:

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1302, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1294     return next(
   1295         iter(
   1296             self.get_iterator(
   (...)
   1299         )
   1300     )
   1301 else:
-> 1302     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1308, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1307 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
-> 1308     model_inputs = self.preprocess(inputs, **preprocess_params)
   1309     model_outputs = self.forward(model_inputs, **forward_params)
   1310     outputs = self.postprocess(model_outputs, **postprocess_params)

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/pipelines/image_text_to_text.py:398, in ImageTextToTextPipeline.preprocess(self, inputs, truncation, padding, max_length, timeout, continue_final_message)
    396 except TypeError:
    397     kwargs.pop(""legacy"", None)
--> 398     model_inputs = self.processor(images=images, text=text, return_tensors=self.framework, **kwargs).to(
    399         dtype=self.torch_dtype
    400     )
    402 model_inputs[""text""] = inputs_text
    404 return model_inputs

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/models/mllama/processing_mllama.py:290, in MllamaProcessor.__call__(self, images, text, audio, videos, **kwargs)
    288 n_images_in_images = [0]
    289 if images is not None:
--> 290     images = make_list_of_images(images)
    291     n_images_in_images = [len(sample) for sample in images]
    293 if text is not None:

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/models/mllama/image_processing_mllama.py:543, in make_list_of_images(images)
    541     output_images = images
    542 else:
--> 543     raise ValueError(
    544         ""Invalid input type. Must be a single image, a list of images, or a list of batches of images.""
    545     )
    546 return output_images

ValueError: Invalid input type. Must be a single image, a list of images, or a list of batches of images.
```

I tried running it just as above as well, with an image input, and that resulted in an `OutOfMemoryError`, which is confusing because the model size is only 166G on disk, and I'm running this in a 4x80G (i.e. 320G) H100 Lambda Labs environment.

```python
---------------------------------------------------------------------------
OutOfMemoryError                          Traceback (most recent call last)
Cell In[6], line 23
      1 # messages = [
      2 #     {
      3 #         ""role"": ""user"",
   (...)
      9 # outputs = pipe(text=messages, max_new_tokens=20, return_full_text=False)
     10 # print(outputs[0][""generated_text""])
     11 messages = [
     12     {
     13         ""role"": ""user"",
   (...)
     21     }
     22 ]
---> 23 outputs = pipe(text=messages, max_new_tokens=20, return_full_text=False)
     24 print(outputs[0][""generated_text""])

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/pipelines/image_text_to_text.py:291, in ImageTextToTextPipeline.__call__(self, images, text, **kwargs)
    285 if isinstance(text, (list, tuple, KeyDataset) if is_torch_available() else (list, tuple)) and isinstance(
    286     text[0], (list, tuple, dict)
    287 ):
    288     # We have one or more prompts in list-of-dicts format, so this is chat mode
    290     if isinstance(text[0], dict):
--> 291         return super().__call__(Chat(text, images), **kwargs)
    292     else:
    293         if images is None:

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1302, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1294     return next(
   1295         iter(
   1296             self.get_iterator(
   (...)
   1299         )
   1300     )
   1301 else:
-> 1302     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1309, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1307 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
   1308     model_inputs = self.preprocess(inputs, **preprocess_params)
-> 1309     model_outputs = self.forward(model_inputs, **forward_params)
   1310     outputs = self.postprocess(model_outputs, **postprocess_params)
   1311     return outputs

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1209, in Pipeline.forward(self, model_inputs, **forward_params)
   1207     with inference_context():
   1208         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
-> 1209         model_outputs = self._forward(model_inputs, **forward_params)
   1210         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu""))
   1211 else:

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/pipelines/image_text_to_text.py:412, in ImageTextToTextPipeline._forward(self, model_inputs, generate_kwargs)
    408 prompt_text = model_inputs.pop(""text"")
    409 input_ids = (
    410     model_inputs[""input_ids""] if ""input_ids"" in model_inputs else model_inputs[""decoder_input_ids""]
    411 )  # for decoder-only models
--> 412 generated_sequence = self.model.generate(**model_inputs, **generate_kwargs)
    414 return {""generated_sequence"": generated_sequence, ""prompt_text"": prompt_text, ""input_ids"": input_ids}

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)
    113 @functools.wraps(func)
    114 def decorate_context(*args, **kwargs):
    115     with ctx_factory():
--> 116         return func(*args, **kwargs)

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2208, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)
   2200     input_ids, model_kwargs = self._expand_inputs_for_generation(
   2201         input_ids=input_ids,
   2202         expand_size=generation_config.num_return_sequences,
   2203         is_encoder_decoder=self.config.is_encoder_decoder,
   2204         **model_kwargs,
   2205     )
   2207     # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)
-> 2208     result = self._sample(
   2209         input_ids,
   2210         logits_processor=prepared_logits_processor,
   2211         stopping_criteria=prepared_stopping_criteria,
   2212         generation_config=generation_config,
   2213         synced_gpus=synced_gpus,
   2214         streamer=streamer,
   2215         **model_kwargs,
   2216     )
   2218 elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):
   2219     # 11. prepare beam search scorer
   2220     beam_scorer = BeamSearchScorer(
   2221         batch_size=batch_size,
   2222         num_beams=generation_config.num_beams,
   (...)
   2227         max_length=generation_config.max_length,
   2228     )

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:3176, in GenerationMixin._sample(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)
   3173 model_inputs.update({""output_hidden_states"": output_hidden_states} if output_hidden_states else {})
   3175 # forward pass to get next token
-> 3176 outputs = self(**model_inputs, return_dict=True)
   3178 # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping
   3179 model_kwargs = self._update_model_kwargs_for_generation(
   3180     outputs,
   3181     model_kwargs,
   3182     is_encoder_decoder=self.config.is_encoder_decoder,
   3183 )

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/accelerate/hooks.py:170, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    168         output = module._old_forward(*args, **kwargs)
    169 else:
--> 170     output = module._old_forward(*args, **kwargs)
    171 return module._hf_hook.post_forward(module, output)

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/models/mllama/modeling_mllama.py:2138, in MllamaForConditionalGeneration.forward(self, input_ids, pixel_values, aspect_ratio_mask, aspect_ratio_ids, attention_mask, cross_attention_mask, cross_attention_states, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)
   2135     cross_attention_mask = cross_attention_mask[:, :, cache_position]
   2136     full_text_row_masked_out_mask = full_text_row_masked_out_mask[:, :, cache_position]
-> 2138 outputs = self.language_model(
   2139     input_ids=input_ids,
   2140     attention_mask=attention_mask,
   2141     position_ids=position_ids,
   2142     cross_attention_states=cross_attention_states,
   2143     cross_attention_mask=cross_attention_mask,
   2144     full_text_row_masked_out_mask=full_text_row_masked_out_mask,
   2145     past_key_values=past_key_values,
   2146     use_cache=use_cache,
   2147     inputs_embeds=inputs_embeds,
   2148     labels=labels,
   2149     output_hidden_states=output_hidden_states,
   2150     output_attentions=output_attentions,
   2151     return_dict=return_dict,
   2152     cache_position=cache_position,
   2153     num_logits_to_keep=num_logits_to_keep,
   2154 )
   2156 return outputs

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/transformers/models/mllama/modeling_mllama.py:1948, in MllamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, cross_attention_states, cross_attention_mask, full_text_row_masked_out_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)
   1931 outputs = self.model(
   1932     input_ids=input_ids,
   1933     cross_attention_states=cross_attention_states,
   (...)
   1944     cache_position=cache_position,
   1945 )
   1947 hidden_states = outputs[0]
-> 1948 logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()
   1950 loss = None
   1951 if labels is not None:
   1952     # Upcast to float if we need to compute the loss to avoid potential precision issues

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    164 def new_forward(module, *args, **kwargs):
--> 165     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
    166     if module._hf_hook.no_grad:
    167         with torch.no_grad():

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/accelerate/hooks.py:355, in AlignDevicesHook.pre_forward(self, module, *args, **kwargs)
    347         if (
    348             value is not None
    349             and self.tied_params_map is not None
    350             and value.data_ptr() in self.tied_params_map
    351             and self.execution_device not in self.tied_params_map[value.data_ptr()]
    352         ):
    353             self.tied_pointers_to_remove.add((value.data_ptr(), self.execution_device))
--> 355         set_module_tensor_to_device(
    356             module,
    357             name,
    358             self.execution_device,
    359             value=value,
    360             fp16_statistics=fp16_statistics,
    361             tied_params_map=self.tied_params_map,
    362         )
    364 return send_to_device(args, self.execution_device), send_to_device(
    365     kwargs, self.execution_device, skip_keys=self.skip_keys
    366 )

File ~/.cache/pypoetry/virtualenvs/fis-eda-hqM-d85b-py3.10/lib/python3.10/site-packages/accelerate/utils/modeling.py:329, in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)
    327             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)
    328 elif isinstance(value, torch.Tensor):
--> 329     new_value = value.to(device)
    330 else:
    331     new_value = torch.tensor(value, device=device)

OutOfMemoryError: CUDA out of memory. Tried to allocate 3.91 GiB. GPU 0 has a total capacity of 79.10 GiB of which 2.12 GiB is free. Including non-PyTorch memory, this process has 76.97 GiB memory in use. Of the allocated memory 75.56 GiB is allocated by PyTorch, and 761.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
``` Thanks for the feedback @knkski! Although it's not really an objective of this pipeline, I think we can try to add support and raise a warning at least, wdyt @Rocketknight1 ?
For the memory problem, that's is strange indeed, I will look into that, and if others have an idea of why this is happening feel free to chime in. Do you manage to use this model on your setup without using the pipeline? @yonigozlan I think that's okay! It might result in a bit of crossover with `text-generation` pipelines, but I think it's fine, and we can deprecate it later and officially move that functionality to `text-generation` if it's a problem. @Rocketknight1 @knkski , text-only inference should be supported now :) @yonigozlan Thanks! Works great for me :rocket: 

I think the extra memory usage is unrelated to this PR, so ignore that :+1:  > That might mean we should take these changes and fold them into text-generation instead. However, that might add additional inputs that would make it harder to synchronize the pipeline with the inference spec - cc @Wauplin / @LysandreJik, how annoying do you think that would be?

X-posting the [slack thread](https://huggingface.slack.com/archives/C014N4749J9/p1729606160808759) (private) about that convo.
IMO better to have both `text-generation` and `image-text-to-text` to be consistent with https://huggingface.co/tasks. There is still some issues with pipeline tests:
- It seems that pipeline model tests are based on ""tiny models"" available on `hf-internal-testing`, but those tiny models don't seem to be added anymore for recent vlms, so they are not being tested. I'm not sure if this is or used to be an automatic or manual process, and if we should start adding those tiny models back again.
- The Kosmos2 tiny model causes some problems: it's configuration has hyper-parameters that are not compatible with each other, namely `latent_query_num=3`, which is a model parameter, should be the same as `num_image_tokens=64`, which is a processor call argument, so can't be set via a json config file (I think?). An easy fix would be to manually change `latent_query_num` to 64 in the tiny model's config in `hf-internal-testing`, but that could make the model not so tiny anymore. Or we could skip the test altogether. @yonigozlan tiny models aren't automatically generated, those are all manually created. Rather than modifying an existing one (which might break existing tests), I'd suggest just making a new tiny model that fits what you want to test and uploading that to `hf-internal-testing`. You can ask to be added to the organization if you don't have permissions! > @yonigozlan tiny models aren't automatically generated, those are all manually created. Rather than modifying an existing one (which might break existing tests), I'd suggest just making a new tiny model that fits what you want to test and uploading that to `hf-internal-testing`. You can ask to be added to the organization if you don't have permissions!

I see, thanks for the explanation! As for adding new tiny model, pipelines use the `tiny_model_summary.json` file to identify tiny models, but it looks like only one tiny model per model architecture can be present in that file, so I'm not sure how to solve the issue with the Kosmos2 tiny model without modifying the current one. @yonigozlan probably the easiest thing to do, in that case, is just to manually upload a new model, don't add it to `tiny_model_summary`, and manually set that model in the `image-text-to-text` tests. You shouldn't need to worry about whatever's in `tiny_model_summary.json` either way!

Also, I was wrong - some of the `tiny` models are automatically created, but in this case I think a manual one just for your pipeline will work a lot better. @ArthurZucker Addressed your comments! I tried to simplify the code a bit without removing any logic. Happy to remove some logic depending on what you think about this https://github.com/huggingface/transformers/pull/34170#discussion_r1816953666 Answered on the issue, I think we should simnplify to ship with not checks, add checks in the corresponding processors Clearly agree that we should rely on the processors. I can make all this logic into a utils or a ProcessorMixin function and adapt it a bit for models requiring some specific input formatiing. Happy to work on that.
I do think however this will take quite a bit of time to validate and make sure we handle BC correctly for all processors, and it seems that there's a need for this pipeline to be shipped quickly (cc @NielsRogge on that).
So I feel we can either:
- Ship the pipeline with no checks and the processors as they are now, with an added warning that each model may have some specific input requirements. Progressively improve the processors afterwards. (That's maybe what you're saying? Just want to confirm)
- Keep the checks in the pipeline for now, remove them once the processors are in a good state.
- Modify the processors first, delay shipping this pipeline. @ArthurZucker I removed most of the preprocessing logic, hopefully the code should be less messy now :) >Ship the pipeline with no checks and the processors as they are now, with an added warning that each model may have some specific input requirements. Progressively improve the processors afterwards. (That's maybe what you're saying? Just want to confirm) 

this is IMO the best Not sure why I didn't see this failure on CircleCI, but running on daily CI runner, I got 

> FAILED tests/models/fuyu/test_modeling_fuyu.py::FuyuModelTest::test_pipeline_image_text_to_text - TypeError: 'int' object is not subscriptable

 OH I see. @yonigozlan , better to rebase (or merge, whatever you prefer) the main branch to have #34391 > Not sure why I didn't see this failure on CircleCI, but running on daily CI runner, I got
> 
> > FAILED tests/models/fuyu/test_modeling_fuyu.py::FuyuModelTest::test_pipeline_image_text_to_text - TypeError: 'int' object is not subscriptable

Thanks! Should be fixed now @ArthurZucker There were some issues with other multimodal pipelines tests where model tests weren't actually being performed previously. I added the tests and made some modifications to owlv2 and fuyu image processors as some backward compatibility wasn't properly handled. This seems to be resolved now, and all tests are passing :) Thanks for all of your inputs! I'll merged this now as the remaining issues/improvements raised seem a bit out of scope for this PR.
Just to recap some of the points that were raised:
- VLMs processors are not fully consistent in terms of what inputs they accept, and some of them don't catch errors that should be caught. Improvements can be made there that would benefit this pipeline as well. I'll open an issue for this to share it as a known limitation, and I'll start working on it asap :).
- Donut doesn't work in this pipeline as processors are not infer in pipelines if they are not in auto.
- Chat templates could be applied directly in conversational models' processor instead of users having to manually do so before making a processor call? Chat inputs could be detected as they are list of dicts.
- Several pipelines have a way to handle detecting inputs in generated text, and removing or adding it. This could be unified in a util, or in generate with an added ""return_input"" flag.
- Most recent models (and vlms in particular) don't have a ""tiny"" version uploaded on hf-internal-testing, which means they are not tested by the CI in the different pipelines that support them.Overall, this looks good! The tests seem good and the pipeline code looks clean! A lot of the code is familiar from the `text-generation` pipeline, with modifications for images.

The only question I have is whether it'll be confusing to have e.g. `image-text-to-text` as well as `image-to-text` and `text-generation` pipelines. In particular, it feels like this pipeline is almost a ""superset"" of `text-generation`, since it can handle both text completions and chat completions with templates, which means it's basically just `text-generation` plus image support. 

That might mean we should take these changes and fold them into `text-generation` instead. However, that might add additional inputs that would make it harder to synchronize the pipeline with the inference spec - cc @wauplin / @LysandreJik, how annoying do you think that would be? Thanks for working on this! I think it's very important, thus we should try to make it a bit more simple. 🤗  Cool! 
2 last nits: 
- postprocess 
- tests: can't we avoid having to manually add which pipeline need to be tested based on all the automappings that we have? 🤗 ","Overall, this looks good! The tests seem good and the pipeline code looks clean! A lot of the code is familiar from the `text-generation` pipeline, with modifications for images.

The only question I have is whether it'll be confusing to have e.g. `image-text-to-text` as well as `image-to-text` and `text-generation` pipelines. In particular, it feels like this pipeline is almost a ""superset"" of `text-generation`, since it can handle both text completions and chat completions with templates, which means it's basically just `text-generation` plus image support. 

That might mean we should take these changes and fold them into `text-generation` instead. However, that might add additional inputs that would make it harder to synchronize the pipeline with the inference spec - cc @wauplin / @LysandreJik, how annoying do you think that would be? Thanks for working on this! I think it's very important, thus we should try to make it a bit more simple. 🤗  Cool! 
2 last nits: 
- postprocess 
- tests: can't we avoid having to manually add which pipeline need to be tested based on all the automappings that we have? 🤗 ","# What does this PR do?
Add image-text-to-text pipeline!

A split of this PR with only model-specific pre and post processing is available [here](https://github.com/huggingface/transformers/pull/32471), in order to reduce the loc count and number of files changed before merging this PR.

Note: The use of a `""legacy""` kwarg to modify the preprocessing of some image-text-to-text models is needed here if we want to integrate those models into this pipeline. However, the way it is handled might not be ideal, so I'm open to suggestion on how to improve this.

the pipeline support the following inputs:
- unbatched images and text - `images=image, text=text`
- batched images and text - `images = [image, image], text= [text, text]`
- several images per prompt (only for models supporting the use of an image token) - `images = [[image, image], [image]] or  images=[image, image, image], text = [""... <image>...<image>..."", ""...<image>...""]`
- Chat templates (for models supporting them).

### TODOs:
- [x] Add pipeline tests in model-specific test files
- [ ] Update tasks documentation?

### Known current limitations/bugs:
- Using prompts without image tokens with models that expect them will throw an error. Should we automatically add image tokens to prompts and display a warning? For now, only a warning is displayed if the model's processor has an image token.
- Using several images per prompt for models who do not support the use of an image token) will raise an uncaught error.
- Donut doesn't work, as there is a problem identifying the correct model type for it
- Idefics3 will raise an uncaught error if no correct image tokens are provided, fixed here https://github.com/huggingface/transformers/pull/34222
- Pixtral with batched input raises `Pipeline with tokenizer without pad_token cannot do batching. You can try to set it with `pipe.tokenizer.pad_token_id = model.config.eos_token_id`.`

### Examples of usage:
```python
>>> from transformers import pipeline
>>> pipe = pipeline(""image-text-to-text"", model=""llava-hf/llava-interleave-qwen-0.5b-hf"")
>>> image = ""./tests/fixtures/tests_samples/COCO/000000039769.png""
>>> text = ""<image> What this is? Assistant: This is""
>>> pipe(image, text=text, max_new_tokens=20)
[
    [
        {
            ""input_text"": ""<image> What this is? Assistant: This is"",
            ""generated_text"": ""<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are sleeping and appear to be comfortable"",
        }
    ]
],
```

```python

>>> from transformers import pipeline
>>> pipe = pipeline(""image-text-to-text"", model=""llava-hf/llava-interleave-qwen-0.5b-hf"")
>>> messages = [
>>>     {
>>>         ""role"": ""user"",
>>>         ""content"": [
>>>             {
>>>                 ""type"": ""image"",
>>>                 ""image"": ""https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"",
>>>             },
>>>             {""type"": ""text"", ""text"": ""Describe this image.""},
>>>         ],
>>>     }
>>> ]
>>> outputs = pipe(text=messages, max_new_tokens=20, return_full_text=False)
>>> print(outputs[0][""generated_text""])
""In the image, a woman is sitting on the sandy beach, her legs crossed in a relaxed manner""
```

```python
>>> from transformers import pipeline
>>> pipe = pipeline(""image-text-to-text"", model=""llava-hf/llava-interleave-qwen-0.5b-hf"")
>>> messages = [
>>>     {
>>>         ""role"": ""user"",
>>>         ""content"": [
>>>             {
>>>                 ""type"": ""image"",
>>>                 ""image"": ""https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"",
>>>             },
>>>             {""type"": ""text"", ""text"": ""Describe this image.""},
>>>         ],
>>>     },
>>>     {
>>>         ""role"": ""assistant"",
>>>         ""content"": [
>>>             {""type"": ""text"", ""text"": ""There is a dog and""},
>>>         ],
>>>     },
>>> ]
>>> outputs = pipe(text=messages, max_new_tokens=20)
>>> print(outputs[0][""generated_text""])
[
    {
        ""role"": ""user"",
        ""content"": [
            {
                ""type"": ""image"",
                ""image"": ""https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"",
            },
            {""type"": ""text"", ""text"": ""Describe this image.""},
        ],
    },
    {
        ""role"": ""assistant"",
        ""content"": [
            {
                ""type"": ""text"",
                ""text"": ""There is a dog and a person in the image. The dog is sitting on the sand, and the person is sitting on"",
            }
        ],
    },
]
```




<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

## Who can review?

@Rocketknight1 @molbap @qubvel @NielsRogge 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","We'll be adding soon chat template support for image/video inputs and the format decided is  like:`{type: ""image"", ""url"": https://}` or `{type: ""image"", ""path"": ""/home/usr/image.png""}`

imo it is better if we do that way for consistency, so users start getting used to the same format across repo Hmm how does multiimage work, do we need to explicitly pass in nested list of images in that case? Can we add pipeline mapping in all VLMs so that they are tested with image-text-to-text pipeline? We have to make sure the pipeline works with all VLMs, and some of the models are very peculiar. For ex in BLIP-2 we have

https://github.com/huggingface/transformers/blob/293e6271c69a48b6a66e68978dd3d37601c04c63/tests/models/blip_2/test_modeling_blip_2.py#L703-L708 can you add a comment on why we need to have `add_special_tokens` to help future ourselves when reading the code? in one of prev PRs it was decided that we should support Paligemma in similar fashion as llava models, thus the user must provide ""image"" token, especially when doing multi-image generation. I don't think we should be raising warning here misleading users, since we also raise warning when no 'image' token is passed (lines 271-275 below) afaik not all models require an image token to be in prompt. BLIP for ex has no requirement and Paligemma only suggests users to use 'image' token. Will this check work for such models? Oh I hadn't seen that, will remove it. Thanks Maybe there is something to change here indeed in the case where text is a string. 
Right now we assume that if text is a string and there is multiple images (non nested), the prompt should be applied to each image independently.
It would maybe make more sense to consider as a multiimage prompt.
But yes in that specific case we need to explicitly pass in a nested list of image `[[image1, image2,...]]` with `text = ""prompt""` , which is weird.
I will try changing that and see if I run into any problems.
 there's an initial check before this:
https://github.com/huggingface/transformers/blob/74375f7a5bf570835b54c607fedfdeb73f7fb28a/src/transformers/pipelines/image_text_to_text.py#L311-L325
so if there's no image tokens in any prompt, we don't enter this logic at all After testing a bit, it looks like most models which work without image tokens also don't accept multiple images for one prompt (kosmos 2, blip/blip2, paligemma, udop, GIT), and the errors are not always caught, so it's not clear to the user why the pipeline crashes.
The one I found that does work is llava-onevision, but then the output seems to be garbage, so I'm not sure we should encouraged this behavior. Wdyt?
On that last point, it looks like a lot of models which support chat templates do not work well with ""raw"" text inputs. Should we raise a warning when a model supports chat template, but the input is raw text?
 Right now I settled on this:
- if there are image tokens, we accept one prompt for multiple images, no matter if the images are nested or not.
- if there are no image tokens and one prompt for multiple images, the prompt will be repeated for each image, and raise a warning. oke, agreed, makes sense! Thanks A lot of these methods are identical and should probably have `#Copied from` to sync them up. We have another PR open that affects this area, specifically the `strip()` methods. You don't need to change anything, but just be aware that you might have to resolve conflicts at some point! ```suggestion
                        ""The number of images in the chat should be the same as the number of images passed to the pipeline.""
``` Maybe slightly clearer? ```suggestion
        raise ValueError(""The number of images in the chat should be the same as the number of images passed to the pipeline."")
``` ```suggestion
        if getattr(self.processor, ""chat_template"", None) is not None:
```
Slightly simpler! ```suggestion
                ""The input data was not formatted as a chat with dicts containing 'role' and 'content' keys, even though ""
                ""this model supports chat. ""
                ""Consider using the chat format for better results. For more information, see https://huggingface.co/docs/transformers/en/chat_templating""
``` Thanks for the heads up! After Slack discussion, I was wrong, keeping this as a separate pipeline from `text-generation` makes sense! I think this PR is almost ready to merge, in that case.

I did a second review, though, and there's one bit I don't understand (cc @zucchini-nlp as well) - it seems like this pipeline uses `Processor` rather than `ImageProcessor`, because you set `_load_processor=True`. However, the pipeline is initialized with `image_processor` here. Which class is actually being loaded? You are right something (multiple things really) was wrong here. This pipeline indeed uses a processor, but I missed that I had to clearly put the image-text-to-text test in `PipelineTesterMixin`, so no model was actually tested on the pipeline. This is done now, although I had to make some adjustments to test batched inputs, as the current tests were not adapted to a pipeline that takes in several inputs.
Thanks a lot for flagging this! Changed this to `or` because unlike `load_tokenizer`, `load_image_processor` and `load_feature_extractor` which can be set to True in the logic right afterwards depending on the type of data involved in the pipeline, if `load_processor` is False at this point, it won't change.
This means that if the model is not in `PROCESSOR_MAPPING`, the processor won't be loaded, and the logic below to         ""try to infer processor from model or config name"" won't be executed.
This is a problem for models who share an architecture with other models, such as Donut, and thus can't be in PROCESSOR_MAPPING. Changing this to `or` fixes the problem, but I would like to make sure I don't break anything by doing this. @qubvel I think you added this logic so I would be glad to have your input on this Update: I am reverting this change as it is causing some problems in other tests. I'll open a PR for it later on. the only difference seems to be that not legacy sets default. (wether there is an image or not)  the token might not be in the vocab itself, let's use convert tokens to id should we use `Llava` instead?  IMO here we should only define `content = content[""image""]` etc and then append I don't really agree with that 😅  Is this a known use-case? We should raise an error otherwise"
34512,2024-10-30T15:11:07Z,2024-10-31T17:34:00Z,ydshieh,1,11,13,105,4,3,2,[],1693.0,0,94975.0,0,0,0,0,4848031.308595,,1,13,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34512). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM 👍 Thank you for fixing

Extra note: L4170 (`model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)`) should also have `attn_implementation=""sdpa""`, in case we update the default. thanks ! ","LGTM 👍 Thank you for fixing

Extra note: L4170 (`model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)`) should also have `attn_implementation=""sdpa""`, in case we update the default. thanks ! ","# What does this PR do?

With `torch.bfloat16` the numerical difference/instability occurs quite often, especially with multiple hidden layers.

This PR first changes `test_eager_matches_sdpa_inference` to create models with only 1 hidden layer. 

**number of failures per 500 runs**
|   | main | n_layer=1 |
| ------------- | -------------: | -------------: |
| llama  | 16  | 2  |
| idefics2   | 391  | 15  |

Then it relaxes the condition a bit: only checks 80% of the sequences. If the results match on those 80%, the test pass.

This makes the test much less flaky. On 500 runs, it pass (for llama, mistral, idefics2 and Llava)

Finally, change the image size of `llava` and `VipLlava` from `30` to `8` so the sequence length is much smaller and avoid numerical issues.
","This seems wrong to me ! we can make it easier by first creating a zero tensor then update the values! 80%: let's make our life easier ... I'm assuming this is needed for VLMs 😅 (because it fetches special VLM kwargs) If possible, it would be nice to leave the original values, or `[1, 7]` (so as to test batched and unbatched) Agreed 😅  yes The problem of having batch size 1 is we might get flaky results.

For this test, I don't know why batch_size = 1 should be tested separately. WDYT? > The problem of having batch size 1 is we might get flaky results.

I think this is a further argument to test batch size 1 :D Let's leave a TODO, if the cause is not obvious do sample is random no? I am using the same value as in `generation_kwargs = {...}` a few line below.

Yes it is random but this method is `test_...._sample` so makes sense."
34536,2024-10-31T15:17:15Z,2024-10-31T16:41:06Z,McPatate,1,0,2,8,2,1,1,['run-benchmark'],2809.0,0,5033.0,0,0,0,0,4851205.335005,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34536). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.sounds good thanks!,sounds good thanks!,"We want to make queries efficient as the content of the benchmarks tables grows, making dashboards more responsive and reducing load on the database. Hence the addition of indexes on the most used fields for filtering/joining.",
34518,2024-10-30T18:23:50Z,2024-10-31T15:47:58Z,philkuz,5,0,2,6,2,2,2,"['Tests', 'Vision', 'run-slow']",2426.0,0,77432.0,0,0,0,0,4854012.075767,,0,2,0,False,"['qubvel', 'HuggingFaceDocBuilderDev', 'ydshieh', 'philkuz']","@philkuz  run-slow commit should be after the run-slow label is added, can you please push it once again? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34518). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > @philkuz  run-slow commit should be after the run-slow label is added, can you please push it once again?

Done! Yes, great!

(And thank you for requesting/triggering the slow tests!) Thanks all!!

On Thu, Oct 31, 2024 at 8:48 AM Yih-Dar ***@***.***> wrote:

> Merged #34518 <https://github.com/huggingface/transformers/pull/34518>
> into main.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/huggingface/transformers/pull/34518#event-15039734920>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABJUBEYVPVLGOCSEQVXGMZDZ6JGMHAVCNFSM6AAAAABQ4XLOP2VHI2DSMVQWIX3LMV45UABCJFZXG5LFIV3GK3TUJZXXI2LGNFRWC5DJN5XDWMJVGAZTSNZTGQ4TEMA>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
Thanks for the fix! The tests are green now!
cc @ydshieh for tests change","Thanks for the fix! The tests are green now!
cc @ydshieh for tests change","# What does this PR do?
Fixes slow tests that are broken in main and discovered in this PR: https://github.com/huggingface/transformers/pull/34103
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@qubvel

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34514,2024-10-30T16:18:08Z,2024-10-31T15:36:13Z,ydshieh,3,0,5,171,24,2,2,[],61877.0,0,84414.0,0,0,0,0,4854573.442981,,1,5,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']","Yes, a helper method is nice. Will update updated. 

So far it doesn't call `gc.collect()` at all (default value `False`).
I would like to see if this would cause issue.
In general, we don't need to call it after each test method (so in `tearDown`) as it is slow.


 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34514). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Yes, this seems like a good speed fix! cc @LysandreJik @ArthurZucker for core maintainer review Smart! Should a helper method be made that only runs on CPU?

Both the gc.collect and the torch device checks could be moved into the `backend_empty_cache` method (or an other method that wraps both)","Yes, this seems like a good speed fix! cc @LysandreJik @ArthurZucker for core maintainer review Smart! Should a helper method be made that only runs on CPU?

Both the gc.collect and the torch device checks could be moved into the `backend_empty_cache` method (or an other method that wraps both)","# What does this PR do?

Let's avoid calling `gc.collect` and `cuda.empty_cache` while the tests are running on CPU:
- those operations are slow
- (actually, in most cases, they are only relevant for integration tests which use large models)

Running on GPT2 tests,

60 seconds on main, 20 seconds on this PR",
34535,2024-10-31T14:36:56Z,2024-10-31T15:42:13Z,gante,1,4,1,2,1,2,1,[],1653.0,0,3921.0,0,0,0,0,4854738.504223,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34535). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?

Computing `inputs_embeds` in `Qwen2VL` is more complex than simply embedding `input_ids` (see `Qwen2VLForConditionalGeneration`), so the basic check doesn't apply :)

Fixes:
```
py.test tests/models/qwen2_vl/test_modeling_qwen2_vl.py::Qwen2VLModelTest::test_generate_from_inputs_embeds_1_beam_search --flake-finder --flake-runs 100
```","This flag skips the check that passing `inputs_embeds = model.get_input_embeddings()(input_ids)` should be equivalent to passing `input_ids` to `generate`.

This is not true in Qwen2VL, whose computation of `inputs_embeds` is more complex OK for me. Just curious: would this be addressed in the future to make it work  Not unless we create a model function to embed inputs, similar to `def get_input_embeddings(self)`.

(I think we should do it eventually, we're starting to have a few models with non-standard inputs embeddings creation!) got it :-) 👍 "
30710,2024-05-08T12:38:36Z,2024-05-13T14:47:58Z,NielsRogge,7,2,2,4,1,4,5,[],2161.0,0,15211181.0,0,0,0,0,4860980.149874,,0,2,0,False,"['qubvel', 'amyeroberts', 'NielsRogge', 'HuggingFaceDocBuilderDev', 'SangbumChoi']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_30710). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. FYI, we usually use much more lower score threshold (e.g. 0.3) to see just tendency of the model. (+ Note that transformers has transformer architecture which have lower confidence level)

https://github.com/facebookresearch/detr/issues/216

https://github.com/IDEA-Research/DINO/blob/main/inference_and_visualization.ipynb @NielsRogge should we merge it, or do you have something to add? I haven't tried the pipeline yet with various thresholds. Do you think we should lower it even more? It could result in too many detections. Wondering what the best default value is @NielsRogge We can always lower it further if necessary. I'd merge as-is and based on observations / feedback we can always update.  With my experiments on cppe-5 dataset `threshold=0.4..0.5` visually looks fine, otherwise, too many FP appear.

With a quick search in other frameworks, I found for some models:
 - In Utlralitics - 0.25 / 0.3
 - In Mmedection - 0.05 / 0.3 / 0.5

But in general, it depends on the model and dataset Ok thanks, will merge in that case.Thanks for updating! 

Agreed, it's not BC but this seems like a better value ","Thanks for updating! 

Agreed, it's not BC but this seems like a better value ","# What does this PR do?

This PR proposes to lower the default threshold of the object detection pipeline. It is currently set to 0.9, which causes a lot of inference widgets to not show any detected objects.

","Should we also update in postprocess?
https://github.com/huggingface/transformers/blob/1872bde7fc6a5d6796bd742bc2dc38eaf8069c5d/src/transformers/pipelines/object_detection.py#L123 Haha sorry I updated the wrong code :D thanks for spotting"
34522,2024-10-31T01:27:53Z,2024-10-31T13:32:46Z,jp1924,2,0,1,12,3,2,2,[],16985.0,0,43493.0,0,0,0,0,4862511.511475,,1,1,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34522). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Pinging actual core maintainer @LysandreJik!Thanks! 

Makes sense since we use the `math.ceil` op when padding. Let's use `math.ceil` for unpadding also to make sure the calculations are done identically

For core maintainers: I don't think this needs a test as it is not something that can be broken again. We have had some issues before with unpadding (that is why `image_sizes` are cast to list) but still some numerical precision errors seem to be left Ok, thank you!","Thanks! 

Makes sense since we use the `math.ceil` op when padding. Let's use `math.ceil` for unpadding also to make sure the calculations are done identically

For core maintainers: I don't think this needs a test as it is not something that can be broken again. We have had some issues before with unpadding (that is why `image_sizes` are cast to list) but still some numerical precision errors seem to be left Ok, thank you!","# What does this PR do?

```python
Image features and image tokens do not match: tokens: 2403, features 2349
  File ""/usr/local/lib/python3.10/dist-packages/transformers/models/llava_next/modeling_llava_next.py"", line 921, in forward
    raise ValueError(
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/root/workspace/others/img_mismatch_error.py"", line 191, in main
    print(model(**find_inputs))
  File ""/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
  File ""/root/workspace/others/img_mismatch_error.py"", line 203, in <module>
    main()
  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main (Current frame)
    return _run_code(code, main_globals, None,
ValueError: Image features and image tokens do not match: tokens: 2403, features 2349
```

```python
    if original_aspect_ratio > current_aspect_ratio:
        scale_factor = current_width / original_width
        new_height = int(original_height * scale_factor)
        padding = (current_height - new_height) // 2
        unpadded_tensor = tensor[:, padding : current_height - padding, :]
    else:
        scale_factor = current_height / original_height
        new_width = int(original_width * scale_factor)
        padding = (current_width - new_width) // 2
        unpadded_tensor = tensor[:, :, padding : current_width - padding]
```

When calculating `new_height` and `new_width`, applying `int()` to values like 59.999999 (which should be treated as 60) results in 59. This leads to incorrect unpadding and consequently causes the img_size mismatch error.

Fixed the issue by adding round to `int(original_width * scale_factor)` like `int(round(original_width * scale_factor, 7))`.

It works fine in my environment for now, but I think we should look into this more carefully. 
What do you all think?

## bug reproduction code

```python
import torch
from PIL import Image

from transformers import (
    AddedToken,
    AutoConfig,
    AutoTokenizer,
    LlavaNextConfig,
    LlavaNextForConditionalGeneration,
    LlavaNextImageProcessor,
    LlavaNextProcessor,
)


device = ""cpu""
dtype = torch.bfloat16
IMG_TOKEN = ""<|image|>""

language_name, vision_name = ""google/gemma-2-9b"", ""google/siglip-so400m-patch14-384""
language_config = AutoConfig.from_pretrained(language_name)
vision_config = AutoConfig.from_pretrained(vision_name).vision_config

vision_config.num_hidden_layers, language_config.num_hidden_layers = 2, 2

tokenizer = AutoTokenizer.from_pretrained(language_name)
tokenizer.add_tokens(AddedToken(IMG_TOKEN, special=True, normalized=False), special_tokens=True)
language_config.vocab_size = len(tokenizer.get_vocab())
image_token_index = tokenizer.convert_tokens_to_ids(IMG_TOKEN)
image_grid_pinpoints = [[768, 768], [384, 768], [384, 1152], [768, 384], [1152, 384]]
vision_feature_select_strategy = ""full""

config = LlavaNextConfig(
    vision_config=vision_config,
    text_config=language_config,
    image_seq_length=vision_config.image_size,
    image_token_index=image_token_index,
    image_grid_pinpoints=image_grid_pinpoints,
    vision_feature_select_strategy=vision_feature_select_strategy,
    _attn_implementation=""eager"",
)
image_processor = LlavaNextImageProcessor.from_pretrained(
    vision_name,
    image_grid_pinpoints=image_grid_pinpoints,
    crop_size={""height"": vision_config.image_size, ""width"": vision_config.image_size},
)

processor = LlavaNextProcessor(
    image_processor=image_processor,
    tokenizer=tokenizer,
    image_token=IMG_TOKEN,
    patch_size=vision_config.patch_size,
    vision_feature_select_strategy=vision_feature_select_strategy,
    image_seq_length=vision_config.image_size,
)
model = LlavaNextForConditionalGeneration(config)
model = model.to(device).to(dtype)

# width, hegith
inputs = (
    processor(
        images=Image.new(""RGB"", (940, 423)),
        text=IMG_TOKEN,
        return_tensors=""pt"",
    )
    .to(device)
    .to(dtype)
)

output = model(**inputs)
```

This code requires the issue #34447 to be resolved first before running. 
Therefore, it is recommended to resolve this issue by installing the following code before execution:
`pip install git+https://github.com/jp1924/transformers.git@Fix--Llava_img_mismatch`


## env
- `transformers` version: 4.47.0.dev0
- Platform: Linux-5.15.0-124-generic-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.26.0
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.4.0+cu121 (True)
- Tensorflow version (GPU?): 2.15.1 (True)
- Flax version (CPU?/GPU?/TPU?): 0.7.0 (cpu)
- Jax version: 0.4.13
- JaxLib version: 0.4.13
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA A100 80GB PCIe



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@zucchini-nlp

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33673,2024-09-24T04:03:18Z,2024-10-31T13:53:23Z,kibitzing,17,5,12,9,1,4,3,[],165506.0,0,3232205.0,0,0,0,0,4861274.742702,,0,12,0,False,"['SunMarc', 'kibitzing', 'HuggingFaceDocBuilderDev', 'muellerzr']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33673). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hello @SunMarc ,

Thank you for taking the time to review my PR! I appreciate your suggestion, it's a great idea and would indeed simplify the code.

However, after investigating and experimenting with this, I found that `self.gradient_state.end_of_dataloader` is always `False` in my case, preventing the step of `self.accelerator` from being reset. 
This happens because:
*  `self.gradient_state.end_of_dataloader` depends on `self.gradient_state.active_dataloader` 
    * `end_of_dataloader` [depends on](https://github.com/huggingface/accelerate/blob/4305033f8035defad0a87cd38e5c918e78510ba5/src/accelerate/state.py#L1197) `in_dataloader` 
    * `in_dataloader` [depends on](https://github.com/huggingface/accelerate/blob/4305033f8035defad0a87cd38e5c918e78510ba5/src/accelerate/state.py#L1250) `active_dataloader`
* and `active_dataloader` is generally [set to `None`](https://github.com/huggingface/accelerate/blob/4305033f8035defad0a87cd38e5c918e78510ba5/src/accelerate/state.py#L1163), unless explicitly configured otherwise.

While it's been interesting to dig deeper, I believe this issue might be outside the scope of the current PR. Hence, I suggest we stick with the previous approach for now.

Additionally, during my testing, I found a bug in the condition `is_last_step_and_steps_less_than_grad_acc`. It should use `or` instead of `and` for the expected behavior. If `and` is used, the last steps are always dropped if` args.gradient_accumulation_steps` is larger than `step_in_epoch`, which is highly likely to happen. I just removed the comparing `step_in_epoch` and `grad_accum_step` part because updating at the last step covers that case as well. I’ve fixed this in the PR because I believe it is directly related to gradient accumulation.

---

\+ It is not modified now but to use `self.accelerator.sync_gradients`, we should also remove https://github.com/huggingface/transformers/blob/f2c388e3f946862f657acc1e21b272ec946fc66c/src/transformers/trainer.py#L4820  because `_do_sync` function check this as well. Regarding the failing test ([run_swag_no_trainer.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/multiple-choice/run_swag_no_trainer.py)), I checked that it is not using the Trainer, which I have fixed. 

Do you have any suggestions or comments on it? After reading the code a bit more, I don't think we are performing a gradient update at the end of each epoch no (see your example) ? I said that because as you saw the condition using `and` in `is_last_step_and_steps_less_than_grad_acc` + we have set `grad_acc_kwargs[""sync_with_dataloader""] = False`. So, I feel like we are doing the right number of update across epoch. The only issue if that we indeed have an issue since there is no N sub-steps between the on_step_begin and on_step_end callbacks. 

```
Epoch 1

Steps 1, 2, 3, 4 (update parameter because (4 % 4 = 0)
Steps 5, 6, 7 (update because it's the last step)
Epoch 2

Step 8 (update parameter because 8 % 4 =0)
Steps 9, 10, 11, 12 (update parameter because 12 % 4 = 0)
Steps 13, 14 (update because it's the last step)
Epoch 3

Step 15, 16 (update parameter because 16 % 4 = 0)
Steps 17, 18, 19, 20 (update parameter because 16 % 4 = 0)
Steps 21 (update because it's the last step)
```

> 
> However, after investigating and experimenting with this, I found that self.gradient_state.end_of_dataloader is always False in my case, preventing the step of self.accelerator from being reset.
> This happens because:
> 
> self.gradient_state.end_of_dataloader depends on self.gradient_state.active_dataloader
> end_of_dataloader [depends on](https://github.com/huggingface/accelerate/blob/4305033f8035defad0a87cd38e5c918e78510ba5/src/accelerate/state.py#L1197) in_dataloader
> in_dataloader [depends on](https://github.com/huggingface/accelerate/blob/4305033f8035defad0a87cd38e5c918e78510ba5/src/accelerate/state.py#L1250) active_dataloader
> and active_dataloader is generally [set to None](https://github.com/huggingface/accelerate/blob/4305033f8035defad0a87cd38e5c918e78510ba5/src/accelerate/state.py#L1163), unless explicitly configured otherwise.

Thanks for exploring ! Could you share a minimal reproducer ? This might indeed require a fix on accelerate side.

> Regarding the failing test ([run_swag_no_trainer.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/multiple-choice/run_swag_no_trainer.py)), I checked that it is not using the Trainer, which I have fixed.

This is probably a flaky test ! 

Do you have any suggestions or comments on it? Thank you for reply @SunMarc, 
I may have misinterpreted the code initially. I agree that there's no issue with the update logic. I will go ahead and revert my changes to keep the condition `is_last_step_and_steps_less_than_grad_acc`.

> Thanks for exploring ! Could you share a minimal reproducer ? This might indeed require a fix on accelerate side.

Sure, I will! > Thank you for reply @SunMarc,
> I may have misinterpreted the code initially. I agree that there's no issue with the update logic. I will go ahead and revert my changes to keep the condition is_last_step_and_steps_less_than_grad_acc.

I mean that the original issue you had (too many updates) might not exist Yes, you are right, that issue does not exist. 
I thought it always update at last, but it doesn't because it is generally blocked by ""steps_less_than_than_grad_acc"" condition. 
Sorry for the confusion. Not an issue ! I was confused also. Still, I will discuss with @muellerzr if it makes sense to switch to back to your idea with the update at each last step, just like how it is coded in accelerate. cc@mueller I'll keep you updated. 

Now, the issue is why self.gradient_state.end_of_dataloader doesn't work as expected. Feel free to open an issue on accelerate library with the reproducer ! Thanks a lot !  Okay, I will create a new issue regarding the `self.gradient_state.end_of_dataloader` and wait for updates on this PR. 
Thanks! Hello @SunMarc, 

I revisited the issue I previously reported regarding `accelerate` and, embarrassingly, it turned out to be a problem with my own codebase. I was using `accelerate` together with a custom `Trainer` but had overridden `get_train_dataloader` without calling `accelerate.prepare(dataloader)`.

To explain the flow in more detail: 
1. `active_dataloader` is set to `None` at first. 
2. When `prepare(dataloader)` is called, it goes into `prepare_data_loader`, where it returns either a `DataLoaderShard` or `DataLoaderShard` that inherits from `DataLoaderStateMixin` as a new dataloader. 
3. These prepared dataloaders add an `active_dataloader` at every `begin()`. 
* Without calling `prepare`, it is clear that the `GradientState` doesn't have an active dataloader

Therefore, **there is no issue with the `accelerate` code itself**. Everything works correctly when `prepare` is called, and I confirmed that at the last batch, `self.gradient_state.end_of_dataloader=True` is set as expected. I also checked this with the pytorch `run_glue.py` example.

Thus, as you suggested, using `if self.accelerator.sync_gradients:` is perfectly fine and results in simpler code.

I’m going to change the current condition to this one and push the update. Hello, I have updated the if condition as per @SunMarc's suggestion.
Do we have any updates regarding the update at each last step ? @SunMarc 
Is there any update on this PR? 
If any additional work is needed, please let me know! (Last bit is resolving the conflicts) Hello, @SunMarc @muellerzr 

Here is a summary of the new commits:

1. I merged the main branch and resolved conflicts.
2. I replaced `total_batched_samples` with `(step + 1)` and removed `steps_in_epoch <= args.gradient_accumulation_steps` condition to simplify, as we discussed before
> I see that in [accelerate](https://github.com/huggingface/accelerate/blob/4305033f8035defad0a87cd38e5c918e78510ba5/src/accelerate/accelerator.py#L1038), we only turn the gradient sync when we are at the end of the dataloader or step+1 % args.gradient_accumulation_steps == 0. Potentially, we can even remove the condition that we placed in transformers.

3. I reverted the previous changes and set `grad_acc_kwargs[""sync_with_dataloader""] = False` as SunMarc mentioned, 
> we will set self.accelerator.gradient_state._set_sync_gradients by ourselves and not rely on the values set by accelerate.accumulate

If there are any issues or additional modifications needed, please let me know!
 failing tests come from main, we are in limbo 🫠  cc @ydshieh  Hello @muellerzr, I merged the main branch and it looks like it passed the tests! 😄 Thanks for the detailed issue @kibitzing! Really nice job explaining what's happening! This looks like the right fix for gradient accumulation. In the past, we had the same behavior as this PR but without updating at the end of the epoch. See this [PR](https://github.com/huggingface/transformers/pull/22098/files) for more information. Since we do update after the end of each epoch, it make sense to not use `total_batched_samples` anymore. I see that in [accelerate](https://github.com/huggingface/accelerate/blob/4305033f8035defad0a87cd38e5c918e78510ba5/src/accelerate/accelerator.py#L1038), we only turn the gradient sync when we are at the end of the dataloader or `step+1 % args.gradient_accumulation_steps == 0`. Potentially, we can even remove the condition that we placed in transformers. 
From
```py
                if (
                    (step + 1) % args.gradient_accumulation_steps == 0
                    or
                    # last step in epoch but step is always smaller than gradient_accumulation_steps
                    is_last_step_and_steps_less_than_grad_acc
                ):
                    # the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered
                    # in accelerate. So, explicitly enable sync gradients to True in that case.
                    if is_last_step_and_steps_less_than_grad_acc:
                        self.accelerator.gradient_state._set_sync_gradients(True)
```
to simply
```python
if self.accelerator.sync_gradients:
```
Can you check if we have the same behavior ? cc @muellerz  Nice! I feel this is a great simplification all around :)  Can you fix the merge comflits. We did a lot of [modification](https://huggingface.co/blog/gradient_accumulation) wrt to grad acc and it makes sense to have this now ! Feel free to ask us any question you have !  A lot better ! Thanks for your patience @kibitzing. If you are happy with the changes, feel free to merge the PR @muellerzr  Nice!","Thanks for the detailed issue @kibitzing! Really nice job explaining what's happening! This looks like the right fix for gradient accumulation. In the past, we had the same behavior as this PR but without updating at the end of the epoch. See this [PR](https://github.com/huggingface/transformers/pull/22098/files) for more information. Since we do update after the end of each epoch, it make sense to not use `total_batched_samples` anymore. I see that in [accelerate](https://github.com/huggingface/accelerate/blob/4305033f8035defad0a87cd38e5c918e78510ba5/src/accelerate/accelerator.py#L1038), we only turn the gradient sync when we are at the end of the dataloader or `step+1 % args.gradient_accumulation_steps == 0`. Potentially, we can even remove the condition that we placed in transformers. 
From
```py
                if (
                    (step + 1) % args.gradient_accumulation_steps == 0
                    or
                    # last step in epoch but step is always smaller than gradient_accumulation_steps
                    is_last_step_and_steps_less_than_grad_acc
                ):
                    # the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered
                    # in accelerate. So, explicitly enable sync gradients to True in that case.
                    if is_last_step_and_steps_less_than_grad_acc:
                        self.accelerator.gradient_state._set_sync_gradients(True)
```
to simply
```python
if self.accelerator.sync_gradients:
```
Can you check if we have the same behavior ? cc @muellerz  Nice! I feel this is a great simplification all around :)  Can you fix the merge comflits. We did a lot of [modification](https://huggingface.co/blog/gradient_accumulation) wrt to grad acc and it makes sense to have this now ! Feel free to ask us any question you have !  A lot better ! Thanks for your patience @kibitzing. If you are happy with the changes, feel free to merge the PR @muellerzr  Nice!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #33671


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@muellerzr and @SunMarc","We decided to perform the grad acc at the end of the dataloader but in trainer, we will set self.accelerator.gradient_state._set_sync_gradients by ourselves and not rely on the values set by accelerate.accumulate If I understand correctly, since we're setting `self.accelerator.gradient_state._set_sync_gradients` by ourselves in the trainer, would it be safer to keep it set to False? Okay, then I'll stick with the `do_sync_step` in the main branch code, and just replace the `total_batched_samples` with `step`. Yeah, let's keep it set to False !  that's right ! "
34413,2024-10-25T11:34:16Z,2024-10-28T18:44:20Z,alex-bene,3,1,4,78,3,1,1,"['Vision', 'Processing']",5119.0,0,525411.0,0,0,0,0,4862612.256944,,0,4,0,False,"['alex-bene', 'HuggingFaceDocBuilderDev', 'ydshieh']","Thanks!

Could you remove the following block, thanks!

(in `tests/models/glpn/test_modeling_glpn.py`)
```python
    @unittest.skip(reason=""Failing after #32550"")
    def test_pipeline_depth_estimation(self):
        pass

    @unittest.skip(reason=""Failing after #32550"")
    def test_pipeline_depth_estimation_fp16(self):
        pass
```

They pass now indeed. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34413). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker  ready to merge?Thanks, LGTM ","Thanks, LGTM ","# What does this PR do?

This PR adds a post_process_depth_estimation method for the image processor of GLPN (thus fixing its inference via the depth pipeline) and updates its docs.

Fixes #34300 and [comment](https://github.com/huggingface/transformers/pull/32550#issuecomment-2429461037) on #32550

## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).

## Who can review?
@ydshieh @qubvel @amyeroberts ","```suggestion
                depth = depth[None, None, ... ]
                depth = torch.nn.functional.interpolate(depth, size=target_size, mode=""bicubic"", align_corners=False)
                depth = depth.squeeze()
```
"
34354,2024-10-23T16:58:00Z,2024-10-30T17:49:47Z,yonigozlan,4,15,9,1584,12,3,2,"['Vision', 'optimization', 'Processing']",1578.0,0,678594.0,0,0,0,0,4862806.329516,,0,9,0,False,"['HuggingFaceDocBuilderDev', 'yonigozlan', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34354). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker Refactored DETR and RT-DETR image processor fast to loop as few times as possible over annotations and images, and added some docs! Does this improve the perf you saw? 😉  > Thanks for iterating with me! 🤗

Thank you!

> Does this improve the perf you saw? 😉

Hmm hard to tell, maybe very slightly when on GPU, as on CPU the potential gains are overshadowed by the processing time. But at least it's cleaner that way! :)Thanks! Just a few nits - saw you were removing the kwargs validation as well, can we also do that for detr?  Thanks for working on this! 
A few more nits, overall good, IMO all your graph should be placed in the documentation as well and not just on the PR description!  Thanks for iterating with me! 🤗 ","Thanks! Just a few nits - saw you were removing the kwargs validation as well, can we also do that for detr?  Thanks for working on this! 
A few more nits, overall good, IMO all your graph should be placed in the documentation as well and not just on the PR description!  Thanks for iterating with me! 🤗 ","# What does this PR do?
Adds a fast image processor for RT-DETR. Follows issue https://github.com/huggingface/transformers/issues/33810.
This image processor is a result of [this work](https://www.notion.so/huggingface2/OptimVision-Optimize-preprocessing-time-10f1384ebcac8091a12debb87fe5f591) on comparing different image processing method.

The diffs look bad but this PR is almost exclusively made up of `# Copied from` based on the fast image processor for DETR!

## Implementation
See https://github.com/huggingface/transformers/pull/34063

## Usage
Except for the fact that it only returns torch tensors, this fast processor is fully compatible with the current one.
It can be instantiated through AutoImageProcessor with use_fast=True, or through the Class directly:
```python
from transformers import AutoImageProcessor

processor = AutoImageProcessor.from_pretrained(""PekingU/rtdetr_r50vd"", use_fast=True)
```
```python
from transformers import RTDetrImageProcessorFast

processor = RTDetrImageProcessorFast.from_pretrained(""PekingU/rtdetr_r50vd"")
```

Usage is the same as the current processor, except for the `device` kwarg:
```python
from torchvision.io import read_image
images = torchvision.io.read_image(image_path)
processor = RTDetrImageProcessorFast.from_pretrained(""PekingU/rtdetr_r50vd"")
images_processed = processor(images , return_tensors=""pt"", device=""cuda"")
```
If `device` is not specified:
- If the input images are tensors, the processing will be done on the device of the images.
- If the inputs are PIL or Numpy images, the processing is done on CPU.

## Performance gains


- Average over 10% of the COCO 2017 validation dataset, with `batch_size=1`.

![benchmark_results_full_pipeline_rt_detr_fast_single](https://github.com/user-attachments/assets/0c88b8c2-49ae-487e-8d0e-9be01cae70d3)


---

- Average over 10% of the COCO 2017 validation dataset, with `batch_size=8`.


![benchmark_results_full_pipeline_rt_detr_fast_batched](https://github.com/user-attachments/assets/a0d31c96-1194-4f6b-abcf-e5ae8088ed6d)


---

## Tests
- The new image processor is tested on all the tests of the current processor.
- I have also added a consistency test for processing on GPU vs CPU.


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

## Who can review?

@ArthurZucker Pinging you directly as there is almost no ""new"" code here.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","`return_segmentation_masks` is unused here, could either reuse the one from `detr_fast` so we have a # Copied from statement here? ```suggestion
``` This was `False` in the init, any reason for the change? I copied what was done in the original rt_detr image processor, but I'm not sure why there is this change either Yes it's a bit weird, `return_segmentation_masks ` is present in several places, but rt-detr does not support segmentation. I added a copied from here, with an Ignore copy for the segmentation part (as otherwise we would need to import/copy a function that would never be used.

EDIT: actually the Ignore copy makes the CI crash, not sure why, so I left it as is for now...  ```suggestion
``` is it not torchvision here? 
 `# Ignore copy` does not work like this 😉  forget about it in this case!  I don't know if there are a lot of annotations, but we are iterating over them 5 times here when probably don't need no?  why not create the dict explicitly : 
```suggestion
    new_target = {
    ""image_id"" :image_id
""class_labels"": classes[keep]
""boxes"": boxes[keep]
""area"":area[keep]
""iscrowd"" :iscrowd[keep]
""orig_size"" : torch.as_tensor(
        [int(image_height), int(image_width)], dtype=torch.int64, device=image.device
    )
``` I don't know how many times we are gonna use these, but as they are not critical but more like utils, they could be inherited / imported from utils! there is again the recurring theme of iterating on the images a bunch of time. Now I don't know if you can avoid it, but if you can let's only iterate through them once! there are more efficient ways to do this 😉  no single letter variables please Yep, I'll reverse the loop order here and elsewhere to avoid iterating on image/annotations several times"
32550,2024-08-09T02:12:26Z,2024-10-22T13:50:54Z,alex-bene,21,30,24,658,13,3,0,"['Vision', 'Processing']",392471.0,0,7211907.0,0,0,0,0,4862627.761851,,0,24,0,False,"['qubvel', 'amyeroberts', 'HuggingFaceDocBuilderDev', 'alex-bene', 'ArthurZucker', 'ydshieh']","I'm not sure who to tag about this, but I have added to the `image_transforms.py` file the function `colorize_depth`, which, given a depth prediction, generates a colored PIL image or numpy image. If this is not the correct place for this function let me know. @NielsRogge Hey everyone, @NielsRogge @amyeroberts @Narsil, is there anything missing from this PR so that it can be merged? @amyeroberts let me know if there's anything else that needs changing. @amyeroberts kind reminder Linking to the outstanding comment here, in case it's been lost in review, as it's always hard to find again in github: https://github.com/huggingface/transformers/pull/32550#discussion_r1757124860 Hey @amyeroberts , I finished up with the last pending comments; let me know if it seems ok. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32550). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hey @amyeroberts , in the docs, I have added a code block and a link inside a ""tip"" block that did not render markdown assets as usual, as seen in the image. Do you have a good idea of how to fix this?

<img width=""821"" alt=""image"" src=""https://github.com/user-attachments/assets/a219ca4e-49a7-420d-9ad7-b6b653995f3c""> @alex-bene Tbh, I'm not sure. I'd suggest just removing the tip tags altogether -- the section follows on directly from the part above, so I don't think this will impact the message in the docs much  > @alex-bene Tbh, I'm not sure. I'd suggest just removing the tip tags altogether -- the section follows on directly from the part above, so I don't think this will impact the message in the docs much

Hey @amyeroberts , I was thinking of doing something like this if it makes sense:
```html
<Tip>
<p>In the <a href=""https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L131"">original implementation</a> ZoeDepth model performs inference on both the original and flipped images and averages out the results. The <code>post_process_depth_estimation</code> function can handle this for us by passing the flipped outputs to the optional <code>outputs_flipped</code> argument:</p>
<pre><code class=""language-Python"">&gt;&gt;&gt; with torch.no_grad():   
...     outputs = model(pixel_values)
...     outputs_flipped = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))
&gt;&gt;&gt; post_processed_output = image_processor.post_process_depth_estimation(
...     outputs,
...     source_sizes=[image.size[::-1]],
...     outputs_flipped=outputs_flipped,
... )
</code></pre>
</Tip>
``` > > @alex-bene Tbh, I'm not sure. I'd suggest just removing the tip tags altogether -- the section follows on directly from the part above, so I don't think this will impact the message in the docs much
> 
> Hey @amyeroberts , I was thinking of doing something like this if it makes sense:
> 
> ```
> <Tip>
> <p>In the <a href=""https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L131"">original implementation</a> ZoeDepth model performs inference on both the original and flipped images and averages out the results. The <code>post_process_depth_estimation</code> function can handle this for us by passing the flipped outputs to the optional <code>outputs_flipped</code> argument:</p>
> <pre><code class=""language-Python"">&gt;&gt;&gt; with torch.no_grad():   
> ...     outputs = model(pixel_values)
> ...     outputs_flipped = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))
> &gt;&gt;&gt; post_processed_output = image_processor.post_process_depth_estimation(
> ...     outputs,
> ...     source_sizes=[image.size[::-1]],
> ...     outputs_flipped=outputs_flipped,
> ... )
> </code></pre>
> </Tip>
> ```

You can always try and see how it renders :)  Yeah, it seems to have done the trick @amyeroberts -- I think we're good!

<img width=""916"" alt=""image"" src=""https://github.com/user-attachments/assets/a69df46b-a9c0-4465-a333-ab7e7dec9b58"">
 @alex-bene Great! Could you resolve the conflicts? Once done I think we're good to merge! 

cc @qubvel  @amyeroberts @qubvel I think we're done, but let me know if there's anything else pending. @qubvel is it now ready to merge? @qubvel @amyeroberts kind reminder @alex-bene thanks for ping!

cc @ArthurZucker for final review Ok thanks, merging as you answered 🤗  Hi @alex-bene Thank you for the contribution!

There are 2 failing tests after this PR is merged to `main`:

(GLPN model)
```bash
FAILED tests/models/glpn/test_modeling_glpn.py::GLPNModelTest::test_pipeline_depth_estimation - AttributeError: 'GLPNImageProcessor' object has no attribute 'post_process_depth_estimation'

FAILED tests/models/glpn/test_modeling_glpn.py::GLPNModelTest::test_pipeline_depth_estimation_fp16 - AttributeError: 'GLPNImageProcessor' object has no attribute 'post_process_depth_estimation'
```

Could you take a look? Thanks in advance.

[job run page](https://app.circleci.com/pipelines/github/huggingface/transformers/108316/workflows/035f3ea5-9d3e-4a61-b9dc-9e454b9a7e74/jobs/1439527)

 Hey @ydshieh , that's because this is a new model not available when we initially added the post processing interface for depth models. I'll try to make the necessary changes until ~tomorrow. Hey @ydshieh , the fix for this is on #34413 Thanks for all the investigation work and adding this logic! 

Most comments are about simplifying the added logic and bringing it more in-line with transformers. Overall looks great!  Thanks for the continued work on this! 

Mostly nits on formatting and docs ","Thanks for all the investigation work and adding this logic! 

Most comments are about simplifying the added logic and bringing it more in-line with transformers. Overall looks great!  Thanks for the continued work on this! 

Mostly nits on formatting and docs ","# What does this PR do?

This PR adds a `post_process_depth_estimation` method for the image processors of depth estimation models, similar to the `post_process_semantic_segmentation` methods for the segmentation models. Also, it updates the depth estimation pipeline to use the new `post_process_depth_estimation` method. Lastly, it adds full support for the ZoeDepth *special* inference (dynamically padded input + inference of the flipped of the input). A small update to the documentation is pending.

Fixes #30917 #32381 

## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?

## Who can review?
@NielsRogge @amyeroberts @Narsil","The post-processing methods for image processors should follow a similar pattern to the tokenizers i.e. accept and return the same array type (numpy arrays or torch tensors) e.g. [like here for batch_decode](https://github.com/huggingface/transformers/blob/cc25757a44d08cc7e3cca32554455cbcfc5db957/src/transformers/tokenization_utils_base.py#L3963). No need to bother with tensorflow tensors at the moment as we have so few TF vision models. We shouldn't be converting to PIL.Image.Image numpy should be returned if that's the input. However, pretty much all the inputs to this method will be torch tensors directly from the model. I'd suggest just implementing for torch tensors and we can add numpy support later if requested. 

```suggestion
``` Let's just let the user specify `vmin` or vmax` which they can derive from a percentile if they want instead of having two arguments which essentially control the same thing Let's just use one of these arguments, rather than have two than control the same thing. In this case I'd remove `invalid_val`  `gamma_corrected` makes it sound like it's flagging whether the input image has already been corrected or not 

```suggestion
        gamma_correction (`bool`, *optional*, defaults to `False`):
            Apply gamma correction to colored image.
``` ```suggestion
``` ```suggestion
``` ```suggestion
``` edit

```suggestion
>>> with torch.no_grad():
``` `value` isn't a great name as it's not descriptive. Let's use something like `depth_map` It's better to use longer, more descriptive names unless abbreviations are v. common and standardized e.g. ""mlp""

```suggestion
        vmin_percentile (`float`, *optional*, defaults to `1.0`):
            use the `vmin_perc`-th percentile as `vmin` (outlier rejection).
        vmax_percentile (`float`, *optional*, defaults to `99.0`):
            use the `vmax_perc`-th percentile as `vmax` (outlier rejection).
``` Let's remove the matplotlib dependancy here  ```suggestion
``` This is unnecessary functionality we should just remove  Is `gamma_corrected` needed for replication results from dpt or zoedepth? If not we should remove  Missing type hint (and for outputs) Order here doesn't match signature  * It's more pythonic to iterate over objects directly instead of indexing
* Library standard is to use clear, explicit var names instead of single letters e.g. `batch_size` instead of `b`

```suggestion
        for depth, target_size, source_size in zip(predicted_depth, target_sizes, source_sizes):
```

I would just create a dummy list for `source_sizes` if it's `None` e.g.:

```
source_sizes = [None] * len(target_sizes) if source_sizes is None else source_sizes 
```  From the code below - I'd have expected source_sizes to default to None and target_sizes to be required  What do 'fh' and 'fw' stand for?  We shouldn't have model-specific logic in the pipelines. The pipelines are for simple, one-line inference so people can get quickly get predictions. It's OK if they don't perfectly replicate results 

```suggestion
``` Same here - we don't want model-specific logic in the pipeline  To be consistent with other control flags

```suggestion
            do_remove_padding (`bool`, *optional*):
``` We can do that, however, this will remove some functionality from the function related to the output depth image colormap, e.g. ""magma"", ""inferno"" etc. We can still easily support ""gray"", and ""gray_r"" though.
This also relates to the internal type used for the inputs processing. If we use matplotlib for the added functionality I talked about, then its inputs should be np.arrays thus I should change the array type in the end to torch.tensor if that was the input. If we do not use matplotlib, then we can also have the internal processing be in numpy or torch depending on the input type. Should we still keep those in `post_process_depth_estimation` ?  Same as before, should we still keep those in `post_process_depth_estimation` ?  The reason I do not have a type here is that I got an error when I was importing `ZoeDepthDepthEstimatorOutput` (`from .modeling_zoedepth import ZoeDepthDepthEstimatorOutput`). Is there a suggested way to import this without getting errors? `source_sizes` is required because it is used to calculate the padding to be removed from around the depth map. `target_sizes` is only used to an additional resizing of the result (and also to match the rest of the `post_process_...` functions. These are parameters from the original implementation and are used in the same file to calculate `pad_h` and `pad_w`. For the implementation in transformers, those parameters have been already set to `3` by default in the pre-processor I don't think we want the matplotlib dependency indeed, just the ""raw"" depth map (as is currently the case for the pipeline).

If a user wants to visualize it, then he can use any visualization framework he/she prefers, like Matplotlib, OpenCV, etc."
34063,2024-10-10T11:11:41Z,2024-10-21T13:05:05Z,yonigozlan,4,22,8,3948,16,3,1,"['Vision', 'optimization', 'Processing']",383083.0,0,1822526.0,0,0,0,0,4862854.663545,,1,8,0,False,"['SangbumChoi', 'HuggingFaceDocBuilderDev', 'yonigozlan']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34063). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @yonigozlan Hi, super excited to see this. just left two questions
1. Figure says it is `RTDetr` but this is `Detr` right?
2. I think this can be also adapted to all vision processor?
3. Can you also elaborate the computing device? Hi @SangbumChoi ! 

  1. Oops, thanks for pointing this out, yep it's a typo, this is indeed Detr (but I'm also working on a fast image processor for RTDetr :) ).
  2. Yes absolutely this is the plan. There are quite a few other image processors based on the Detr one so their fast image processor should be out soon.
  3. This was on a A10 GPU!  @yonigozlan thanks! I would love support this task if you need some help. Also if you can share the comparison script for compile + visualize plot etc... it would be very helpful!Very nice work! I added a couple comments, in addition to one also on the page you linked rel to performance of .to(device).to(dtype) VS .to(device, dtype), finding the second one to be always faster when moving to a cuda device, fwiw  LGTM in general, but having 1600 LOC for an image processor is quite intense! 
Great performance gains tho
","Very nice work! I added a couple comments, in addition to one also on the page you linked rel to performance of .to(device).to(dtype) VS .to(device, dtype), finding the second one to be always faster when moving to a cuda device, fwiw  LGTM in general, but having 1600 LOC for an image processor is quite intense! 
Great performance gains tho
","# What does this PR do?
Adds a fast image processors for DETR. Follows issue #33810.
This image processor is a result of [this work](https://www.notion.so/huggingface2/OptimVision-Optimize-preprocessing-time-10f1384ebcac8091a12debb87fe5f591) on comparing different image processing method.

The processing methods use only [torchvision transforms](https://pytorch.org/vision/stable/transforms.html) (either v1 or v2, depending on the torchvision version) and torch tensors.
Just like the current DETR image processor, this processor can also process object detection or segmentation annotations. This processing also uses only torch tensors and torchvision transforms.
The post-processing methods have not been modified from the original image processor.

## Implementation
A previous fast image processor implementation for VIT ([link](https://github.com/huggingface/transformers/blob/c9afee539204f5e658d03e63a1df3aacb4cab305/src/transformers/models/vit/image_processing_vit_fast.py#L50)) uses torchvision transform classes and `Compose` to create a one step processing class. However this poses two problems:
- The torchvision v2 Transforms are only torch compile/scripting compatible in their functional forms and not in their Class form ([source](https://pytorch.org/vision/stable/transforms.html#torchscript-support)).
- A one step processing class is not possible when the processing depends on the input, like it's the case for DETR for resizing and padding.

So this implementation uses the functional forms of torchvision transforms, and it's structure is very similar to the current DETR image processor.

All the numpy/PIL operations have been converted to torch or torchvision operations, and like the VIT fast image processor, this processor only accept `return_tensors = ""pt""`

The processor call function accept a `device` kwarg, as processing can be performed on both CPU and GPU, but is much faster on GPU. 
I wanted to add device as an `init` argument, but that would make the signatures of fast and slow processors different, which make some tests fails.

## Usage
Except for the fact that it only returns torch tensors, this fast processor is fully compatible with the current one.
It can be instantiated through AutoImageProcessor with use_fast=True, or through the Class directly:
```python
from transformers import AutoImageProcessor

processor = AutoImageProcessor.from_pretrained(""facebook/detr-resnet-50"", use_fast=True)
```
```python
from transformers import DetrImageProcessorFast

processor = DetrImageProcessorFast.from_pretrained(""facebook/detr-resnet-50"")
```

Usage is the same as the current processor, except for the `device` kwarg:
```python
from torchvision.io import read_image
images = torchvision.io.read_image(image_path)
processor = DetrImageProcessorFast.from_pretrained(""facebook/detr-resnet-50"")
images_processed = processor(images , return_tensors=""pt"", device=""cuda"")
```
If `device` is not specified:
- If the input images are tensors, the processing will be done on the device of the images.
- If the inputs are PIL or Numpy images, the processing is done on CPU.

## Performance gains

### Main Takeaways
#### Processing speedup
- **~60x faster processing on GPU (single image)**
- **~80x faster processing on GPU (batch_size=8)**
- **~5x faster processing on CPU (single image)**
- **~2.6x faster processing on CPU (batch_size=8)**
#### Inference pass speedup (GPU)
- **~2.2x speedup on whole model inference pass (single image, eager)**
- **~3.2x speedup on whole model inference pass (single image, compiled)**
- **~2.4x speedup on whole model inference pass (batch_size=8, eager)**

---

- Average over 100 runs on the same 480x640 image. No padding needed, as ""all"" the images have the same size.

![benchmark_results_full_pipeline_detr_fast](https://github.com/user-attachments/assets/46129c0c-02ac-485e-9211-afeb68e5fe22)

---

- Average over 10% of the COCO 2017 validation dataset, with  `batch_size=8`. Padding needed, as the images have different sizes, and the DETR processor resize them using ""shortest_edge""/""longest_edge"", resulting in different sized resized images.

![benchmark_results_full_pipeline_detr_fast_batched](https://github.com/user-attachments/assets/d3c7136d-3f43-45d6-b25f-52e28d36c3b8)

---

- Average over 10% of the COCO 2017 validation dataset, with `batch_size=8`. Forcing padding to 1333x1333 (=""longest_edge""), as otherwise torch.compile needs to recompile if the different batches have different max sizes.
(I'm not sure what is going wrong when using the compiled model with the current processor)

![benchmark_results_full_pipeline_detr_fast_batched_compiled](https://github.com/user-attachments/assets/1a2b82a9-44e9-4084-b29f-c072137a1e59)

---

- Average over 10% of the COCO 2017 validation dataset, with `batch_size=1`. Forcing padding to 1333x1333 for comparison with batched inputs

![benchmark_results_full_pipeline_detr_fast_padded](https://github.com/user-attachments/assets/b80b0c77-d81e-40a1-a765-82827f6f24d7)

---

## Tests
- The new image processor is tested on all the tests of the current processor.
- I have also added two consistency tests (panoptic and detection) for processing on GPU vs CPU.


---

Looking forward to your feedback!
I was also wondering if we should adopt a more modular approach to the fast image processors, as there is quite a lot of repetition with the ""slow"" processor for now. It looks like something like this was done for Fast tokenizers? If someone that worked on Fast tokenizers has any advice on that I'll gladly hear them 🤗.
There will also be the question of how to advertise this ""use_fast"" option to users, and if we want to make it default eventually when torchvision is available?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Hmm, I guess this is to support PIL's interpolation mode that is an enum - can't we reuse that rather? This method doesn't have any tensor <-> array change right?
```suggestion
# Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio
``` :rocket: 
```suggestion
    Constructs a fast Detr image processor.
``` Copied from Detr slow image processor I think and btw (for later) these methods should be slightly :broom: :broom: rel to variable naming for oh + ow  All methods below are # Copied from Detr image processor more readable refactor - but maybe we can access it in a more explicit way than ratios[0] and ratios[1]?  we can use the deprecate_kwarg util here Dove a bit here, nice refactor and improvement - I think if we can move to a more modular way of building processors, as we try to do with models, it'll reduce loc count greatly and improve readability. A `modular_image_processing_detr_fast` that inherits from the necessary funcs and then build automatically the `image_processing_detr_fast` would be amazing for instance, cc @ArthurZucker for visibility (not a requirement on this PR imo).  :white_check_mark:  It's interesting the git diff is so large here - unless I'm wrong we are generalizing the tests to integrate a list of processors, fast and slow, but the tests do not change? Yes I'm guessing it's because of the change in indentation, but all I changed is testing a list of processors instead of only one Yes I was thinking about this too, because the loc count is huge compare to the actual changes. I might be wrong but it looks like something similar was done to reduce loc count for fast tokenizers? Not sure who to ping here for clarifications Oh yes you are right, and I just realized that something like this is already defined here:
https://github.com/huggingface/transformers/blob/fa3f2db5c7405a742fcb8f686d3754f70db00977/src/transformers/image_utils.py#L60-L67 Yes indeed. I'm not sure if I should import those functions directly from `image_processing_detr.py` instead of using `# Copied from` at this point, wdyt? We typically don't import from another file, but it'd be nice to do it yes - in general we should be able to import from an `...utils` file (say `processing_utils.py`, `image_processing_utils.py`,   `image_processing_utils_fast.py` ) more generic methods instead of redefining them for each model. cc @ArthurZucker for this pattern type as we discussed it before I agree but using it causes the `test_image_processor_preprocess_arguments` to fail. I'm guessing this tests does not work well with the way decorators calls the function and the arguments Yeah! Kind of like we have a `PreTrainedModel` we can have a `ImageProcessorMixin` that would be a bit more powerfull! 

For fast tokenizers, we just rely on the `tokenizers` library directly, so it's a bit different Yep I think it's good to think about how we can refactor a bit! This is kind of void of copied from, which I am a bit suprised about! Most methods are very similar to the ones in image_processing_detr, but slightly changed to work with tensors instead of numpy arrays. I haven't changed the post-processing methods at all (as they are not as much of a speed bottleneck), so they are all added with copied from (end of the file)  ah okay, got it! "
34291,2024-10-21T19:05:52Z,2024-10-29T15:09:18Z,abhi-glitchhg,6,9,19,49,7,4,2,[],234446.0,0,840455.0,0,0,0,0,4866075.764498,,0,19,0,False,"['HuggingFaceDocBuilderDev', 'abhi-glitchhg', 'ArthurZucker']","cc @molbap  Hi, thanks for the reply. I was waiting for confirmation if this was indeed a bug or intended behaviour. I will fix the lint and other errors.  
Thanks.
abhijit In the #33974 , @ArthurZucker mentioned the messy initialisation of swinlayer class, i would not like to touch it in this pr. 

Personally i think  the initialisation looks good. But if you think we should make it simpler, i m happy to tackle it in another pr ```
FAILED tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py::DeiT2RobertaModelTest::test_encoder_decoder_model_generate - AssertionError: torch.Size([13, 21]) != (13, 20)
FAILED tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py::ViT2BertModelTest::test_encoder_decoder_model_generate - AssertionError: torch.Size([13, 21]) != (13, 20)
FAILED tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py::Swin2BartModelTest::test_encoder_decoder_model_generate - AssertionError: torch.Size([13, 21]) != (13, 20)
FAILED tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py::ViT2TrOCR::test_encoder_decoder_model_generate - AssertionError: torch.Size([13, 21]) != (13, 20)
FAILED tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py::LayoutLMv32TrOCR::test_encoder_decoder_model_generate - AssertionError: torch.Size([13, 28]) != (13, 20)
FAILED tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py::VIT2GPT2Test::test_encoder_decoder_model_generate - AssertionError: torch.Size([13, 27]) != (13, 20)
FAILED tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py::Donut2GPT2Test::test_encoder_decoder_model_generate - AssertionError: torch.Size([13, 27]) != (13, 20)
```

Hmm, these tests are failing in CI. 

Edit: I guess these were unrelated to my changes, merged the main branch and these errors have gone
 The CI is green. Yey! ig now the pr is ready for review @molbap 

Btw i really loved the infra for CI/CD, tests run fast! Linting tools work amazingly!  I also recently watched @ArthurZucker's  pytorch conference talk and now i **really** understand the pain points mentioned in the video. 

Thank you guys for maintaining such a high-impact library!
 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34291). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Hi and thanks for the PR! I think this is a work in progress? If sofeel free to pass the PR in `draft` mode or name it [WIP], and ping me again when you want another review! I think you correctly identified this bug :) when you're done, you can run `make fixup` to run the linter and make `check doe quality` happy in the CI Thanks, it's cleaner! left a couple comments, let me know what you think LGTM - revamp of layers init to depend on (config, layer_idx) TBD in a follow-up PR! cc @ArthurZucker for final review Nice and simple! Thanks 🤗 ","Hi and thanks for the PR! I think this is a work in progress? If sofeel free to pass the PR in `draft` mode or name it [WIP], and ping me again when you want another review! I think you correctly identified this bug :) when you're done, you can run `make fixup` to run the linter and make `check doe quality` happy in the CI Thanks, it's cleaner! left a couple comments, let me know what you think LGTM - revamp of layers init to depend on (config, layer_idx) TBD in a follow-up PR! cc @ArthurZucker for final review Nice and simple! Thanks 🤗 ","# What does this PR do?

This PR fixes #33974 . 


As i had mentioned in the issue, I feel that swin transformer implementation has incorrect implementation of stochastic depth decay.


According to the official implementation, drop_prob for every SwinLayer is different. 

https://github.com/microsoft/Swin-Transformer/blob/f82860bfb5225915aca09c3227159ee9e1df874d/models/swin_transformer.py#L544

https://github.com/microsoft/Swin-Transformer/blob/f82860bfb5225915aca09c3227159ee9e1df874d/models/swin_transformer.py#L558

https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py#L397-#L408

But in transformers, we were using a constant value that is picked from the config file. I feel that implementations in transformers should be closer to the official ones.  This also applies for the SwinV2 model.  (and maybe swin2sr as well)

Please do look into this and let me know. Also i have changed the variable names as well. I am very bad at naming, so any suggestions for the argument names are welcome 😄 

 

- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. -> #33974 

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@amyeroberts, @qubvel @ArthurZucker 
","self.num_layers will be undefined for this class, will do the change required you can pick the number of layers from the `config` object! I think this `dpr` is undefined?  ```suggestion
``` this is all because of ruff linter nice fix - and aligned with hiera & focalnet which also have a varying drop_path per layer iirc As mentioned in the other linked PR I think it'd be nice to refactor the init of all of these layers - the reason is that we're trying to have not too many args, and here we're adding yet another one. `input_resolution`, `num_heads` etc can all be derived from `config` and `layer_idx` (albeit through `grid_size` for `input_resolution`). We should modify this in a follow-up PR, or in this one if you feel up to it!  happy to do it follow-up pr sounds good then!"
34226,2024-10-17T16:39:30Z,2024-10-30T19:37:39Z,anshumangahlot,2,6,4,81,1,1,1,['Documentation'],1120604.0,0,1133889.0,0,0,0,0,4927024.127461,,0,4,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']","cc @stevhliu The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34226). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Cool, thanks for updating! Left a few comments about formatting, otherwise LGTM! 🤗 

Make sure to also rebase on main to fix the failing CI test","Cool, thanks for updating! Left a few comments about formatting, otherwise LGTM! 🤗 

Make sure to also rebase on main to fix the failing CI test","This PR introduces a guide for translating the Transformers documentation into multiple languages as part of the project’s effort to make machine learning more accessible globally.

### Key Changes:

- **New Translation Instructions**: Added detailed step-by-step instructions for contributing translations, covering everything from opening issues to forking the repository, and translating relevant files.

	

- **File Organization**: Included a guide on how to copy the English documentation to a new language folder using ISO 639-1 or ISO 639-2 language codes.

	

- **_toctree.yml Guidance**: Provided instructions on editing or creating the _toctree.yml file to correctly render the documentation’s table of contents for the new language.

	

- **Collaboration Encouragement**: Emphasized the importance of collaboration, inviting contributors to share glossaries and tag team members for assistance.

### Files Modified:

- Added docs/source/translation_guide.md: This file contains the full translation guide, with detailed instructions on getting started, collaborating, and submitting translations.

**Additional Notes:**

- The guide encourages new contributors to open issues before starting a translation, ensuring no duplicate efforts.

- Resources and tips for effective translation are included to help users maintain consistent terminology.

- I’ve added links to relevant documentation, including language code references and GitHub commands.
","Not necessary to capitalize the other letters here ```suggestion
## Open an Issue
``` ```suggestion
## Fork the Repository
``` ```suggestion
## Copy-paste the English version with a new language code
``` ```suggestion
## Start translating
``` ```suggestion
## Collaborate and share
```"
34440,2024-10-27T02:06:15Z,2024-10-30T15:55:16Z,fzyzcjy,1,0,1,2,1,1,1,[],162438.0,0,308942.0,0,0,0,0,4940366.609018,,0,1,0,False,['fzyzcjy'],You are welcome!Thanks!,Thanks!,"# What does this PR do?

Hi thanks for the library! When using the Trainer (and TrainingArguments) I see an extra space. Thus this super very tiny PR :)

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34465,2024-10-28T13:19:09Z,2024-10-30T13:54:10Z,ydshieh,2,0,2,4,2,1,1,[],1599.0,0,174903.0,0,0,0,0,4947633.149038,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34465). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. OKThanks, I think waiting for 2.5.1 might be better as there are fixes for mixed precision ˜I think! ","Thanks, I think waiting for 2.5.1 might be better as there are fixes for mixed precision ˜I think! ","# What does this PR do?

It's time to switch.",
34200,2024-10-16T18:41:26Z,2024-10-30T15:52:17Z,VladOS95-cyber,2,13,6,95,4,3,3,[],27.0,0,1199452.0,0,0,0,0,4940547.183847,,0,6,0,False,['VladOS95-cyber'],"Hi @SunMarc! This PR is ready for review, please, take a look. Hey @SunMarc! Do you have any questions or comments regarding this PR?Thanks for your work ! Left a question  Thanks for digging into that ! LGTM !  LGTM! Thanks a lot @VladOS95-cyber ",Thanks for your work ! Left a question  Thanks for digging into that ! LGTM !  LGTM! Thanks a lot @VladOS95-cyber ,"# What does this PR do?

Add GGUF support for Mamba

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Link: https://github.com/huggingface/transformers/issues/33260
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Regarding the task @SunMarc @LysandreJik @ArthurZucker.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","what do you mean there is no ""lm_head"" ? We should have the same architecture no ?  Hey @SunMarc! If I am not mistaken, it is because of this logic in tensor modification in https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py: (L2979-2985)                                                      
 `# assuming token_embd.weight is seen before output.weight`
        `if self._tok_embd is not None and new_name == output_name:`
            `if torch.equal(self._tok_embd, data_torch):`
                `logger.debug(f""{output_name} is equivalent to {tok_embd_name}, omitting"")`
                `return []`
       ` elif new_name == tok_embd_name:`
            `self._tok_embd = data_torch`
It just does not include `output.weight` (which is `lm_head.weight` eventually) in quantization.
I would like to say that it is not the only one situation where quantized model does not have lm_head.weight. For example, Starcoder2 does not have it either. They removed the lm_head since it is the same of the embedding weights. This is also done in the original model. You can check the safetensors file. Still after loading the model, we should have the `lm_head`. This is linked to `tie_word_embeddings = True` in the config file. The default value is `True`. Would you like to have a look at what's happening and potentially fix the issue for starcoder2 also ? Thanks !  We should have the same layers. If a layer is not in converted_state_dict, we should raise an error in this test instead.  Hey @SunMarc! So, in this case, if lm_head has the same weights as embedding layer, and if gguf file does not contain output.weight which is lm_head we should create this tensor explicitly using the same weights from embedding weights then?  That's right. In transformers code, we link the lm_head to the embedding weights.  Ok, got it, thanks! I'll fix that Hey @SunMarc! I added lm_head layer creation for Mamba and Starcoder2 and did some unit tests refactoring, please, take a look We shouldn't have to create the layer no ? when calling automodelforcausallm, we should have them by default. Also I think you pushed the wrong branch. I don't see the modification Hey @SunMarc, yes, sorry, small mess happened. Now you should see latest changes.  So if I understand, if we don't set the tensor, the lm_head won't be created or just that there is no weights ? Could you print the model to see the architecture ? We have the function tie_weights() to tie the weights of the lm_head and the embedding. Maybe that could be useful ?  @SunMarc, No, you were right, I was confused a little bit by keys comparison between original model and gguf one and I saw that tie.weights() was performed on original model, but later, actually, it is performed on gguf too. So, we definetely do not need to create anything explicitly. I fixed my latest changes, now, everything should be ok Nice ! "
34486,2024-10-29T10:50:00Z,2024-10-30T13:17:20Z,molbap,3,7,21,83,5,2,1,['run-slow'],1577.0,0,95242.0,0,0,0,0,4949843.973557,,0,21,0,False,"['molbap', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34486). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Tests (nonslow, just launched the slow ones) are green - there was a mismatch in the tests between images per batch and images per sample which I think I fiixed, added a test as well for previous issue. cc @ArthurZucker for review all green incl. slow now - fixed slow tests that were broken before (well one fix for a wrong config key at init, one that actually can't work, and a skip for torchscript) lmk! Thanks, LGTM let's not revert integration test Good to go ! Thanks 🤗 ","Thanks, LGTM let's not revert integration test Good to go ! Thanks 🤗 ","# What does this PR do?

Should fix https://github.com/huggingface/transformers/issues/34204 . 
","why did this one have to go away ? 😓  many reasons, it was looking up an image url that did not exist (pixtral-vl instead of llava-vl) to predict something that pixtral model does not predict, also it was testing `VisionModel` that does not support `generate` Yep it needs to be in the `Llava` tests! Can you move it around 👀  sure! https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/tests/models/llava/test_modeling_llava.py#L620-L663 

looks like pixtral is already tested under llava tests properly - since it's a llava model it's enough to test it there, no?  ah then good job and sorry! no worries- it's hard to see when tests are measuring a model through another one, but it's convenient on our side. I can move all pixtral related tests to pixtral testing so we don't forget it, wdyt? It's a bit more loc but it's easier to track down"
34334,2024-10-23T07:20:28Z,2024-10-30T09:11:50Z,zucchini-nlp,1,4,4,19,1,2,1,[],179735.0,0,611483.0,0,0,0,0,4964576.434525,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34334). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34304 and adds info about lm-head resizing. Maybe also fixes https://github.com/huggingface/transformers/issues/33819?","This should already be done internally if you use the correct flag for resize token embedding  from what I see we don;t let users to specify which embeddings to resize and use `input_embeddings` by default. In case weights are tied (not case of mllama) we also resize output embeddings

Or you mean there is another method similar to `resize_token_embeddings`? Might have overlooked that We use the output of `get_input_embeddings` which should always be the input embedding and by default the output embedding from `get_output_emebdding` is resized when you are tied. But you are right, you can't only resize the lm head. 

Tho ther might be some util function you can re-use no? 🤗 Feel free to merge! right, there was a way to hide all the ugly code in private methods"
34493,2024-10-29T15:36:28Z,2024-10-30T09:03:42Z,gpetho,0,0,5,4,1,1,1,[],,0,62834.0,0,0,0,0,4965065.996035,,0,5,0,False,[],LGTM thanks 🤗 ,LGTM thanks 🤗 ,This PR fixes issue https://github.com/huggingface/transformers/issues/34437,
34464,2024-10-28T11:21:49Z,2024-10-30T10:59:08Z,gante,3,24,17,2609,46,4,3,[],1653.0,0,171443.0,0,0,0,0,4958136.622514,,0,17,0,False,"['HuggingFaceDocBuilderDev', 'gante', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34464). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. For tests where `is_flaky` is removed, you ran it with `--flake-finder --flake-runs 500` right? (at least for some models ?) > For tests where is_flaky is removed, you ran it with --flake-finder --flake-runs 500 right? (at least for some models ?)

@ydshieh yes :D The pattern was actually added in a recent PR: https://github.com/huggingface/transformers/pull/34386Great clean up, good to see redundant code killed! Left a few questions and also I think we had `test_embeds_match_ids_generate` overwritten in all VLMs so prob those can be removed now

Overall LGTM, thanks

EDIT: forgot to mention that a few models might not be running the new tests because they don't have `GenerationTesterMixin`. Working on adding that on a few VLMs currently and noticed that tests from here might fail for them. Guess it's okay, i'll skip/fix the failing ones after enabling generation tests Long due! Thanks everyone for reviewing and @gante for the cleanup! ","Great clean up, good to see redundant code killed! Left a few questions and also I think we had `test_embeds_match_ids_generate` overwritten in all VLMs so prob those can be removed now

Overall LGTM, thanks

EDIT: forgot to mention that a few models might not be running the new tests because they don't have `GenerationTesterMixin`. Working on adding that on a few VLMs currently and noticed that tests from here might fail for them. Guess it's okay, i'll skip/fix the failing ones after enabling generation tests Long due! Thanks everyone for reviewing and @gante for the cleanup! ","Moves `generate` tests incorrectly placed in the general mixin to `GenerationTesterMixin`. In the process, removes redundant tests and streamlines repeated logic 👀 

⚠️ reviewer: start by reviewing `test_modeling_utils.py`, the last file in the diff. In it, I explain what happened to each test and why -- most were actually deleted, as they were redundant.

After this PR:
✅ fewer redundant tests and less code duplication
✅ fewer flaky tests
✅ faster tests
✅ more test coverage

In a follow-up PR:
👉 Fix failing upgraded generate+FA2 test
👉 Fix failing added generate+torch.compile test

Closes #32913","(changes in this file were done for end-to-end compilation) Many `generate` tests wanted to check equivalence of something, and were incorrectly tagged as flaky. This function contains the correct equivalence check -- as a result, several `@is_flaky` were removed :D

(I honestly don't know why I haven't implemented this before, instead of adding `@is_flaky` 🙃 ) We actually don't need `main_input`, we can obtain the batch size directly from the many outputs of `generate` Related to the deleted `inputs_embeds` tests -- clarifies that this test also runs for VLMs Merges this existing test, which checked the static cache returned by generate, with the deleted static cache equivalence test.

One generate call is enough to run both checks :) Instead of having a test for `torch.compile` against large checkpoints, let's do it against dummy checkpoints -- the importance of the test is to confirm a) we can compile AND b) compiled is equivalent to non-compiled.

Testing `model.forward` compilation is very similar to testing `model.generate` compilation, so `parameterized` is used. A few modifications were added to make the test more efficient (e.g. compile only once)

Note: this test doesn't have to be slow, but we currently have a few failures. Let's fix them in a follow-up PR The test to check that SDPA = eager is the same as the test to check that FA2 = eager. However, parameterized doesn't work here: we want different `pytest.mark` decorators for each test. As such, the original test was moved to a helper function. We already had a test that checks that generating from `inputs_embeds` is equivalent to generating from `input_ids` -- `test_generate_from_inputs_embeds`

This test was deleted. We have:
- a test that checks that we can generate with left-padding (`test_left_padding_compatibility`)
- a test that checks that we can generate with FA2 (`test_eager_matches_fa2_generate`, added in this PR)
- a test that checks that we can run the forward pass in FA2 with left-padding (`test_flash_attn_2_inference_equivalence`)

As such, this test was redundant. From `generate`'s point of view, FA2 is a modification to `model.forward`, so if `model.forward` is equivalent then so is `model.generate`

Deleted. Similar as above (we have individual checks in place). Moreover, generating with right-padding doesn't make sense on most models :D 

Deleted. This test was pretty toothless -- it didn't even check the actual output of `generate`.

I've added `test_eager_matches_fa2_generate` in its place, which checks the outputs. It's a clone of the SDPA generate equivalence test, except that it uses FA2. Similar to the FA2 + padding tests above.

In this case, we check that we can reuse a cache in `test_generate_continue_from_past_key_values`, no need for a FA2-specific test. 

Deleted. Merged the checks in this test [check that the outputs are the same] with the checks in `test_generate_with_static_cache` [check that the shapes are as expected] This test was running against very large checkpoints, and we had to manually specify one in `_torch_compile_test_ckpt` 👀 

We were indeed missing a mixin test for `generate` with `model.forward` compiled, see the new `test_generate_compile`. The most important part of this test is to confirm that compiled == not compiled, we have checks for other parts of the problem (including checks to see whether large checkpoints are working as expected)

This heavy test was, in practice, deleted Same as above (uses heavy checkpoints)

We now have a benchmarks page, much better to track performance than a test. one q: afaik the other test is skipped for encoder-decoder models? Can we enable it just to be sure we cover all models with generation abilities? This was testing that FA2 with packing works in generate, and unfortunately we don't have equivalent forward test. We could extend the `test_fa2_position_ids test` with this specific case Good point, thank you for catching that! I'll update `test_generate_from_inputs_embeds` so as to support VLMs ~Makes sense, I'll make sure there is one packing FA2 test (testing the forward pass, and not `generate` -- `generate` does not support packing)~ > RUN_SLOW=1 python3 -m pytest -v tests/models/llama/test_modeling_llama.py -k ""test_generate_compile""

gives me 

> ling_llama.py::LlamaModelTest::test_generate_compile_1_end_to_end - RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run. Stack trace: File ""/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py"", line 38, ...


 ✅ `test_generate_from_inputs_embeds` updated for VLMs, overwrites deleted. @zucchini-nlp have a look at the updated test -- you might need to add VLMs to the list of added special cases in the test @ydshieh have a look at the `TODO` in the `@parameterized` of this test -- both parameterizations have broken cases, whose cause comes from outside this PR.

This PR is already quite big, I'd rather fix the root causes of both parameterizations in a follow-up PR 🤗 This PR moves the tests to the right place, and doesn't touch modeling code @zucchini-nlp actually `test_flash_attn_2_fp32_ln` covers the case without attention mask, no need to add more test cases ah ok!"
34425,2024-10-26T02:03:41Z,2024-10-30T08:36:46Z,guangy10,3,0,2,43,1,2,2,"['run-slow', 'ExecuTorch']",304013.0,0,369185.0,0,0,0,0,4966683.533206,,0,2,0,False,"['qubvel', 'HuggingFaceDocBuilderDev', 'guangy10']","I left comments in https://github.com/huggingface/transformers/pull/34475 that are relevant for this PR too, let me know WDYT? > I left comments in #34475 that are relevant for this PR too, let me know WDYT?

@qubvel Comments addressed in both PRs The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34425). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks ! Thanks!",Thanks ! Thanks!,"# What does this PR do?

Roberta is ExecuTorch compatible.

Unit Test
`RUN_SLOW=1 pytest tests/models/roberta/test_modeling_roberta.py -k test_export -v`
```
tests/models/roberta/test_modeling_roberta.py::RobertaModelIntegrationTest::test_export PASSED                                                                                          [100%]
```

E2E test in ExecuTorch
Patch https://github.com/pytorch/executorch/pull/6509 
`python -m extension.export_util.export_hf_model -hfm=""FacebookAI/roberta-base"" -lm=""masked_lm""`
```
Saved exported program to ./roberta.pte
```
`./cmake-out/backends/xnnpack/xnn_executor_runner --model_path roberta.pte`
```
I 00:00:00.109478 executorch:executor_runner.cpp:82] Model file roberta.pte is loaded.
I 00:00:00.109501 executorch:executor_runner.cpp:91] Using method forward
I 00:00:00.109503 executorch:executor_runner.cpp:138] Setting up planned buffer 0, size 15620352.
I 00:00:00.175029 executorch:executor_runner.cpp:161] Method loaded.
I 00:00:00.175050 executorch:executor_runner.cpp:171] Inputs prepared.
I 00:00:00.295228 executorch:executor_runner.cpp:180] Model executed successfully.
I 00:00:00.295249 executorch:executor_runner.cpp:184] 1 outputs:
Output 0: tensor(sizes=[1, 64, 50265], [
  16.5879, -5.25161, 20.0616, -5.65803, 15.4071, 8.26069, 8.98625, 8.78171, 7.48042, 8.91317,
  6.00829, 8.17103, 8.48503, 8.62202, 5.53155, 5.49756, 5.09422, 7.248, 6.07587, 7.18935,
  8.06838, 3.08847, 9.98008, 8.40343, 7.12966, 6.48628, 5.33241, 5.92863, 5.37339, 7.89109,
  6.99939, 5.9172, 3.59078, 4.93762, 3.97946, 8.68409, 10.6187, 6.07593, 8.58189, 4.42053,
  1.91258, 3.9684, 3.2536, 8.25442, 7.25367, 6.57186, 5.89913, 4.0703, 6.80745, 3.6514,
  7.43827, 4.52306, 5.09739, 6.20495, 4.08807, 4.82672, 4.47396, 3.85133, 1.97933, 3.59948,
  2.08338, 3.59931, 5.50763, 3.78655, 1.23716, 3.04796, 5.43055, 3.84411, 8.29853, 6.54947,
  4.42576, 4.48108, 6.27531, 7.78458, 2.88165, 6.08975, 6.0315, 2.08121, 3.42374, 6.22524,
  5.07051, 4.25636, 1.95587, 2.93666, 3.4033, 7.57293, 5.60298, 3.42511, 4.89115, 5.73852,
  5.88404, 5.91935, 2.0786, 5.62394, 3.55468, 1.60399, 7.11961, 5.17408, 4.68167, 4.39651,
  ...,
  -2.30653, -0.993726, -1.52527, -0.210215, -3.31757, -0.429947, -1.09775, -1.0864, -2.42128, -0.966061,
  -1.29405, -3.63411, 0.521034, -4.05964, -1.3553, -1.00719, -2.55122, -2.55572, -0.884924, -0.995036,
  2.63108, 0.277828, -0.956355, 2.10707, 0.445026, -0.647644, -1.52773, -2.82471, 0.285436, -2.86462,
  1.486, -1.24808, -2.69333, -0.898781, -0.363611, -0.440042, 0.290613, -2.46375, 0.0467077, -1.67636,
  -1.78017, -0.534887, -0.840572, -0.841681, 0.203331, -1.8875, -0.707412, -1.04936, -2.30359, -1.30266,
  -0.842846, -1.84217, -0.338074, -2.47592, -0.244073, -0.906605, 0.330401, -1.50474, -0.97063, -1.35738,
  -1.49982, -1.21406, -0.134308, -1.63609, -0.708895, -1.05977, -2.13565, -1.86365, -1.1758, -0.780443,
  0.914404, 1.55187, -1.61172, -5.54678, 1.74232, -1.38653, -0.71281, -0.163013, -1.4589, -0.548585,
  -2.33784, -0.12623, -1.24065, -1.42141, -0.534155, -1.93391, -1.22874, 0.372179, 1.15677, -0.0187027,
  0.293305, -0.949133, -2.06938, -0.898005, -2.56974, -0.92074, -1.57755, -2.57184, -1.35508, 6.52663,
])
```

## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. #33841
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker
@qubvel
",
34382,2024-10-24T15:13:54Z,2024-10-29T18:45:15Z,Rocketknight1,3,3,3,50,7,3,2,[],46.0,0,444682.0,0,0,0,0,5016574.038983,,0,3,0,False,"['Wauplin', 'HuggingFaceDocBuilderDev', 'Rocketknight1']",cc @LysandreJik @Wauplin for review! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34382). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. (x-linking https://github.com/huggingface/transformers/pull/34290#issuecomment-2432768995 where this PR was discussed)Thanks @Rocketknight1! Looks good. Let's wait for another review though Yes! Thanks @Rocketknight1 ,Thanks @Rocketknight1! Looks good. Let's wait for another review though Yes! Thanks @Rocketknight1 ,"As part of the pipeline sync project, this PR allows some arguments in `transformers` that don't have to be in the Hub spec. This is limited to arguments that do not actually affect the core pipeline, like `timeout`, which controls the timeout for downloading images passed as path arguments.","Maybe over-engineering things here but I would make this list pipeline-specific  (feel free to ignore though, we can also do it next time a ""transformers""-only arg is added) Yeah, it shouldn't be too painful to make that change when we need to!"
34416,2024-10-25T14:49:16Z,2024-10-29T18:08:42Z,yonigozlan,2,0,3,13,1,2,2,[],1659.0,0,357566.0,0,0,0,0,5018768.542723,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'yonigozlan']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34416). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Tests are failing for unrelated (I think) reasonsThanks for the fix! I ought to do some documentation for how the kwargs merging logic works - the logic flow is a bit tricky indeed Nice! Feel free to merge when ready,Thanks for the fix! I ought to do some documentation for how the kwargs merging logic works - the logic flow is a bit tricky indeed Nice! Feel free to merge when ready,"# What does this PR do?
Follows this PR https://github.com/huggingface/transformers/pull/34285
The warnings are displayed for all kwargs not in common, even if they are used in other modalities.
This PR only changes the logic of the warning, the kwargs handling logic should already be correct.


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->



## Who can review?

Cc @molbap , sorry I think you may have hinted at this problem previously but I did not pick up on it.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34298,2024-10-21T20:45:05Z,2024-10-29T17:29:24Z,AlekseyLobanov,6,3,2,3,1,2,1,[],56.0,0,679460.0,0,0,0,0,5021126.679462,,0,2,0,False,"['Rocketknight1', 'gante', 'AlekseyLobanov']","It is not a typo but still a small change with existing tests This looks good now! It should be simpler and more performant, and shouldn't change which strings it matches.

I have one more suggestion, though. This regex looks like it would fail in cases when the strings ""try"" and ""except"" occur internally in some lines. This is probably very rare, but maybe we should change the regex to `\n\s*try` and `\n\s*except` to be secure against that?

To be clear, this isn't a bug with your PR - it's an issue with the regex that already existed! >regex looks like it would fail in cases ...

Any regex should fail because even valid Python source is not defined by a regular language.
For example, code below also should be ignored, I believe.

```python
""""""
try:
    pass
except:
    pass
""""""
# code in multi line string literal
```

Should I add it in another PR with more tests? This PR is more about performance improvements.

Maybe we can wait for @gante to reply, because he wrote this lines. >Want me to merge this one now, and if you think you can improve it further, you can open a new PR with added test(s)?

Can we just merge this for now? It will be great.
 > Maybe we can wait for @gante to reply, because he wrote this lines.

Hehe not true (https://github.com/huggingface/transformers/pull/23725 is the source of these lines), but the diff looks good to me :) Yes, the diff looks good to me. Thanks for the PR!@AlekseyLobanov yes, good point - we can keep this PR simple. Want me to merge this one now, and if you think you can improve it further, you can open a new PR with added test(s)?","@AlekseyLobanov yes, good point - we can keep this PR simple. Want me to merge this one now, and if you think you can improve it further, you can open a new PR with added test(s)?","# What does this PR do?

Just found that one of the Regular Expressions may be very slow (O(N^3)) on some special inputs.

Something like `'try:' + ' ' * 1500` may take a lot of time. I replaced it with the same logic but faster


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@gante


<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
    content = re.sub(r""\s*try\s*:.*?except.*?:"", """", content, flags=re.MULTILINE | re.DOTALL)
``` Looks good to me - can we take this even further and remove the whitespace matching here as well? With `re.DOTALL` they should be equivalent. Also, `re.MULTILINE` only affects [the handling of start and end characters](https://docs.python.org/3/library/re.html#re.MULTILINE), which aren't used here. Maybe we can drop that as well? Yes, definitely. I removed  `re.MULTILINE` and extra `\s*`. Tests are fine."
34420,2024-10-26T00:12:36Z,2024-10-29T16:42:40Z,dependabot[bot],1,0,1,2,1,1,1,"['dependencies', 'python']",1585.0,0,318607.0,0,0,0,0,5023928.975655,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34420). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"Bumps [werkzeug](https://github.com/pallets/werkzeug) from 3.0.3 to 3.0.6.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/pallets/werkzeug/releases"">werkzeug's releases</a>.</em></p>
<blockquote>
<h2>3.0.6</h2>
<p>This is the Werkzeug 3.0.6 security fix release, which fixes security issues but does not otherwise change behavior and should not result in breaking changes.</p>
<p>PyPI: <a href=""https://pypi.org/project/Werkzeug/3.0.6/"">https://pypi.org/project/Werkzeug/3.0.6/</a>
Changes: <a href=""https://werkzeug.palletsprojects.com/en/stable/changes/#version-3-0-6"">https://werkzeug.palletsprojects.com/en/stable/changes/#version-3-0-6</a></p>
<ul>
<li>Fix how <code>max_form_memory_size</code> is applied when parsing large non-file fields. <a href=""https://github.com/advisories/GHSA-q34m-jh98-gwm2"">GHSA-q34m-jh98-gwm2</a></li>
<li><code>safe_join</code> catches certain paths on Windows that were not caught by <code>ntpath.isabs</code> on Python &lt; 3.11. <a href=""https://github.com/advisories/GHSA-f9vj-2wh5-fj8j"">GHSA-f9vj-2wh5-fj8j</a></li>
</ul>
<h2>3.0.5</h2>
<p>This is the Werkzeug 3.0.5 fix release, which fixes bugs but does not otherwise change behavior and should not result in breaking changes.</p>
<p>PyPI: <a href=""https://pypi.org/project/Werkzeug/3.0.5/"">https://pypi.org/project/Werkzeug/3.0.5/</a>
Changes: <a href=""https://werkzeug.palletsprojects.com/en/stable/changes/#version-3-0-5"">https://werkzeug.palletsprojects.com/en/stable/changes/#version-3-0-5</a>
Milestone: <a href=""https://github.com/pallets/werkzeug/milestone/37?closed=1"">https://github.com/pallets/werkzeug/milestone/37?closed=1</a></p>
<ul>
<li>The Watchdog reloader ignores file closed no write events. <a href=""https://redirect.github.com/pallets/werkzeug/issues/2945"">#2945</a></li>
<li>Logging works with client addresses containing an IPv6 scope. <a href=""https://redirect.github.com/pallets/werkzeug/issues/2952"">#2952</a></li>
<li>Ignore invalid authorization parameters. <a href=""https://redirect.github.com/pallets/werkzeug/issues/2955"">#2955</a></li>
<li>Improve type annotation fore <code>SharedDataMiddleware</code>. <a href=""https://redirect.github.com/pallets/werkzeug/issues/2958"">#2958</a></li>
<li>Compatibility with Python 3.13 when generating debugger pin and the current UID does not have an associated name. <a href=""https://redirect.github.com/pallets/werkzeug/issues/2957"">#2957</a></li>
</ul>
<h2>3.0.4</h2>
<p>This is the Werkzeug 3.0.4 fix release, which fixes bugs but does not otherwise change behavior and should not result in breaking changes.</p>
<p>PyPI: <a href=""https://pypi.org/project/Werkzeug/3.0.4/"">https://pypi.org/project/Werkzeug/3.0.4/</a>
Changes: <a href=""https://werkzeug.palletsprojects.com/en/3.0.x/changes/#version-3-0-4"">https://werkzeug.palletsprojects.com/en/3.0.x/changes/#version-3-0-4</a>
Milestone: <a href=""https://github.com/pallets/werkzeug/milestone/36?closed=1"">https://github.com/pallets/werkzeug/milestone/36?closed=1</a></p>
<ul>
<li>Restore behavior where parsing <code>multipart/x-www-form-urlencoded</code> data with
invalid UTF-8 bytes in the body results in no form data parsed rather than a
413 error. <a href=""https://redirect.github.com/pallets/werkzeug/issues/2930"">#2930</a></li>
<li>Improve <code>parse_options_header</code> performance when parsing unterminated
quoted string values. <a href=""https://redirect.github.com/pallets/werkzeug/issues/2904"">#2904</a></li>
<li>Debugger pin auth is synchronized across threads/processes when tracking
failed entries. <a href=""https://redirect.github.com/pallets/werkzeug/issues/2916"">#2916</a></li>
<li>Dev server handles unexpected <code>SSLEOFError</code> due to issue in Python &lt; 3.13.
<a href=""https://redirect.github.com/pallets/werkzeug/issues/2926"">#2926</a></li>
<li>Debugger pin auth works when the URL already contains a query string.
<a href=""https://redirect.github.com/pallets/werkzeug/issues/2918"">#2918</a></li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/pallets/werkzeug/blob/main/CHANGES.rst"">werkzeug's changelog</a>.</em></p>
<blockquote>
<h2>Version 3.0.6</h2>
<p>Released 2024-10-25</p>
<ul>
<li>Fix how <code>max_form_memory_size</code> is applied when parsing large non-file
fields. :ghsa:<code>q34m-jh98-gwm2</code></li>
<li><code>safe_join</code> catches certain paths on Windows that were not caught by
<code>ntpath.isabs</code> on Python &lt; 3.11. :ghsa:<code>f9vj-2wh5-fj8j</code></li>
</ul>
<h2>Version 3.0.5</h2>
<p>Released 2024-10-24</p>
<ul>
<li>The Watchdog reloader ignores file closed no write events. :issue:<code>2945</code></li>
<li>Logging works with client addresses containing an IPv6 scope :issue:<code>2952</code></li>
<li>Ignore invalid authorization parameters. :issue:<code>2955</code></li>
<li>Improve type annotation fore <code>SharedDataMiddleware</code>. :issue:<code>2958</code></li>
<li>Compatibility with Python 3.13 when generating debugger pin and the current
UID does not have an associated name. :issue:<code>2957</code></li>
</ul>
<h2>Version 3.0.4</h2>
<p>Released 2024-08-21</p>
<ul>
<li>Restore behavior where parsing <code>multipart/x-www-form-urlencoded</code> data with
invalid UTF-8 bytes in the body results in no form data parsed rather than a
413 error. :issue:<code>2930</code></li>
<li>Improve <code>parse_options_header</code> performance when parsing unterminated
quoted string values. :issue:<code>2904</code></li>
<li>Debugger pin auth is synchronized across threads/processes when tracking
failed entries. :issue:<code>2916</code></li>
<li>Dev server handles unexpected <code>SSLEOFError</code> due to issue in Python &lt; 3.13.
:issue:<code>2926</code></li>
<li>Debugger pin auth works when the URL already contains a query string.
:issue:<code>2918</code></li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/pallets/werkzeug/commit/5eaefc3996aa5cc8c5237d8b82f1b89eed6ea624""><code>5eaefc3</code></a> release version 3.0.6</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/2767bcb10a7dd1c297d812cc5e6d11a474c1f092""><code>2767bcb</code></a> Merge commit from fork</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/87cc78a25f782f8c59fbde786840a00cf0d09b3d""><code>87cc78a</code></a> catch special absolute path on Windows Python &lt; 3.11</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/50cfeebcb0727e18cc52ffbeb125f4a66551179b""><code>50cfeeb</code></a> Merge commit from fork</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/8760275afb72bd10b57d92cb4d52abf759b2f3a7""><code>8760275</code></a> apply max_form_memory_size another level up in the parser</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/8d6a12e2af542a553853c870d106884a3cd1f73b""><code>8d6a12e</code></a> start version 3.0.6</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/a7b121abc781b9a6557ca204f23247db654d0253""><code>a7b121a</code></a> release version 3.0.5 (<a href=""https://redirect.github.com/pallets/werkzeug/issues/2961"">#2961</a>)</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/9caf72ac060181a3171d91fd12279e071df430ca""><code>9caf72a</code></a> release version 3.0.5</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/e28a2451e99457ce71e460af276a02f27a3bdba1""><code>e28a245</code></a> catch OSError from getpass.getuser (<a href=""https://redirect.github.com/pallets/werkzeug/issues/2960"">#2960</a>)</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/e6b4cce97eef17716004625bcf6754fa930f2618""><code>e6b4cce</code></a> catch OSError from getpass.getuser</li>
<li>Additional commits viewable in <a href=""https://github.com/pallets/werkzeug/compare/3.0.3...3.0.6"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=werkzeug&package-manager=pip&previous-version=3.0.3&new-version=3.0.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/huggingface/transformers/network/alerts).

</details>",
34358,2024-10-23T20:26:03Z,2024-10-29T15:23:16Z,apoorvkh,10,0,10,124,2,3,4,['trainer'],62939.0,0,502803.0,0,0,0,0,5026126.657276,,0,10,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev', 'muellerzr', 'apoorvkh']","If you do `pip install -e .[quality]; make fixup` it should fix the style failures The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34358). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Great, thanks both!

@muellerzr I had already run `make fixup` but it results in the following error(s). I think these are unrelated to this PR?

<details><summary><code>make repo-consistency</code></summary>

```
python utils/check_modular_conversion.py

Differences found between the generated code and src/transformers/models/glm/modeling_glm.py:

   1 --- src/transformers/models/glm/modeling_glm.py_generated
   2 +++ src/transformers/models/glm/modeling_glm.py
   3 @@ -38,7 +38,6 @@
   4  )
   5  from ...modeling_utils import PreTrainedModel
   6  from ...utils import (
   7 -    add_code_sample_docstrings,
   8      add_start_docstrings,
   9      add_start_docstrings_to_model_forward,
  10      is_flash_attn_2_available,
  11 @@ -1223,11 +1222,6 @@
  12          self.model.embed_tokens = value
  13  
  14      @add_start_docstrings_to_model_forward(GLM_INPUTS_DOCSTRING)
  15 -    @add_code_sample_docstrings(
  16 -        checkpoint=_CHECKPOINT_FOR_DOC,
  17 -        output_type=TokenClassifierOutput,
  18 -        config_class=_CONFIG_FOR_DOC,
  19 -    )
  20      def forward(
  21          self,
  22          input_ids: Optional[torch.LongTensor] = None,
```
</details>

<details><summary><code>make quality</code></summary>

```
Traceback (most recent call last):
  File ""transformers/src/transformers/utils/import_utils.py"", line 1778, in _get_module
    return importlib.import_module(""."" + module_name, self.__name__)
  File ""transformers/.pixi/envs/default/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 783, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""transformers/src/transformers/convert_graph_to_onnx.py"", line 23, in <module>
    from transformers.pipelines import Pipeline, pipeline
  File ""transformers/src/transformers/pipelines/__init__.py"", line 50, in <module>
    from .automatic_speech_recognition import AutomaticSpeechRecognitionPipeline
  File ""transformers/src/transformers/pipelines/automatic_speech_recognition.py"", line 23, in <module>
    from .audio_utils import ffmpeg_read
  File ""transformers/src/transformers/pipelines/audio_utils.py"", line 54, in <module>
    ffmpeg_additional_args: Optional[list[str]] = None,
TypeError: 'type' object is not subscriptable
```
</details>

`make style` does seems to work (resulting in no changes).

edit: resolved by #34360 @SunMarc I am writing a codebase that uses `Trainer` for training with arbitrary models / hyper-parameters, especially using methods like FSDP / Deepspeed. (Happy to post a link shortly :) )

I believe the `optimizers` argument pre-supposes that a model is copied/sharded onto the right devices. But if we rely on Trainer to do the copying/sharding, the model parameters won't be available earlier (when defining the `optimizers` argument). I think that necessitates using `optimizer_cls` and `optimizer_kwargs` in `Trainer`. This PR simply tries to expose those options more conveniently :) Most checks pass now! A few tests in `tests_torch_and_flax` fail, but this does not seem specific to this PR. Makes sense ! Thanks for explaining ! Maybe you can add a note explaining that in the description of the arg. As for the CI, this PR is indeed unrelated. We will fix it shortly !  For sure! Done. Sorry for the wait ! When the CI is fixed from our side, we will merge the PR @apoorvkh Sounds great, thanks! Sure, how do those changes to `trainer.md` look?Nice! Seems like a great fix :)  Thanks for the PR ! LGTM ! I'm curious about  why the optimizers args is not enough for you use case ?  Thanks, would you like to add a bit of documentation for discoverability? With an example use case maybe? 🤗  LGTM thanks","Nice! Seems like a great fix :)  Thanks for the PR ! LGTM ! I'm curious about  why the optimizers args is not enough for you use case ?  Thanks, would you like to add a bit of documentation for discoverability? With an example use case maybe? 🤗  LGTM thanks","# What does this PR do?

Currently, `Trainer` must be extended (as follows) to provide a custom `torch.optim.Optimizer`. The existing `optimizers` argument for `Trainer` assumes the model is already initialized on the correct devices (which is usually handled by `Trainer`).

```python
class CustomOptimizerTrainer(Trainer):
    @staticmethod
    def get_optimizer_cls_and_kwargs(args: HfTrainingArguments, model=None) -> tuple[type[torch.optim.Optimizer], dict[str, Any]]:
        optimizer = torch.optim.AdamW
        optimizer_kwargs = {
            ""lr"": 4e-3,
            ""betas"": (0.9, 0.999),
            ""weight_decay"": 0.05,
        }
        return optimizer, optimizer_kwargs
```

This PR adds an `optimizer_cls_and_kwargs` argument to `Trainer`. This simply allows a user to pass `Type[torch.optim.Optimizer]` and `Dict[str, Any]` when initializing the `Trainer` rather than having to extend the class (as above).

I am making this PR after #31875 (cc: @amyeroberts @muellerzr)

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?",
34476,2024-10-29T00:18:14Z,2024-10-29T15:22:13Z,guangy10,0,0,1,46,1,1,1,"['run-slow', 'ExecuTorch']",,0,54240.0,0,0,0,0,5028759.646692,,0,1,0,False,[],Thanks 🤗 ,Thanks 🤗 ,"# What does this PR do?

Albert is ExecuTorch compatible.

Unit Test:
`RUN_SLOW=1 pytest tests/models/albert/test_modeling_albert.py -k test_export -v`
```
tests/models/albert/test_modeling_albert.py::AlbertModelIntegrationTest::test_export PASSED                                                                                             [100%]
```

E2E test in ExecuTorch:
Patch https://github.com/pytorch/executorch/pull/6509 
`python -m extension.export_util.export_hf_model -hfm=""albert/albert-base-v2"" -lm masked_lm`
```
Saved exported program to ./albert.pte
```
`./cmake-out/backends/xnnpack/xnn_executor_runner --model_path albert.pte`
```
I 00:00:00.051666 executorch:executor_runner.cpp:82] Model file albert.pte is loaded.
I 00:00:00.051701 executorch:executor_runner.cpp:91] Using method forward
I 00:00:00.051704 executorch:executor_runner.cpp:138] Setting up planned buffer 0, size 12005376.
I 00:00:00.101731 executorch:executor_runner.cpp:161] Method loaded.
I 00:00:00.101775 executorch:executor_runner.cpp:171] Inputs prepared.
I 00:00:00.251130 executorch:executor_runner.cpp:180] Model executed successfully.
I 00:00:00.251148 executorch:executor_runner.cpp:184] 1 outputs:
Output 0: tensor(sizes=[1, 64, 30000], [
  7.12213, 16.4255, -9.69697, 0.315882, 7.49277, 8.37929, 8.01692, 12.2838, 8.11998, 12.4227,
  7.5468, -5.25646, -5.68964, 11.3917, 8.85877, 8.94538, 5.69543, 7.87437, 10.1869, 6.47921,
  5.09051, 8.5898, 7.79427, 1.2211, 3.30417, 3.22097, 1.58806, 9.30696, 1.07529, 4.84525,
  2.17895, 8.81211, -1.02848, -3.64258, 6.78737, 4.30354, 1.65078, 3.47092, 11.7028, 7.89638,
  5.70505, -1.05684, 8.3248, 12.2657, 4.26686, 10.2256, -1.99968, 2.86684, 1.18797, 16.2016,
  1.63196, 5.46712, 2.33064, 7.08936, 0.676241, 6.57334, 1.04902, 0.281277, 12.6735, -1.04131,
  4.93435, -5.3161, 10.982, 2.07643, -1.98044, 1.17825, -6.78902, -0.594365, 9.06238, 11.7988,
  6.41249, 2.30598, 2.37509, 8.14539, 0.708781, 0.270195, -0.437858, -3.87035, -3.94704, 12.5791,
  0.291936, 5.41188, -2.38334, -4.61858, 2.57807, -0.0342076, -2.09207, 3.3832, 4.2705, -5.35976,
  6.55041, -5.35834, 0.0824419, 10.0817, -11.5175, 7.71341, 14.2482, -2.19647, 0.258341, 13.5795,
  ...,
  -26.6734, -15.8391, -9.05885, -22.9564, -14.1135, -14.3582, -1.38681, -22.967, -6.46937, -5.23052,
  -15.8735, -0.781886, 1.96928, -0.801466, -13.4606, -9.3534, -7.63344, -18.6456, -14.0491, -10.0933,
  -10.3132, -11.3254, -12.3537, -4.23457, -9.51285, -19.6473, -14.6648, -5.87785, -2.96578, -14.0239,
  -0.557438, -5.21334, -5.5204, 1.0429, -8.47772, -12.8267, -8.01721, -15.3659, -15.7359, -14.8388,
  -11.0749, -14.5002, -22.6418, -7.16905, -5.90876, -12.3513, -9.51316, -21.9345, -18.6938, -6.07597,
  -11.0177, 0.404317, -6.31417, -8.48093, -7.75292, -7.26334, -14.5192, -9.14845, -10.1494, -5.35306,
  -2.2068, -12.4971, -20.1255, -3.67846, -8.99902, -11.6741, -13.5727, -0.0831118, -12.0526, -7.48546,
  -18.1656, -12.0559, -6.95208, 1.14825, -11.53, -13.2759, -9.91268, -8.80736, -3.15759, -4.27456,
  -6.43947, -7.06724, -8.69398, -13.4397, -4.94796, -9.45768, -11.02, -11.3739, -10.9547, -18.7554,
  -25.8251, -12.1951, -6.00279, -9.81018, -5.64514, -20.6445, -12.1152, -7.1209, -13.5729, -8.33296,
])
```


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. #33836
- [x] Did you write any new necessary tests?


## Who can review?

@ArthurZucker
@qubvel
",
34473,2024-10-28T20:38:15Z,2024-10-29T15:14:31Z,guangy10,0,0,1,43,1,1,1,"['run-slow', 'ExecuTorch']",,0,66976.0,0,0,0,0,5029222.882493,,0,1,0,False,[],LGTM thanks,LGTM thanks,"# What does this PR do?

MobileBERT is ExecuTorch compatible.

Unit Test:
`RUN_SLOW=1 pytest tests/models/bert/test_modeling_mobilebert.py -k test_export -v`
```
tests/models/mobilebert/test_modeling_mobilebert.py::MobileBertModelIntegrationTests::test_export PASSED                                                                                [100%]
```

E2E test in ExecuTorch:
Patch https://github.com/pytorch/executorch/pull/6509 
`python -m extension.export_util.export_hf_model -hfm=""google/mobilebert-uncased"" -lm=""masked_lm""`
```
Saved exported program to ./mobilebert.pte
```
`./cmake-out/backends/xnnpack/xnn_executor_runner --model_path mobilebert.pte`
```
I 00:00:00.052851 executorch:executor_runner.cpp:82] Model file mobilebert.pte is loaded.
I 00:00:00.052931 executorch:executor_runner.cpp:91] Using method forward
I 00:00:00.052941 executorch:executor_runner.cpp:138] Setting up planned buffer 0, size 78562304.
I 00:00:00.080190 executorch:executor_runner.cpp:161] Method loaded.
I 00:00:00.080223 executorch:executor_runner.cpp:171] Inputs prepared.
I 00:00:01.315332 executorch:executor_runner.cpp:180] Model executed successfully.
I 00:00:01.315349 executorch:executor_runner.cpp:184] 1 outputs:
Output 0: tensor(sizes=[1, 64, 30522], [
  -8.54569, -9.2325, -9.63534, -9.15334, -8.9625, -8.23703, -8.13457, -8.46437, -9.27599, -10.4376,
  -9.40925, -8.32582, -7.68392, -9.18805, -9.3336, -8.71841, -9.52881, -8.03673, -10.02, -9.08982,
  -10.1946, -9.01746, -8.93738, -8.80435, -9.85641, -10.1131, -8.68367, -7.66197, -11.1961, -9.6375,
  -10.6121, -9.54771, -9.9602, -9.8043, -8.37304, -9.33157, -10.0574, -9.67762, -8.67488, -9.94721,
  -8.77431, -11.054, -9.39442, -9.91193, -8.54864, -10.2091, -9.69447, -10.2535, -8.71598, -8.85885,
  -10.5134, -10.151, -9.6825, -10.2545, -9.03317, -9.3754, -10.1931, -8.01357, -7.8706, -9.44086,
  -7.66918, -8.66947, -8.13562, -8.58915, -11.6404, -8.62273, -8.92262, -9.81593, -8.25118, -8.82181,
  -11.4176, -9.58474, -8.92113, -8.37405, -8.7471, -8.637, -9.41958, -8.99523, -10.1699, -9.5519,
  -8.48485, -9.90429, -8.82682, -9.10106, -9.65399, -9.55595, -9.16413, -7.88924, -8.53024, -8.77796,
  -9.7509, -9.06045, -10.9217, -10.7008, -8.35735, -10.2375, -9.26975, -8.89627, -9.15771, -9.24441,
  ...,
  -7.41324, -7.98777, -6.96755, -5.78335, -6.45162, -6.61997, -7.14526, -7.84212, -7.80403, -6.78576,
  -6.23739, -6.91561, -5.49068, -6.96955, -6.45688, -6.2613, -5.91608, -6.87685, -6.08245, -6.84436,
  -7.63106, -6.86492, -7.27272, -7.66639, -5.61715, -6.88419, -6.54495, -7.39954, -4.94468, -7.13938,
  -6.0773, -6.17809, -6.97764, -7.4307, -6.80996, -6.7768, -7.37503, -7.60815, -7.17836, -6.44587,
  -7.20048, -5.83285, -6.8438, -5.48847, -6.91492, -6.98491, -5.9548, -7.00031, -6.17852, -6.701,
  -6.21065, -7.01716, -7.04772, -7.45687, -7.57249, -5.39035, -6.14504, -6.50262, -6.42711, -7.29622,
  -6.24016, -7.12543, -7.03201, -7.88257, -7.04191, -7.0761, -6.95057, -6.0472, -5.97595, -6.85439,
  -5.8476, -6.37267, -6.20555, -7.30387, -6.25161, -7.53724, -6.54707, -6.81088, -6.88549, -6.02146,
  -6.34617, -6.92539, -7.22334, -7.04436, -6.80034, -6.05272, -6.70695, -8.43641, -0.241071, -2.15037,
  -6.41449, -5.42762, -6.04412, -7.70756, -7.53018, -6.31514, -7.02132, -6.56385, -7.70365, 0.30994,
])
```


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. #33843
- [x] Did you write any new necessary tests?


## Who can review?

@ArthurZucker
@qubvel
",
29439,2024-03-04T13:18:18Z,2024-03-06T23:57:23Z,currybab,4,2,3,4,2,2,1,[],211159.0,0,20653788.0,0,0,0,0,5032008.847246,,0,3,0,False,"['Tran-Thanh1991', 'ArthurZucker']","FYI @fxmarty and @gante !  I have not tested this with `compile` but the dtype should be alright to check / we can always check self.dtype to not be input dependant Hi everybody. I'm having the same error, does anyone know how to fix it?

RuntimeError: User specified an unsupported autocast device_type 'cuda:0'

![image](https://github.com/user-attachments/assets/184e3163-dfac-41f4-b670-ccb84783fb8a)
 Do you have a reproducer? That is indeed a problem. Was not aware that `autocast` is not available for `mps`. 
We probably need to do a patch for this!
I think we can use `cpu` device even if the tensors are not on CPU no?  LGTM thank you for the prompt fix","That is indeed a problem. Was not aware that `autocast` is not available for `mps`. 
We probably need to do a patch for this!
I think we can use `cpu` device even if the tensors are not on CPU no?  LGTM thank you for the prompt fix","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #29431

The issue on MPS devices was caused by the merge of #29285 in version 4.38.2.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
        device_type = device_type if isinstance(device_type, str) and device_type != ""mps"" else ""cpu""
``` it seems to work well in my M1 Pro. I'll add a commit for this."
34376,2024-10-24T12:57:53Z,2024-10-25T07:44:09Z,IlyasMoutawwakil,6,0,4,22,10,1,1,[],18.0,0,436865.0,0,0,0,0,5032557.587227,,0,4,0,False,"['HuggingFaceDocBuilderDev', 'IlyasMoutawwakil', 'ArthurZucker']",@ArthurZucker  same as https://github.com/huggingface/transformers/pull/31696 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34376). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Could you run make fix-copies?  neat ! love this command 😄 feel free to merge!Thanks! ,Thanks! ,"# What does this PR do?
All of these changes were introduced in `4.46` via #33586, which broke them
",
34281,2024-10-21T13:02:22Z,2024-10-29T13:31:36Z,wavy-jung,4,0,1,17,2,1,1,[],134630.0,0,694565.0,0,0,0,0,5033788.875829,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker', 'wavy-jung']","@ArthurZucker I literally have no idea why the CI check fails at glm modeling code. I did not make any changes to the glm models. Can you help me with this or can you give me a brief guideline? `make fix-copies` might be needed as mistral is copied here and there @ArthurZucker I did `fix-copies` & `rebase`. It seems all the checks are passed. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34281). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Hey! Sounds good, could you make sure to fix the cis and rebase? 🤗  These were fixed on main! You can now rebase and should be good! Thanks 🤗 ","Hey! Sounds good, could you make sure to fix the cis and rebase? 🤗  These were fixed on main! You can now rebase and should be good! Thanks 🤗 ","# What does this PR do?
- manual `head_dim` setting for `mixtral` model
- same as `llama` and `mistral` implementation
- worked well when I tested with sparse upcycled llama model

Fixes #34261 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker @gante",
34424,2024-10-26T01:55:30Z,2024-10-29T13:30:02Z,guangy10,0,0,1,42,1,1,1,[],,0,300873.0,0,0,0,0,5035493.791505,,0,1,0,False,[],LGTM thanks 🤗 ,LGTM thanks 🤗 ,"# What does this PR do?

Bert is ExecuTorch compatible.

Unit Test:
`RUN_SLOW=1 pytest tests/models/bert/test_modeling_bert.py -k test_export -v`
```
tests/models/bert/test_modeling_bert.py::BertModelIntegrationTest::test_export PASSED                                                                                                   [100%]
```

E2E test in ExecuTorch:
Patch https://github.com/pytorch/executorch/pull/6509 
`python -m extension.export_util.export_hf_model -hfm=""google-bert/bert-base-uncased"" -lm=""masked_lm""`
```
Saved exported program to ./bert.pte
```
`./cmake-out/backends/xnnpack/xnn_executor_runner --model_path bert.pte`
```
I 00:00:00.130046 executorch:executor_runner.cpp:82] Model file bert.pte is loaded.
I 00:00:00.130076 executorch:executor_runner.cpp:91] Using method forward
I 00:00:00.130077 executorch:executor_runner.cpp:138] Setting up planned buffer 0, size 11155968.
I 00:00:00.187901 executorch:executor_runner.cpp:161] Method loaded.
I 00:00:00.187940 executorch:executor_runner.cpp:171] Inputs prepared.
I 00:00:00.303509 executorch:executor_runner.cpp:180] Model executed successfully.
I 00:00:00.303528 executorch:executor_runner.cpp:184] 1 outputs:
Output 0: tensor(sizes=[1, 64, 30522], [
  -5.31, -5.25653, -5.62139, -5.35147, -5.58368, -5.76769, -5.39854, -5.49406, -5.41472, -5.60033,
  -5.60824, -5.64769, -5.70689, -5.62003, -5.36711, -5.81403, -5.40622, -5.50556, -5.37618, -5.25235,
  -5.4938, -5.61887, -5.47021, -5.50617, -5.24071, -5.41838, -5.49885, -5.6491, -5.62601, -4.98599,
  -5.64714, -5.54412, -5.54532, -5.72253, -5.06812, -5.47294, -5.34288, -5.27819, -5.37329, -5.43782,
  -5.53609, -5.46165, -5.21018, -5.28851, -5.38127, -4.95377, -5.5431, -5.67045, -5.67165, -5.92957,
  -5.32795, -5.5024, -5.20931, -5.71709, -5.74185, -5.28685, -5.38797, -5.52771, -5.33859, -5.84168,
  -5.23825, -5.49683, -5.65671, -5.70298, -5.40745, -5.49082, -5.45573, -5.2697, -5.46876, -5.2727,
  -5.54258, -5.2412, -5.71677, -5.37888, -5.76725, -5.6436, -5.3792, -5.68382, -5.87468, -5.57073,
  -5.62629, -5.48719, -4.89835, -5.87864, -5.73316, -5.2621, -5.5839, -5.54559, -5.56973, -5.56077,
  -5.28284, -5.11866, -5.77843, -5.373, -5.9074, -5.17517, -5.66992, -5.49333, -5.10405, -5.38464,
  ...,
  -5.03352, -5.74726, -3.90867, -2.13927, -5.53828, -3.65071, -9.55079, -5.52908, -5.65968, -1.65796,
  -2.86744, -4.80233, -5.33614, -4.94629, -4.69176, -3.87113, -7.70568, -5.79979, -5.22023, -4.59109,
  -1.37544, -2.02489, -3.94916, -3.0996, -7.91293, -1.34026, -3.35818, -4.66463, -4.11608, -4.97319,
  -6.91719, -6.17901, -4.73535, -7.47833, -2.26579, -4.29045, -2.38045, -6.78713, -5.87505, -2.10658,
  -7.45086, -3.71908, -4.90981, -4.69094, -1.40817, -3.86502, -4.08434, -1.32019, -5.07113, -9.31685,
  -5.29724, -6.87862, -3.78542, -5.81963, -7.6481, -4.51615, -7.94964, -4.45573, -2.57971, -7.64688,
  -2.73065, -5.26966, -5.91819, -4.89055, -3.93077, -4.92867, -3.93575, -5.32761, -4.06602, -2.97809,
  -4.35003, -2.96373, -5.43266, -2.50787, -3.88847, -4.5942, -4.9843, -7.62539, -9.25849, -3.7758,
  -6.6124, -3.78246, -3.56245, -3.91384, -3.44218, -4.49679, -5.97564, -3.77184, -5.44054, -4.61596,
  -3.69274, -6.37589, -5.36977, -4.21502, -5.9998, -6.23449, -3.90434, -4.50337, -6.30893, -1.40617,
])
```


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. #32507
- [x] Did you write any new necessary tests?


## Who can review?

@ArthurZucker
@qubvel
",
34387,2024-10-24T16:28:44Z,2024-10-29T10:10:10Z,Framartin,1,0,3,15,1,1,1,[],410861.0,0,413487.0,0,0,0,0,5043286.590541,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34387). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks,Thanks,"# What does this PR do?

This pull request fixes an issue in the code of the Perplexity of fixed-length models documentation. The current implementation computes the arithmetic mean of the negative log-likelihood (NLL) per sequence chunk without accounting for the varying number of tokens in each chunk. This can lead to an inaccurate average NLL per token, as tokens in smaller chunks (like the last one) have a higher weight in the final perplexity calculation.  The PR computes the arithmetic mean of NLL per token. The perplexity error is small in the provided example (`16.4541` vs. `16.4443`), but I guess that it can be larger for shorter sequences.


## Before submitting

- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? 
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@stevhliu  @ArthurZucker


",
34409,2024-10-25T10:35:53Z,2024-10-29T10:41:04Z,SunMarc,1,1,6,29,2,2,2,[],1598.0,0,345914.0,0,0,0,0,5045630.964892,,0,6,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34409). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.I don't have enough background to judge if the fix is good, but the test should cover it, as the `generate` call itself would raise an error without the fix. Thanks, noting as it will be in a patch","I don't have enough background to judge if the fix is good, but the test should cover it, as the `generate` call itself would raise an error without the fix. Thanks, noting as it will be in a patch","# What does this PR do?
This PR fixes the regression that @BenjaminBossan found out. It was caused by this [PR](https://github.com/huggingface/transformers/pull/33141). Basically, `old_param` was always going to be `None` with the current logic. Hence, we don't set the correct dtype to the parameter `param = param.to(old_param.dtype)`. This caused a dtype mismatch with torchao. 

```
[...]
    return torch.nn.functional.linear(input_tensor, weight_tensor, bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: self and mat2 must have the same dtype, but got Float and Half
```

To reproduce
```
import torch
from transformers import AutoModelForCausalLM, TorchAoConfig

quantization_config = TorchAoConfig(quant_type=""int8_dynamic_activation_int8_weight"")
model_id = ""facebook/opt-125m""
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)
inputs = torch.arange(10).view(-1, 1)
model(inputs)
```

I tested that hqq serialization and the new test that I added both passed ","😅 I knew it! 
It was a bit too specific "
34266,2024-10-20T09:44:42Z,2024-10-29T10:40:41Z,hlky,3,4,5,10,1,2,1,"['HACKTOBERFEST-ACCEPTED', 'Modular']",659.0,0,780965.0,0,0,0,0,5045652.28113,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker', 'hlky']","~~Please note the test failure here is unrelated, looks like `modeling_glm` needs to be overwritten with modular generated code.~~ The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34266). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks for your contribution!LGTM maybe using os.path will simplify the separation! 🤗 ",LGTM maybe using os.path will simplify the separation! 🤗 ,"# What does this PR do?

This PR fixes some issues with Modular Converter on Windows.

1. Relative path regex. `rf""(src{os.sep}transformers{os.sep}.*|examples{os.sep}.*)""` results in `(src\transformers\.*|examples\.*)`, due to `\t` and `\.` this regex fails, we replace `\` on `os.path.abspath` instead, and use `(src/transformers/.*|examples/.*)` as the regex.
~~2. Relative path in auto generated message. On Windows this generates as `This file was automatically generated from src\transformers\models\florence2\modular_florence2.py.` This could potentially cause issues elsewhere, and it's better to be standardized, so when `os.sep` == `\` we replace `os.sep` with `/`.~~
3. `open` encoding. On Windows the default encoding is `cp1252` which doesn't work with `🚨`, so we use `encoding=utf-8` instead for all `open`. This particular fix is in effect in many other places in the codebase.

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker
","I think we'd better use `os.path.join` no? This would directly support correct separation?  It's the same separation as `os.sep` with `os.path.join`. This is a python/regex issue due to the combination of raw and formatted.

An alternate solution would be replacing `\\` on `os.path.abspath`, then regex can just use `/`:
```python
relative_path = re.search(
    r""(src/transformers/.*|examples/.*)"", os.path.abspath(modular_file).replace(""\\"", ""/"")
).group(1)
``` looks simpler let's go with this! Done, thanks!"
32031,2024-07-17T18:37:13Z,2024-07-17T19:20:39Z,moses,1,0,1,4,1,1,1,[],974.0,0,8954037.0,0,0,0,0,5048629.852954,,0,1,0,False,['leloykun'],Hi! I think you should be tagging this? https://github.com/huggingface/transformers/issues/32030Thanks for fixing! ,Thanks for fixing! ,"Fixes https://github.com/huggingface/transformers/issues/32030 
(previously tagged ~#32013~ )

The existing logic has a typo in it, where, for example, it selects a sigmoid classification function if the problem type is ""multi_label_classification"" or num_labels is 1. The num_labels condition overrides the problem_type condition, so in practice, I don't think this error will cause a bug, but it's confusing.

This fix corrects the problem type conditions.

Please review at your convenience @Narsil 
",
34213,2024-10-17T09:44:49Z,2024-10-29T09:48:58Z,ydshieh,2,6,8,61,3,3,1,[],601394.0,0,1037050.0,0,0,0,0,5048761.198395,,0,8,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34213). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. On some docker environment we use, `python` won't work and we have to use `python3`. And in other environments, the situation is inversed. That's why I go for `sys.executable` 👀 Looks super complicated, thanks for diving! 
Just missing some doc for me 🤗  Okay, on my side I don't really see how this can be ""hacked"" now, `sys.executable` is supposed to be safe, we could also just put `python` here no? ","Looks super complicated, thanks for diving! 
Just missing some doc for me 🤗  Okay, on my side I don't really see how this can be ""hacked"" now, `sys.executable` is supposed to be safe, we could also just put `python` here no? ","# What does this PR do?

In order to make `running (some individual) tests in a subprocess` work with GPU (cuda), if we use `multiprocessing`, we have to use `spawn`, but this uses pickle and has some limitations that make the necessary changes to the test methods quite big. 

This PR tries to use the `subprocess` module to implement a simpler way to run (some individual) tests in a subprocess.","let's document this for futur's sake!  This seems vulnerable to injection with the command being built from f-strings
I don't think it's particularly risky but in the current form it's highly likely that we'll get a huntr report :) Maybe we can mark as subprocess instead, and have a separate command to run these no?  I got the idea of what you mentioned.

`test` is gettting from `"" "".join(os.environ.get(""PYTEST_CURRENT_TEST"")` with is a processed variable from `pytest` (so it is already passed pytest's collection).

`sys.executable` should not be risk I think, and we have other places using it like

> cmd = [sys.executable, ""-c"", ""\n"".join(commands)]

Can't say 100% for sure, but risk level is likely low

 The idea here is not to to run these tests (very very few tests) separated from the other tests, but to change the behavior of its execution logic (run in a subprocess) even if a suite of tests is collected by `pytest`.

If we use separate command to run those (very few) tests, it means we need to add new jobs (or steps) - which is already not very good IMO, and will get new separated test reports which is probably making the testing statistic less unified. Okay, this would have fixed the issue mentioned by Lysandre, and we don't need a new job, maybe we can run them after the normal ones? "
34482,2024-10-29T07:17:08Z,2024-10-29T08:39:06Z,techkang,1,0,2,11,1,1,1,[],6468.0,0,6470.0,0,0,0,0,5051403.285938,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34482). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.THanks, timout seems to be gone","THanks, timout seems to be gone","# What does this PR do?

The model used in generation config test is so that that it may caused timeout. Use a small model instead.
https://app.circleci.com/pipelines/github/huggingface/transformers/108968/workflows/c1171c15-32ef-448c-a7ee-660db7dfcfb3/jobs/1448804?invite=true#step-112-20764_41

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34458,2024-10-28T06:51:52Z,2024-10-29T07:26:04Z,zucchini-nlp,3,2,4,8,3,3,2,[],1556.0,0,88452.0,0,0,0,0,5057338.0212,,0,4,0,False,"['techkang', 'HuggingFaceDocBuilderDev', 'zucchini-nlp']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34458). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. btw I can't merge this while other tests are failing, retriggering didn't help I think the model used for testing is too large, which casued the timeout error. I proposed a new PR: https://github.com/huggingface/transformers/pull/34482 trying to fixed it by using a small model instead.Thanks for the prompt fix! Thank you!",Thanks for the prompt fix! Thank you!,"# What does this PR do?

Fixes CI which is currently red on many PRs. Caused by https://github.com/huggingface/transformers/pull/34043 and https://github.com/huggingface/transformers/pull/34026 when not all tests were fetched for run ","should it be `input_ids.shape[-1] + 5` or using `max_new_tokens` instead? yeah, could also be max-new-tokens. Interestingly after the linked PR `max_length` behaves same way as `max_new_tokens`"
34369,2024-10-24T09:41:17Z,2024-10-29T06:57:10Z,zucchini-nlp,1,3,4,72,4,3,2,[],1611.0,0,422153.0,0,0,0,0,5059072.543699,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34369). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM 👍  Thanks!,LGTM 👍  Thanks!,"# What does this PR do?

Fixes one part of the test that was not being triggered when we run it with `pytest` because it had typo in `cache_position(S)`. Fixing the typo resulted in a whole bunch of errors most of which are caused by apparently non-deterministic output when setting the `repetition_penalty`. Thus I moved the test for beam-search + input-embeds to another place

The main idea when writing this test was to make sure beam search works with inputs embeds for decoder-only models that now support new cache format, so I think the change should be fine
","so this part was never executed, because of the typo in `cache_positions` -- correct? nit: `max_new_tokens` can be moved to `generation_kwargs` yep :)"
34449,2024-10-27T14:36:54Z,2024-10-28T17:48:18Z,ShubhamJagtap2000,2,0,1,8,1,1,1,[],147.0,0,103640.0,0,0,0,0,5100649.276776,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'ShubhamJagtap2000']","Hello @stevhliu,

I have raised this PR to suggest some improvements to the docs. However, I haven't raised any ISSUE prior for this change for approval, I hope it is fine for this nature of the issue. 

I observed a wrong linting method to code-block in https://github.com/huggingface/transformers/tree/main/docs and did a change to it. Please review.

Please let me know if I should be raising the issue for this PR.

Thank you,
Shubham J. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34449). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for improving! 🤗 ",Thanks for improving! 🤗 ,"Enhance user experience using py-linting

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34287,2024-10-21T16:38:33Z,2024-10-28T18:33:17Z,McPatate,1,0,6,1606,3,1,1,"['Benchmarks', 'run-benchmark']",242938.0,0,611686.0,0,0,0,0,5103705.146852,,1,6,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34287). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.```
psql: error: could not connect to server: Connection timed out
	Is the server running on host ""***"" (10.80.43.30) and accepting
	TCP/IP connections on port 5432?
```
appart from that let's goo!

I think we should fix the issue of the action triggerring randomly as well before merging 👀 

","```
psql: error: could not connect to server: Connection timed out
	Is the server running on host ""***"" (10.80.43.30) and accepting
	TCP/IP connections on port 5432?
```
appart from that let's goo!

I think we should fix the issue of the action triggerring randomly as well before merging 👀 

","### What does this PR do?

Add A100 runner group for benchmarks CI.",
34373,2024-10-24T12:39:52Z,2024-10-28T17:59:38Z,techkang,7,11,12,32,2,4,5,[],5474.0,0,364799.0,0,0,0,0,5105714.348648,,0,12,0,False,"['techkang', 'HuggingFaceDocBuilderDev', 'muellerzr', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34373). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @muellerzr @SunMarc I fixed one typo to make ruff happy and pass auto check. And added more explanation to the argument. Pls check again, thank you. One required tests failed but it's due to timeout. Can any one help to fix it? Thanks a lot. world_size is unavailable when only using TF backend. Using try/except to catch errors.

CI still enconunted four errors, but the same error appeared at the recently merged PR. So it may not caused by this PR.

CI errors of this PR: [ci_error](https://app.circleci.com/pipelines/github/huggingface/transformers/108706/workflows/6ef932a9-5847-4736-b178-836b2e1407ba/jobs/1445381/tests)

CI errors for former [PR](https://github.com/huggingface/transformers/pull/34377): [ci_error](https://app.circleci.com/pipelines/github/huggingface/transformers/108644/workflows/ccfcf63a-13d3-4e43-a7fd-16fb3c16ead8/jobs/1444457/tests) We'll merge once our CI allows us to 😅  cc @ydshieh  Thanks @techkang 🤗 Thanks! It's a good start, let's improve it a little :) Thanks! We're quite close. Two more nits This indeed looks like the correct solution! Thanks!  Thanks for your work ! LGTM !  Thanks! Overall LG2M, cc @SunMarc  Thanks ! LGTM ! Just a nit  Thanks! Rebase to make sure we have green CIs, will merge anyways!","Thanks! It's a good start, let's improve it a little :) Thanks! We're quite close. Two more nits This indeed looks like the correct solution! Thanks!  Thanks for your work ! LGTM !  Thanks! Overall LG2M, cc @SunMarc  Thanks ! LGTM ! Just a nit  Thanks! Rebase to make sure we have green CIs, will merge anyways!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->
Fix [https://github.com/huggingface/transformers/issues/34242](https://github.com/huggingface/transformers/issues/34242)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@muellerzr pls help to check.
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Thanks! We also need the case for when we don't define this, e.g. it's passed to the model `forward()`. So what would be better is to perform the `gather` much earlier, and pass the new `num_items_in_batch` as part of the call to `compute_loss`. 

And then perform the `loss *=` where we call `loss *= self.args.gradient_accumulation_steps` later (right before we call `accelerator.backward()`) Thanks for advice! Already fixed it, please check again. During the `__post_init__` we call `setup_devices`. We can change `average_tokens_across_devices` value to `False` if the world size < 1 I think!

This then simplifies it earlier to just be `if self.args.average_tokens_across_devices` Both of these chunks I think can be under a `if num_items_in_batch is not None and self.model_accepts_loss_kwargs`, since both need to be valid for the `loss *= self.args.gradient_accumulation_steps` Thanks, already fixed it, please check.
 I think gradient accumulation is orthogonal to DDP, and used a new if statement. Please check my code. Thanks. It is, it's a matter of `self.model.accepts_loss_kwargs` Maybe we could share a bit more why this arg could be useful ?  Done, please review my code. We modified a lot how loss is computed, are we sure that this is loss is the same as the one applied ?  Good catch, it should be `loss.detach() / self.args.gradient_accumulation_steps / self.accelerator.num_processes` (dividing by num processes if and only if we did our loss function num tokens logic)"
33980,2024-10-05T17:15:11Z,2024-10-28T17:46:49Z,Jwaminju,4,0,4,64,2,1,1,[],274451.0,0,1989098.0,0,0,0,0,5106496.392419,,0,4,0,False,"['Jwaminju', 'stevhliu', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33980). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hello, I'm looking into how to esolve the setup_and_quality failure. Thanks in advance! 

>  Some diff and their modeling code did not match. Hey, it looks like the `setup_and_quality` test is passing now. The `build_pr_documentation` issue has been resolved on main, so if you wouldn't mind, can you rebase your branch on it to get the fix? 🤗  Hi, @stevhliu! 
Thanks you so much for your review :) I have rebased the branch!
Best Regards,
Minju💯 Feel free to change this PR from a draft whenever you're ready!",💯 Feel free to change this PR from a draft whenever you're ready!,"# What does this PR do?

Translated the `model_doc/barthez.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang 


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)
@stevhliu May you please review this PR?",
34322,2024-10-22T21:12:41Z,2024-10-28T16:14:07Z,h3110Fr13nd,2,2,3,16,2,2,1,[],2011.0,0,500487.0,0,0,0,0,5112058.344467,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'h3110Fr13nd']","Also, in `modelling_mistral.py` there's a typo in input docstring

```diff
attention_mask
              ...
-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
            `past_key_values`).
```

as suggested by docs of past_key_values and the forward args
```python
past_key_values
            ...
            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
            of shape `(batch_size, sequence_length)`.
```
@stevhliu  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34322). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the update :)

If you can rebase on main, that should fix the failing CI tests.","Thanks for the update :)

If you can rebase on main, that should fix the failing CI tests.","# What does this PR do?
Mamba2 and Mistral models had Input docstrings not exactly matching with forwrd pass of their respective models.
This doesn't have a big impact.
But I'm trying to add modular models. Which I'm subclassing from Mistral.  https://github.com/huggingface/transformers/issues/33916
And documentation mismatch is somehow  not overriding. Which seems to be a lacking in Mistral model.


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@stevhliu 

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
            Indices indicating the position of the input sequence tokens in the sequence. Unlike `position_ids`,
            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
            the complete sequence length.
``` Rebased and changed. Thanks. @stevhliu "
34140,2024-10-13T21:22:50Z,2024-10-28T12:50:17Z,eljandoubi,7,0,34,47,3,2,2,[],62472.0,0,1265247.0,0,0,0,0,5124290.559318,,1,34,0,False,"['HuggingFaceDocBuilderDev', 'muellerzr', 'eljandoubi']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34140). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. The required tests are distributed tests. We need to verify FSDP functionality with and without FP8 mixed precision. The appropriate test file might be `tests/trainer/test_trainer_fsdp.py`. Is the `test/trainer` folder included in the CI tests? Where can I check the results for `test_trainer_fsdp.py`? @muellerzr @SunMarc  @eljandoubi we can't run them on the normal CI since GPU runners are not part of PR's. 

Instead when ready I'll pull the PR down and run it myself @muellerzr Thank you for the information. I have tested the branch in my code on a multi-node, multi-GPU setup using FSDP mode, both with and without FP8 mixed precision, and it worked as expected. Please let me know if you encounter any issues on your end.  @muellerzr Any updates regarding this PR? @muellerzr  I have done `make fixup`.Nice :) Can we add a test in `tests/test_trainer.py`? We can set env variables to configure Accelerate properly (`ACCELERATE_MIXED_PRECISION=""fp8""` will auto-use TE) Thanks! Can you do `pip install -e .[quality]` followed by `make fixup`? I'll then pull it locally to test on my 4090 system and we should be set! Don't worry we'll merge as is, failing tests are unrelated!","Nice :) Can we add a test in `tests/test_trainer.py`? We can set env variables to configure Accelerate properly (`ACCELERATE_MIXED_PRECISION=""fp8""` will auto-use TE) Thanks! Can you do `pip install -e .[quality]` followed by `make fixup`? I'll then pull it locally to test on my 4090 system and we should be set! Don't worry we'll merge as is, failing tests are unrelated!","# What does this PR do?

It passes the model and  the optimizer to accelerate.prepare in order to enable fp8 mixed precision, if any.

Fixes #34024 

## Who can review?

Library:

- trainer: @muellerzr and @SunMarc 

 -->
",
34343,2024-10-23T11:05:09Z,2024-10-28T12:23:52Z,zeus2611,1,3,6,12,1,4,3,[],103034.0,0,443133.0,1,0,0,0,5119465.637503,,0,6,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34343). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks! Let's tweak the logic a bit to fully use the accelerate API Thanks for fixing @zeus2611 ! Nice work  Thanks for fixing!,Thanks! Let's tweak the logic a bit to fully use the accelerate API Thanks for fixing @zeus2611 ! Nice work  Thanks for fixing!,"# What does this PR do?

Fixes # (issue)
#31457 as mentioned in #33345 
Updated the prediction_loop method in the Trainer class to correctly handle batch size when using DataLoaderShard. This ensures that the batch size is retrieved from total_batch_size for distributed training scenarios, preventing TypeError related to NoneType during evaluation.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@ArthurZucker @muellerzr @SunMarc","It can be more than `DataLoaderShard`. Let's instead check:

```suggestion
        batch_size = dataloader.total_batch_size if getattr(dataloader, ""_is_accelerate_prepared"", False) else dataloader.batch_size
``` Unused import here is part of the failure. Can you run `pip install -e .[quality]; make fixup` Applied the fix and reformated the previous change."
34404,2024-10-25T09:17:03Z,2024-10-28T11:01:05Z,ydshieh,1,1,3,8,3,2,1,[],1550.0,0,265444.0,0,0,0,0,5130841.667464,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34404). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.🤗 ,🤗 ,"# What does this PR do?

I forgot a place that needs an update too.",This is a fix for **not having all new failing tests being saved into** `new_model_failures.json`
34381,2024-10-24T14:47:19Z,2024-10-28T10:59:46Z,ydshieh,1,0,1,4,2,1,1,[],1607.0,0,331949.0,0,0,0,0,5130921.501984,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34381). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks! Not sure we'll ever need to unpin 👁️ 👁️ ,Thanks! Not sure we'll ever need to unpin 👁️ 👁️ ,"# What does this PR do?

After #34314, `tensorflow_probability==0.24` was selected and installed (under the pip dependency) and some tests (in particular, TF tapas) failed with `tensorflow==2.13`.

Let's pin `tensorflow_probability<0.22`",
34374,2024-10-24T12:41:15Z,2024-10-28T10:24:56Z,IlyasMoutawwakil,4,0,5,71,2,2,1,[],1189.0,0,338391.0,0,0,0,0,5132043.634026,,0,5,0,False,"['echarlaix', 'HuggingFaceDocBuilderDev', 'IlyasMoutawwakil']","In transformers v4.46 generation for Pix2Struct fail when setting `use_cache=True`


Code to reproduce this issue : https://huggingface.co/docs/transformers/v4.46.0/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration.forward.example with `use_cache=True`
```
from PIL import Image
import requests
from transformers import AutoProcessor, Pix2StructForConditionalGeneration
model_id = ""google/pix2struct-textcaps-base""

processor = AutoProcessor.from_pretrained(model_id)
model = Pix2StructForConditionalGeneration.from_pretrained(model_id)
url = ""https://www.ilankelman.org/stopsigns/australia.jpg""
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors=""pt"")
generated_ids = model.generate(**inputs, max_new_tokens=50, use_cache=True)
``` The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34374). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker I added a small quick test there's one thing I don't understand, running the same test with T5 passes even without eval mode, but it was necessary to get it passing for pix2struct.Thanks for fixing! Very bad that we don't have generation test mixin in Pix2Struct 😢 , will add tests in the next batch with some left-over VLMs Thanks! can you add the snippet as a test? 🤗  Thanks, did not have the chance to approve but LGTM 🤗 ","Thanks for fixing! Very bad that we don't have generation test mixin in Pix2Struct 😢 , will add tests in the next batch with some left-over VLMs Thanks! can you add the snippet as a test? 🤗  Thanks, did not have the chance to approve but LGTM 🤗 ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

In #34089 some stuff might have not being copied correctly from T5. In pix2struct case past_key_value was being overridden by the first attention block, resulting in failure in cross attention.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
32600,2024-08-11T17:25:28Z,2024-09-25T18:05:01Z,manuelsh,21,10,54,893,16,3,1,['run-slow'],84650.0,0,6499206.0,0,0,0,0,5347777.1901,,0,54,0,False,"['manuelsh', 'HuggingFaceDocBuilderDev', 'amyeroberts']","@amyeroberts I have included the interpolation of positional embeddings in all the following models, and their respective tests:

1. altclip
2. bridgetower
3. chineseclip
4. clip
5. clipseg
6. kosmos_2
7. x_clip
8. git

Thanks! OK, I've:
- added your three suggestions (thanks!)
- I haven't removed the ` @unittest.skip(reason=""GitForCausalLM does not support inputs_embeds in generate method"")...` lines, as per my comment, please let me know
- I have run the slow tests with the git command sent. However, I don't think it is running the right slow tests as I just detected some errors on them which I am fixing now (for example in the ""bridgetower"" one)

Please don't merge yet, just need some time to check and potentially fix the tests. GIT model test still requires to be fixed. Getting this error:

```
tests.models.git.test_modeling_git.GitModelIntegrationTest.test_inference_interpolate_pos_encoding failed with error: <class 'IndexError'> index out of range in self
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/unittest/case.py"", line 59, in testPartExecutor
    yield
  File ""/usr/local/lib/python3.10/unittest/case.py"", line 591, in run
    self._callTestMethod(testMethod)
  File ""/usr/local/lib/python3.10/unittest/case.py"", line 549, in _callTestMethod
    method()
  File ""/usr/src/app/transformers/tests/models/git/test_modeling_git.py"", line 588, in test_inference_interpolate_pos_encoding
    outputs = model(**inputs, interpolate_pos_encoding=True)
  File ""/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/src/app/transformers/src/transformers/models/git/modeling_git.py"", line 1302, in forward
    embedding_output = self.embeddings(
  File ""/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/src/app/transformers/src/transformers/models/git/modeling_git.py"", line 115, in forward
    embeddings = self.word_embeddings(input_ids)
  File ""/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/local/lib/python3.10/site-packages/torch/nn/modules/sparse.py"", line 164, in forward
    return F.embedding(
  File ""/usr/local/lib/python3.10/site-packages/torch/nn/functional.py"", line 2267, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
```

due to `input_ids` having values out of range (`tensor([[49406,   768,   568,   530,   518,  2867, 49407]], dtype=torch.int32)`). In concrete `49406` and `49407` are not accepted. Not sure why the `processor` is adding them.

Still on it. @manuelsh Have you included the most recent updates from `main`?  Did it and still getting the same error. These two tokens (49406 and 49407) are special tokens added by the processor, they are  `<|startoftext|>` and `<|endoftext|>`. I can also see that the `word_embeddings` tensor has a dimension of `Embedding(30522, 768, padding_idx=0)`, i.e. a vocabulary of 30522.

I will do further debugging once I find time. In the meantime any suggestion is appreciated.
 @amyeroberts I found the source of the issue: the pre-trained model for GIT needed to be updated to the correct one. I think this should make it!

However, now I am getting the following integration error coming from `feature_extraction_audio_spectrogram_transformer`, even if I've synced the branch with the latest changes. I don't get anything related to this when I do `make fixup` or `make repo-consistency`.

```
Traceback (most recent call last):
  File ""/root/transformers/utils/check_repo.py"", line 1198, in <module>
    check_repo_quality()
  File ""/root/transformers/utils/check_repo.py"", line 1186, in check_repo_quality
    check_all_auto_object_names_being_defined()
  File ""/root/transformers/utils/check_repo.py"", line 742, in check_all_auto_object_names_being_defined
    if not hasattr(transformers, class_name):
  File ""/root/transformers/src/transformers/utils/import_utils.py"", line 1631, in __getattr__
    value = getattr(module, name)
  File ""/root/transformers/src/transformers/utils/import_utils.py"", line 1630, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File ""/root/transformers/src/transformers/utils/import_utils.py"", line 1642, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.audio_spectrogram_transformer.feature_extraction_audio_spectrogram_transformer because of the following error (look up to see its traceback):
libtorch_cuda.so: cannot open shared object file: No such file or directory

Exited with code exit status 1
``` Hi @amyeroberts I update main again with all changes and now it seems that all tests are passed, so it's ready to merge! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32600). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @manuelsh Great! As there's been a few changes since the last slow run, we'll need to do another `git commit --allow-empty -m ""[run_slow] altclip, bridgetower, chinese_clip, clip, clipseg, git, kosmos2, x_clip""`. Once those are all passing we're good to merge!  @amyeroberts I did a couple of fixes (one in another non related test, `test_inference_image_segmentation` in clipseg and another in GIT) and now all tests run. Hi @amyeroberts , I wonder if there is something missing or we can merge it. @manuelsh Thanks for all the work so far on this! Yes, there's a final iteration we'll need to do -- otherwise the code all looks good. 

Last week we merged in #33226. This fixed an issue in a lot of our vision models, which were using `scale_factor` in the `nn.functional.interpolate` call instead of `size`. This is in part needed to enable exporting to onnx and hence making the models compatible with transformers.js. 

Could you update the interpolate functions to use this updated logic flow? The tests shouldn't be affected.  @amyeroberts done, all `interpolate_pos_encoding` functions updated. @manuelsh Just the failing slow tests to address!  @amyeroberts , I think it is not just substituting `interpolate_pos_encoding` function, but one needs to adapt it, as the `position_embeddings` tensor from #33226 is different from the `position_embedding` object in the code of this PR (note the `s`).

I believe I can make them work with 

`self.position_embeddings = self.position_embedding.weight.unsqueeze(0)`

but now all my tests are crashing for different reasons (different tensors outputs for example) and this will take longer.

Why not getting back to the previous working commit ([d44e070](https://github.com/huggingface/transformers/pull/32600/commits/d44e07030f7ef63ab79e134f8f6650ec1a546147)), merge it, and then open another PR like #33226 but for the clip family models?

I would be happy to contribute to it. @amyeroberts I was able to fix all tests with the new function, so no need to do an additional PR. Please review. @manuelsh OK, great. Just the merge conflict to resolve an a final slow run to confirm everything passes now.  @amyeroberts I have resolved the conflicts, run the tests in slow, corrected one test in clipseg model not related to this PR that was not working `test_inference_image_segmentation`, and requested another slow run. Hopefully that's it! @amyeroberts there were two tensors to correct in clipseg. Done now. If you can kindly approve the run for clipseg slow tests. @manuelsh Done and all passing - let's merge! Thanks for this addition and patience on iterating on this :)  Fantastic! Glad to see it. Thank you!Thanks for adding - looks great! 

Just a handful of small nits. Before merge, we'll need to run the slow tests for the models affected. Could you trigger this by running `git commit --allow-empty -m ""[run_slow] altclip, bridgetower, chinese_clip, clip, clipseg, git, kosmos2, x_clip""` Beautiful - thanks for adding this capability to our models and for iterating on a solution! ","Thanks for adding - looks great! 

Just a handful of small nits. Before merge, we'll need to run the slow tests for the models affected. Could you trigger this by running `git commit --allow-empty -m ""[run_slow] altclip, bridgetower, chinese_clip, clip, clipseg, git, kosmos2, x_clip""` Beautiful - thanks for adding this capability to our models and for iterating on a solution! ","@amyeroberts as there were some conflicts with merging with main on #31900 (possibly due to the `make` scripts), I have reimplemented all the changes of #30783 in a new branch, which is rebased to main.","Nice :)  Let's remove this as this logic is independent of this PR 

```suggestion
``` ```suggestion
``` ```suggestion
        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):
``` ```suggestion
        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):
``` If I remove this, I will get the following error from the CI pipeline:

```
FAILED tests/models/git/test_modeling_git.py::GitModelTest::test_inputs_embeds_matches_input_ids_with_generate - ValueError: You passed `inputs_embeds` to `.generate()`, but the model class GitForCausalLM doesn't have its forwarding implemented. See the GPT2 implementation for an example (https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!
```
as shown [here](https://app.circleci.com/pipelines/github/huggingface/transformers/100559/workflows/33f186fa-5789-48ce-ab89-58d4eb6305eb/jobs/1339108?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-checks-link&utm_content=summary) Could you rebase on main? I believe this has been resolved upstream  This should still be removed as this tests is unrelated to this PR  Was this tested? I'm getting user reports of error `Input image size (352*352) doesn't match model (224*224).` regardless of what size of image is put in - I'm assuming somewhere in ClipSeg code it resizes to 352 - which makes sense considering docs seem to indicate that 352 is an expected value https://huggingface.co/docs/transformers/model_doc/clipseg#transformers.CLIPSegForImageSegmentation.forward.example while indeed also showing that image_size is 224 https://huggingface.co/docs/transformers/model_doc/clipseg#transformers.CLIPSegVisionConfig.image_size

ie: using ClipSeg exactly as documented will necessarily throw this error.

The original clipseg model built by huggingface staff defines image_size 224 in config https://huggingface.co/CIDAS/clipseg-rd64-refined/blob/main/config.json#L127
and the preprocessor size as 352 https://huggingface.co/CIDAS/clipseg-rd64-refined/blob/main/preprocessor_config.json#L20


So either this check is wrong, or official configs have been wrong for years, or there's meant to be some handling of the sizes between the two that's gone missing

EDIT: Posted issue https://github.com/huggingface/transformers/issues/34415 You need to add ` interpolate_pos_encoding=True` in the signature when calling the model as discussed in the issue https://github.com/huggingface/transformers/issues/34415."
34325,2024-10-22T22:46:45Z,2024-10-25T15:52:46Z,stevhliu,2,0,1,10,1,2,1,[],37059.0,0,234364.0,0,0,0,0,5372542.968579,,0,1,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev']","Clean! cc @gante @ArthurZucker let's not forget to also update here when new cache classes are added The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34325). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Great idea to use the links for the docs, and I think the name references are also useful for people checking the docstring in code. Thank you!","Great idea to use the links for the docs, and I think the name references are also useful for people checking the docstring in code. Thank you!",Follow up to comment [here](https://github.com/huggingface/transformers/pull/34299#issuecomment-2428741017) by @pcuenca to explicitly list the cache methods.,
33624,2024-09-20T14:22:39Z,2024-10-16T09:21:49Z,ylacombe,6,30,88,5355,24,3,2,['run-slow'],1018529.0,0,3053329.0,0,0,0,0,5348624.915883,,0,88,0,False,"['HuggingFaceDocBuilderDev', 'ylacombe']","Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). cc @gante and @ArthurZucker , there's still a few failing tests, but I think it'd be good to have a first review, given the model complexity  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33624). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. For the remaining failures: 

- quality issue seems linked to `VipLlavaForConditionalGeneration`

- tokenization tests - cc @ArthurZucker  - do you think it's linked to our offline discussion ?
<details><summary>Details tokenization</summary>
<p>
FAILED tests/models/moshi/test_tokenization_moshi.py::MoshiTokenizationTest::test_added_tokens_do_lower_case - AssertionError: '▁' == '▁'
FAILED tests/models/moshi/test_tokenization_moshi.py::MoshiTokenizationTest::test_alignement_methods - AssertionError: 0 != 5
FAILED tests/models/moshi/test_tokenization_moshi.py::MoshiTokenizationTest::test_training_new_tokenizer - AssertionError: 'This▁is▁the▁first▁sentence' != 'This is the first sentence'
FAILED tests/models/moshi/test_tokenization_moshi.py::MoshiTokenizationTest::test_training_new_tokenizer_with_special_tokens_change - AssertionError: 'This▁is▁the▁first▁sentence' != 'This is the first sentence'
</p>
</details> 

- sdpa and eager attention mismatch - I'm not really sure where it's coming from: the model (`MoshiDecoderTest`) is almost the same as `Gemma`

<details><summary>Details mismatch</summary>
<p>
FAILED tests/models/moshi/test_modeling_moshi.py::MoshiDecoderTest::test_eager_matches_sdpa_inference_0_float16 - AssertionError: False is not true : padding_side=left, use_mask=False, batch_size=1, enable_kernels=False: mean relative difference: 1.938e+00, torch atol = 0.005, torch rtol = 0.005
FAILED tests/models/moshi/test_modeling_moshi.py::MoshiDecoderTest::test_eager_matches_sdpa_inference_1_bfloat16 - AssertionError: False is not true : padding_side=left, use_mask=False, batch_size=1, enable_kernels=False: mean relative difference: 8.164e-01, torch atol = 0.01, torch rtol = 0.01
FAILED tests/models/moshi/test_modeling_moshi.py::MoshiDecoderTest::test_eager_matches_sdpa_inference_2_float32 - AssertionError: False is not true : padding_side=left, use_mask=False, batch_size=1, enable_kernels=False: mean relative difference: 6.824e-01, torch atol = 1e-06, torch rtol = 0....
</p>
</details> 

 Hey @SunMarc, we could benefit from your help here: When using device_map=""auto"", I'm facing a lot of issues, see [here](https://github.com/huggingface/transformers/actions/runs/11294837205/job/31416241230?pr=33624) and [here](https://github.com/huggingface/transformers/actions/runs/11294837205/job/31416241531?pr=33624). Everything passes locally though.
 Everything's green, I'll merge
Thanks @ArthurZucker and @SunMarc for the support!the generation part makes sense to me!

suggestion: because it is quite convoluted with nested generate calls, adding a block diagram explaining the workflow and linking it to the docstring in `def generate()` will likely make the life easier for us (long-term maintenance) and our user (can quickly understand what's going on) Small review, as always great work very much transformers like! 🤗 
From scanning, `use_flexible_linear` can be simplified in some places no? (ctrl f not super good on gh hub tho)  Very complex model! 
2 main blocking comments for me:
- einsums 
- embed_tokens looped. Let's think a bit, if not possible I don't mind skipping, but it's a huge loss there! (we had something similar for TILE embedding in Mllama!) let's synch on this 🤗  Thanks for your hard work! ","the generation part makes sense to me!

suggestion: because it is quite convoluted with nested generate calls, adding a block diagram explaining the workflow and linking it to the docstring in `def generate()` will likely make the life easier for us (long-term maintenance) and our user (can quickly understand what's going on) Small review, as always great work very much transformers like! 🤗 
From scanning, `use_flexible_linear` can be simplified in some places no? (ctrl f not super good on gh hub tho)  Very complex model! 
2 main blocking comments for me:
- einsums 
- embed_tokens looped. Let's think a bit, if not possible I don't mind skipping, but it's a huge loss there! (we had something similar for TILE embedding in Mllama!) let's synch on this 🤗  Thanks for your hard work! ","# What does this PR do?

Moshi is the latest Kyutai model. It is a streaming speech-to-speech model, that can also do an inner dialogue (i.e it outputs text as well).

In particular, it means that Moshi deals with 3 streams of information:
1. The user's audio
2. Moshi's audio
3. Moshi's textual output

Similarly to `Musicgen`, audio is represented with audio codebooks, which can be interpreted like tokens. The main difference between text tokens and audio codebooks is that audio codebooks introduce an additional dimension of information.
Text tokens are typically of dim `(batch_size, sequence_length)` but audio tokens are of dim `(batch_size, num_codebooks, sequence_length)`.
![image](https://github.com/user-attachments/assets/e790311d-11dc-4292-936e-51683c2b3fe4)

--------

It's made of 3 components:

**1. The main decoder (Helium in the paper)**

Here, it corresponds to `MoshiForCausalLM`. It is strictly a classic text LLM, that uses an architecture similar to `Gemma`. In other words, it takes text tokens, embeds them, pass them through the decoder and a language head, to get text logits.

**2. The depth decoder**

On its own, it's also a classic LLM, but this time, instead of generating over the time dimension, it generates over the codebook dimension.

It also means that its context length is `num_codebooks` -> it can't generate more than `num_codebooks`.

Another interesting difference with a classic LLM is that each timestamp (here it correspond to each codebook) got its own set of Linear Layers and Embeddings.

**3. Mimi**

It's the audio encoder from Kyutai, that has recently been integrated to transformers, which is used to ""tokenize"" audio. It has the same use that `Encodec` has in `Musicgen`.

--------

## Architecture choice:

1. `MoshiForCausalLM` corresponds to the main decoder, it can be used as a textual LLM.


2. `MoshiDepthDecoder` is the depth decoder mentioned above


3. `MoshiForConditionalGeneration` encapsulates the main decoder, the depth decoder and the audio encoder.

Conceptually, `MoshiForConditionalGeneration` takes as input one stream of text and two streams of audio inputs - what the user has said so far, and what the model have generated so far - and generates two streams - a text stream and an audio stream.

**How does it work:**

-> The input streams are embedded and combined into `inputs_embeds`.

-> `inputs_embeds` is passed through the main decoder. There's nothing special done here, it's the same operation as Gemma or so on. 

-> The main decoder outputs `text logits` but also its `last hidden state` which is called `temporal context` in the picture above. 

-> the depth decoder switches the dimension on which we generate (codebooks instead of time). It uses the token generated from `text logits`  and the `temporal context` to auto-regressively generate audio codebooks.



","cc @gante, this is the model that'll use a weird generation!

This is roughly how I envision generation. It works already, but there'll be some changes that will make the code a bit heavier I actually would like to allow dynamic outputs depending on the type of generation (beam, sample) etc., do you think I can do a nested `ModelOutput` ? My suggestion would be to make it as close as possible to the return structure from the original `generate`. Users transitioning from other models to `moshi` would then have as little friction as possible 🤗  that's quite ugly and a bit weird, would be nice if we could avoid this That's interesting `use_flexible_linear` is always True, I don't see where it's False so not needed for the DecodeLayer class no?  always `True` AFA i can read ```suggestion
# Copyright 2024 The HuggingFace Team. All rights reserved.
``` let's push to internal testing! ```suggestion
        # TODO: @ArthurZucker 
``` That's the word for it indeed! Since the depth decoder is really lightweight, they use this to emulate position embedding  It's actually quite the opposite, `MoshiDecoderLayer` has `use_flexible_linear=False` by default (in the main decoder thus). It's used only in the depth decoder. Same for this, in the main decoder, it doesn't use flexible linear It's super ugly, I agree!

Since it inherits from `GenerationMixin`, the latter will use a bunch of usual parameter names (e.g `hidden_size`) to initialize a lot of internal tools (e.g Cache). So we have to, somehow, either change the config or to modify how the internal tools work.

Note that another option would have been to use a nested config, but by experience, I found it a bit confusing + it adds an higher burden of maintenance + it introduces a lot of intricate `if/else`  in the modeling code Open to other suggestions! ```suggestion
    def __init__(self, config: MoshiConfig, layer_idx: int, use_flexible_linear, use_rope=True):
```
let's remove the default to be more explicit please ! 🤗  Would probably just use a small dict without nested config for that particular part!  yep sorry  it's gonna be super inneficient 😅
Pretty sure you can concat the embeddings as the order by which you a access it is linear: concat self.embed_tokens. concat dim is -1 instead of usual 0 (don't extend vocab size but embed dim)  unrelated but lots of PRs are adressing it !  `split=False` I am unsure about! ```suggestion
from typing import TYPE_CHECKING

from ...utils import _LazyModule
from ...utils.import_utils import define_import_structure


if TYPE_CHECKING:
    from .configuration_moshi import *
    from .modeling_moshi import *
else:
    import sys
    _file = globals()[""__file__""]
    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)

```
you should only need to define:
```python
__all__ = [""MoshiForCausalLM"", ""MoshiForConditionalGeneration"", ""MoshiModel"", ""MoshiPreTrainedModel""] 
``` 
for model and config for config that is a bit weird, we usually bass `audio_encoder_config` but this is more convenient for you i guess?  I guess this is the `depth_encoder_config` Not sure we want to scare people off, let's remove this and test this in the tests!  ```suggestion
# Copyright 2024 Kyutai AI and The HuggingFace Inc. team. All rights reserved.
``` ```suggestion
``` oh oh, we don't want einsums that's for sure. 
We have lots of instance where we remove this and code is a lot more understandable ```suggestion
        self._attn_implementation = config._attn_implementation
        self.gradient_checkpointing = False
        self.config = config
``` same comment about concatenating IMO you always access by range should be possible no? "
34405,2024-10-25T09:33:30Z,2024-10-25T15:52:29Z,rudydel,1,0,1,6,1,1,1,[],21290.0,0,22739.0,0,0,0,0,5372564.298073,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34405). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the fixes!,Thanks for the fixes!,"# What does this PR do?

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR, like @stevhliu in this case!",
34414,2024-10-25T13:20:07Z,2024-10-25T14:23:20Z,matthewdouglas,4,0,2,6,2,1,1,['Tests'],1658.0,0,8374.0,0,0,0,0,5373333.012215,,0,2,0,False,"['BenjaminBossan', 'SunMarc', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34414). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I saw that the issue was with the `OPTSdpaAttention` but why it doesn't work with it ? Any ideas ? cc @matthewdouglas  The issue was that the old check `""OPTAttention"" in repr(type(module))` didn't match any layer anymore, hence no LoRA layers were added, hence no trainable parameters were found, resulting in the error. Oh, I read the PR too fast. Thanks ! It makes sense now Thanks for identifying the issue and providing a fix. I tested the change locally and the tests now pass for me.",Thanks for identifying the issue and providing a fix. I tested the change locally and the tests now pass for me.,"# What does this PR do?

Fixes a test failure that resulted from the introduction of `OPTSdpaAttention` as the default implementation in #33298. Discussed internally on Slack.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@BenjaminBossan @SunMarc 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34408,2024-10-25T10:33:38Z,2024-10-25T15:14:07Z,ydshieh,1,0,2,23,2,1,1,[],1549.0,0,16832.0,0,0,0,0,5374864.861432,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34408). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.SGTM!,SGTM!,"# What does this PR do?

CUDA don't like this and will affect many subsequential tests.",
34386,2024-10-24T16:08:31Z,2024-10-25T10:55:07Z,gante,1,6,3,1031,22,3,2,[],1660.0,0,67599.0,0,0,0,0,5390405.588355,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34386). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice! This is sooooo great! ❤️ ,Nice! This is sooooo great! ❤️ ,"# What does this PR do?

`test_eager_matches_sdpa_generate` has failed in our new failure reporting system ([here](https://huggingface.co/datasets/hf-internal-testing/transformers_daily_ci/raw/ccbc621b1791f7713e2d8e977a481844a4a97140/2024-10-24/ci_results_run_models_gpu/new_model_failures_with_bad_commit_grouped_by_authors.json), cc @ydshieh )

Having a look at the test, the cause for flakiness was clear: we are using random models with `generate`, and tiny perturbations can result in a different sampled token, causing generation to go in a different direction and ultimately failing the check (eager generate == sdpa generate).

This PR:
1. Moves the test to `GenerationTesterMixin`, as it calls `generate`
2. Adds logic to handle the flakiness and do the correct check. If the generation is different, check the logits in the first different token. Despite resulting in a different token, the logits should be nearly identical -- if they are, the test pass.
3. Remove most overwrites, which only existed to handle flakiness through `is_flaky()`.

________________________________

The following test commands were run:
1. `RUN_SLOW=1 py.test tests/models/ -k test_eager_matches_sdpa_generate` ✅ 
2. `RUN_SLOW=1 py.test tests/models/gpt2/test_modeling_gpt2.py::GPT2ModelTest::test_eager_matches_sdpa_generate --flake-finder --flake-runs 500` ✅ ","(this is mostly copy-paste, going to comment the sections that are changed) Uses `self.prepare_config_and_inputs_for_generate()` instead, which enables us to pass a dictionary of inputs to `generate` (better input control than simply using `inputs_dict[model_class.main_input_name]`) Uses dictionaries -> more compact flakiness handling as explained in the PR header with #34282 we won't have to init 2 models. 
Also this is memory hungry TBH we can do it one model at a time, going to change the test.

After Flex attention becomes the norm, we probably won't need this test"
34372,2024-10-24T11:59:11Z,2024-10-25T10:46:46Z,gante,1,2,4,24,3,2,1,[],2285.0,0,82058.0,0,0,0,0,5390907.268555,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34372). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.🤗 ,🤗 ,"# What does this PR do?

Better example for applying SynthID watermarking.

(the original example was missing `return_tensor='pt'`, so I took the chance to improve other aspects)","```suggestion
    ... )
``` ```suggestion
    ... )
```"
34391,2024-10-24T18:39:55Z,2024-10-25T10:32:40Z,ydshieh,1,0,3,4,1,1,1,[],3842.0,0,57167.0,0,0,0,0,5391754.909415,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34391). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Looks good to me! ,Looks good to me! ,"# What does this PR do?

Avoid surprise of missing tests!

See the effect on [this run](https://app.circleci.com/pipelines/github/huggingface/transformers/108673/workflows/668bb1e2-bd66-4308-8f5a-a16d5e688079/jobs/1444870/tests)",
34380,2024-10-24T14:32:45Z,2024-10-24T16:34:28Z,michaelbenayoun,1,0,2,7,6,2,3,[],1666.0,0,70169.0,0,0,0,0,5393583.84105,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34380). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice! :) I am ashamed by how simple the fix is ! 
Can you remove the `skip` related to it? 🤗 

THanks! Thanks let's merge!","Nice! :) I am ashamed by how simple the fix is ! 
Can you remove the `skip` related to it? 🤗 

THanks! Thanks let's merge!","# What does this PR do?

As per title. This was introduced in #34283.

cc @ArthurZucker @muellerzr 
",
34305,2024-10-22T05:02:54Z,2024-10-25T09:02:07Z,zucchini-nlp,1,0,3,21,3,1,1,[],273224.0,0,273553.0,0,0,0,0,5397191.379247,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34305). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.thanks for the fix!,thanks for the fix!,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/33976 and fixes https://github.com/huggingface/transformers/issues/33662. As per title, fixes input merging when the text is right padded. 

I didn't add a test because we'll get rid of the old logic for merging in a few minor releases, but until then we should try to fix existing bugs if it's an easy fix.

Tested with 
```python
from transformers import AutoTokenizer, LlavaForConditionalGeneration
import torch

tokenizer = AutoTokenizer.from_pretrained(""llava-hf/llava-1.5-7b-hf"", use_fast=True)

model = LlavaForConditionalGeneration.from_pretrained(""llava-hf/llava-1.5-7b-hf"", device_map=0, torch_dtype=torch.float16)
model.eval()



prompt1 = ""System prompt<image>Caption 1<pad><pad><pad><pad><pad>""
prompt2 = ""System prompt<image>Caption 2<image>Caption 3""           # 12 tokens

print(""`System Prompt` tokens: "", tokenizer.encode(""System prompt"", add_special_tokens=False, truncation=False))
print(""`Caption 1` tokens: "", tokenizer.encode(""Caption 1"", add_special_tokens=False, truncation=False))

# Tokenize the prompt
input_ids = [
	tokenizer.encode(prompt1, return_tensors=""pt"", add_special_tokens=False, truncation=False).squeeze(0),
	tokenizer.encode(prompt2, return_tensors=""pt"", add_special_tokens=False, truncation=False).squeeze(0),
]
input_ids = torch.stack(input_ids)
assert isinstance(input_ids, torch.Tensor)

print(f""Input IDs: {input_ids.shape}"")
print(f""Input IDs: {input_ids}"")


with torch.no_grad():
	input_embeddings = model.get_input_embeddings()(input_ids.to('cuda'))

	embedded_images = torch.randn(3, 5, 4096, device='cuda', dtype=torch.float16)
	attention_mask = torch.ones(2, 12, device='cuda', dtype=torch.bool)
	result = model._merge_input_ids_with_image_features(embedded_images, input_embeddings, input_ids, attention_mask, None)[0]
	print(result.shape)

	print(""`System Prompt` diff: "", (result[0, :2] - input_embeddings[0, :2]).abs().max())
	print(""`image` diff: "", (result[0, 2:7] - embedded_images[0]).abs().max())
	print(""`Caption 1` diff: "", (result[0, 7:11] - input_embeddings[0, 3:7]).abs().max())

	print(""`System Prompt` diff: "", (result[1, :2] - input_embeddings[1, :2]).abs().max())
	print(""`image` diff: "", (result[1, 2:7] - embedded_images[1]).abs().max())
	print(""`Caption 2` diff: "", (result[1, 7:11] - input_embeddings[1, 3:7]).abs().max())
	print(""`image` diff: "", (result[1, 11:16] - embedded_images[2]).abs().max())
	print(""`Caption 3` diff: "", (result[1, 16:20] - input_embeddings[1, 8:12]).abs().max())
```",
34222,2024-10-17T13:54:29Z,2024-10-25T00:00:14Z,yonigozlan,2,4,4,216,7,3,2,[],439438.0,0,641145.0,0,0,0,0,5429705.178103,,1,4,0,False,"['HuggingFaceDocBuilderDev', 'yonigozlan']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34222). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Added tests @ArthurZucker :)Good job! This is a clean way to pass images so it's great that it's supported !
<img src=""https://media4.giphy.com/media/mGK1g88HZRa2FlKGbz/giphy.gif""/> Thanks, can you -properly test the different error scenario as it is done in: https://github.com/huggingface/transformers/blob/e4c19d7a6c6e41ed1beb9290d610b9c92222058a/tests/models/mllama/test_processor_mllama.py#L275 Thanks, missing one more!","Good job! This is a clean way to pass images so it's great that it's supported !
<img src=""https://media4.giphy.com/media/mGK1g88HZRa2FlKGbz/giphy.gif""/> Thanks, can you -properly test the different error scenario as it is done in: https://github.com/huggingface/transformers/blob/e4c19d7a6c6e41ed1beb9290d610b9c92222058a/tests/models/mllama/test_processor_mllama.py#L275 Thanks, missing one more!","# What does this PR do?
Unlike most other image-text-to-text models, Idefics2 and Idefics3 do not support inputs such as:
`images=[image1, image2], text=[""text1 <image>"", ""text2 <image>""]`
But needs images to be nested to follow the number of tokens in each prompt:
`images=[[image1], [image2]], text=[""text1 <image>"", ""text2 <image>""]`

This PR adds support for the former, and also for deducing the nesting of a flat image lists to follow the number of image tokens in each prompt, meaning that this is also supported:
`images=[image1, image2, image3], text=[""text1 <image>"", ""text2 <image><image>""]`

This is done mostly to have a consistent batching behavior in the [image-text-to-text pipeline](https://github.com/huggingface/transformers/pull/34170) between Idefics2/3 and other models.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->
## Who can review?

@andimarafioti 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","indexes can be determined from a cumsum here no? what about the case when there is 1 image token in 1 prompt, and 1 image: 
```python

        text = [
            ""This is a test sentence."",
            ""In this other sentence we try some good things<image>"",
        ]
``` Hmmm I think this won't raise an error here, I will try Oh it does raise an error, because it creates an empty list which is not accepted as an image, my bad!
Added the tests and synced them with a copied from"
33600,2024-09-19T17:34:47Z,2024-10-02T13:46:27Z,yonigozlan,4,4,6,95,5,3,2,['run-slow'],1522.0,0,3045138.0,0,0,0,0,5431695.131062,,0,6,0,False,"['HuggingFaceDocBuilderDev', 'yonigozlan', 'philkuz']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33600). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). Hi, I'm coming to this PR a month late, but I noticed there are some changes here that I think would be great to transfer over to other models that are down-stream of DeformableDetr, specifically Mask2Former.

I would love to contribute these changes, but to do this properly, I'd love if I could get more context on a few things that I couldn't discover from this PR, because the Notion doc is not public :)

1. How do you run the eager/compiled versions of these models? I tried a very naive pass at pulling a DeformableDetr model into `torch.compile` with no arguments, but that failed. Is there another path?
2. What is the underlying motivation here? Do you see users compile these models to run them faster in production environments?

For context, I'm trying to use the new-ish [torch.export() non-strict path](https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#non-strict-export) to export Mask2Former and I noticed that the changes in this PR include some of the same changes I would need to make to the Mask2Former modeling code. I would love to contribute these changes and have them align with the optimization objective you have in this PR! Hi @philkuz!
Great to see some interest in propagating these changes and making Transformers models more torch compile compatible :).
Here is a [published version of the notion page](https://huggingface2.notion.site/Vision-models-compilation-and-OptimVision-first-results-e2b47dabd1804cdc9cac5537ae5e9363), I haven't had the time to polish it and it's not quite up to date, but hopefully it can help answer some of your questions.

As for the motivation, in general I think ensuring Transformers models are fully optimized for compilation sets a nice standard. There has been more and more issues and feature requests related to torch compilation, so there seems to be growing interest in using it in production. With torch.compile and torch.export gaining traction, especially for performance-critical applications like real-time vision, I can see these optimizations becoming increasingly valuable.Looks great! 

Just two things before being merge ready: 
* We'll need to run the slow tests for these models
* Removing the baseline file Looks good to me! Just some nits and questions :)

Can you please provide a bit more details on what was changed and why that leads to a better performance? As far as I see the general idea of this PR is to avoid `.item()` or am I missing anything else?
 Thanks for adding and iterating! ","Looks great! 

Just two things before being merge ready: 
* We'll need to run the slow tests for these models
* Removing the baseline file Looks good to me! Just some nits and questions :)

Can you please provide a bit more details on what was changed and why that leads to a better performance? As far as I see the general idea of this PR is to avoid `.item()` or am I missing anything else?
 Thanks for adding and iterating! ","# What does this PR do?
This PR is part of an ongoing effort to optimize the inference speed of Transformers' vision models. (along with #33412 for now).
For more info on the specific issues these optimizations target and how they help improve the inference speed of compiled models, you can check out this [Notion page](https://www.notion.so/huggingface2/Vision-models-compilation-and-OptimVision-first-results-e2b47dabd1804cdc9cac5537ae5e9363).

The following metrics are for model inference only! Pre/post-processing time are not measured here. Currently, Transformers image processors seem to be a big bottleneck for inference speed, so end-to-end inference will sadly be much slower than this.

A few things to note:
- Other models which use deformable attention were modified due to ""copied from"", but the modifications should be non breaking. I haven't measured it but those models could also be slightly faster due to it :).
- Grounding Dino in particular could really benefit from more changes than those that were simply copied, but this could be part of another PR.
- Contrary to the optimization done on RT-DETR, here  both eager and compiled models are improved :).
- I have no idea what is going on with the compiled fp16 model without custom kernel. The trace shows full use of the GPU, and similar changes were made on RT-DETR with considerable gain in the same settings. If anyone has a theory I would be glad to hear it :).
- Deformable DETR (unlike RT-DETR) uses the custom kernel for deformable attention by default. But the user needs to have `ninja-build` installed for it to work.

![benchmark_results_deformable_detr_nightly](https://github.com/user-attachments/assets/6b4c7051-5fa7-4e4d-96bc-e4616b470e3b)


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@qubvel @molbap @amyeroberts @NielsRogge 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","To delete?  Just wondering, why should we use float16 instead of float32? shouldn't it be `pixel_values.dtype`? in case it holds here :)

```suggestion
        total_elements = sum(height * width for height, width in spatial_shapes_list)
``` Oops absolutely, thanks for catching that."
34388,2024-10-24T17:00:25Z,2024-10-24T17:17:52Z,Cyrilvallez,1,0,2,22,3,1,1,[],1972.0,0,1974.0,0,0,0,0,5452921.342927,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34388). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?

Fix duplicated variable coming from the recent changes and CI issues + fix duplicated import at the same time
",
34383,2024-10-24T15:32:31Z,2024-10-24T17:07:23Z,ydshieh,1,0,2,25,3,1,1,[],1739.0,0,5694.0,0,0,0,0,5454476.120916,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34383). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.👏🏻 thanks for the nits!,👏🏻 thanks for the nits!,"# What does this PR do?

Avoid having things like

```python
        ""gpt2"": {
            ""single-gpu"": []
        },
```
(see [for example](https://huggingface.co/datasets/hf-internal-testing/transformers_daily_ci/raw/ccbc621b1791f7713e2d8e977a481844a4a97140/2024-10-24/ci_results_run_models_gpu/new_model_failures_with_bad_commit_grouped_by_authors.json)) 

and add a few team members",
34377,2024-10-24T13:22:49Z,2024-10-24T16:42:03Z,Cyrilvallez,2,0,9,22,5,2,2,[],1218.0,0,11956.0,0,0,0,0,5455997.479218,,0,9,0,False,"['HuggingFaceDocBuilderDev', 'Cyrilvallez']",cc @ArthurZucker  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34377). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.ah nice catch! Makes sense 👍 ,ah nice catch! Makes sense 👍 ,"# What does this PR do?

This corrects the new defaults set in https://github.com/huggingface/transformers/pull/34026. The previous PR had the effect that if `max_length` is explicitly set by the user in generate, it is automatically overriden to a new value which is extremely confusing and not wanted.  
This adds a check to avoid this scenario.
",
34030,2024-10-08T19:56:43Z,2024-10-24T15:40:26Z,winstxnhdw,4,0,2,11,1,1,1,[],52119.0,0,1370352.0,0,0,0,0,5456367.509598,,0,2,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'winstxnhdw', 'ArthurZucker']",cc @ArthurZucker as you reviewed the other PR The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34030). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Bumping @ArthurZucker  This seems to break our CI tho not 100% sureYop sorry for the delay! Thanks 🤗 ,Yop sorry for the delay! Thanks 🤗 ,"# What does this PR do?

Removes a redundant if-condition as discussed in #33644. I have kept the argument type change to `Iterable` as the function can correctly accept any `Iterable`, not just `List`.",
34068,2024-10-10T14:34:03Z,2024-10-24T15:56:41Z,BenjaminBossan,5,0,1,102,2,1,1,[],1659.0,0,1214570.0,0,0,0,0,5458710.186201,,0,1,0,False,"['BenjaminBossan', 'LysandreJik', 'ArthurZucker']","@ArthurZucker Could you please review or suggest a reviewer? Ping @ArthurZucker  @SunMarc can you take a look as well if you have some bandwidth? Thanks for the review.

> Thanks, I am wondering why we won't init the model with adapter, load with transformers, then just do the matching we usually have in transformers, but that's more for me!

I'm not sure what you mean there, but if there is something I can look into, LMK.

Is there anything missing to merge the PR? If not, as I don't have rights, could you merge please :bow:  Don't worry it's for me ! 🤗 Thanks, I am wondering why we won't init the model with adapter, load with transformers, then just do the matching we usually have in transformers, but that's more for me! 
🤗 ","Thanks, I am wondering why we won't init the model with adapter, load with transformers, then just do the matching we usually have in transformers, but that's more for me! 
🤗 ","# What does this PR do?

When loading a LoRA adapter, so far, there was only a warning when there were unexpected keys in the checkpoint. Now, there is also a warning when there are missing keys.

This change is consistent with
https://github.com/huggingface/peft/pull/2118 in PEFT and the planned PR https://github.com/huggingface/diffusers/pull/9622 in diffusers.

Apart from this change, the error message for unexpected keys was slightly altered for consistency (it should be more readable now). Also, besides adding a test for the missing keys warning, a test for unexpected keys warning was also added, as it was missing so far.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?
",
34285,2024-10-21T14:52:29Z,2024-10-24T15:46:39Z,yonigozlan,1,2,1,6,1,3,2,[],85414.0,0,262450.0,0,0,0,0,5459325.631297,,0,1,0,False,['yonigozlan'],"@molbap Just tested with `Wav2Vec2Processor`, No handling conflict to report when using padding or other overlapping names as far as I see ;) Just curious, did you try with text/audio processors with overlapping names? No conflict in the handling logic? other than that LGTM! thanks for the fix :) ","Just curious, did you try with text/audio processors with overlapping names? No conflict in the handling logic? other than that LGTM! thanks for the fix :) ","# What does this PR do?
When a processor kwarg that is not supported by any modality is passed to a ProcessorMixin call, it is added to CommonKwargs whether or not it is supported by the ProcessorMixin's CommonKwargs object.
This means that any kwargs passed to a processor call will that is not supported by any modality will still be added to all modality kwargs, which can lead to errors. Example:
```python
from transformers import LlavaProcessor, LlavaForConditionalGeneration
processor = LlavaProcessor.from_pretrained(""llava-hf/llava-interleave-qwen-0.5b-hf"")
outputs = processor(text=""Hello, my dog is cute"", return_tensors=""pt"", test=True)
```
gives
```
output_kwargs: {'text_kwargs': {'padding': False, 'return_tensors': 'pt', 'test': True}, 'images_kwargs': {'return_tensors': 'pt', 'test': True}, 'audio_kwargs': {'return_tensors': 'pt', 'test': True}, 'videos_kwargs': {'return_tensors': 'pt', 'test': True}, 'common_kwargs': {'return_tensors': 'pt', 'test': True}}
```
And a `TypeError`

After this fix:
```
Keyword argument test is not a valid argument for this processor and will be ignored.
output_kwargs {'text_kwargs': {'padding': False, 'return_tensors': 'pt'}, 'images_kwargs': {'return_tensors': 'pt'}, 'audio_kwargs': {'return_tensors': 'pt'}, 'videos_kwargs': {'return_tensors': 'pt'}, 'common_kwargs': {'return_tensors': 'pt'}}
```

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->


## Who can review?

@molbap 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","I see! Good catch, no simpler way of catching these keys? (I assume not because of python version dependencies) I don't thinks so, I used the same format as the tests for catching modality keys just above, which seems to be robust across python versions"
34360,2024-10-23T20:56:50Z,2024-10-24T15:28:51Z,h3110Fr13nd,1,2,3,11,2,4,3,[],2711.0,0,66722.0,0,0,0,0,5460392.699001,,0,3,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34360). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM, thanks for fixing! Thanks! LGTM thanks 🤗 ","LGTM, thanks for fixing! Thanks! LGTM thanks 🤗 ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #34359 
Passes `make repo-consistency` by adding the _checkpoint_for_doc


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@Cyrilvallez @stevhliu @ArthurZucker ",Maybe use the non-chat version ([THUDM/glm-4-9b](https://huggingface.co/THUDM/glm-4-9b)) for token classification? Seems fair point. Updated as requested. @stevhliu 
34375,2024-10-24T12:53:47Z,2024-10-24T13:22:50Z,yonigozlan,1,0,1,4,1,1,1,[],1582.0,1,1743.0,0,0,0,0,5467955.084023,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34375). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.thanks!@,thanks!@,"# What does this PR do?

Fix an unprotected import in image_processing_detr_fast

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

## Who can review?

@ArthurZucker
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34333,2024-10-23T07:17:07Z,2024-10-24T12:42:48Z,furtnerthomas,0,0,1,2,1,1,1,[],,0,105941.0,0,0,0,0,5470358.747851,,0,1,0,False,[],Sure! Thanks @furtnerthomas ,Sure! Thanks @furtnerthomas ,"# What does this PR do?

add ""code generation"" to list of use cases in natural language processing section


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.


",
33389,2024-09-09T13:41:28Z,2024-10-24T13:11:00Z,junejae,5,15,11,204,5,3,3,[],181514.0,0,3886172.0,0,0,0,0,5468667.097477,,0,11,0,False,"['HuggingFaceDocBuilderDev', 'SunMarc', 'junejae', 'bonlime']","Does your code allow loading T5 Encoder XXL (the one used in Flux)? 
Example files here:
https://huggingface.co/city96/t5-v1_1-xxl-encoder-gguf/tree/main @bonlime Yes, it works with AutoModelForTextEncoding class. I've tested with T5 encoder of the exact repo you linked, but I didn't commit the independent test code block for T5 encoder since the code could be dirty with embedding vector style example output. @SunMarc
I've resolved conflicts and added more tests. Could you please review it again? Make sure to fix the CI also !  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33389). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for your work @junejae and sorry for the delay ! Just a few nits. There are a few merge conflits, can you fix them also ?  Nice, thanks a lot !  Boom, awesome! Thanks @junejae ","Thanks for your work @junejae and sorry for the delay ! Just a few nits. There are a few merge conflits, can you fix them also ?  Nice, thanks a lot !  Boom, awesome! Thanks @junejae ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Add T5 GGUF loading support

Due to the nature of T5's architecture, I decided to replicate [gguf's](https://github.com/ggerganov/llama.cpp/blob/54f376d0b92c6ff6feb1fa2ef8ed2022348100ba/convert_hf_to_gguf.py#L178) conversion [logic](https://github.com/ggerganov/llama.cpp/blob/4db04784f96757d74f74c8c110c2a00d55e33514/gguf-py/gguf/tensor_mapping.py#L568), so the final code gets messy.
I tried to avoid any logical conflicts between t5's and existing model architectures, but feel free to edit codes if you find any mistakes that I haven't noticed out.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Link: https://github.com/huggingface/transformers/issues/33260
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@SunMarc @LysandreJik @ArthurZucker , could you please review this PR?
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion

```
Remove whitespace from blank line Why do we need to hardcode these ?  Can you also add one test for a fp16 or fp32 gguf model, so that we can compare that the original model on transformers is the same as the gguf model ? We recently added bloom with these kind of tests so feel free to check there !  Thanks for adding this ! T5 architecture is a bit more complicated than llama or qwen, so it makes sense to do that.  Sure! I just added fp16 test and weights conversion test! I've found that almost every gguf versions of t5 models out there are actually flan-t5, and they need to set those config values when they get back to transformer's t5 class.
I didn't want to get deeper with unexpected risks, so I just hardcoded them. Therefore, feel free to fix it in clever way! Let's test all layers
```suggestion

``` Nice  Thanks for the explanation. Can you add a comment to explain this ? This is probably something that we need to fix in the future !  @SunMarc 
I've tested your suggestion before committing, and I found that only one type of weights have issue with `torch.testing.assert_close`.

Those weights' type is `DenseReluDense.wo`, and the error log is like this:
```
AssertionError: Tensor-likes are not close!

Mismatched elements: 401401 / 524288 (76.6%)
Greatest absolute difference: 0.0037021636962890625 at index (367, 773) (up to 1e-05 allowed)
Greatest relative difference: 0.0004879237385466695 at index (17, 619) (up to 1.3e-06 allowed)
```

I'm not familiar with situation like this, so I may need your help. Do you have any ideas? It looks like the `DenseReluDense.wo` weights are not the same in the gguf and the original model. I'm not sure why this is not the case. Could you print the weights to compare them ? I don't know if this is just an accuracy issue or the weights are totally different Sorry for delay.
I've printed both of them and I think it is just an accuracy issue.
```
# gguf model (repetitio/flan-t5-small, flan-t5-small-f16.gguf)
tensor([[ 2.2873e-02, -1.6003e-01,  1.9275e-01,  ...,  4.2480e-01,
          2.3706e-01, -1.1108e-01],
        [-1.1694e-01,  3.3911e-01, -2.9694e-02,  ..., -2.0239e-01,
          3.3862e-01,  2.6587e-01],
        [ 3.5913e-01, -3.4106e-01, -3.6597e-01,  ..., -1.2695e-01,
         -2.2125e-02, -5.1819e-02],
        ...,
        [ 5.2216e-02, -2.9443e-01, -1.6882e-01,  ..., -8.0688e-02,
         -2.5391e-01, -8.2779e-04],
        [ 3.0542e-01, -3.6335e-03,  5.0879e-01,  ...,  7.5317e-02,
          3.4326e-01,  3.1470e-01],
        [ 1.2158e+00,  5.9113e-02, -3.2568e-01,  ..., -3.1323e-01,
          2.7026e-01,  1.9165e-01]], device='cuda:0')

# original model (google/flan-t5-small)
tensor([[ 2.2874e-02, -1.6004e-01,  1.9277e-01,  ...,  4.2484e-01,
          2.3705e-01, -1.1111e-01],
        [-1.1692e-01,  3.3915e-01, -2.9694e-02,  ..., -2.0241e-01,
          3.3858e-01,  2.6584e-01],
        [ 3.5917e-01, -3.4098e-01, -3.6591e-01,  ..., -1.2691e-01,
         -2.2127e-02, -5.1824e-02],
        ...,
        [ 5.2229e-02, -2.9446e-01, -1.6881e-01,  ..., -8.0677e-02,
         -2.5395e-01, -8.2794e-04],
        [ 3.0539e-01, -3.6335e-03,  5.0860e-01,  ...,  7.5305e-02,
          3.4325e-01,  3.1474e-01],
        [ 1.2160e+00,  5.9106e-02, -3.2575e-01,  ..., -3.1322e-01,
          2.7032e-01,  1.9162e-01]], device='cuda:0')
``` Feel free to update the rtol so that the tests pass !  I left a fixme comment on this! Cool, I've raised rtol and atol to 5e-04!"
34342,2024-10-23T10:27:53Z,2024-10-24T12:29:33Z,LysandreJik,1,0,2,6,1,1,1,[],1547.0,0,93703.0,0,0,0,0,5471151.143562,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34342). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks,Thanks,,
34371,2024-10-24T10:04:46Z,2024-10-24T11:44:53Z,zucchini-nlp,1,0,1,3,2,2,2,[],1569.0,0,6007.0,0,0,0,0,5473835.769725,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34371). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thank you for the fix and the feedback!,Thank you for the fix and the feedback!,"# What does this PR do?

Fixes failures caused by some previously merged PRs from our internal daily CI feedback. Some errors like `eager_matched_sdpa` were flaky, so no need to fix those. 

InstructBLIP was failing because earlier we dispatched in ""eager"" by default and now we dispatch on ""sdpa"" for LLM

@ydshieh the new feature for CI feedback is perfect, and super easy to navigate and locate when the tests started failing. Thanks!",
34026,2024-10-08T11:12:56Z,2024-10-24T09:11:55Z,ArthurZucker,1,0,8,19,2,1,1,[],2332.0,0,1375148.0,0,0,0,0,5483005.303761,,0,8,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34026). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Ok!,Ok!,"# What does this PR do?
```python
>>> from transformers import pipeline
>>> pipe = pipeline(""text-generation"", model=""meta-llama/Llama-3.2-1B"")
>>> pipe(""Hey how are you doing my lord? Let's have fun! More and more tokens should be done. However long the input is"")
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
[{'generated_text': ""Hey how are you doing my lord? Let's have fun! More and more tokens should be done. However long the input is.""}]
```
this no longer fails saying that max_length is 20. We always generate 20 tokens more than the input. 
Also device should be inferred automatically",
34350,2024-10-23T15:09:44Z,2024-10-23T20:18:52Z,gante,1,15,52,2318,15,2,1,[],2800.0,0,18550.0,0,0,0,0,5529395.65849,,0,52,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34350). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.First path! Looks really good! Thanks for updating! 🤗,First path! Looks really good! Thanks for updating! 🤗,"# What does this PR do?

Adds SynthID, a watermarking by DeepMind.

https://deepmind.google/technologies/synthid/

Applying watermarking and using a detector is added to `transfomers`. Training a detector is added as a research project.
","let's import these explictly (`get_tokenized_uwm_outputs` etc)  same comment let's use explicit imports! same comment! same comment let's use explicit imports I have no idea what tpr and fpr means, let's either be explicit, or have a small docstring  we are passing the same kwargs (are almost) for all of these, let's try to re-use them, defining process_kwrags !  same here, we should have a small function doing these 3 operations  !  small docstring is welcome here as well!  no asserts let's raise a proper error here
 these are usually initialized in the `_init_weights` function rather than here!  Ihere we init with zeros or empty would be a lot better if we can avoid einsums! 🤗 
```
(i, j, k, l) x (i, j, k, l) -> (i, j, k)
```
would be:
```
(i, j, k, 1, l) x (i, j, k, l, 1) -> (i, j, k,1)
```
so:
```
self.delta[.., None,:] @ x.transpose(-2,-1)[..., None])
```
 expanded docstring 👍  It's not common, but we do have this pattern in other places ([e.g.](https://github.com/huggingface/transformers/blob/26370a4f18a44af529a84dd4c7a7fe7c37e67ed5/src/transformers/models/mllama/modeling_mllama.py#L928))

(I also have no idea how to set this specific initialization in `_init_weights ` 😅 ) Good idea!

(the correct form is then `(self.delta[.., None,:] @ x[..., None]).squeeze()`)"
34351,2024-10-23T16:02:29Z,2024-10-23T16:33:53Z,ArthurZucker,2,0,7,94,2,1,1,[],1691.0,0,3239.0,0,0,0,0,5541542.790221,,0,7,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker']",Let me sync The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34351). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?
Skips init container when your PR does not have the `run-benchmark` tag",
34324,2024-10-22T22:24:30Z,2024-10-23T08:52:52Z,stevhliu,1,0,1,1,1,1,1,[],1613.0,0,61993.0,0,0,0,0,5546268.323015,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34324). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks!,Thanks!,One more fix to the Korean `toctree` which was missing a `title` key,
34316,2024-10-22T14:30:49Z,2024-10-23T15:27:52Z,ydshieh,2,0,1,8,1,1,1,[],1689.0,0,89826.0,0,0,0,0,5546856.599282,,1,1,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34316). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. It's mentioned 

https://github.com/huggingface/transformers/pull/32550#issuecomment-2429494566

with the contributor replied.

I will open the issue if there is no further action from there this week.Please open an issue and ping the team member that should take a look so that we don't forget it :)",Please open an issue and ping the team member that should take a look so that we don't forget it :),"# What does this PR do?

Failing after #32550",
34208,2024-10-17T03:20:45Z,2024-10-23T14:48:22Z,aymeric-roucher,1,0,1,2,1,1,1,['Agents'],542826.0,1,559659.0,0,0,0,0,5549228.718515,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34208). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks @aymeric-roucher ,Thanks @aymeric-roucher ,"# What does this PR do?

Fixes #34056 by adding support for boolean tool args.

cc @LysandreJik @paulblgr


",
34308,2024-10-22T11:00:23Z,2024-10-23T09:15:36Z,FilipposVentirozos,4,0,2,2,1,0,0,[],12791.0,0,80114.0,0,0,0,0,5569196.295667,,0,2,0,False,"['LysandreJik', 'FilipposVentirozos']","Hey @FilipposVentirozos, have you tried running the script with DeBERTa? Does it run/train successfully? Hello @LysandreJik  yes it runs as intended. That is Deberta version 1 (microsoft/deberta-base). The newer versions (i.e. v2, v3) still not supported cause of the need of the slow tokenisation instead, may look into it more later. I tried again with `microsoft/deberta-v3-base` and it actually works. FYI, I tested it locally on my Mac and on my custom dataset and on Conll2003 that was for train, eval and predict. If you would like me to do any other experiment, please let me know. Ok let's merge it then! Thanks for your efforts",,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes in the token_classification example the support for Deberta. Since it was not included in an 'if' clause. Specifically, I added there since it's mentioned that Deberta needs the 'add_prefix_space' to be True and that 'if' clause covers this parameter. I tested it and it works well.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
32838,2024-08-15T20:38:37Z,2024-10-22T13:56:41Z,mikamerath,13,24,11,34,2,4,1,['Audio'],47122.0,0,5907244.0,0,0,0,0,5582573.502641,,0,11,0,False,"['HuggingFaceDocBuilderDev', 'mikamerath', 'amyeroberts']","cc @ylacombe  @amyeroberts @ylacombe The project I'm working on is depending on this change. If there's any clarifications I can make about the motivation of this feature please let me know 😄  @amyeroberts @ylacombe thank you for looking at this. It is hard for me to get a minimal working example because the problem is a little bit convoluted. Today or tomorrow, I can attach a GIF showing the problem and I'll try to come up with a minimal script that reproduces the error. 
![NotWorking](https://github.com/user-attachments/assets/b326d770-a661-4cda-8215-351b55729248)
![Working](https://github.com/user-attachments/assets/49c3f1fb-36f8-444e-a0f2-8fd298537d39)


@amyeroberts @ylacombe Sorry for the long wait. This bug is hard to reproduce with a minimal example so I'll try to explain the motivation and what is causing it. We're using Whisper's speech to text for a project that has a local client/server connection. This connection uses IPC (which in turn uses STDIN) to pass messages back and forth.

When running our project, we would notice that part of the logs would include the message `Enter command: <target>|all <time>|-1 <command>[ <argument>]`. While investigating the error, I stumbled upon [This StackExchange Post](https://unix.stackexchange.com/questions/36310/strange-errors-when-using-ffmpeg-in-a-loop) that discussed similar errors when using ffmpeg. One of the solutions mentioned that ffmpeg uses STDIN by default and passing `-nostdin` which disables interaction. When adding `-nostdin` to the end of the arguments in `ffmpeg_command` the problem goes away, as demonstrated by the pictures attached at the top of this message. The first photo shows the error as it occurs now, while the second photo shows the behavior when this change has been made.

So I wanted to make a minimal change that would allow this feature to solve this issue. The default behavior stays the same, but it adds the ability to pass `-nostdin` so ffmpeg is not in interactive mode. I have tested it thoroughly with the present changes and it works perfectly. If you have additional questions or concerns, I'm happy to answer them. @amyeroberts @ylacombe Any updates on your thoughts? @mikamerath Thanks for providing this information! This seems like quite a niche use-case and not an issue that hasn't been brought up before. 

I'd propose one of two things: 
* Adapting the microphone logic such that any additional arguments can be passed to ffmpeg to configure its usage. This way, we don't have to add and handle a new flag for everything in ffmeg. 
* Don't merge this PR and reopen if other people in the community request this feature in the future @amyeroberts Sounds good, I'll update this pull request this weekend to allow any additional arguments to be passed to ffmpeg. @amyeroberts The code has been updated to allow additional arguments to be passed into `ffmpeg_microphone_live` by passing a list of additional arguments via `ffmpeg_additional_args`  @amyeroberts I believe everything should be good now. Let me know what you think. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32838). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker Yes, I'm planning on adding a test for passing in arguments and not passing in arguments. Does that seem like it covers everything that you would want to see? @ArthurZucker Tests have been added. Is there anything else I need to do?Hi @mikamerath, thanks for opening a PR! 

I've left some comments on the diff of this PR. The PR will need an approval from @ylacombe to see if it's something that the audio team would want to add. 

Requirement for a personal project isn't sufficient enough reason for adding something into the library. The great thing about open source is you can build upon and adapt the code to your own needs! The PR would be better motivated if you could provide a working example (code snippet) to demonstrate where this is useful and would be useful for other users  Hey @mikamerath, thanks for opening up this PR!

As @amyeroberts suggested, could you explain in a few words why this PR is necessary? As I see it, the changes you make are not used anywhere else in the code base.

I've left a tiny question, besides Amy's remarks. I'll give a proper review once we agree on the next steps!

cc @eustlb if you want to give your opinion! Thanks for adding! 

Overall structure I think is OK - mainly some nits here and there on removing unnecessary formatting changes, making sure docstrings are as useful as possible and removing default empty lists as function defaults  She is OOO for a while, thanks for your contribution! IMO it's missing a small test but otherwise LGTM! 
Can you add one? 🤗  Thanks, sorry for the delay, merging! ","Hi @mikamerath, thanks for opening a PR! 

I've left some comments on the diff of this PR. The PR will need an approval from @ylacombe to see if it's something that the audio team would want to add. 

Requirement for a personal project isn't sufficient enough reason for adding something into the library. The great thing about open source is you can build upon and adapt the code to your own needs! The PR would be better motivated if you could provide a working example (code snippet) to demonstrate where this is useful and would be useful for other users  Hey @mikamerath, thanks for opening up this PR!

As @amyeroberts suggested, could you explain in a few words why this PR is necessary? As I see it, the changes you make are not used anywhere else in the code base.

I've left a tiny question, besides Amy's remarks. I'll give a proper review once we agree on the next steps!

cc @eustlb if you want to give your opinion! Thanks for adding! 

Overall structure I think is OK - mainly some nits here and there on removing unnecessary formatting changes, making sure docstrings are as useful as possible and removing default empty lists as function defaults  She is OOO for a while, thanks for your contribution! IMO it's missing a small test but otherwise LGTM! 
Can you add one? 🤗  Thanks, sorry for the delay, merging! ","# What does this PR do?
Gives the options for developers using audio_utils to fun `ffmpeg_microphone_live` as a background process instead of defaulting to STDIN which is the default option for ffmpeg. Useful for cases when running a different process that is using STDIN. The default case behavior is the same and now `ffmpeg_microphone_live` takes a boolean argument that allows the ffmpeg equivalent of passing in `-nostdin`.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #30061 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

- pipelines: @Narsil

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","There's no need to convert to string here  Is this right? Shouldn't the default be `-stdin`?  ```suggestion
            The boolean to determine whether or not to run the ffmpeg command in the background. If set to `True`,
            ffmpeg will not use STDIN as its input (useful for when other processes are using STDIN). If unset,
            ffmpeg will run using STDIN.
``` no need for big if/else here 

```suggestion
    microphone = ffmpeg_microphone(
        sampling_rate,
        chunk_s,
        format_for_conversion=format_for_conversion,
        ffmpeg_input_device=ffmpeg_input_device,
        background_arg=""-nostdin"" if not run_in_background else """",
    )
``` ""background_arg"" isn't very descriptive. It would be better to just propogate `run_in_background` to this function too if there's only two possible options and then set the correct string within this function  I'm not sure to follow here. If `run_in_background=False`, the microphone device is never defined? I'm pretty sure that it'd raise an error since it's used in the subsequent part of the code.
 You're right, the if statement should be removed, thanks for catching that! ffmpeg uses stdin to read its input by default, -nostdin allows ffmpeg to run as a background process You're right, I'm just propagating run_in_background now No mutables as default arguments please! 

The right way to do this is have this unset to `None` by default and then default to an empty list if unset 

```suggestion
    ffmpeg_additional_args: Optional[List[str]] = None,
```

then later on

```py
    ffmpeg_additional_args = [] if ffmpeg_additional_args is None else ffmpeg_additional_args
``` nit - formatting change shouldn't be applied here 

```suggestion
        with subprocess.Popen(ffmpeg_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE) as ffmpeg_process:
``` nit - same here 

```suggestion
        raise ValueError(""ffmpeg was not found but is required to load audio files from filename"") from error
``` better to specify what should type of objects can contained in the list 

```suggestion
        ffmpeg_additional_args (`List[str]`, *optional*):
``` It would be good to show a specific example here to show the user how to pass in the arguments e.g. `[""-nostdin""]`. How should the user pass in if the arguments are a flag then a series of options e.g. `ffmpeg -flag argument_1 argument_2`?  nit 

```suggestion
        print(""ffmpeg was not found. Please install it or make sure it is in your system PATH."")
``` nit 

```suggestion
        ffmpeg_devices = subprocess.run(command, text=True, stderr=subprocess.PIPE, encoding=""utf-8"")
        microphone_lines = [line for line in ffmpeg_devices.stderr.splitlines() if ""(audio)"" in line]
``` ```suggestion
        raise ValueError(""ffmpeg was not found but is required to stream audio files from filename"") from error
``` ```suggestion
        with subprocess.Popen(ffmpeg_command, stdout=subprocess.PIPE, bufsize=bufsize) as ffmpeg_process:
``` ```suggestion
def chunk_bytes_iter(iterator, chunk_len: int, stride: Tuple[int, int], stream: bool = False):
``` ```suggestion
    for item in chunk_bytes_iter(microphone, chunk_len, stride=(stride_left, stride_right), stream=True):
``` ```suggestion
        raise ValueError(f""Unhandled format `{format_for_conversion}`. Please use `s16le` or `f32le`"")
``` This shouldn't be removed  Same comments here - there should be an example of how to pass in multiple arguments in the docstring 

```suggestion
        ffmpeg_additional_args (`List[str]`, *optional*):
``` Same comment here - no mutable object defaults 

```suggestion
    ffmpeg_additional_args: Optional[List[str]] = None,
```"
33823,2024-09-30T15:46:08Z,2024-10-18T15:41:13Z,Cyrilvallez,7,12,75,2979,19,2,1,['run-slow'],64949.0,0,1929549.0,0,0,0,0,5603418.19433,,0,75,0,False,"['HuggingFaceDocBuilderDev', 'liyucheng09', 'zRzRzRzRzRzRzR', 'Cyrilvallez', 'ArthurZucker']","Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). Ready for last review @ArthurZucker, `setup_and_quality` fail because of the `__all__` issue, but will pass once https://github.com/huggingface/transformers/pull/33859 is merged. Thank you very much for your help. I also saw this [huggingface](https://huggingface.co/THUDM/glm-4-9b-chat/discussions/81) PR.
I have replied, and some of the code may need to be modified. Perhaps we can work together to improve it and merge this work.

Thank you again for your support! Of course!  > LGTM anything missing before we merge?

No, only issue are the docstrings in the configuration, but this will be solved with the auto-docstrings. In the meantime, I just moved the config outside modular to please the CIs. Confimed that slow tests pass for the model. Merging. @Cyrilvallez Hi Cyril, you PR for the 1M version of the model got an unexpected generation. Please refer to here for more information: [https://huggingface.co/THUDM/glm-4-9b-chat-1m/discussions/17](https://huggingface.co/THUDM/glm-4-9b-chat-1m/discussions/17).Very very nice! LGTM anything missing before we merge? ",Very very nice! LGTM anything missing before we merge? ,GLM model!,"```suggestion
# Copyright 2024 The HuggingFace Team. All rights reserved.
``` cool! Let's setup good standards however, see MLLAMA, full explicit regex are more informative IMO! 🤗  Let's try to use ** here for attributes that have the same name ```suggestion
class GlmSdpaAttention(GraniteSdpaAttention):
``` I think this should be enough holy molly so nice!  why do we have to overwrite this one?  Cool! In general the least we have to overwrite the better!  meaning are there ways to remove some of the tests you added?  I didn't to avoid adding unused fields, but I refactored to make that block nicer to read. Unfortunately, based on the random inputs there may be some times when one of the cases fail - I overwrote it to add the `flaky` decorator (which allows the test to consistently pass) Unfortunately no -- based on the random seed, some are failing from time to time, and they need to be `flaky` to consistently pass"
34139,2024-10-13T19:14:54Z,2024-10-22T17:26:16Z,h3110Fr13nd,2,0,3,93,13,1,1,[],167373.0,0,771082.0,0,0,0,0,5626159.295322,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'h3110Fr13nd']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34139). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Very nice, thanks!
> 
> The Qwen2 checkpoint should be [Qwen/Qwen2-7B](https://huggingface.co/Qwen/Qwen2-7B) and the Qwen2MoE checkpoint should be [Qwen/Qwen2-57B-A14B](https://huggingface.co/Qwen/Qwen2-57B-A14B).

Done. @stevhliu Very nice, thanks!

The Qwen2 checkpoint should be [Qwen/Qwen2-7B](https://huggingface.co/Qwen/Qwen2-7B) and the Qwen2MoE checkpoint should be [Qwen/Qwen2-57B-A14B](https://huggingface.co/Qwen/Qwen2-57B-A14B). Thanks! 👍 ","Very nice, thanks!

The Qwen2 checkpoint should be [Qwen/Qwen2-7B](https://huggingface.co/Qwen/Qwen2-7B) and the Qwen2MoE checkpoint should be [Qwen/Qwen2-57B-A14B](https://huggingface.co/Qwen/Qwen2-57B-A14B). Thanks! 👍 ","# What does this PR do?

I've add Examples of `LlamaForTokenClassification` and all it's dependent models.
The models include
- Llama
- Mistral
- Mixttral
- Nemotron
- Persimmon
- Qwen2
- Qwen2Moe
- StableLM
- StarCoder2
- Gemma (Modular)
- Gemma2 (Modular)

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@stevhliu Can you please review.",
33981,2024-10-05T17:39:54Z,2024-10-22T16:46:52Z,Jwaminju,0,4,4,90,2,3,1,[],,0,1465619.0,0,0,0,0,5628523.639146,,0,4,0,False,[],"번역 고생하셨습니다! 리뷰 1개 올렸습니다. LGTM, thanks! 🤗 ","번역 고생하셨습니다! 리뷰 1개 올렸습니다. LGTM, thanks! 🤗 ","# What does this PR do?

Translated the `model_doc/bartpho.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang 


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)
@stevhliu May you please review this PR?","주석 부분도 번역이 필요할 것 같습니다! (애매하네요 😅) ```suggestion
...     features = bartpho(**input_ids)  # 이제 모델 출력은 튜플입니다
``` 감사합니다! 수정했습니다! ```suggestion
      - local: model_doc/bartpho
```"
33890,2024-10-02T14:27:49Z,2024-10-22T16:46:31Z,ahnjj,0,2,6,82,2,2,1,[],,0,1736322.0,0,0,0,0,5628546.188773,,0,6,0,False,[],"Thanks, just need to fix the toctree error as pointed out! 🤗 ","Thanks, just need to fix the toctree error as pointed out! 🤗 ","# What does this PR do?

Translated the `bert japanese.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [ ] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR? ","```suggestion
      - local: model_doc/bert-japanese

```
<img width=""469"" alt=""image"" src=""https://github.com/user-attachments/assets/338ac810-237b-4969-9ead-b04c9f831cb7"">

local 부분에는 경로가 와야합니다.
en 폴더 하위의 _toctree.yml에서 그대로 복사해서 사용하시면 됩니다. ```suggestion
        title: 일본어 Bert
```"
33888,2024-10-02T14:00:14Z,2024-10-22T16:46:20Z,ahnjj,0,5,9,35,2,4,1,[],,0,1737966.0,0,0,0,0,5628557.766523,,0,9,0,False,[],"fix: docs suggestion Nice, thank you! Just need to fix the toctree as pointed out by @fabxoe!","fix: docs suggestion Nice, thank you! Just need to fix the toctree as pointed out by @fabxoe!","# What does this PR do?

Translated the `executorch.md file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [ ] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
 @stevhliu May you please review this PR? ","```suggestion
`torch.export`를 사용하여 🤗 Transformers를 익스포트 할 수 있도록  통합 지점이 개발되고 있습니다. 이 통합의 목표는 익스포트뿐만 아니라, 익스포트한 아티팩트가 `ExecuTorch`에서 효율적으로 실행될 수 있도록 더 축소하고 최적화하는 것입니다. 특히 모바일 및 엣지 유즈케이스에 중점을 두고 있습니다.
``` ```suggestion
    - local: main_classes/executorch
``` ```suggestion
    - local:  main_classes/feature_extractor
``` ```suggestion
      title: 특성 추출기
``` ```suggestion
    - local: main_classes/feature_extractor
```"
34235,2024-10-18T06:43:59Z,2024-10-22T16:46:08Z,faaany,0,0,1,8,1,1,1,[],,0,381729.0,0,0,0,0,5628570.655358,,0,1,0,False,[],"LGTM, thanks! I'll merge this once we've resolved the failing `build_pr_documentation` CI test :)","LGTM, thanks! I'll merge this once we've resolved the failing `build_pr_documentation` CI test :)","## What does this PR do?
As can be seen from the fix, the correct variable names for tensorflow and pytorch should be used. 

cc @stevhliu

",
34299,2024-10-21T21:14:43Z,2024-10-22T09:03:26Z,stevhliu,1,0,1,2,1,1,1,[],43432.0,0,69087.0,0,0,0,0,5629769.114952,,0,1,0,False,['pcuenca'],"Just a note that [this won't be expanded](https://github.com/huggingface/transformers/blob/93352e81f5019abaa52f7bdc2e3284779e864367/src/transformers/generation/configuration_utils.py#L173). Maybe an opportunity to list the cache methods explicitly and provide one-liners for each one, in a followup PR? Happy to contribute if you're busy.Damn, thanks for solving this @stevhliu!","Damn, thanks for solving this @stevhliu!",Fixes #34273 #33756  to display all the `GenerationConfig` parameters as shown [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34297/en/main_classes/text_generation). Can be merged after https://github.com/huggingface/transformers/pull/34293 which fixes the failing `build_pr_documentation` test.,
34237,2024-10-18T08:33:24Z,2024-10-22T16:02:42Z,akakakakakaa,2,0,2,10,1,3,3,[],45250.0,0,372558.0,0,0,0,0,5631177.915626,,0,2,0,False,"['akakakakakaa', 'larin92']","should probably remove `self` at [line 968](https://github.com/huggingface/transformers/pull/34237/files#diff-ed55888e6665791fe92cc8fc0c499da54f4ace6738551cd9a2591881cda076deR968) as well Thank you for pointing out what I missedIndeed, thanks for fixing !  Thank you!","Indeed, thanks for fixing !  Thank you!","# What does this PR do?

https://github.com/huggingface/transformers/pull/33514
this PR adds group_by_length support for evaluation. But, this part uses self.eval_dataset instead of eval_dataset. So, if eval_dataset is dictionary it fails.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
- trainer: @muellerzr and @SunMarc",
34236,2024-10-18T08:24:46Z,2024-10-22T15:57:44Z,yonigozlan,1,0,2,26,2,3,3,[],38543.0,0,372779.0,0,0,0,0,5631475.765298,,0,2,0,False,['yonigozlan'],"Added one test for llava processor :). I could add one for every vlms processor that use chat template, but as they all use the same underlying `apply_chat_template`, I thought it was not worth the diffs. Wdyt?LGTM, thanks! Maybe also cc @Rocketknight1  Yes, LGTM too! Cool! Can we have a small test please? 🤗 
 Thanks 😉 ","LGTM, thanks! Maybe also cc @Rocketknight1  Yes, LGTM too! Cool! Can we have a small test please? 🤗 
 Thanks 😉 ","# What does this PR do?

The `content`  field for an image-text-to-text model is a list, which is not currently taken into account when `continue_final_message` is set to True in tokenization_utils_base.
Split from [image-text-to-text PR](https://github.com/huggingface/transformers/pull/34170)

Reproduce error:

```python
from transformers import LlavaProcessor, LlavaForConditionalGeneration
import torch
from PIL import Image
import requests

processor = LlavaProcessor.from_pretrained(""llava-hf/llava-interleave-qwen-0.5b-hf"")

model = LlavaForConditionalGeneration.from_pretrained(""llava-hf/llava-interleave-qwen-0.5b-hf"", torch_dtype=torch.float16, low_cpu_mem_usage=True)
model.to(""cuda:0"")


# Define a chat history and use `apply_chat_template` to get correctly formatted prompt
# Each value in ""content"" has to be a list of dicts with types (""text"", ""image"")
messages = [
    {
        ""role"": ""user"",
        ""content"": [
            {
                ""type"": ""image"",
                ""image"": ""https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"",
            },
            {""type"": ""text"", ""text"": ""Describe this image.""},
        ],
    },
    {
        ""role"": ""assistant"",
        ""content"": [
            {""type"": ""text"", ""text"": ""There is a dog and""},
        ],
    },
]
prompt = processor.apply_chat_template(messages, continue_final_message=True)

inputs = processor(text=prompt, return_tensors=""pt"").to(""cuda:0"").to(torch.float16)

# autoregressively complete prompt
output = model.generate(**inputs, max_new_tokens=100)

print(processor.decode(output[0], skip_special_tokens=True))
```

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

@zucchini-nlp @ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34279,2024-10-21T10:16:00Z,2024-10-22T14:34:17Z,cecheta,3,0,1,10,1,1,1,[],83144.0,0,101897.0,0,0,0,0,5636484.138632,,0,1,0,False,"['LysandreJik', 'cecheta']","Could you just run the code quality tool to ensure that the code quality passes? You can install them with the following, from the root of your clone:
```
pip install -e "".[quality]""
```
And then run them with:
```
make fixup
``` > Could you just run the code quality tool to ensure that the code quality passes? You can install them with the following, from the root of your clone:
> 
> ```
> pip install -e "".[quality]""
> ```
> 
> And then run them with:
> 
> ```
> make fixup
> ```

@LysandreJik I've rebased my branch and run the tool, but it's still failing. However, I don't think it's related to my change

![image](https://github.com/user-attachments/assets/4a189b1d-6265-4e1e-8796-d50191a08513)
 Seems good! Thanks for your changes, mergingThanks!",Thanks!,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

This PR adds `MLFLOW_MAX_LOG_PARAMS` environment variable, to allow the user to limit the number of parameters logged to MLflow.

<!-- Remove if not applicable -->

Closes #34276 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

_Copying from #29032_

@muellerzr @pacman100 @noise-field @amyeroberts 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34181,2024-10-15T18:32:16Z,2024-10-22T13:53:01Z,guangy10,0,0,1,64,1,1,1,[],,0,588045.0,0,0,0,0,5638960.907509,,0,1,0,False,[],cc @ydshieh our slow tests load is gonna 🚀 ,cc @ydshieh our slow tests load is gonna 🚀 ,"# What does this PR do?

`OLMo-1B-hf` is compatible with `ExecuTorch`.

## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. #33840
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker
",
34102,2024-10-11T23:40:03Z,2024-10-22T13:52:23Z,guangy10,1,0,1,55,1,1,1,[],320402.0,0,915140.0,0,0,0,0,5638999.723976,,0,1,0,False,['guangy10'],@ArthurZucker do you mind reviewing this PR?LGTM sorry for the delay! 🤗 ,LGTM sorry for the delay! 🤗 ,"# What does this PR do?

Qwen2.5 is compatible with `ExecuTorch`.

## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. #33833
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker
",
34166,2024-10-14T21:59:35Z,2024-10-22T13:49:21Z,pbelcak,0,0,2,3,1,1,1,[],,0,661787.0,0,0,0,0,5639181.128058,,0,2,0,False,[],Sounds good! Thanks for updating ,Sounds good! Thanks for updating ,"Fix: tensor of examples of the same length triggers invalid stacking

# What does this PR do?

This PR fixes the issue that occurs when `DataCollatorForLanguageModelling` is used on whole tensors (e.g. `input_ids` being a long tensor of shape `(4, 2048)`). When this happens, `torch.stack` fails because it is expecting `examples` argument to be a tuple of tensors and not a tensor on its own.
Note that if the preprocessing step applied to the dataset (e.g. batched `dataset.map`) produces tensors of varying length, this occurs only in the very rare (and difficult to track down) special case when the collator happens to come across two tensors of exactly the same length. This is what occurred to me on the GSM8k dataset in roughly one out of every three runs.

@ArthurZucker",
33877,2024-10-01T23:11:33Z,2024-10-22T13:14:07Z,YHallouard,8,7,7,2,1,4,1,[],13.0,0,1779966.0,0,0,0,0,5639885.375632,,0,7,0,False,"['ArthurZucker', 'HuggingFaceDocBuilderDev', 'YHallouard']","Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). Hi @SangbumChoi, how are you ? I saw that you were the main maintainer of RT-DETR. Thank you very much for your work !

I propose a fix for the anchor generation to avoid a bug in onnx inference : `Error: Type parameter (T) of Optype (Where) bound to different types (tensor(float) and tensor(double) in node (/model/decoder/Where)`. It has already been discussed here: https://github.com/lyuwenyu/RT-DETR/issues/307

What do you feel about it ?  > Thanks for the contribution! For me it looks great,
> 
> 1. I think it would be helpful to add some ONNX conversion description script in `rt_detr.md`?
> 
> Requesting @qubvel for the final approvement!

Thanks for your reply ! Yes it would be interesting, what do you think about adding a section : 

----


## ONNX Tips
See `RTDetrConfig` in this section. (put the link to that section)
```python
from transformers.models.rt_detr.import RTDetrConfig, RTDetrOnnxConfig  # type: ignore[import-untyped]
from transformers.onnx.convert import export_pytorch  # type: ignore[import-untyped]

rtdetr_onnx_config = RTDetrOnnxConfig(config=RTDetrConfig(), task=""object-detection"")

export_pytorch(
    preprocessor=preprocessor,
    model=model,
    config=rtdetr_onnx_config,
    opset=17,
    output=output_path,
    tokenizer=None,
    device=""cuda"",
)
``` > t will be better to add this config to [Optimum](https://github.com/huggingface/optimum),

Hi @qubvel, honestly I wasn't aware of this repository ! Very interesting discovery ! 
I see how to put it in optimum, but since there is no `ORTModelForObjectDetection` yet in Optimum (thing i can work on maybe in the future). What do you think about putting this config (RTDetrOnnxConfig) in transformers and in optimum ?

But I can rework the `rt_detr.md`Onnx section, specifying in transformer section that is deprecated and also put a optimum section Hey @YHallouard sorry for the confusion, I think we need better doc on this  😢 let's not add anything that we know is already deprecated! If you need guidance on ONNX and Optimum contribution for this, I am sure @michaelbenayoun will be happy to help!  Hi @ArthurZucker, @michaelbenayoun, 

No problem, I removed th Onnx config and openned a pull request in optimum. https://github.com/huggingface/optimum/pull/2040 Hi @ArthurZucker, should I run a `run-slow` ? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33877). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the contribution! For me it looks great, 
1. I think it would be helpful to add some ONNX conversion description script in `rt_detr.md`? 

Requesting @qubvel for the final approvement! Hi @YHallouard, thanks a lot for your contribution! Overall looks good to me!

The only concern I have is that `onnx` export in transformers is [no longer maintained](https://huggingface.co/docs/transformers/serialization#exporting-a-model-with-transformersonnx) and at some moment we might end up removing it. 
It will be better to add this config to [Optimum](https://github.com/huggingface/optimum), leaving here just a snippet of code on how to export the model using Optimum.

cc @ArthurZucker regarding onnx Thanks! 🤗  LGTM now thanks for updating!","Thanks for the contribution! For me it looks great, 
1. I think it would be helpful to add some ONNX conversion description script in `rt_detr.md`? 

Requesting @qubvel for the final approvement! Hi @YHallouard, thanks a lot for your contribution! Overall looks good to me!

The only concern I have is that `onnx` export in transformers is [no longer maintained](https://huggingface.co/docs/transformers/serialization#exporting-a-model-with-transformersonnx) and at some moment we might end up removing it. 
It will be better to add this config to [Optimum](https://github.com/huggingface/optimum), leaving here just a snippet of code on how to export the model using Optimum.

cc @ArthurZucker regarding onnx Thanks! 🤗  LGTM now thanks for updating!","# What does this PR do?

- ~~Implement `RTDetrOnnxConfig` for RT-DETR~~
- Fix Error `Error: Type parameter (T) of Optype (Where) bound to different types (tensor(float) and tensor(double) in node (/model/decoder/Where)` during onnx inference. Already fixed on lyuwenyu/RT-DETR (https://github.com/lyuwenyu/RT-DETR/issues/307)

Fixes


## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. https://github.com/lyuwenyu/RT-DETR/issues/307
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [X] Did you write any new necessary tests? No, I didn't found any tests related to OnnxConfig. But I still open to add tests if you think it is required.


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
        anchors = torch.where(valid_mask, anchors, torch.tensor(float(""inf""), dtype=dtype, device=device))
``` 2ea9b757c2217344da37173fde41eb9a1be66496 Added some doc f15740647f341555e65720e942586419ad481d45 :) Tell me what do you think cc @yonigozlan re dtype/device for torch.compile ```suggestion
         
        anchors = torch.where(valid_mask, anchors, torch.finfo(dtype).min, dtype=dtype, device=device))
```
if the dtype is the dtype of `valid_mask` then this would make more sense ! Otherwise the min (float32 -inf) is not gonna be the same! dtype is the dtype of `anchors`, `valid_mask` is a condition, dtypes are the same but just by construction. 

But you're right, `torch.finfo(dtype).max` is good :)  1aa73fecfa800d828b16206e38da95bd8420cfe8"
34093,2024-10-11T14:16:22Z,2024-10-22T13:37:21Z,Cyrilvallez,3,0,1,230,9,1,1,[],8270.0,0,948062.0,0,0,0,0,5639900.735829,,0,1,0,False,"['Cyrilvallez', 'ArthurZucker']","#30642 should have been using `SlidingWindow` + test this no?  Could you add a test using your script?  Actually it does not depend on the Cache class used. The lines
```python
slicing_tokens = 1 - self.config.sliding_window

past_key = past_key_value[self.layer_idx][0]
past_value = past_key_value[self.layer_idx][1]

past_key = past_key[:, :, slicing_tokens:, :].contiguous()
past_value = past_value[:, :, slicing_tokens:, :].contiguous()
```

are just not doing anything. They are not modifying the Cache in-place, and then they are never reused later. I have no clue why they are still here.

`SlidingWindowCache` does not even have `__getitem__`, so doing

```python
from transformers import SlidingWindowCache, MistralConfig

past_key_value = SlidingWindowCache(MistralConfig(), batch_size=1, max_cache_len=100)
past_key = past_key_value[0][0]
```

raises `TypeError: 'SlidingWindowCache' object is not subscriptable`... Nice thanks",Nice thanks,"# What does this PR do?

This PR fixes FA2 attention for models using `sliding_window`. Currently, the removed snippet was not doing anything except assigning unused variables and slicing the potential `attention_mask`, which would result in wrong attention computation. Indeed, as soon as the `attention_mask` was not only `1`s (thus not `None`), and the sequence length was longer than the sliding window, we would incorrectly slice it.  

The fix is either to correctly slice the K-V, or to do nothing and rely on the `sliding_window` arg in `_flash_attention_forward`. I took the second option because it is easier and avoids unnecesary code.

If you run the following:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, StaticCache
import torch

device = 3

model_name = 'mistralai/Mistral-7B-v0.1'
dtype = torch.bfloat16
attn = 'flash_attention_2'

model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=attn,
                                            torch_dtype=dtype, low_cpu_mem_usage=True).cuda(device)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token


with open('text.txt') as file:
    long_text = file.read()
inputs = tokenizer.encode(long_text, return_tensors='pt').to(device=device)


generation_kwargs = {
    ""max_new_tokens"": 50,
    ""eos_token_id"": None,
    ""do_sample"": False,
}


inputs0 = tokenizer.encode(long_text, return_tensors='pt')[:, :4200]
# Get text slighly longer than sliding window
text = tokenizer.batch_decode(inputs0, skip_special_tokens=True)[0]
inputs = tokenizer([text, 'My favorite condiment is undeniably ketchup because it is very nice and'], padding=True, return_tensors='pt')

input_ids = inputs['input_ids'].to(device)
attention_mask = inputs['attention_mask'].to(device)

outputs = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)
tokenizer.batch_decode(outputs[:, 4200:], skip_special_tokens=True)
```

we would previously get gibberish:

```python
>>> ['and behavioralongraphyeterms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n', 't2222222222222222222222222222222222222222222222222']
```

and now (which is the same as when using `sdpa` attention:

```python
>>> ['and physical differences. Dogs have undergone a process of domestication and artificial selection, which has led to a range of morphological and behavioral changes.\n\nPhysical Differences\nDogs exhibit a wide range of physical',
 'tasty. I love to eat it with my fries, burgers, and other foods. I also love to use it as a dip for my fries.\n\nI have been using ketchup for a long time now,']
```

Slow tests for Mistral are all good (two failing, but on main as well)

@ArthurZucker 
",
34065,2024-10-10T12:54:22Z,2024-10-22T13:11:54Z,SunMarc,2,0,1,4,2,1,1,[],285.0,0,1037854.0,0,0,0,0,5641429.201695,,1,1,0,False,"['LysandreJik', 'ArthurZucker']","I can't cc @LysandreJik 👀 
 Done!",,"# What does this PR do?

This PR changes the PR templates. @MekkCyber will help with quantization maintenance from now ! 

Can you also add @MekkCyber to https://github.com/orgs/huggingface/teams/transformers team @ArthurZucker ? ",
34288,2024-10-21T17:10:52Z,2024-10-22T12:33:49Z,Rocketknight1,2,2,2,68,3,3,2,[],646.0,0,69806.0,0,0,0,0,5643688.515279,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']","cc @LysandreJik @Wauplin - this was a simple one, no Hub changes needed! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34288). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thank you! :fire: ",Thank you! :fire: ,,This one seems to be lacking a disclaimer; can you add it? Added!
34293,2024-10-21T19:20:34Z,2024-10-22T09:05:56Z,regisss,1,0,1,2,1,2,2,[],1633.0,0,53357.0,0,0,0,0,5652355.661375,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34293). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks so much!

@ArthurZucker ok to merge despite the unrelated failing test? Thanks!","Thanks so much!

@ArthurZucker ok to merge despite the unrelated failing test? Thanks!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Bad indentation.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34089,2024-10-11T11:11:05Z,2024-10-22T06:23:53Z,zucchini-nlp,1,2,34,3921,22,3,2,[],852644.0,0,933168.0,0,0,0,0,5665914.536583,,0,34,0,False,['zucchini-nlp'],"Will merge this later today if we are all okay. The comment for tokenization is added within code with a TODO for us, in case it is not a bug we can remove the comment laterThank you for this refactor 🙏 

I've left a question for us to double-check, but approving since it looks good to me.

______
⚠️ before merging, make sure to run the following slow tests to confirm that there are no regressions vs `main`: {all touched models} + {llama, whisper, caches} NICE! 🔥 thanks for adding the compile integration tests 🤗 ","Thank you for this refactor 🙏 

I've left a question for us to double-check, but approving since it looks good to me.

______
⚠️ before merging, make sure to run the following slow tests to confirm that there are no regressions vs `main`: {all touched models} + {llama, whisper, caches} NICE! 🔥 thanks for adding the compile integration tests 🤗 ","# What does this PR do?

Same as https://github.com/huggingface/transformers/pull/33754, I accidentally force pushed to wrong branch and the PR got closed 🙃 ","Is this a new failure? 

If yes: can you confirm a) the generated tokens are different b) compare the logprobs of the different tokens before/after the change, to see whether it can be explained by tiny fluctuations?

If no: can you add a comment with the PR that causes the difference? 🙏  Added a comment, the regression was caused by changes in the tokenization. So nothing breaks in terms of modeling or generation. Not sure if the fix in tokenization was intended to break T5 (i.e. it actually fixes T5) or we need another PR to fix tokenization back, so I'll leave a comment for future us "
34175,2024-10-15T14:08:23Z,2024-10-22T05:56:35Z,zucchini-nlp,1,0,4,788,9,1,1,[],1619.0,0,575293.0,0,0,0,0,5667552.533547,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34175). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks 🤗 ,Thanks 🤗 ,"# What does this PR do?

As mentioned in https://github.com/huggingface/transformers/issues/33948, this PR simply refactors code a bit to make it more modular, Specifically we now will have special public methods for obtaining image/video features that users can easily overwrite if they want to modify the process. In any way this makes less code in forward and more standardization in API",
34180,2024-10-15T18:17:28Z,2024-10-18T15:54:58Z,Rocketknight1,11,2,11,4,1,3,1,[],104.0,0,529481.0,0,0,0,0,5698420.81028,,0,11,0,False,"['LysandreJik', 'molbap', 'Rocketknight1', 'yonigozlan']","cc @NielsRogge UDOP seems to be failing in the CI because the image type is `uint8`, which causes an overflow error when it's scaled by a negative number during normalization. I'm guessing this means we need to set the dtype to be `float32` there, but I'm not sure exactly where to make the change. Figured it out, the cause is:

- `do_resize` and `do_rescale` both `True`
- Specific `PIL` version

Because `PIL` is used for the resize, the output of the resize can sometimes by `np.uint8`, the standard image format, rather than `np.float32`, which is necessary once data is being rescaled. cc @NielsRogge @LysandreJik for review! cc @qubvel or @molbap for CV review as well I'd appreciate a quick review on this one, since it's blocking other PRs, and it's only one line! Hey @Rocketknight1, sorry about that, quick review to unblock: I'd be more pro-having the modification within the definition of `self.resize`, in order to keep the pattern consistent with the other classes. 
You could even catch the PIL version causing issues, parse it with `packaging.version`, and if it's above/below the version causing issues do the dtype switch in that case, right? + commenting on why this is needed! @molbap I think it's simpler to just do `.astype(np.float32)`, firstly because I can't figure out exactly which version in the CI triggers this issue, and secondly because `.astype(np.float32)` on a `float32` array [does not copy or modify the array](https://numpy.org/devdocs/reference/generated/numpy.ndarray.astype.html) if we use `copy=False`, so there's no performance penalty to just calling it anyway.

Still, I'll move it inside `self.resize()`! Investigated further, and the issue is not where I thought! The cause is actually this PR to Numpy, which changes type promotion rules: https://github.com/numpy/numpy/issues/25639

In fact, `uint8` is the expected dtype and this hasn't changed. What **has** changed is that this line:

`rescaled_image = image * scale`

fails in cases where `image` is `uint8` and `scale` is a negative integer, which occurs in the UDOP tests. Previously, this would silently promote the dtype to `np.int16`, but this is no longer the case. The reason we don't see more problems is that multiplying a `uint8` array by a positive/negative **float** still silently promotes the array, and this is much more common. Only UDOP uses a negative **integer** scale in its tests.

This means the fix actually needs to go in `image_transforms.py`, one sec! Pushed the fix - now we always manually upcast to `np.float64`. This was what happens silently 99% of the time anyway, but now we make it explicit, which also fixes the cases where promotion doesn't silently happen anymore. Thanks @Rocketknight1 ! cc @yonigozlan here, on the negative scale x uint8 issue I mentioned last time (just FYI) Thanks for flagging this! I'll also change the ProcessorMixin tests to make the type promotion more explicit in case they change it again :) Thanks, our ci has been red because of this for quite a while!","Thanks, our ci has been red because of this for quite a while!","The UDOP processor uses the LayoutLMV3 image processor, but this results in dtype errors in the CI. Trying to fix it in this PR!","Shouldn't this be fixed in the modeling code? Would be inconsistent compared to all the other image processing classes we have Unfortunately, I think the conversion to `uint8` happens inside `self.resize()` (with some PIL versions), and then the error is thrown when we try to scale the data afterwards. It's quite hard for me to tell exactly where it's happening, though, because I can't reproduce the issue locally - it only occurs on the CI machine!

I'm not 100% sure but I think what happens is:

1) data before resize is `np.float32`
2) `self.resize()` creates a `PIL.Image` to do the resize
3) On some PIL versions this converts data to uint8, on others it remains as float32
4) The output dtype therefore depends on the PIL version"
32238,2024-07-26T06:53:14Z,2024-10-22T04:54:44Z,zucchini-nlp,19,30,74,2679,65,5,1,['run-slow'],1570.0,0,7596090.0,0,0,0,0,5671266.040722,,0,74,0,False,"['amyeroberts', 'HuggingFaceDocBuilderDev', 'nbroad1881', 'zucchini-nlp', 'ydshieh', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32238). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Regarding

```
MODEL_MAPPING.get(type(config.text_config), None)
            if text_model_cls is not None:
```
it could probably reuse

```
def _get_model_class(config, model_mapping):
```

But both will be somehow slow as you mentioned, and sometimes give unexpected errors (because loading a modeling module file). But we might have someway to avoid that. @ydshieh @qubvel I made some changes after our last discussion that changing class attr is not good. So now we don't change the attr per se, but we change the config's `attn_impl` manually while setting sdpa. I checked that it works this way also. The last question is whether to set it to SDPA when both sub-configs support it or at least one supports it. 

I will add more tests if we agree on the design


And also, regarding
> But both will be somehow slow as you mentioned, and sometimes give unexpected errors (because loading a modeling module file). But we might have someway to avoid that.

unfortunately we can't do that, cause it raises error if key is not in the dict and we have not auto-mappable models. So we rather try to get and if not simple continue.  
> unfortunately we can't do that, cause it raises error if key is not in the dict and we have not auto-mappable models. So we rather try to get and if not simple continue.

but why not try except around the usage of `_get_model_class`? I don't feel very strong here, but just it is something well designed in order to get the model class which is good if we can reuse it. Hmm, not sure how good if to use a `try except`, can it be another source of hidden bugs even if we use `except AttributeError`? I'm okay with any way, as long as it's consistent with the library and doesn't cause bugs As we discussed internally, I made the loading logic model-agnostic. So now we check if the config contains any `PretrainedConfig` inside and set attn implementation to each `PretrainedConfig`. This solved vision-encoder-decoder problem we had in slack and makes us more versatile for possible future multimodal models.

I added some tests in vision enc-dec models where only one sub-config supports SDPA and when both support. Also added tests in Idefics (which were skipped earlier) to verify that LM has its SDPA dispatched even though other modules do not.

TODO in the next few hours or tomorrow:
- Add similar tests to encoder-decoder models, not only vision enc-dec
- Accept from users a config with separate `attn_implementation` already set, such that `attn_implementation={""vision_config"": ""sdpa"", ""text_config"": ""eager""}` I made final changes to the PR and made sure all tests are passing and all composite models were modified. The main points are:

- we have a `Is-composite` flag for models so that we know if we want to dispatch on each sub-config or not. The flag was needed for models like Chameleon that have a sub-config, but it's for VAE which will not supports SDPA i guess
- attn implementation can be set as prev (""sdpa"") or as a dict in case of composite models ({""encoder"": ""sdpa"", ""decoder"": ""eager""}). For composite models, in first case we try to dispatch requested attn in all sub-models and raise error if not supported. In second case we dispatch the different attn for each sub-model as requested and also raise error if not supported
- Idefics and MusicGen models can get rid of custom code for attn implementation now
- For testing I had to add another flag `is_multimodal` (maybe will change to `is_composite`) and check each sub-config's attn implementation is set correctly. Also added tests in enc-dec models
- For some models which were not automappable, I had to add them to mapping so that we can get from config to class, and obtain `supports_sdpa` flag


One last thing to consider is that this might result in a bit of slow-down as we try to do automapping for each sub-model and dispatch separately. I have another idea, we can stop dispatching in each sub-model and instead simply replace ""str"" attn implementation with ""dict"" as `attn_implementation = {""encoder"": requested_attn, ""decoder"": requested_attn}`. Then inside modeling we could do `AutoModel.from_config(config, attn_impl = config._attn_impl[""encoder""])`. I testes with a few models and verified it works same way as current PR but did not introduce those changes yet. First, I want to get a general feedback if the PR is doing what we want. cc @amyeroberts, would like to know if I addressed your concerns about hidden magic code Made updates and removed automapping, seems we don't need it to dispatch in submodules. Added more tests, specifically for FA2 and for the new way of setting attn impl (as a dict in composite models). All the tests I added/touched are passing for me locally + non-slow tests in all models.

Some FA2/SDPA tests are failing but those seem to be failing disregarding the current changes. Requesting re-review, failing tests are not related and should have been skipped

UPDATE: Ran slow tests on LLaVA, Paligemma, Llama, Idefics1 and 2. All is green Will [this warning](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/discussions/23) stop appearing after this PR is merged? @ncoop57 not really in the current state, because we always call `from_config(attn_implementation=""my_attn_impl"")` to initialize a sub-component and we don't indicate `dtype`. Actually, now that I look into that, seems like we could call simply `SubModule(config)` instead of `_from_config`. The only thing is that directly calling class won't work with deepspeed, according to code. I'll check this out, should be also fixed in scope of this PR TODO: New VLMs were added while PR was in progress, so we need to propagate new design to:
- [ ] Qwen2-VL
- [ ] LLava-Onevision
- [ ] Idefics2
- [ ] Some more coming soon


In the meanwhile, @amyeroberts or @ArthurZucker , can I get a review on general design ? >  i.e. if it's compatible in one block but not the other an error will still be raised?

Yes this is the behavior now. Sorry I forgot to update PR description so let me describe the expected behavior.

For non-composite models nothing changes and the attention is dispatched same way as before. For composite models:

- if user just loads the model without any explicit attention (`Model.from_pretrained(model_id)`), then we dispatch SDPA in any of the sub-models whenever possible. In case any/none of the vision/text models support SDPA, we silently fallback to eager. No warnings, no errors, which is in line with non-composite models
- if user specifies required attention, we check if all of the sub-models support it. An error will be raised on sub-model level if the attention is not yet supported, so instead of `Llava doesn't support FA2`, we'll raise `CLIP doesn't support FA2`
- if user specifies attention for each sub-model when loading, as a dict (`Model.from_pretrained(model_id, attn_implementation={""vision_config"": ""eager"", ""text_config"": ""sdpa""})`), then we dispatch required attention to indicated models. We raise an error if any of the requested attentions is not supported by the indicated sub-model. When using attn implementation as a dict the user has to indicate attn for all sub-models, no defaults will be available. And the dict keys have to be identical to the `ModelConfig` attributes for sub-configs. In other words, if the config has `config.text_config`, then the dict key must be `text_config` to correctly dispatch > * if user specifies required attention, we check if all of the sub-models support it. An error will be raised on sub-model level if the attention is not yet supported, so instead of `Llava doesn't support FA2`, we'll raise `CLIP doesn't support FA2`

@zucchini-nlp Great, thanks for explaining. Behaviour all sounds good to me!  I think now this is ready for review, @ArthurZucker 

I rebased main and updated all new models, addressed some comments from Amy and added a few more tests Related to #33953 as well, will have a look, but having to change modeling code means we are doing something wrong as it should be a layer of abstraction!  Cool thanks! The modeling is mostly changed to get the attn implementation from dict and for IDEFICS (1 and 2), which fall out of standards and enforce hard requirements on sdpa. Hope we can get this merged soon, as it is getting more of a problem when new quirky models are added @ArthurZucker I made changes as you suggested but I want to warn that it still needed a few extra logic. Making this PR revealed soo many issues. So we have two options now to indicate attn for composite models:

**Attn implementation as a dict:**
The prev solution when we simply copy paste the user-requested attn in all sub-configs, and form a dict. So that the general `config.attn = Dict(text_config: None, vision_config: None)`
+ no need for flag whether attn implementation is set or not, because when we have a dict we always use the very first requested attn in all sub-configs. We just need to save it as a dict once and then go back to that dict to ""get()""
+ we don't need to unset attn implementation when saving so one can set a different attn when loading any model back.
- BUT we change a lot of model code to pass in explictly (attn_implementation=config._attn_implementation[""text_config""]) and we need to account for cases like Chameleon or DBRX where the sub-configs don't even need attn

**Attn implementation directly propagated in corresponding configs:**
Current solution when we simply propagate the attn to all configs we can find, and don't touch the general config
+ no need to change any modeling code, except for general clean up
- BUT we need a flag attn_implementation_autoset  so we don't dispatch attn on the same config twice. Also we would need to unset the flag when saving. See below for reason

Imagine we have a composite or nested config. Currently we set attn in three places: `from_pretrained`, `from_config` and `init`. When we don't require any attn (`None`) and load with `model.from_pretrained(id)`, internally we will call `autoset_attn`  and set SDPA to the general config and the sub-configs will get `None` propagated. But then the same method `autoset_attn`  is called second time when we init the class, and this time we'll try to propagate SDPA to all sub-configs (because general config has SDPA already set). If any sub-module has no SDPA support, an error will be raised as it was requested directly. From user perspective they did nothing wrong, but they see an error.
So the decision is to either stop autosetting attn for each config once it was set, which is happening now. Or in prev version I had attn implementations saved as dict, which basically did the same. When we got a dict attn, we had nothing to set and simply returned If we find a clean way to auto_set attn implementation would be better in the futur!  merging tomorrow, the tests pass locally in most cases except fro the cases not touched by this PR. For ex some FA2/sdpa equivalence tests are failing for me in `main` branchThanks for working on this! Not sure I fully understand the logic at this moment, so I left a few comments below Really great piece of work - thanks for taking the time and energy to tackle this tricky behaviour and find such a nice solution! 

Some general comments about idefics and a iterating over the config. Otherwise looks great!  Overall structure looks great to me. 

Just some small comments on the PR atm, although I was scanning more high-level. 

One question I have is about the checks we have for an individual model versus composite models. My understanding at the moment is that if I have a single model where I do: `Model.from_pretrained(checkpoint, attn_implementation=""flash_attention_2)""`, then we end up calling `Model._check_and_enable_flash_attn_2` which will verify if FA2 is available and if the model is FA2 compatible. 

Am I right in saying if `Model` is a composite model, e.g. with a vision block and a language block, and I called `Model.from_pretrained(checkpoint, attn_implementation=""flash_attention_2)""` then: 
* Both blocks would have their attention assigned as flash attention 2?
* Verification would be run for those individual blocks respectively, as `cls._autoset_attn_implementation` is called in `_from_config`? i.e. if it's compatible in one block but not the other an error will still be raised?  Thanks, it is indeed important work!  Modeling changes are just perfect 😉 before we have a big refactor this is super super welcome","Thanks for working on this! Not sure I fully understand the logic at this moment, so I left a few comments below Really great piece of work - thanks for taking the time and energy to tackle this tricky behaviour and find such a nice solution! 

Some general comments about idefics and a iterating over the config. Otherwise looks great!  Overall structure looks great to me. 

Just some small comments on the PR atm, although I was scanning more high-level. 

One question I have is about the checks we have for an individual model versus composite models. My understanding at the moment is that if I have a single model where I do: `Model.from_pretrained(checkpoint, attn_implementation=""flash_attention_2)""`, then we end up calling `Model._check_and_enable_flash_attn_2` which will verify if FA2 is available and if the model is FA2 compatible. 

Am I right in saying if `Model` is a composite model, e.g. with a vision block and a language block, and I called `Model.from_pretrained(checkpoint, attn_implementation=""flash_attention_2)""` then: 
* Both blocks would have their attention assigned as flash attention 2?
* Verification would be run for those individual blocks respectively, as `cls._autoset_attn_implementation` is called in `_from_config`? i.e. if it's compatible in one block but not the other an error will still be raised?  Thanks, it is indeed important work!  Modeling changes are just perfect 😉 before we have a big refactor this is super super welcome","# What does this PR do?

Related to #32221 and fixes #30565. As described in the issue, currently MultiModal models set attn implementation only in the LM backbone, and not in vision backbone. While working on it, I found another bug. Specifically, using a property to set SDPA flag to VLM class doesn't work because we don't know what is `self.LM` before init the VLM. I tried class property, but that would throw error that `self has no attribute LM`. 

This PR makes a few modifications to how attn impl is set in modeling to fix the above issues.

More precisely:

- If the model is composite, i.e. has 2 or more PreTrainedConfigs as part of the main config (text/vision config), then we set the requested attention implementation to each of the sub-configs as a dict. For ex, in LLaVa case:
```python
model.from_pretrained(id, attn_implementation=""sdpa"")

# internally we do the following, where the second line dispatches SDPA on LM backbone
# if we can't dispatch, an error is raised as it should be when loading an LLM with unsupported attention (i.e. llama doesn't support SDPA)
self.config._attn_implementation = {""text_config"": ""sdpa"", ""vision_config"": ""sdpa""}
AutoModelForCausalLM._from_config(config.text_config, attn_implementation=config._attn_implementation[""text_config""])
```

- If user just loads the composite model without any explicit attention, then we dispatch SDPA in any of the sub-models whenever possible. In case any/none of the vision/text models support SDPA, we silently fallback to eager. No warnings, no errors, which is in line with non-composite models
```python
# suppose LM backbone doesn't support SDPA here, then we'll use eager silently
# but dispatch SDPA on vision backbone
model.from_pretrained(id, attn_implementation=None)
```

- The users are also free now to set different attentions in different backbones. We can set eager on vision model and sdpa on LM. That can be done if one passes a dict `attn_implementation`. And the dict keys have to be identical to the ModelConfig attribute names for sub-configs. In other words, if the config has config.text_config, then the dict key must be ""text_config"" to correctly dispatch

```python
model.from_pretrained(id,  {""text_config"": ""sdpa"", ""vision_config"": ""eager""})
```
- For non-composite models nothing changes and everything works as before


NOTE:
I had a few rare cases where the model was not really composite but had several sub-configs. For example DBRX, and I decided to get rid of the sub-configs there. I am not sure what is the right way to deprecate that, so any comments on that welcome
Also, some models have nested configs, where a vision config is part of text config (Qwen2-VL). Only three models are like that, mostly because I didn;t review properly when the model was added and didn't insist on changing stuff. For those models, we don't consider them as composite and just leave it working as it was
","I guess `config.vision_config._attn_implementation` shouldn't be there, this branch related to `config.text_config._attn_implementation` only?
Should we have it under the next ""if""? The logic with `cls._supports_sdpa`, is it correct and intended to be this way?

text_model_cls, vision_model_cls -> cls._supports_sdpa
None, None -> ?
None, True -> True
True, None -> True
None, False -> False
False, None -> False

If both vision and text model classes are not None:
True, False -> False
False, True -> False
False, False -> False
True, True -> True > If one sub-model supports SDPA while other doesn't, an error will be raised following the typical SDPA-dispatch path.

Not sure I got this, whats the difference with just calling `AutoModel.from_config(..., attn_implementation=config._attn_implementation)` for each sub-model?
I guess the purpose was to make each sub-model independently dispatch to sdpa/flash attention without the error. Right! Fixed that one! Would be nice to have a test on VLM side when vision-sdpa is not supported, let me see if I can arrange it Yes. The check was needed because some custom backbones are not auto-mappable, like IDEFICS vision model which is not a PretrainedModel but a simple `nn.Module`  Yes, so that if one sub-model can dispatch on SDPA, it will dispatch. But the other will not. At the same time the general `config._attn_implementation` flag in for ex. LLaVa will be ""sdpa"" , and will not be used while init  Yes, the test to handle different cases would help to check if everything works as expected. We can probably make a separate test just for this method and come up with dummy models that support and do not support sdpa/fa2 Should it be this way (and the same for text model class)?
```diff
-                cls._autoset_attn_implementation(
+                vision_model_cls._autoset_attn_implementation(
``` It is very bad to modify a class attribute: it will lead bugs and nightmares 😰   very difficult to detect.

One example is the `InstructBlip`, see #27867 and #25910.

I see you need it in the call below to `_check_and_enable_sdpa`. Got to think another way ...



 BTW, 

```
        # If one sub-model supports SDPA while other doesn't, an error will be raised following the
        # typical SDPA-dispatch path.
```

If I understand correctly, the call to `_check_and_enable_sdpa`  and `_autoset_attn_implementation` will return without error, right? The error occur only when we try to init. the model with this config? Yes, this is actually better than the current way cause we don;t have to reset `supports_sdpa` flag every time Yes, the error for SDPA is only when the user specifically sets `attn_implementation=sdpa` when loading the model. Dispatching SDPA on sub-configs of VLM will follow the same path so that we raise error that ""VisionMode doesn't support sdpa"" only if user specifically requested sdpa. Otherwise we dispath to eager by default ```suggestion
        initializer_range (`float`, *optional*, defaults to 0.02):
``` I don't think we should return both keys and values, as this means it behaves differently from dictionaries which config objects can be. It's better to by pythonic with duck typing and keep the interfaces similar 

```suggestion
        for attr in copy.deepcopy(self.__dict__):
            yield attr
```

To reinforce this, we could add a corresponding `items()` method too  nit (here and anywhere this comment is copied)

```suggestion
    # We define this flag here because in VLMs these flags depend on which LM/vision models are used
``` nit

```suggestion
    # This flag is used by tests and is set to False because LM/vision models used in tests don't support SDPA
``` I don't think we want these to be importable from the top level of the init, at least not as-is for two reasons: 

* Naming - the pattern is normally `XxxModel` or `XxxForYyy`. It lends high precedence for these sub-modules to show under autocomplete if someone does `from transformers import ...`. As they don't match the standard pattern it's not clear how they fit in. I'm not convinced someone would want to use them individually either (in particular the perceiver module). 
* The perceiver resampler can't be loaded in through its own config, it requires the config of the entire idefics2 model  nit

```suggestion
                # sigLip has one shared cls attr for all models
                vision_attn = text_attn = ""sdpa"" if model._supports_sdpa else ""eager""
``` Why remove `GenerationTesterMixin`? I'm guessing because before `all_generative_model_classes` wasn't defined and this is multimodal but just want to make sure I've understood the change properly  Yes, I wouldn't import it also. I think after the next iteration of changes I'll try to get rid of extra `AutoMap` for sub-configs, this can be removed. That works for most models, but haven't really thought of Idefics

If that's not successful, I'm not sure if we can auto-map from config to model without having a top-level import 🤔   Yep, because that triggered all generation tests which are yet not ready for VLMs, and would fail in most cases If the automap change doesn't work we can try: 

* For `Idefics2VisionTransformer`, rename to `Idefics2VisionModel` so it's more in line with CLIP submodules
* For the perceiver, move more values into the config (I remember not doing this because of dependencies on the text and vision model params) such that it's stand-alone.  Why are we deep copying here? This will just yield the keys of the dictionary, which should be strings and therefore immutable  Why do we need to get torch's default type here? I'm not sure I understand this comment. The check on `_is_composite` doesn't appear to relate to whether attn impl is used at all? because of the above comment about FA2 warnings that model dtype should be half-precision. When we call `_from_config` within modeling code, we don't pass `dtype`. In general it doesn't break anything and the correct dtype is used, but dispatching attention implementation checks for this particular kwarg from inputs. So when the kwarg is None, it assumed we are not in half-precision and raises a warning that ""FA2 behavior might be bad and unexpected"" Hmm, it has been long since I made this. Let me check out if the requirement to cope still holds or we can get rid of it This is for specific cases when a model is composite (has several standalone models) but not all of the components are a Transformer. So the sub-component doesn't/can/t have any attention mechanism and has nothing to dispatch. 

Example: Chameleon with its VQ-VAE or CLIPText models. 

In case of CLIP, the model is mostly loaded with the general config (not text config). So when a `ClipTextModel.from_pretrained()` is loaded it relies on general `ClipConfig` and we replace the config's attention implementation with a dict for each sub-model (text vs vision config). In later steps we fail to dispatch because we are dispatching on `self.config._attn_implementation` and  `attention implementation should be a string and one of the following....`. Hope this explains why. I thinkg for Chameleon we could find a way without `is_composite` attribute but in case of CLIP-like models that would be harder

 in DBRX having several nested configs causes errors as the new logic assumes that each config needs its own attn implementation. I am not sure if it is a hard requirement to have it as a PreTrainedConfig

I want to deprecate it somehow so we end up with a simple dict for ""ffn_params"" and for ""attn_params"" overwrite to set ""sdpa"" no matter what because that is the only attention that IDEFICS has, and basically `attn_implementation` is not used anywhere except for when preparing the causal mask"
34252,2024-10-18T20:52:05Z,2024-10-21T17:21:52Z,andimarafioti,0,0,1,6,1,1,1,[],,0,246590.0,0,0,0,0,5712835.527183,,2,1,0,False,[],Looks good to me :),Looks good to me :),"The method `model_download_tool` was called `model_download_counter` earlier in the tutorial, this raises an error when following the code.

# What does this PR do?

This PR fixes a small typo in the documentation. I was following the code here and realized that the method had different names during the tutorial but always refers to the same thing.


## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @


Models:

- text models: @ArthurZucker

Documentation: @stevhliu

 -->
",
34248,2024-10-18T13:41:44Z,2024-10-21T13:35:57Z,Rocketknight1,0,0,1,29,1,1,1,[],,0,258856.0,0,0,0,0,5726391.743029,,0,1,0,False,[],Clean!,Clean!,"The chat template docs describe using generation prompts, but I realize it's a little unclear for model authors after reading #34209. This PR adds a section on writing generation prompts.",
34211,2024-10-17T08:33:41Z,2024-10-21T12:55:27Z,yonigozlan,0,0,2,45,2,1,1,[],,0,361306.0,0,0,0,0,5728825.086372,,0,2,0,False,[],Importing from utils is usually better as it has our own layer! 🤗 thanks,Importing from utils is usually better as it has our own layer! 🤗 thanks,"# What does this PR do?
Currently, the modeling file of Paligemma uses `from ...utils import logging`, and the processing file uses `import logging`.
That means if we want to use a modular file to add a model inheriting from Paligemma, depending on what import `logging` we add to the modular file, we either have: 
- An error as one logging is used to create a logger with `logging.get_logger` and the other with `logging.getLogger`
- A style issue as logging is imported from two different places in the same file

I'm not sure if we want to modify modular transformers to handle this, or if it's better to enforce the use of the same logging module across the files of a model. Wdyt @ArthurZucker 
",
34244,2024-10-18T10:18:01Z,2024-10-21T10:15:37Z,gante,0,2,2,159,6,2,0,[],,0,262572.0,0,0,0,0,5734899.656119,,0,2,0,False,[],"Only missing docs and test, super super nice otherwise!","Only missing docs and test, super super nice otherwise!","# What does this PR do?
","should we add `_supports_early_exist` ? `hasattr(model, ""active_layers"") ` IMO it depends on how the model is structured/trained 👀 

- if the model is expected to have early exit at ANY layer because the lm head is compatible with all layers -> there is no way to detect unless we manually add an argument in the config, which is... brittle. Probably I would suggest to not do any check for now?
- If the model is expected to have early exit on specific layers, store those layers in the config and check that attribute here.

WDYT?"
34228,2024-10-17T16:59:31Z,2024-10-21T08:00:14Z,zucchini-nlp,0,2,2,83,3,2,1,[],,0,313244.0,0,0,0,0,5746538.878894,,0,2,0,False,[],"If the check in my comment below passes, then LGTM :)","If the check in my comment below passes, then LGTM :)","# What does this PR do?

Precedes https://github.com/huggingface/transformers/pull/34174/files. 

Before merging the linked PR, we need to stop relying on `main_input_name` as in some cases (BLIP) the main input is not text but rather audio/video/image. So we fail to check generated text length

We can make some assumptions here:
- Decoder-only models always require text as main input, because we generate from given input id(s). So we can always get `input_ids` and use its shape in tests
- Encoder-decoder can take whatever modality input and still generate text. But we don't need the input length in that case to verify generated length
- We still need seq length in checking attentions/hidden states. But we don't have to try and infer it from inputs, because we already register all length in model tester. This PR introduces similar way of getting seq length as in `test_modeling_common`  + add a `text_seq_length`. The latter is for two models that are special, Reformer which changes seq length in train vs inference and GIT which looks like an old-style VLM where images are concated in modeling code
- ","I'm not 100% sure this holds every time -- for instance, `git` is a decoder-only with image as main input. Can you please run `py.test tests/models -k test_greedy_generate` and confirm? 

If it does, then I'm happy with the change :D  GIT takes both image and text, and has its main-input-name as `input_ids`. The problem with GIT was same was in other VLMs prev, that the images were added to input during forward pass thus changing seq length for attentions/hidden states. But the generated length is still based off the `input_ids`

Yes, I ran all `-k greedy` tests and was successful on my end. Will verify once more just to be sure :)"
34147,2024-10-14T03:13:08Z,2024-10-18T16:15:26Z,ringohoffman,0,2,2,37,9,3,2,[],,0,392539.0,0,0,0,0,5976027.346313,,0,2,0,False,[],"LGTM, thank you for the fix 🙏  🤗 thanks for bookeeping hehe 
","LGTM, thank you for the fix 🙏  🤗 thanks for bookeeping hehe 
","

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

A few updates that were missed in #31292 and #33902.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

cc: @Cyrilvallez @gante @ArthurZucker ","This can do into the `if labels is not None:` block below, I believe, keeping the code more readable :) 

(setting some values to -inf is irrelevant for the cast operation) Okay, moved it for you."
34201,2024-10-16T21:43:47Z,2024-10-18T11:54:55Z,byi8220,1,0,2,5,1,2,2,[],73852.0,0,145505.0,0,0,0,0,5983622.683753,,0,2,0,False,['byi8220'],"ran `make style`, should pass nowThanks for spotting this ! LGTM. To fix the CI, please do `make style` !  Thanks!","Thanks for spotting this ! LGTM. To fix the CI, please do `make style` !  Thanks!","# What does this PR do?
This PR is a 1 line diff which fixes the `require_torch_up_to_2_accelerators`.

This decorator was malformed, which lead to tests annotated by this to be not run at all, and silently passing.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@muellerzr @SunMarc
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34154,2024-10-14T09:47:20Z,2024-10-18T12:12:15Z,LysandreJik,1,4,5,14,4,1,0,[],353773.0,0,354296.0,0,0,0,0,5990619.980967,,0,5,0,False,['LysandreJik'],"I appreciate the review @lewtun, thank you :grin: ",,,"turbo nit (although i realise these reports are tiresome :))
```suggestion
        help=""path to marian model sub dir. yaml.load will be used to load the configuration file, please be wary of which file you're loading."",
``` ```suggestion
            ""Given the files are in the pickle format, please be wary of passing it files you trust.""
``` ```suggestion
        help=""Path to the original config file. yaml.load will be used to load the file, please be wary of which file you're loading."",
``` ```suggestion
        ""Given the files are in the pickle format, please be wary of passing it files you trust."",
```"
34225,2024-10-17T16:26:13Z,2024-10-18T10:17:30Z,zucchini-nlp,0,0,1,12,3,1,1,[],,0,64277.0,0,0,0,0,5997506.286711,,0,1,0,False,[],Ok understood! No need for the test. Thanks!,Ok understood! No need for the test. Thanks!,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34223. Since BLIP expansion happens after the text has been tokenized for specific reasons, we missed this bug. 

I can add one more test for batched expansion but I don't think we need it since this won't probably break again and was a silly bug",
34198,2024-10-16T14:52:28Z,2024-10-17T21:01:57Z,muellerzr,1,20,20,449,2,6,1,[],38009.0,0,151069.0,1,0,0,0,6002740.356213,,1,20,0,False,['muellerzr'],"A bit more context, full fine-tuning does **NOT SEEM TO BE IMPACTED BY THIS** (when padding). I am looking into how this directly affects TRL, however *things are not as bad as they may seem*. 

(Below is an example CausalLM result comparing grad accum 4, bs 8 vs bs 32 both before and after this fix)

![image](https://github.com/user-attachments/assets/20eeabfe-47f1-4f3d-9f24-3a7161afd259)
LGTM, IMO a regression test on the grad norms could be fairly nice!  Just a denominator change in the test case Feel free to merge! ","LGTM, IMO a regression test on the grad norms could be fairly nice!  Just a denominator change in the test case Feel free to merge! ","# What does this PR do?

In conjunction with https://github.com/huggingface/transformers/pull/34191, this PR solves the other half of what's needed:

1. Letting users pass in their own loss functions directly to the Trainer via `compute_loss`
2. Prefetching the first `gradient_accumulation_steps` worth of data each complete step and marking how many samples were seen (`num_items_in_batch`), which can be passed to a loss function if it takes in `num_items_seen` (name TBD)

A bit of feedback needed we need to coordinate:

* Should it be called `num_items_in_batch` and then passed through to the loss functions as such? Or is there a better name we can think of

Fixes https://github.com/huggingface/trl/issues/2175


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@LysandreJik @ArthurZucker 

","let's make this more readable! mmmm if people don't pass a loss, we won't use the model's default?  needs to be documented somewhere! I already quickly discussed this with Zach, so this is a more general questions to other reviewers:

Would this line be work for all the different task types we support? Specifically, can we always skip the first item in the sequence, i.e. is the `[..., 1:]` part valid? `clean` did this one 🫠  We will, it stays in `inputs` and gets passed to the models `forward()` aren't there a lot of downstream trainers that extend this, that we should pause on renaming this? for example https://github.com/huggingface/trl/blob/a67f2143c38d6520be8735463ce715ad5c281db8/trl/trainer/cpo_trainer.py#L826 I'll reverse this then (store as `_compute_loss` and keep `compute_loss`) you can split in 3-4 lines 🎐  ```suggestion
                        loss, outputs = self.compute_loss_func(model, inputs, return_outputs=True)
```
a typo maybe! ```suggestion
``` We should either recommend this, or recommend overloading `compute_loss`. Might be better to not add this?  ```suggestion
        self.compute_loss_func = compute_loss_func
```
we should be consistent as well and rename for all the additional code, can we have a separate function? 🤗  ```suggestion
            if self.compute_loss_func is not None:
``` thanks, btw, @lewtun we should open a PR to add `num_items_in_batch` or `**kwargs` to `compute_loss` in trl otherwise this will still break trl too For casual auto regressive models it works but won't work in other ones Although this is a test, it's probably better to not divide by `.numel()` or the number of items, but do the pure mean, since there might be padding tokens.

```suggestion
    if num_items_in_batch is None or disable_num_items_in_batch:
        loss = nn.functional.cross_entropy(shift_logits, shift_labels, ignore_index=-100, reduction=""mean"")
    else:
        loss = nn.functional.cross_entropy(shift_logits, shift_labels, ignore_index=-100, reduction=""sum"")
        loss = loss / num_items_in_batch
``` thanks @winglian ! "
34230,2024-10-17T21:25:07Z,2024-10-17T21:38:35Z,ArthurZucker,0,0,5,92,5,0,0,[],,0,810.0,0,0,0,0,6043040.808225,,0,5,0,False,[],,,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33778,2024-09-27T19:26:45Z,2024-10-17T20:37:37Z,pcuenca,4,3,12,553,1,2,0,[],1732.0,0,1732255.0,0,0,0,0,6046698.450961,,0,12,0,False,"['HuggingFaceDocBuilderDev', 'pcuenca', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33778). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. This is now ready for review @ArthurZucker. I exported the Llama 3.2 instruct, base and guard models and verified that the configuration files were identical to what's currently in the Hub. I converted Llama 2 and only saw a couple of new properties from the default configuration (for example, `mlp_bias`) that look fine to me. The only interesting difference is that the converted weights are now exported in `bfloat16`, while [this repo](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main) was exported in `float16`. I believe `bfloat16` is correct, because the [original weights](https://huggingface.co/meta-llama/Llama-2-7b-chat/tree/main) were released with that dtype (and it's consistent with newer models). Let me know if you think it's necessary to override the dtype in this case, but I think it could potentially mess things up when converting fine-tunes exported in the original format.

I found the error message for tokenizer (< 3.x) conversion to be unclear, so I added a explicit reference to requiring `sentencepiece`.

I didn't keep a local copy of the Llama 1 research weights, so I can't test that code path now. I re-requested access to download the weights from Meta, not sure how long it'll take. No worries, LGTM and I think we can merge! 🤗 Thanks, LGTM, what's best is usually to try converting at least llama1 7B, and llama2 7B as well! 🤗 this way we are certain there is not regression ","Thanks, LGTM, what's best is usually to try converting at least llama1 7B, and llama2 7B as well! 🤗 this way we are certain there is not regression ","# What does this PR do?

Update the existing Llama conversion script to support the new 1B and 3B models.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?
","Is it acceptable to load from a well-known tokenizer @ArthurZucker? This way they'd always be in sync, and having the very long template in the script is a bit noisy. If we use a revision, fine by me! The Llama Guard 3 model uses a different rope scale and requires a different chat template. An alternative would be to use `--guard` (which would imply `--instruct`) instead of adding a new version, but I'm not sure if code would be clearer."
34101,2024-10-11T19:39:45Z,2024-10-17T15:33:19Z,guangy10,6,0,1,69,1,1,1,[],12042.0,0,505305.0,0,0,0,0,6063269.338656,,0,1,0,False,"['guangy10', 'HuggingFaceDocBuilderDev', 'ArthurZucker']",Verified on `torch>=2.4.0` including the upcoming `torch==2.5.0` @ArthurZucker I see the original tokenizer for `LLama3` is moved under [`original/tokenizer.model`](https://huggingface.co/meta-llama/Llama-3.2-1B/tree/main/original) dir. Do you know how the `tokenizer.model` is converted to `tokenizer.json` and maybe other files? Does `transformers` have a util script that converts between these formats? Asking because I need to generate a `tokenizer.model` or `tokenizer.bin` file that ExecuTorch runtime can recognize for models that only have `tokenizer.json`. @ArthurZucker do you mind reviewing this PR? Hey sorry for being late! We convert them using `convert_slow`'s TikToken converter! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34101). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.🤗 ,🤗 ,"# What does this PR do?

Llama 2&# is compatible with `ExecuTorch`. 

Note that LLama2&3 in ExecuTorch repo has been fully optimized for SOTA perf using its own model definition and optimization. You can read details in https://github.com/pytorch/executorch/tree/main/examples/models/llama2. The work here is to make the Llama model compatible with `ExecuTorch` using HuggingFace's model definition.

### Additional Test in `ExecuTorch`
Running `Llama-3.2-1B` E2E:
`cmake-out/examples/models/llama2/llama_main --tokenizer_path=tokenizer_llama3_1b.model --model_path=llama3_1b.pte --prompt=""My name is""`
```
I 00:00:00.000599 executorch:cpuinfo_utils.cpp:61] Reading file /sys/devices/soc0/image_version
I 00:00:00.000638 executorch:cpuinfo_utils.cpp:77] Failed to open midr file /sys/devices/soc0/image_version
I 00:00:00.000643 executorch:cpuinfo_utils.cpp:157] Number of efficient cores 4
I 00:00:00.000645 executorch:main.cpp:69] Resetting threadpool with num threads = 6
I 00:00:00.002350 executorch:runner.cpp:59] Creating LLaMa runner: model_path=llama3_1b.pte, tokenizer_path=tokenizer_llama3_1b.model
I 00:00:01.442476 executorch:runner.cpp:88] Reading metadata from model
I 00:00:01.442494 executorch:runner.cpp:111] Methond use_sdpa_with_kv_cache not found, using the default value 0
I 00:00:01.442496 executorch:runner.cpp:113] Metadata: use_sdpa_with_kv_cache = 0
I 00:00:01.442501 executorch:runner.cpp:113] Metadata: use_kv_cache = 1
I 00:00:01.442503 executorch:runner.cpp:113] Metadata: get_vocab_size = 128256
I 00:00:01.442505 executorch:runner.cpp:113] Metadata: get_bos_id = 128000
I 00:00:01.442507 executorch:runner.cpp:113] Metadata: get_max_seq_len = 123
I 00:00:01.442508 executorch:runner.cpp:111] Methond enable_dynamic_shape not found, using the default value 0
I 00:00:01.442509 executorch:runner.cpp:113] Metadata: enable_dynamic_shape = 0
I 00:00:01.442512 executorch:runner.cpp:174] RSS after loading model: 0.000000 MiB (0 if unsupported)
I 00:00:01.691377 executorch:runner.cpp:243] RSS after prompt prefill: 0.000000 MiB (0 if unsupported)
My name is Alex.
I am a retired Army Captain, former air traffic controller and current writer.
I have been a reader for many years, and I have always enjoyed stories that had the ability to take me away from my daily routine and into a world of my imagination. It was my life that I chose, not the other way around. When I saw the cover for The Devil’s Triangle and the idea that a man was trying to save a child, I knew I had to read the book. I was not disappointed.
I thought this book was beautifully written, and I enjoyed reading about a man who was
I 00:00:11.182142 executorch:runner.cpp:257] RSS after finishing text generation: 0.000000 MiB (0 if unsupported)
PyTorchObserver {""prompt_tokens"":3,""generated_tokens"":119,""model_load_start_ms"":1728683099291,""model_load_end_ms"":1728683100732,""inference_start_ms"":1728683100732,""inference_end_ms"":1728683110471,""prompt_eval_end_ms"":1728683100980,""first_token_ms"":1728683100980,""aggregate_sampling_time_ms"":90,""SCALING_FACTOR_UNITS_PER_SECOND"":1000}
I 00:00:11.182185 executorch:stats.h:104] 	Prompt Tokens: 3    Generated Tokens: 119
I 00:00:11.182186 executorch:stats.h:110] 	Model Load Time:		1.441000 (seconds)
I 00:00:11.182188 executorch:stats.h:120] 	Total inference time:		9.739000 (seconds)		 Rate: 	12.218914 (tokens/second)
I 00:00:11.182190 executorch:stats.h:128] 		Prompt evaluation:	0.248000 (seconds)		 Rate: 	12.096774 (tokens/second)
I 00:00:11.182192 executorch:stats.h:139] 		Generated 119 tokens:	9.491000 (seconds)		 Rate: 	12.538194 (tokens/second)
I 00:00:11.182193 executorch:stats.h:147] 	Time to first generated token:	0.248000 (seconds)
I 00:00:11.182194 executorch:stats.h:154] 	Sampling time over 122 tokens:	0.090000 (seconds)
```


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. #32505
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker
",
34199,2024-10-16T18:24:08Z,2024-10-17T15:53:48Z,gante,3,7,5,1274,64,4,3,[],617.0,0,77383.0,0,0,0,0,6063729.356071,,0,5,0,False,['gante'],"@zucchini-nlp / @ylacombe : i've tagged you both so that yoach can double-check audio models, and raushan the others 🤗  > Are you planning to run slow tests for every models btw ?

Yes :) Now that's approved, I will be running slow tests before merging @ylacombe  Ran slow tests on all models with significant changes, no regressions -- merging :) 🧼 super clean! Great clean up! LGTM, thanks for this @gante! 
Most of the audio models' `prepare_inputs_for_generation` were untouched, but for the ones that were, it looks like it'll work.

Special thanks for Moshi, the new  `prepare_inputs_for_generation` covers every small edge case (no base model, no `get_output_embeddings`, doing post-processing). It's much cleaner to read now.

Are you planning to run slow tests for every models btw ? 
","🧼 super clean! Great clean up! LGTM, thanks for this @gante! 
Most of the audio models' `prepare_inputs_for_generation` were untouched, but for the ones that were, it looks like it'll work.

Special thanks for Moshi, the new  `prepare_inputs_for_generation` covers every small edge case (no base model, no `get_output_embeddings`, doing post-processing). It's much cleaner to read now.

Are you planning to run slow tests for every models btw ? 
","# What does this PR do?

Closes #32685 🙌 

This PR does a final pass over the remaining `prepare_inputs_for_generation`:
- makes a few adjustments to the general function to handle trivial corner cases
- removes the cases where the general function is equivalent
- adds a comment on the functions that can't be removed, so we can quickly a) remember that there is a general function b) why the general function doesn't work on the model


👉 After this PR, let's aim at overwriting `prepare_inputs_for_generate` as few times as possible, so we can quickly roll out model-agnostic upgrades 🏎️ and minimize bugs 🐛  ","Some models have extra kwargs. With `**kwargs` we can make the generalization in `GenerationMixin.prepare_inputs_for_generate` :) ```suggestion
        # Overwritten -- Moshi has custom post-processing
``` for my own understanding, is this needed for multi-gpu setting? I think most VLMs do smth similar when we pass pixel values only in pre-fill stage. I am thinking that for Idefics it can also be a check on `if cache_position[0] == 0` as we don't support multi-turn dialogues. So I am think we can find a way to generalize for VLMs in a subsequent PR :)

About moving the logic to modeling, I think we want to discourage anyone to pass both  This change is needed because `moshi` doesn't have `get_output_embeddings()`, and creating `get_output_embeddings()` there would be a bit ambiguous.

`self.dtype` works just as fine and is more versatile :) Next I will be working on separating prefil from non-prefil.

Perhaps if I add a flag `prefil: bool`, we can sort most VLMs!  oh dtype, not device 😄 "
33987,2024-10-06T12:05:57Z,2024-10-17T15:27:34Z,vrnvu,5,2,3,76,9,2,3,[],68969.0,0,962497.0,0,0,0,0,6065306.717865,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'vrnvu', 'ArthurZucker']","Regarding the failing test: 
```python
VideoClassificationPipeline requires the PyAv library but it was not found in your environment. You can install it with:
```
pip install av
```
Please note that you may need to restart your runtime after installation.
FAILED tests/models/timesformer/test_modeling_timesformer.py::TimesformerModelTest::test_pipeline_video_classification_fp16 - ImportError: 
VideoClassificationPipeline requires the PyAv library but it was not found in your environment. You can install it with:
```
pip install av
```
Please note that you may need to restart your runtime after installation.
```
the workflow for pipelines needs a small fix! Fixed the requirement, I wrongly confused ""vision"" with ""video"" and thought I didn't need the `av` requirement. 🤕 

How can I troubleshoot them? Are they flaky?

I dont see how they are related with this PR. But the tests you edited are skipped by the CIs because they need av (which was not installed) The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33987). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Okay merging then! 🤗 Very welcome for this! Thanks for the cleanup 🤗 
Could you confirm that the `test_pipeline_video_classification` tests work well locally? 
 Thanks, yep the other tests are unrelated! Small nit, LGTM otherwise!","Very welcome for this! Thanks for the cleanup 🤗 
Could you confirm that the `test_pipeline_video_classification` tests work well locally? 
 Thanks, yep the other tests are unrelated! Small nit, LGTM otherwise!","The only usage that was left of decord is removed in this PR

- Context:, project abandoned for 2 years: https://github.com/dmlc/decord
- Adds toil to local dev (https://github.com/huggingface/transformers/issues/22617)
  - https://github.com/huggingface/transformers/issues?q=is%3Aissue+decord
- Already removed from a couple of places in favor of av: https://github.com/huggingface/transformers/pulls?q=is%3Apr+decord

The helper functions `read_video_pyav` and `sample_frame_indices` are the used in all the documentation https://github.com/search?q=repo%3Ahuggingface%2Ftransformers%20read_video_pyav&type=code and `video_classification` https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/video_classification.py#L132

","```suggestion
                frames.append(frame.to_rgb())
```
any reason we don't use this? 
or directly convert before stacking?  Good question, we return the transformation at the end to directly transform the numpy data structure

```
return np.stack([x.to_ndarray(format=""rgb24"") for x in frames])
```

It's more efficient than converting the frames to rgb:
https://github.com/PyAV-Org/PyAV/blob/main/av/video/frame.pyx#L252
https://github.com/PyAV-Org/PyAV/blob/main/av/audio/frame.pyx#L168
"
28687,2024-01-24T21:38:14Z,2024-01-31T12:02:07Z,patrickvonplaten,12,30,30,831,3,2,1,[],128683.0,0,23045938.0,0,0,0,0,6065930.076724,,1,30,0,False,"['sanchit-gandhi', 'canberk17', 'Kimahriman', 'HuggingFaceDocBuilderDev', 'MonolithFoundation', 'ArthurZucker', 'patrickvonplaten', 'RitchieP']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_28687). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Still need to make sure that ""Whisper default language detection behavior"" stays the same. See: https://huggingface.co/openai/whisper-large-v3/discussions/71#65b78d5a34297095bcecfe78 **Update**:

This PR now does two additonal things:
- `generation_config` is always preferred over `config.json`
- By default multilingual Whisper does language detection followed by transcription

cc @sanchit-gandhi  - [x] TODO(Patrick) Make sure to run all slow tests Hi there @patrickvonplaten  I started getting the following error when I tried finetune whisper-tiny model, which points  me to this PR  here :

> Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.
> 
>  ValueError: Multiple languages detected when trying to predict the most likely target language for transcription. It is currently not supported to transcribe to different languages in a single batch. Please make sure to either force a single language by passing `language='...'` or make sure all input audio is of the same language.


I am simply following the instructions here by @sanchit-gandhi :
https://huggingface.co/blog/fine-tune-whisper#prepare-environment

I used exactly the same code  here ( https://colab.research.google.com/drive/1Ffd2ZnaaRfM02A0QiW08blufw0qFdlIu?usp=sharing ) previously to push same model (https://huggingface.co/ckandemir/whisper-tiny-tr) ,and this week I wanted to experiment with different parameters started getting the the error above,  I was wondering if you could help me understand the issue here. 
 Hey @canberk17 - this can be fixed by specifying the language in the `generation_config` as follows:
```python
model.generation_config.language = ""<|hi|>""  # for hindi
model.generation_config.task = ""transcribe""
```
Provided you run these two lines before calling `trainer.train()`, you'll de-activate automatic language detection during inference and be able to run the code as before. I'll update the blog post + Colab to reflect these changes! Hi @sanchit-gandhi , thank you so much for clarifying ! I just started hitting the
```
ValueError: Multiple languages detected when trying to predict the most likely target language for transcription. It is currently not supported to transcribe to different languages in a single batch. Please make sure to either force a single language by passing language='...' or make sure all input audio is of the same language.
```

error as well due to the change in behavior. Setting `language='en'` seems to fix this, however when using `return_language=True` I now get all the returned languages as English, and not the source language(s) that I was getting previously. Is that expected? Okay I think I get it now, since you're tracking `init_tokens` as a list of tokens, you're just requiring all items in a batch to have the same language somewhat arbitrarily. Any plans to address that? That's a pretty major breaking change that prevents batched inference on mixed language audio, especially when Whisper is our method for detecting the audio language. Can the language tokens just be tracked as a batch tensor instead of a single token, and that would fix the whole issue?

What I can't figure out is exactly how this worked before. I see the `forced_decoder_ids` gets a `(1, None)` entry for the language token, but I can't see where the `None` is handled when creating the `init_tokens`. I see it might get handled in the `ForceTokensLogitsProcessor`? But it seems like it would err out when creating the `decoder_input_ids`. > Hey @canberk17 - this can be fixed by specifying the language in the `generation_config` as follows:
> 
> ```python
> model.generation_config.language = ""<|hi|>""  # for hindi
> model.generation_config.task = ""transcribe""
> ```
> 
> Provided you run these two lines before calling `trainer.train()`, you'll de-activate automatic language detection during inference and be able to run the code as before. I'll update the blog post + Colab to reflect these changes!

Hi @sanchit-gandhi , I tried implementing this method, but when I ran `trainer.train()`, whenever it reaches the evaluation step, it tells me this message.

>You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.

Any idea why this is happening? ValueError: A custom logits processor of type <class 'transformers.generation.logits_process.ForceTokensLogitsProcessor'> with values <transformers.generation.logits_process.ForceTokensLogitsProcessor object at 0x7f4230cfac50> has been passed to `.generate()`, but it has already been created with the values <transformers.generation.logits_process.ForceTokensLogitsProcessor object at 0x7f422829c510>. <transformers.generation.logits_process.ForceTokensLogitsProcessor object at 0x7f422829c510> has been created by passing the corresponding arguments to generate or by the model's config default values. If you just want to change the default values of logits processor consider passing them as arguments to `.generate()` instead of using a custom logits processor.


what's the way to use it now? Hey! Could you open a new issue with a repro? 🤗 Very nice PR @patrickvonplaten, a great clean-up of `forced_decoder_ids` -> `decoder_input_ids`. Thank you also for the deprecation notes for 4.39 - I'll make sure to execute these when the version comes round. Overall looks good - just some minor nits and suggestions","Very nice PR @patrickvonplaten, a great clean-up of `forced_decoder_ids` -> `decoder_input_ids`. Thank you also for the deprecation notes for 4.39 - I'll make sure to execute these when the version comes round. Overall looks good - just some minor nits and suggestions","# What does this PR do?

This PR refactors `forced_decoder_ids` making sure that we now always pass prompted ids as `decoder_input_ids` into generate for Whisper. The whole idea of forcing ids instead of just passing them as initial tokens was a bad design choice and we should try to move away from it.

In addition, Whisper prompting is improved by:
- Not allowing `prompt_ids` to be passed as a numpy array
- Enable `prompt_ids` for long-form generation with two modes:
  - a) prompt only the first segment
  - b) prompt every segment

While a) is the only supported case in the original Whisper repo b) can be very useful as can be seen in the added slow test [here](https://github.com/huggingface/transformers/pull/28687/files#r1467376608).


This is the final code PR regarding Whisper for Transformers. In the next weeks focus will be put on writing nice docs, tutorials and blog posts.","Now that we don't use `forced_decoder_ids` anymore for whisper generate -> we need to potentially cut the length of `timestamps` according to the actual length of the input We now directly pass the prompt tokens also in short-form generate this function is heavily rewritten to make sure that `forced_decoder_ids` + specified `task` and/or `language` create the correct `init_tokens` prompt conditioning.  We have to types of prompt_conditioning for long-form:
- 1. We condition only the first segment on the prompt
- 2. We condition all segments on the prompt The original OAI whisper codebase only has i., but ii. has some very nice advantages in certain scenarios.  We're leaving the `ForceTokensLogitsProcessor` here for backwards compatibility for short-form. But it should be removed in 4.39. This didn't make sense before as the audio is only 4 seconds long. Seems like changing from `forced_decoder_inputs` to `decoder_input_ids` correts this actually. Previously we allowed passing numpy arrays into `model.generate(...)` which I think was not intentional and a bug (see here: https://github.com/huggingface/transformers/pull/22496/files#r1467370182). 

If people feel strongly I can make it backward compatible though inside generate, but we should definitely not allow passing numpy arrays into a nn.Module  Tiny punctuation change in the expected results coming from the inherant randomness when using temperature fallback This test shows the advantages of both `first-segment` and `all-segments` prompt ids for long-form generation. (nit) can go in the docstring
```suggestion
        prompt_condition_type: Optional[str] = None,
``` No longer need to define `batch_size` on L611 after defining here: https://github.com/huggingface/transformers/blob/1ed2880e41fea1b037313bb7e3102df213a4168e/src/transformers/models/whisper/generation_whisper.py#L611 Looks good! Good with me to prioritise the generation config over the config (and as discussed offline at https://huggingface.slack.com/archives/C02G13FEMDH/p1706544739238239?thread_ts=1706447656.541159&cid=C02G13FEMDH) There could feasibly be an instance where the forced decoder ids start at position 2, e.g.
```python
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from datasets import load_dataset, Audio

model = WhisperForConditionalGeneration.from_pretrained(""openai/whisper-tiny"")
processor = WhisperProcessor.from_pretrained(""openai/whisper-tiny"")

dataset = load_dataset(""mozilla-foundation/common_voice_16_1"", ""de"", split=""validation"", streaming=True)
dataset = dataset.cast_column(""audio"", Audio(16_000))

sample = next(iter(dataset))
input_features = processor(sample[""audio""][""array""], return_tensors=""pt"").input_features

# generate with default forced ids (en + transcribe)
pred_ids = model.generate(input_features)
pred_text = processor.batch_decode(pred_ids)
print(pred_text)

# generate with forced ids starting at idx 2
model.config.forced_decoder_ids = [[2, 50359]]
pred_ids = model.generate(input_features)
pred_text = processor.batch_decode(pred_ids)
print(pred_text)
```
**Print output:**
```python
['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> His cabin is located on the street of Alpenminister at the moment.<|endoftext|>']
['<|startoftranscript|><|de|><|transcribe|><|notimestamps|> Seine Gebine rohnen heute auf dem Friedhof von Alpenminister bei Gemunden.<|endoftext|>']
```

=> the current logic is a bit brittle in that it would fail for such examples Nice! ```suggestion
                    ""Multiple languages detected when trying to predict the most likely target language for transcription. It is currently not supported to transcribe to different languages in a single batch. Please make sure to either force a single language by passing `language='...'` or make sure all input audio is of the same language.""
``` Should we also have a warning:
```python
elif forced_decoder_ids is not None and return_timestamps is not None:
```
=> since this also gets overridden by the `return_timestamps` argument (e.g. here we always override the forced ids with the setting of `return_timestamps`) Good for me to follow the original codebase implementation where the prediction of the lang ids are not included in the beam for beam search (but rather done as a standalone forward pass) -> think it's very unlikely that beam search would switch to a more probably language during decoding Very nice! This is a mistake no?
```suggestion
                is_low_temperature = temperature is None and temperature < 0.5
``` Good with me - agree that this is a ""bug"" that we can fix This test should be decorated with `@require_torchaudio`
```suggestion
    @require_torchaudio
    def test_language_detection(self):
``` Very nice - this could go in our Whisper blog post / detailed long-form guide (or even the new lib) No think this was correct the way it was 😅  I'd prefer keeping the comment here actually as this way one can quickly see what options are allowed The whole test class has a `@require_torchaudio` Added a logger.info(...) further down Hmm, but isn't that the expected behavior here actually? For the 2nd case, we don't define a language, but say the model should transcribe => so it detects German and transcribes.

When an ""incorrectly"" forced_decoder_ids is passed, a warning is thrown, but the forced_decoder_ids are still used => I improved the warning message though to make it clearer."
34214,2024-10-17T09:49:47Z,2024-10-17T14:45:07Z,schoennenbeck,2,0,3,2,1,1,1,[],1163.0,0,17736.0,0,0,0,0,6067839.378698,,0,3,0,False,"['schoennenbeck', 'Rocketknight1']","Alternative or possibly more robust way would be to only do the stripping if the full final message string cannot be found in the rendered chat as is. @schoennenbeck merged! Thanks again for a clean and helpful PR!@schoennenbeck this is a really good fix, thank you! I think we don't need the more robust solution - because of how whitespace is tokenized, I think it will be quite hard to end your message in multiple spaces and still get a good continuation. 

(We're having some CI issues, but hopefully we'll resolve them later today and then I can merge this)","@schoennenbeck this is a really good fix, thank you! I think we don't need the more robust solution - because of how whitespace is tokenized, I think it will be quite hard to end your message in multiple spaces and still get a good continuation. 

(We're having some CI issues, but hopefully we'll resolve them later today and then I can merge this)","For a lot of tokenizers in `Tokenizer.apply_chat_template` with `continue_final_message=True` we get a ""ValueError: substring not found"" if the final message starts or ends in some whitespace. Here is some example code that exhibits the issue:

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(""hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"")
messages=[
    {""role"": ""user"", ""content"": ""What is the capital of France?""},
    {""role"": ""assistant"", ""content"": ""Great question! The capital of France is ""}
]
tokenizer.apply_chat_template(
    messages, add_generation_prompt=False, 
    continue_final_message=True
)
```

This is due to the fact that the `apply_chat_template`-method looks for the full final message in the rendered chat but many modern chat templates (in particular the Llama3.1-chat-template) actually trim messages before rendering.

This PR strips the final message before looking it up in the rendered string. This fixes the issue. However, this means that the continuation can now happen at a slightly different spot than the user intended. I believe this is the best way to address this issue but I would also be open to simply raise a more descriptive error in case this happens so the user can strip the last message themselves to handle this. 

Either way I think the current failure mode is not ideal.

If required I could also add a test for this behaviour.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

## Potential reviewers (based on affected area)
- tokenizers: @ArthurZucker
- chat templates: @Rocketknight1

",
34023,2024-10-08T09:23:52Z,2024-10-17T14:41:55Z,chrsmcgrr,4,0,2,8,4,2,2,['run-slow'],262974.0,0,796683.0,0,0,0,0,6068047.949853,,0,2,0,False,"['chrsmcgrr', 'gante', 'ylacombe', 'zucchini-nlp']","Seems okey to me, compile stuff usually complain on in-place modification to tensor Thanks @zucchini-nlp, requesting a core maintainer review then ! @ylacombe @zucchini-nlp these errors seem unrelated to the slow tests. Is this expected? LGTM 👍 

(I've rerun CI in case it was a transient error)This LGTM, but I'd like another opinion for a torch compile expert as well! 

Maybe @gante or @zucchini-nlp ? Thanks for your help Let's run slow tests just in case. You can do it by pushing an empty commit like this: `git commit --allow-empty -m ""[run-slow] hubert, unispeech, unispeech_sat, wav2vec2""`
 Ok, thanks! @ylacombe feel free to merge once you're satisfied with the tests","This LGTM, but I'd like another opinion for a torch compile expert as well! 

Maybe @gante or @zucchini-nlp ? Thanks for your help Let's run slow tests just in case. You can do it by pushing an empty commit like this: `git commit --allow-empty -m ""[run-slow] hubert, unispeech, unispeech_sat, wav2vec2""`
 Ok, thanks! @ylacombe feel free to merge once you're satisfied with the tests","# What does this PR do?

Fixes #34022 by implementing the masking of the hidden states using an elementwise multiplication rather than indexing with assignment.

The torch.export functionality seems to mark the tensor as frozen even though the update is legal.

This change is a workaround for now to allow the export of the model as a FxGraph. Further investigation is required to find the real solution in pytorch.

Tagging:
@ylacombe, @eustlb

Please let me know if someone else is more appropriate to review this PR. 
",
34076,2024-10-10T17:37:47Z,2024-10-15T12:32:10Z,gante,0,0,1,9,1,2,2,[],,0,592713.0,0,0,0,0,6069584.203915,,0,1,0,False,[],Thanks for fixing this! LGTM!  👁️ 👁️ this should help with all cache issues I think nice!,Thanks for fixing this! LGTM!  👁️ 👁️ this should help with all cache issues I think nice!,"# What does this PR do?

In `generate`, we create many auxiliary tensors for internal operations. Prior to this commit, the assumption was that the model's output (`logits`) and these auxiliary tensors were on the same device. In practice, this is the same as assuming that `input_ids` (or other main input, depending on the model) would be on the same device as `lm_head`.

This assumption doesn't always hold, even if the user follows our suggested examples. For instance, if we do `model_inputs = model_inputs.to(model.device)`, as we suggest in many examples, we move `model_inputs` to the same device as in the first set of parameters in `model.parameters()`, which is far from guaranteed to be the same device as `lm_head`. ([PreTrainedModel.device source code](https://github.com/huggingface/transformers/blob/24b82f3cd56d5eeb26e649207aac3ce8a7d75bdc/src/transformers/modeling_utils.py#L1072), [.parameters() docs](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters))

To make the assumption hold, this PR moves the `logits` returned by the model to the same device as `input_ids`, which should fix most (if not all) of device issues in multi-device settings.

Example of command fixed by this PR: `CUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 python -m pytest tests/quantization/bnb/test_mixed_int8.py -k test_generate_quality_config -vv` ✅ 

_________________________________

### ⚠️ caveats

The following script now runs. 

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""distilgpt2"")
model = AutoModelForCausalLM.from_pretrained(""distilgpt2"", device_map=""auto"")

inputs = tokenizer([""The quick brown""], return_tensors=""pt"")
gen_out = model.generate(**inputs, do_sample=False)
```

Note that `input_ids` is on the CPU, so some internal ops in `generate` will run on the CPU (while the forward pass happens on GPU). We throw a [loud warning](https://github.com/huggingface/transformers/blob/24b82f3cd56d5eeb26e649207aac3ce8a7d75bdc/src/transformers/generation/utils.py#L2090), but this may still be a source of issues (`generate` running slower than expected because there are CPU ops and device transfers)",
34171,2024-10-15T09:07:20Z,2024-10-17T14:11:52Z,ydshieh,2,0,6,443,6,1,1,[],2854.0,0,191074.0,0,0,0,0,6069850.53729,,0,6,0,False,"['HuggingFaceDocBuilderDev', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34171). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. You mean the (new) GitHub Action yml file, right?

I have been quite careful recently and this new file has no (actual) new stuff than other existing workflow files :-)Boom! Does Guillaume need to take a look at the github runner file to check for security?",Boom! Does Guillaume need to take a look at the github runner file to check for security?,"# What does this PR do?

Use `git bisect run` to find the commit for a failed test, and ping team member using Slack's keyword mention (`GH_ydshieh` for example)

The scope is limitied

- Currently only focus on the `single-gpu` tests
- During each `git bisect run`, we only check a single (failed) test like 
    > tests/models/vit/test_modeling_vit.py::ViTModelTest::test_foo`
   
   (but some failures only occur in a test suite of that model being run)
",
34079,2024-10-10T21:14:28Z,2024-10-17T13:11:34Z,amosyou,3,0,3,12,2,1,2,[],77228.0,0,577409.0,0,0,0,0,6071887.98844,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'amosyou']",ready for another review @SunMarc bump in case you forgot! should be ok to merge The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34079). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks ! Could you also change all references of `load_in_8bit_fp32_cpu_offload` to `llm_int8_enable_fp32_cpu_offload` ? I saw it elsewhere too.  LGTM ! Thanks for iterating ! ,Thanks ! Could you also change all references of `load_in_8bit_fp32_cpu_offload` to `llm_int8_enable_fp32_cpu_offload` ? I saw it elsewhere too.  LGTM ! Thanks for iterating ! ,"# What does this PR do?

The bitsandbytes config was introduced in #21579, with the `llm_int8_enable_fp32_cpu_offload` argument to enable cpu offloading. However, the environment validation function for bitsandbytes configs has a warning message that suggests using the argument `load_in_8bit_fp32_cpu_offload`, which is incorrect. This PR fixes this inconsistency.

cc @SunMarc",
34220,2024-10-17T12:28:13Z,2024-10-17T12:43:48Z,pcuenca,1,0,1,63,1,0,0,[],941.0,0,942.0,0,0,0,0,6075131.146197,,0,1,0,False,['ArthurZucker'],Thanks!,,"Rough draft. Tested the different pieces in a notebook, but not yet in the script.",
33514,2024-09-16T17:01:17Z,2024-10-17T12:43:29Z,larin92,3,6,1,21,1,4,3,[],91350.0,0,2662932.0,0,0,0,0,6075157.990428,,0,1,0,False,"['LysandreJik', 'larin92']","Thanks @larin92 for the PR! Pinging @muellerzr and @SunMarc for review :) checked failed test, it's not related to this PR @SunMarc , rebased on top of 0f49deacbff3e57cde45222842c0db6375e4fa43 (as latest commit with all checks passed)Thanks @larin92 ! Left a few comments cc @muellerzr  LGTM ! I'm just not sure if we should only use `group_by_length` for both training and evals. cc @muellerzr 

Could you also rebase the PR to main in order to pass the CI ? cc @larin92  Thanks! Should be a straightforward speedup for users doing this :) Nice!","Thanks @larin92 ! Left a few comments cc @muellerzr  LGTM ! I'm just not sure if we should only use `group_by_length` for both training and evals. cc @muellerzr 

Could you also rebase the PR to main in order to pass the CI ? cc @larin92  Thanks! Should be a straightforward speedup for users doing this :) Nice!","Trainer didn't support grouping batches by length for evaluation, which made evaluation slow with `eval_batch_size`>1.

Updated `trainer._get_eval_sampler()` method was based off of `trainer._get_train_sampler()`.","We can't just remove this part We should keep that too Did you test this sampler in the eval stage ? How fast it is compared to what we had before ?  for my dataset, eval stage:
- `eval_batch_size=8`  current transformers version ~4min, this patched version ~2min

current transformers version of `_get_eval_sampler()` doesn't account for `group_by_length` arg for batching, while `_get_train_sampler()` does account for it returned it returned it"
34193,2024-10-16T10:31:56Z,2024-10-16T19:25:19Z,SunMarc,0,0,1,68,2,2,2,[],,0,32004.0,0,0,0,0,6137447.0502,,0,1,0,False,[],😢 ,😢 ,Reverts huggingface/transformers#34032,
34032,2024-10-09T08:38:24Z,2024-10-15T11:48:10Z,Itssshikhar,7,3,6,68,2,4,3,[],17012.0,0,628208.0,0,0,0,0,6152856.443467,,0,6,0,False,"['SunMarc', 'Qubitium', 'HuggingFaceDocBuilderDev', 'Itssshikhar']","Thanks for the PR ! Could you explain a bit more why this PR fixes the issue that you linked ? Thanks  Yeah, sure. 

There is a similar issue in Pytorch (https://github.com/pytorch/pytorch/issues/113496) which causes the same error. Reason being, initialization error in the forward pass, which causes FSDP to fail. 

The Fix seems fairly simple, as we just have to run forward pass once using dummy values, before initializing FSDP. @muellerzr @SunMarc  
All the tests have passed, but one remains `tests_non_models` that requires to have CUDA. 

It would be great if you guys can see to it once and if there's anything from my end that needs to be done?!

Thanks! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34032). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Regression as result of this merge for `trl`/`sft` + fsdp training on 2x gpu. 

```TypeError: LlamaForCausalLM.forward() got an unexpected keyword argument 'args'```

```
trl 0.11.4
accelerate 1.0.1
transformers 4.46.0.dev
```

```
File ""/python/ai/train/sft_trainer.py"", line 380, in <module>
    trainer = SFTTrainer(
              ^^^^^^^^^^^
  File ""/python/ai/train/sft_trainer.py"", line 380, in <module>
    trainer = SFTTrainer(
              ^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py"", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py"", line 401, in __init__
    super().__init__(
  File ""/root/miniconda3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py"", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/transformers/utils/deprecation.py"", line 165, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py"", line 401, in __init__
    super().__init__(
  File ""/root/miniconda3/lib/python3.11/site-packages/transformers/trainer.py"", line 639, in __init__
    self.model = _init_fsdp(self.model, self.accelerator, self.args.device)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/transformers/utils/deprecation.py"", line 165, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/transformers/trainer.py"", line 305, in _init_fsdp
    _ = model(**dummy_input)
        ^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/transformers/trainer.py"", line 639, in __init__
    self.model = _init_fsdp(self.model, self.accelerator, self.args.device)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/transformers/trainer.py"", line 305, in _init_fsdp
    _ = model(**dummy_input)
        ^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 863, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 863, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 820, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 808, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 820, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/amp/autocast_mode.py"", line 43, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
TypeError: LlamaForCausalLM.forward() got an unexpected keyword argument 'args'
  File ""/root/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 808, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/torch/amp/autocast_mode.py"", line 43, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
TypeError: LlamaForCausalLM.forward() got an unexpected keyword argument 'args'
``` Thanks for the heads-up @Qubitium @ringohoffman ! I will revert this PR !  Thanks for info @Qubitium @ringohoffman on the PR. I'll try to resolve the errors.Thanks! Fix makes sense to me, thanks for the explanation. Could you document and add the link to that issue on top of the `_init_fsdp` func so we can fully trace why this is needed? 

Also please do `pip install -e .[quality]; make fixup` and this will fix the quality tests.  LGTM! I left a comment to show what to fix in order to pass the CI !  This PR broke the test in [`tests/trainer/test_trainer_fsdp.py`](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer_fsdp.py), which actually tests initializing a trainer using FSDP.

* https://github.com/huggingface/transformers/pull/33483#issuecomment-2415765515

Given that there are also some flaws in the logic of this PR, it might be worth reverting this so it can be properly relanded.

@SunMarc ","Thanks! Fix makes sense to me, thanks for the explanation. Could you document and add the link to that issue on top of the `_init_fsdp` func so we can fully trace why this is needed? 

Also please do `pip install -e .[quality]; make fixup` and this will fix the quality tests.  LGTM! I left a comment to show what to fix in order to pass the CI !  This PR broke the test in [`tests/trainer/test_trainer_fsdp.py`](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer_fsdp.py), which actually tests initializing a trainer using FSDP.

* https://github.com/huggingface/transformers/pull/33483#issuecomment-2415765515

Given that there are also some flaws in the logic of this PR, it might be worth reverting this so it can be properly relanded.

@SunMarc ","Addresses the issue with Fully Sharded Data Parallel (FSDP) initialization when resuming training from a checkpoint. It implements a solution by adding a dummy forward pass during the initialization process. 

Fixes #31892 

Added tests in the test_trainer.py file to ensure proper FSDP initialization

@muellerzr @SunMarc  I am creating a draft PR, let me know if there anymore changes that I can make

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","You need to pass a @require_cuda decorator for this test !  These are the variable names inside of forward... not the parameters to forward. I think you probably meant to do something like `inspect.signature`. Not every parameter to forward is a tensor, but you are sending in a tensor for every value."
34165,2024-10-14T19:00:03Z,2024-10-16T15:01:19Z,RezaRahemtola,6,0,2,6,1,2,2,[],49654.0,0,158476.0,0,0,0,0,6153290.013957,,0,2,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'Rocketknight1', 'zucchini-nlp']","Also cc @zucchini-nlp  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34165). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @zucchini-nlp @LysandreJik this PR is quite straightforward, and I think it's okay to accept - are you okay with me merging it once tests pass? I am not qualified for review here, so whatever you find reasonable @Rocketknight1  :) ~In that case, since this is a simple bugfix, I think it's okay to merge. One last step, though - @RezaRahemtola can you fix up the code quality tests? To do that:~

~1) `pip install transformers[quality]`~
~2) `make style` in the transformers directory~
~3) commit and push~

Never mind - this is actually a CI issue. Rerunning it, you don't need to do anything @RezaRahemtola ! Documentation build is failing but this PR doesn't touch the docs at all, so I'm assuming that's an issue with the doc builder. Merging, and thanks @RezaRahemtola for the PR!Yes, this looks good to me! For clarity, this function is called when auto-generating JSON schema for functions passed as tools to the chat template interface.

The main thing to be wary of here is that this change might cause slightly different behaviour on systems where `Image` and `Tensor` types are unavailable. However, I think we can safely assume that when the user has defined a function with `Image` or `Tensor` type hints in the header, that means they have the relevant libraries. Let's go!","Yes, this looks good to me! For clarity, this function is called when auto-generating JSON schema for functions passed as tools to the chat template interface.

The main thing to be wary of here is that this change might cause slightly different behaviour on systems where `Image` and `Tensor` types are unavailable. However, I think we can safely assume that when the user has defined a function with `Image` or `Tensor` type hints in the header, that means they have the relevant libraries. Let's go!","# What does this PR do?

This PR adds conditional checks to the types mapping definitions to avoid defining types that are conditionally imported previously, if the same check doesn't succeed.

This allows a user (like me) to use `transformers` for simple stuff like [tokenizers](https://huggingface.co/docs/transformers/fast_tokenizers#use-tokenizers-from--tokenizers) without needed to install PIL and/or torch if not needed (I'm not using the [tokenizers](https://pypi.org/project/tokenizers/) library directly as it doesn't include chat templating functions, and I'm not using torch/tensorflow/JAX as I'm just calling models run locally with llamacpp, installing one of those just for chat templating seems overkill).

Before, a simple call to `tokenizer.apply_chat_template` would result in this error:
```text
  File ""...../venv/lib/python3.11/site-packages/transformers/utils/chat_template_utils.py"", line 79, in _get_json_schema_type
    Image: {""type"": ""image""},
    ^^^^^
NameError: name 'Image' is not defined
```

With this fix, there's not error: if PIL (or torch) isn't installed, it's [not imported](https://github.com/huggingface/transformers/blob/fa3f2db5c7405a742fcb8f686d3754f70db00977/src/transformers/utils/chat_template_utils.py#L35) (already the case) and not used (added here).

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

- tokenizers: @ArthurZucker
- chat templates: @Rocketknight1





",
34177,2024-10-15T15:09:33Z,2024-10-16T14:48:52Z,yonigozlan,1,0,2,3,1,2,2,[],2456.0,0,85159.0,0,0,0,0,6154037.056125,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34177). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice catch, thanks! Thanks!","Nice catch, thanks! Thanks!","# What does this PR do?
`llava-onevision` -> `llava_onevision`
qwen2_vl is also missing in tokenization_auto
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->


<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34197,2024-10-16T12:19:25Z,2024-10-16T14:13:41Z,steveepreston,1,0,1,2,1,1,1,[],1135.0,0,6856.0,0,0,0,0,6156149.55978,,1,1,0,False,['steveepreston'],"@SunMarc Thank you too!
@muellerzr Please review this if possible, Thanks!Thanks for reverting this PR ! ",Thanks for reverting this PR ! ,"# What does this PR do?

Fixes #34176 

Starting from `v4.45.0` Trainer was not able to train on TPU VMs.
so i tested commits between `v4.44.2` and `v4.45.0` one by one, then found this 1 line change commit [46d09af](https://github.com/huggingface/transformers/commit/46d09af4fc62db59cb836099d17a2c6102b39835) causes this error.
You can see full test result on #34176.


## Before submitting
- [x] This PR fixes a bug on code, not a typo
- [x] Read Pull Request Guideline
- [x] Discussed before on github issue.
- [x] No Document update is required. 1 line change.
- [x] Did you write any new necessary tests?


## Reviewers
Trainer: @muellerzr and @SunMarc
Deepspeed: HF Trainer/Accelerate: @muellerzr",
32048,2024-07-18T09:39:56Z,2024-08-07T09:28:18Z,hanwen-sun,6,0,3,2,1,3,3,[],7350.0,0,7786382.0,0,0,0,0,6162193.126056,,0,3,0,False,"['steveepreston', 'HuggingFaceDocBuilderDev', 'amyeroberts', 'hanwen-sun']","cc @muellerzr @SunMarc  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32048). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > I don't think we can do this directly. AFAICT this was enabled in 0.28 but the required version in our setup.py for accelerate is 0.21. Instead we'll need to add a condition checking the accelerate version is >= 0.28

So I need to add a condition checking that accelerate version is >= 0.28? @hanwen-sun Yes - you should add `is_accelerate_available(""0.28.0"")` to the if statement  > @hanwen-sun Yes - you should add `is_accelerate_available(""0.28.0"")` to the if statement

thank you, I have updated. Created #34197 to revert this change due to #34176 issue.Much appreciated for the enable! I don't think we can do this directly. AFAICT this was enabled in 0.28 but the required version in our setup.py for accelerate is 0.21. Instead we'll need to add a condition checking the accelerate version is >= 0.28 Nice !  Thanks! ",Much appreciated for the enable! I don't think we can do this directly. AFAICT this was enabled in 0.28 but the required version in our setup.py for accelerate is 0.21. Instead we'll need to add a condition checking the accelerate version is >= 0.28 Nice !  Thanks! ,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

We have supported xla fsdp in accelerator, so we should enable xla fsdp in transformers.
the support of xla fsdp in accelerate: https://github.com/huggingface/accelerate/pull/2176  and  https://github.com/huggingface/accelerate/pull/2941


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33931,2024-10-03T20:50:56Z,2024-10-16T12:22:55Z,alpertunga-bile,9,0,16,61,4,2,2,[],150519.0,0,1092719.0,0,0,0,0,6162796.271996,,1,16,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'gante', 'alpertunga-bile', 'ArthurZucker']","cc @gante! 🤗  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33931). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. The red CI should get fixed after #33950 is merged. At that point, I'd like to kindly ask you to rebase and force-push, to re-run CI 🤗   @e can see token healing tests being run in the CI 👍  from https://app.circleci.com/pipelines/github/huggingface/transformers/106787/workflows/081efefd-ecd1-4eed-8496-b4d03d6e492e/jobs/1417470/parallel-runs/7?filterBy=ALL :
<img width=""1024"" alt=""Screenshot 2024-10-07 at 14 09 12"" src=""https://github.com/user-attachments/assets/63e29061-396c-492d-9908-b9b960bc88bf"">

<img width=""927"" alt=""Screenshot 2024-10-07 at 14 10 33"" src=""https://github.com/user-attachments/assets/8cffe94c-5fd2-45a5-9cbe-ae721e96a639"">

 > The red CI should get fixed after #33950 is merged. At that point, I'd like to kindly ask you to rebase and force-push, to re-run CI 🤗

Uhh, my bad. Sorry, i understand it wrong. I will add comments to my changes and push it to re-run the CI when #33950 is merged. cc @ArthurZucker as we just discussed it in person @gante I ran CI again after the #33950 PR was merged but now a test named ```pipelines_torch``` is failing. After which PR do I need to run CI again? @alpertunga-bile sorry about that :( https://github.com/huggingface/transformers/pull/34067 -- this would be the PR > @alpertunga-bile sorry about that :( #34067 -- this would be the PR

Thanks, it is passed through the tests now.@alpertunga-bile thank you for pinning the issue, finding the cause and problems with the test setup, and opening a PR with the fix -- contributors like you make `transformers` shine 💛 

LGTM 👍 

Note for our future selves, in case we need more context about PR: check [this comment](https://github.com/huggingface/transformers/issues/33827#issuecomment-2386059973) Nice, let's merge 🚀 ","@alpertunga-bile thank you for pinning the issue, finding the cause and problems with the test setup, and opening a PR with the fix -- contributors like you make `transformers` shine 💛 

LGTM 👍 

Note for our future selves, in case we need more context about PR: check [this comment](https://github.com/huggingface/transformers/issues/33827#issuecomment-2386059973) Nice, let's merge 🚀 ","# What does this PR do?

Hello! As I described in the #33827; token healing functionality is failing in usage and tests.

This PR is fixing the token healing tests and usage bugs.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #33827 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@gante 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
31629,2024-06-26T06:29:32Z,2024-07-23T13:56:42Z,RhuiDih,17,12,13,226,20,3,2,[],1372175.0,0,9681859.0,0,0,0,0,6178942.171012,,0,13,0,False,"['qibao77', 'HuggingFaceDocBuilderDev', 'olisicky', 'mayank31398', 'ArthurZucker', 'MahmoudAshraf97', 'RhuiDih', 'wynterl']","@ArthurZucker #31446 has just merged, let me rebase and update this PR @ArthurZucker Rebase is done.

I have move the codes to `modeling_flash_attention_utils.py` and added required comments/description.

Regarding the control flow, ideally the `cu_seq_len` should be prepared in data collator and model should accept it as arg to trigger `flash_attn_varlen_func` without relying on `position_ids`, but this is the quick & fast way to do padding free.
 I think loss computation is wrong for everything
it needs to be:
https://github.com/ibm-granite/dolomite-engine/blob/7e2527528ac54cb3388040a3052ef3b6d845a40d/dolomite_engine/hf_models/models/gpt_dolomite/main.py#L185-L191 @mayank31398 
it can be done within the model or when labels preparation in data collator
https://github.com/huggingface/transformers/blob/808fd638182289cf1903a1d4c536f00c48809fb8/src/transformers/data/data_collator.py#L1634-L1637
 Hello @mayank31398 We are providing the capability to define the labels as the user wishes in the data collator in this PR via [transformers/src/transformers/data/data_collator.py](https://github.com/huggingface/transformers/blob/808fd638182289cf1903a1d4c536f00c48809fb8/src/transformers/data/data_collator.py#L1634-L1637) @ArthurZucker
I have added tests for the new data collator and the models which already have FA2 tests in place. For the trainer, i can't find existing FA2 + Trainer testing combo, could you point me there ? No worries I think the tests you added are enough The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_31629). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi, thank you for this! I tried to use it without `DataCollatorWithFlattening` but with data prepared with `input_ids` and `position_ids`,  but I encounter `RuntimeError: CUDA error: an illegal memory access was encountered` just after `elif position_ids is not None and not (torch.diff(position_ids, dim=-1) >= 0).all() and query_length != 1:` was called. 

I am using: 
* llama3
* flash_attn version: 2.5.8
* deepspeed - ZeRO1
* transformers 4.44.1

Thank you! @olisicky Hi, could you provide a minimal code to reproduce the error ? that would help greatly > @olisicky Hi, could you provide a minimal code to reproduce the error ? that would help greatly

Hi @RhuiDih . I found a mistake in my data as I was preparing a minimal code for you:D. So thank you for encouraging me to do so! As I am preparing data without `DataCollatorWithFlattening` I kept padding there to `context_length` even for bs=1 and it resulted in plenty of 0 in `position_ids` tensor on indexes corresponding to PAD tokens. 

However, I have an additional question. You are setting `-100` in labels for the first token in each sequence in the batch. I wonder why. I thought that the use of `position_ids` itself should reduce cross-example attention but here you mentioned that the -100 should prevent the last token of previous example predicting the first token of next example.

If I would have a sequences which start with BOS token and then it will be ignored during training by setting -100 in labels, then I would lost the importance of this first token. Is it necessary? Or maybe shouldn't we use some dummy token there rather then a part of the sequence itself?

Thank you very much! @olisicky 
The setting of `-100` is due to [causal loss implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1221-L1229), if we were flattening all sequences into batch of `1` without setting `-100` accordingly, it would incur unwanted loss.

First token of `input_ids` is still used to produce `logits`, hence no importance is lost. What we want to ignore, its the first token of `labels` due to the mentioned causal loss implementation.
Hope it is clear.

 > @olisicky The setting of `-100` is due to [causal loss implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1221-L1229), if we were flattening all sequences into batch of `1` without setting `-100` accordingly, it would incur unwanted loss.
> 
> First token of `input_ids` is still used to produce `logits`, hence no importance is lost. What we want to ignore, its the first token of `labels` due to the mentioned causal loss implementation. Hope it is clear.

Sorry for late reply. Yes, thank you. I will try it.  Is packing usable in transformers `generate` method? AFAIK only padding is supported Packing is supported with the collator introduced. We don't pack when generating tho, PRs are welcome for this! 🤗  > # What does this PR do?
> Improve throughput as well as train time and memory utilization for instruction tuning by enabling padding-free and attention mask-free attention.
> 
> Specifically, this PR adds the capability to utilize `position_ids` in FlashAttention2 `_flash_attention_forward()` in the case of packing (`attention_mask=None`) to models which use `position_ids` in their respective `DecoderLayer` implementations.
> 
> This PR also adds a new off-the-shelf data collator `DataCollatorWithFlattening` which packs the examples in a mini batch into one long sequence and return `position_ids` as well and turns the first token of `labels` to -100 to prevent the last token of previous example predicting the first token of next example.
> 
> This enables the following:
> 
> 1. Use of packing for instruction tuning without incorrect cross-example attention
> 2. Significant increase in training throughput  and reduction in memory utilization
> 
> **Example Result 1** dataset: OrcaMath subset setup: FSDP with 8 GPUs
> 
> Model	DataProcess	Time	Throughput (token/s)	Memory (MB)
> Llama2-7B	Padding	790	1269	22305
> Llama2-7B	ThisPR	574	1746	20950
> Mistral-7B	Padding	812	1216	23603
> Mistral-7B	ThisPR	596	1658	22409
> **Example Result 2** dataset: FLAN subset setup: FSDP with 8 GPUs
> 
> Model	DataProcess	Time	Throughput (token/s)	Memory (MB)
> Llama2-7B	Padding	1526	771	29234
> Llama2-7B	ThisPR	809	1455	23854
> Mistral-7B	Padding	742	742	30625
> Mistral-7B	ThisPR	1408	1408	24549
> ## Before submitting
> * [ ]  This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
> * [ ]  Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
>   Pull Request section?
> * [ ]  Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
>   to it if that's the case.
> * [ ]  Did you make sure to update the documentation with your changes? Here are the
>   [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
>   [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
> * [ ]  Did you write any new necessary tests?
> 
> ## Who can review?
> Models:
> 
> * text models: @ArthurZucker
> 
> Edit - add more description on data collator

How to use this feature for pretraining? @qibao7 usually one would truncate and pack to max length during pretraining, this PR will not benefit pretraining, unless most of your pretraining data are shortThanks! I think we can wait a tad bit, #31446 is almost ready for merge!  This LGTM! Only thing missing is a test! 
Testing this codepath in particuler, + datacollator in the trainer tests maybe?  LGTM, I just want a second look from our @fxmarty . 
This should improve performances for everyone. but let's make sure we don't break BC! WDYT @fxmarty ?  Looks good!

I strongly think this should be documented somewhere, maybe https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#flash-attention-2. 🚀 nice getting this done!","Thanks! I think we can wait a tad bit, #31446 is almost ready for merge!  This LGTM! Only thing missing is a test! 
Testing this codepath in particuler, + datacollator in the trainer tests maybe?  LGTM, I just want a second look from our @fxmarty . 
This should improve performances for everyone. but let's make sure we don't break BC! WDYT @fxmarty ?  Looks good!

I strongly think this should be documented somewhere, maybe https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#flash-attention-2. 🚀 nice getting this done!","# What does this PR do?

Improve throughput as well as train time and memory utilization for instruction tuning by enabling padding-free and attention mask-free attention. 

Specifically, this PR adds the capability to utilize `position_ids` in FlashAttention2 `_flash_attention_forward()` in the case of packing (`attention_mask=None`) to models which use `position_ids` in their respective `DecoderLayer` implementations.  

This PR also adds a new off-the-shelf data collator `DataCollatorWithFlattening` which packs the examples in a mini batch into one long sequence and return `position_ids` as well and turns the first token of `labels` to -100 to prevent the last token of previous example predicting the first token of next example.

This enables the following:
1. Use of packing for instruction tuning without incorrect cross-example attention 
2. Significant increase in training throughput  and reduction in memory utilization

**Example Result 1**
dataset: OrcaMath subset
setup: FSDP with 8 GPUs
| Model | DataProcess | Time | Throughput (token/s) | Memory (MB) |
| - | - | - | - | - |
| Llama2-7B | Padding  | 790 | 1269 | 22305 |
| Llama2-7B | ThisPR | 574 | 1746 | 20950 |
| Mistral-7B | Padding | 812 | 1216 | 23603 |
| Mistral-7B | ThisPR | 596 | 1658 | 22409 |

**Example Result 2**
dataset: FLAN subset
setup: FSDP with 8 GPUs
| Model | DataProcess | Time | Throughput (token/s) | Memory (MB) |
| - | - | - | - | - |
| Llama2-7B | Padding  | 1526 | 771 | 29234 |
| Llama2-7B | ThisPR | 809 | 1455 | 23854 |
| Mistral-7B | Padding | 742 | 742 | 30625 |
| Mistral-7B | ThisPR | 1408 | 1408 | 24549 |

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review?
Models:
- text models: @ArthurZucker

Edit - add more description on data collator","this is an input dependent control flow which won't be supported by compile, but compile is already not supported. 
This needs a comment: we are checking that the input is not padded, and that we are doing prefill right?  we never import from another modeling code in transformers, we use ""copied from"" or we define the general function in `falsh_attention_utils` for example  this needs to be documented to explain what is happening  I think we keep track of a list of models that support FA2, if needed for guidance here!  this is an input dependent control flow, but we already don't support compile with FA2 AFAIK. 
Alright for now!  As this is already in `test_modeling_common.py`, adding this here is not needed. same for all I am not a big fan as having unpadded inputs as `[1, total_seqlens]`. To me, it would make more sense to have support for one dimensional tensors as `input_ids ([total_tokens])`, which becomes after embedding `inputs_embeds ([total_tokens, hidden_size])`, as anyway flash attention varlen frontend expects 3D tensors, not 4D.

Thus
```
    query = query.view(-1, query.size(-2), query.size(-1))
    key = key.view(-1, key.size(-2), key.size(-1))
    value = value.view(-1, value.size(-2), value.size(-1))
```
would not be needed, and neither calculating the cumulative seqlens at each layer. This maybe can be left for an other PR. How should a user make sure? no easy way, the user will have to make sure the model supports FA2 and have `position_ids`
until the model accepting `cu_seq_len` which will allow every model remove padding once for all, i think this serve as a good warning i have removed them in latest commit very much agree it is less than ideal that `cu_seq_len` is calculated again in every layers. It can be done in `data_collator` or in the first `forward()` and passed along all `DecoderLayer`, not sure how it links to switching from `[bs, seq_len, ... ]` to just `[seq_len, ...]`. This removal of `bs` dimension requires substantial change to the model implementation, should HF decide to do this switch, those 3 lines can be just removed."
34043,2024-10-09T14:51:36Z,2024-10-16T07:25:26Z,zucchini-nlp,2,2,7,118,7,3,2,[],79715.0,0,578030.0,0,0,0,0,6180647.350905,,0,7,0,False,"['HuggingFaceDocBuilderDev', 'zucchini-nlp']","I am trying to make sure that IDEFICS models start using library standards, and hopefully after we enable generation tests it will be easier to catch those bugs when adding the model 🤗  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34043). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.It's always the same models causing problems 👁️ @leot13 👁️  LGTM, except for that nit in the `generate` body :)

Thank you for fixing 💪 ","It's always the same models causing problems 👁️ @leot13 👁️  LGTM, except for that nit in the `generate` body :)

Thank you for fixing 💪 ","# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34033 and enables tests for VLMs. Prev tests were all skipped because we had a hard check for CausalLM Mapping","the subsequent step of preparing cache checks generation config, and we miss the step of adding a Cache class. Fails for IDEFICS only because it doesn't prepare cache in ""forward"" if ""use_cache"", but rather assume that there's already a correct cache provided This change updates `generation_config`, which means we would have two variables containing the source of truth for this flag (`generation_config.use_cache` and `model_kwargs[""use_cache""]`). More than one source of truth usually leads to bugs 🐛 We also know that we need `model_kwargs[""use_cache""]` for the forward pass.

To avoid multiple sources of truth, I'd suggest one of the following: 
1. let's use `model_kwargs[""use_cache""]` in all places after this if/else, instead of also using `generation_config.use_cache`.
2. (I have a preference for this one) Let's use `generation_config.use_cache` everywhere, removing `model_kwargs[""use_cache""]` from most places in the `generate` function. Before calling the decoding methods, let's add `model_kwargs[""use_cache""] = generation_config.use_cache`, since we need this for the forward pass and `generation_config` barely gets used in the decoding methods

(I know this issue predates your change 🤗 But since we're touching it, let's do the most sustainable change)

"
32318,2024-07-30T09:00:35Z,2024-08-26T13:16:45Z,simonJJJ,14,30,103,3828,22,6,1,['New model'],164360.0,0,6733472.0,0,0,0,0,6180667.667763,,0,103,0,False,"['ShuaiBai623', 'HuggingFaceDocBuilderDev', 'zucchini-nlp', 'ArthurZucker', 'simonJJJ']","hi @zucchini-nlp, tidy all files and all test cases were passed. hi @zucchini-nlp, I think we resolved all the reviews, let me know if you have more questions about merging this pr. @zucchini-nlp let me know if u have any questions about merging this pr Hi, @ArthurZucker , let us know if u have any questions about merging this pr 👀 Reviewing asap!  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32318). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @ArthurZucker, I think we are all good for merging this PR? Totally forgot about this, can we swap order of input args for the processor so that it is 'images, text, ......' ? We are doing processor standardization and it'll be easier to have ot correct orders from the beginning, instead of deprecating one more model. I'll take of the whole standardization forQenVLProcessor kwargs later  > Totally forgot about this, can we swap order of input args for the processor so that it is 'images, text, ......' ? We are doing processor standardization and it'll be easier to have ot correct orders from the beginning, instead of deprecating one more model. I'll take of the whole standardization forQenVLProcessor kwargs later

What is the correct order? Alphabetic order? No, it's just the inputs that should be in 'image, text, video ', whilw now it is 'text, images, video,...'. Then you can leave order of other kwargs as it is, we'll take care of the rest > No, it's just the inputs that should be in 'image, text, video ', whilw now it is 'text, images, video,...'. Then you can leave order of other kwargs as it is, we'll take care of the rest

done. Yep gimme a minute to check the new changes and merge accordingly!  Thanks a lot for bearing with me, we'll actually take care of changing that in another PR, let's merge 🤗 
Compile won't be supported out of the box but it's alright otherwise! 
Congrats team for the awesome model!  
 > Thanks a lot for bearing with me, we'll actually take care of changing that in another PR, let's merge 🤗 Compile won't be supported out of the box but it's alright otherwise! Congrats team for the awesome model!

thanks a lot! I really appreciate you guys effort!Your are missing a few files for the automapping to work! would recommend running `transformers-cli add-new-model-like` and overwrite the config, md etc with what you have here! 

Then you should be able to ping @zucchini-nlp for a review on this new multimodal model!  Great addition! Yes, after adding auto maps and md files, feel free to tag for review. Let me know if you need any help with that Thanks for working on this! Great to see more multimodal LLMs. 

My main concern in the current implementation is the chat template format. I wouldn't recommend passing images/processing kwargs in the template. Also, we would need some changes to be consistent with transformers models, left more comments below Thanks a lot for iterating on this! Left some comments, mostly nits

I guess we can ask for core maintainer review after comments are addressed Awesome, thanks for adding the model! Looks good to me, left one tiny comment. Feel free to request review from @ ArthurZucker after that :) 🔥 Congrats for such an amazing model! 
Lots of interesting new tehcniques that deserve some attention! 
Just want to make sure this can be easily re-used, just like Llava was, so making the code readable and understandable is super important! 
","Your are missing a few files for the automapping to work! would recommend running `transformers-cli add-new-model-like` and overwrite the config, md etc with what you have here! 

Then you should be able to ping @zucchini-nlp for a review on this new multimodal model!  Great addition! Yes, after adding auto maps and md files, feel free to tag for review. Let me know if you need any help with that Thanks for working on this! Great to see more multimodal LLMs. 

My main concern in the current implementation is the chat template format. I wouldn't recommend passing images/processing kwargs in the template. Also, we would need some changes to be consistent with transformers models, left more comments below Thanks a lot for iterating on this! Left some comments, mostly nits

I guess we can ask for core maintainer review after comments are addressed Awesome, thanks for adding the model! Looks good to me, left one tiny comment. Feel free to request review from @ ArthurZucker after that :) 🔥 Congrats for such an amazing model! 
Lots of interesting new tehcniques that deserve some attention! 
Just want to make sure this can be easily re-used, just like Llava was, so making the code readable and understandable is super important! 
","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","This is not consistent with docs, the `md` file in docs uses hyphen and here is underscore. I suggest to use underscore everywhere as `qwen_vl.md` to be consistent with other models We need to add paper abstract with arxiv urls, and who contributed the model. These should be available through template when doing `transformers-cli add-mode-like` This doesn't seem right, image processing args should be passed to the processor when calling. I see your point about using different resolution for each image (cmiiw), but the general idea of chat templates in VLMs is to format prompt correctly Same goes here, passing the image to the conversation, be it PIL/array/url is a bit different from what is usually done. I see why this is easy for users, and we might consider changing chat templates if there's a strong bias towards this design. But for shipping model, I'd like to have images passed to processor while templates only leave placeholders for images to be merged. You take a look at Idefics2 which also does interleaving at any position  Additionally, can format conversations to make it more readable :) nit: `tokenize=False` should not be required, it's False by default on processors afaik Example of what will be printed and also example of how formatted prompt looks like is needed here. So that users know how the general chat format for QwenVL looks like from skimming the docs Yay more Video LLMs! Same comments as in image, about processing args and videos that should not be in the template Better phrase it as a section called `Flash-Attention 2 for speed optimization` and add a description of how to use FA2. Here we can make a `Tip` section with this exact phrase  we shouldn't import image processor here, because it has a requirement `if_vision_available`. I see you already added it next to other image processors below, so let's remove this one remove image processor, same as above QwenVL is a VLM so should be mapped in AutoMapForVision2Seq, where other VLMs are. That way users can load it with `AutoModelForVision2Seq.from_pretrained(""qwen2_vl"")` Instead of having a dict `DEFAULT_VALUE` we can make a QwenVLVisionConfig class and fill it with default values. That way if `vision_config` is None it will be initialized with QwenVLVisionConfig by default, otherwise it can be a dict or as a QwenVLVisionConfig in the input This doesn't look right. The error message says `larger than factor` but we check with a fixed int, `< 10`

The second condition also has 100 in error, but check for 200. Also we should use the absolute aspect ratio in error msg, otherwise the error can be `must be smaller than 200 but we got 1/201` which doesn't make sense for anyone who encounters the error Hmm, does this mean users can pass a single video frame to videos?

In general we'll have to move this helper to `image_utils` instead of defining in each file, but let's leave it here for now Using one letter variables is not recommended, can we do `height, width` I guess this will be cleaned up after modifying chat template and removing vision info at inputs The general API for image processors is supposed to be agnostic of whether the framework used is torch/tf/jax. So we can't force `torch` here.

Is there any reason for not using numpy, I believe reshape and permute should be doable in numpy also  some args in docstring are not used by the processor, have to remove them I see now why the make-batched fn is modified. In case the model should handle videos and images at the same time, I recommend to follow trend from `video_llava` and add two inputs args: images and videos. IIUC the videos should be processed same as images, so the only thing to handle is to iterate over each video frame as if it were and image, as you did below Can we add `**kwargs` here, it will help me to be more BC during the refactoring. Also, the `vision_token_id` should be settable by user from `init`, either as token-str or token-id

We'll soon make a VLMTokenizer, it will hopefully make things with special vlms tokens a lot easier Same here, can we make inputs as `text, images, videos` to be consistent. And here we can also pass those `resize` args from conversation-dict which will be used by image processor  We don't have default templates anymore, each model should have its own template in the config Hopefully to be removed so we don't extract any vision info from chat template I am not sure how good is it to support video path as input, since there are many video decoding frameworks from which we currently use `av` in the docs. Though it is not perfect ofc.

Making this kind of method forces users to install a specific framework to read videos, in this case torchvision. I understand that this makes model usage less-code, but I am biased towards following the usual trend and asking users to pass in already processed videos, in lists of PIL/array/tensors.

We might want to support videos as urls/paths in the future, but right now the video models are not yet standard and I have it in my TODO list to figure out a better way for all video-based models Same as below applies to images, let's expect that users pass PIL/array/tensor only Nice! Hmm, wondering why is that the case? We don't have to pass cache position usually, it should be handled within modeling code. The only exception is when continuing generation from existing past-key-values, which is not the case for most tests"
33516,2024-09-16T17:32:28Z,2024-10-15T18:21:22Z,cjfghk5697,1,6,5,102,2,3,1,[],1926827.0,0,2508534.0,1,0,0,0,6227693.560265,,0,5,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33516). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks, we can merge once the conflict is resolved! 🤗 ","Thanks, we can merge once the conflict is resolved! 🤗 ","# What does this PR do?

Translated the `blip-2.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR? ","```suggestion
# BLIP-2[[blip-2]]
```

TOC가 누락되어 추가하였습니다. ```suggestion
BLIP-2 모델은 Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi의 [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) 논문에서 제안되었습니다. BLIP-2는 동결된 사전 학습 이미지 인코더와 대규모 언어 모델(LLM)을 연결하는 12층의 경량 Transformer 인코더를 학습시켜, 여러 비전-언어 작업에서 SOTA(현재 최고의 성능)을 달성했습니다. 특히, BLIP-2는 800억 개의 파라미터를 가진 Flamingo 모델보다 제로샷 VQAv2에서 8.7% 더 높은 성능을 기록했으며, 학습 가능한 파라미터 수는 Flamingo보다 54배 적습니다.
```
조금 더 자연스러운 한국말로 바꿔보았습니다.  ```suggestion
<small> BLIP-2 구조. <a href=""https://arxiv.org/abs/2301.12597"">원본 논문</a> 에서 발췌. </small>
```
엄청 중요한건 아닌데, original paper는 `원본 논문`이 좀 더 맞지 않나 싶어 제안드립니다.
 ```suggestion
- BLIP-2는 이미지와 조건에 따라 텍스트 프롬프트를 입력받아 조건부 텍스트를 생성합니다. 추론 시 [`generate`] 메소드를 사용하는 것이 권장됩니다.
``` '조건부 텍스트를 생성합니다' 혹은 '조건부 텍스트 생성을 수행합니다'가 보다 자연스러울 것 같습니다!  이외에는 완벽한 것 같습니다. 수고 많으셨습니다 :)"
33483,2024-09-13T22:11:56Z,2024-10-10T18:09:04Z,ringohoffman,9,8,17,532,26,4,4,[],283.0,0,2792546.0,0,0,0,0,6186113.856996,,0,17,0,False,"['LysandreJik', 'ringohoffman', 'YeLuoSuiYou']","I should also mention that my script fails when using `attn_implementation=""sdpa""` with an off-by-one error. I don't see this error when using `attn_implementation=""flash_attention_2""`...

```
[rank1]: Traceback (most recent call last):
[rank1]:   File ""/home/matthew/transformers/fsdp_generate.py"", line 64, in <module>
[rank1]:     main()
[rank1]:   File ""/home/matthew/transformers/fsdp_generate.py"", line 52, in main
[rank1]:     generated_text = fsdp_model.module.generate(
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File ""/home/matthew/transformers/src/transformers/generation/utils.py"", line 2048, in generate
[rank1]:     result = self._sample(
[rank1]:   File ""/home/matthew/transformers/src/transformers/generation/utils.py"", line 3001, in _sample
[rank1]:     outputs = self(**model_inputs, return_dict=True)
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File ""/home/matthew/transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py"", line 1038, in forward
[rank1]:     transformer_outputs = self.transformer(
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File ""/home/matthew/transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py"", line 801, in forward
[rank1]:     outputs = block(
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 863, in forward
[rank1]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File ""/home/matthew/transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py"", line 512, in forward
[rank1]:     attn_outputs = self.attn(
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File ""/home/matthew/transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py"", line 462, in forward
[rank1]:     return self.attention(
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File ""/home/matthew/.conda/envs/transformers310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File ""/home/matthew/transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py"", line 314, in forward
[rank1]:     attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
[rank1]:   File ""/home/matthew/transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py"", line 278, in _attn
[rank1]:     attn_weights = attn_weights + causal_mask
[rank1]: RuntimeError: The size of tensor a (21) must match the size of tensor b (20) at non-singleton dimension 3
``` The CI failures seem to be from flaky tests. This should be good for review! Hey @gante and @ArthurZucker, just following up. Do you think you will be able to review this? @SunMarc and @muellerzr could you take a look here please? Okay, I gave it a stab based on these files + the example in the description. I added a test for FSDP and FSDP2.

https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer_distributed.py
https://github.com/huggingface/transformers/blob/main/tests/generation/test_utils.py
 Hi, I found that maybe have a bug of trainer fsdp init, when I use pytest to test the test code `tests/trainer/test_trainer_fsdp.py`, I got error.
![image](https://github.com/user-attachments/assets/f3c37d01-ff45-481d-8676-28bee0726670)
 may u git me some advice how can i avoid it? @ringohoffman 
 
 my packages version are
 torch                    2.4.1
 accelerate            0.34.2
 transformers        4.46.0.dev0
 thx a lot @YeLuoSuiYou Seems like it was due to the logic added in this PR https://github.com/huggingface/transformers/pull/34032 > @YeLuoSuiYou Seems like it was due to the logic added in this PR #34032@YeLuoSuiYou似乎是由于此 PR 中添加的逻辑所致 #34032

thx a lot, maybe I should wait the normal version to fix it? btw, thx much for the function provided by this PR! > @YeLuoSuiYou Seems like it was due to the logic added in this PR #34032@YeLuoSuiYou似乎是由于此 PR 中添加的逻辑所致 #34032

I commet this function, the test work well done, thx!Looks great to me! I am just wondering if we can add a small test, fine to merge first as this looks like it would affect a lot of users! Thanks for adding this! This is very helpful! I left a comment. Can you have a second look @muellerzr ? 
Also, I think that we can create a doc to explain how to perform generate when the model is initialized under fsdp or deepspeed ! I saw a lot of users bieng confused about this. 
Also, if you want to dig deeper, that would be nice to have a comparison with ddp/deepspeed that was done here: https://github.com/huggingface/trl/pull/1483#issue-2207767196 Thanks, this looks good to me, just one suggestion please to make sure we can test the Accelerate base case as well Thanks! These tests look great!","Looks great to me! I am just wondering if we can add a small test, fine to merge first as this looks like it would affect a lot of users! Thanks for adding this! This is very helpful! I left a comment. Can you have a second look @muellerzr ? 
Also, I think that we can create a doc to explain how to perform generate when the model is initialized under fsdp or deepspeed ! I saw a lot of users bieng confused about this. 
Also, if you want to dig deeper, that would be nice to have a comparison with ddp/deepspeed that was done here: https://github.com/huggingface/trl/pull/1483#issue-2207767196 Thanks, this looks good to me, just one suggestion please to make sure we can test the Accelerate base case as well Thanks! These tests look great!","

# What does this PR do?

<!-- Remove if not applicable -->

Fixes #30228

Related:

* https://github.com/pytorch/pytorch/issues/100069
* https://github.com/pytorch/pytorch/issues/123962

Similar to DeepSpeed ZeRO Stage 3, when using FSDP with multiple GPUs and differently sized data per rank, the ranks reach different synchronization points at the same time, leading to deadlock.

To avoid this, we can automatically set `synced_gpus` to `True` if we detect that a `PreTrainedModel` is being managed by FSDP using `_is_fsdp_managed_module`, which was added in 2.0.0 for `torch.compile`: https://github.com/pytorch/pytorch/blob/v2.0.0/torch/distributed/fsdp/_dynamo_utils.py

To facilitate this, I created a module called `transformers.integrations.fsdp` containing the function `is_fsdp_managed_module` which returns `True` if a `Module` has `_is_fsdp_managed_module` set to `True` on it or if the `Module` itself is a `FullyShardedDataParallel` instance.

Here is the script I used to test my fix:

```bash
OMP_NUM_THREADS=2 \
TOKENIZERS_PARALLELISM=false \
CUDA_VISIBLE_DEVICES=6,7 \
torchrun \
    --rdzv-backend=c10d \
    --rdzv-endpoint=localhost:0 \
    --nnodes=1 \
    --nproc-per-node=2 \
    fsdp_generate.py
```

**fsdp_generate.py**
```python
import functools

import torch
import torch.distributed
import torch.distributed.fsdp
import torch.distributed.fsdp.wrap
import transformers
import transformers.models.gpt_neo.modeling_gpt_neo


def main() -> None:
    torch.distributed.init_process_group(world_size=2)
    device = torch.device(torch.distributed.get_rank())
    torch.cuda.set_device(device)

    pretrained_model_name_or_path = ""EleutherAI/gpt-neo-125m""
    model = transformers.AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path,
        device_map=device,
        attn_implementation=""flash_attention_2"",  # I'm using flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu123torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
        torch_dtype=torch.bfloat16,
    )
    assert isinstance(model, transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM)

    tokenizer = transformers.AutoTokenizer.from_pretrained(
        pretrained_model_name_or_path,
    )
    tokenizer.pad_token_id = tokenizer.eos_token_id

    fsdp_model = torch.distributed.fsdp.FullyShardedDataParallel(
        model,
        auto_wrap_policy=functools.partial(
            torch.distributed.fsdp.wrap.transformer_auto_wrap_policy,
            transformer_layer_cls={
                transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoBlock
            },
        ),
        limit_all_gathers=True,
        use_orig_params=True,  # required to overcome the error ""The tensor has a non-zero number of elements, but its data is not allocated yet"" ... PreTrainedModel.generate is probably using some torch.compile-wrapped function
    )

    data_by_rank = {  # differently sized causes FSDP to hang
        0: ""Hello world!"",
        1: ""The quick brown fox jumps over the lazy dog.""
    }

    batch = tokenizer(
        data_by_rank[torch.distributed.get_rank()],
        return_tensors=""pt"",
        return_attention_mask=True,
    ).to(device)

    with torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params(fsdp_model):  # required to overcome to the error ""'weight' must be 2-D""
        generated_text = fsdp_model.module.generate(
            input_ids=batch[""input_ids""],
            attention_mask=batch[""attention_mask""],
            max_length=20,
            # synced_gpus=True,  # currently, True is required to use differently sized data with FSDP + generate (current default is False)
        )

    torch.distributed.barrier()
    torch.distributed.destroy_process_group()


if __name__ == ""__main__"":
    main()
```

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?



## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@gante
@ArthurZucker

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
# Copyright 2024 The HuggingFace Team. All rights reserved.
```
😉  We don't want to import fsdp here as it will increase transformers loading time. Maybe put it in the function ?  Can't we use `is_fsdp_enabled` just like how we do it for deepspeed ? I guess users will use fsdp in trainer + accelerate, so that should work. But I understand that it won't work with your general script. cc @muellerzr  Sure! > But I understand that it won't work with your general script

Right, since `is_fsdp_enabled` is a `Trainer` attribute, it would only work if you are using a `Trainer`, whereas this function works in all scenarios, including when using a `Trainer`.
 Generally I think this solution is okay with me. 

Being this generic means that a user could manually wrap in FSDP, or do so with Accelerate. One thing I'd like to see from the tests is making sure that it works via core Accelerate as well. 

E.g.:

```python
from accelerate import Accelerator
from accelerate import FullyShardedDataParallelPlugin as FSDPPlugin

plugin = FSDPPlugin(...)
accelerator = Accelerator(plugin=plugin)
model = accelerator.prepare(model)
...
``` Okay, I used the `accelerate` CLI and showed using FSDP inside the Trainer . Let me know what you think. @muellerzr how is this looking? Have you had a chance to look at the test I wrote?"
33817,2024-09-30T12:37:06Z,2024-10-15T18:21:05Z,yijun-lee,2,1,5,53,2,2,1,[],139369.0,0,1316639.0,0,0,0,0,6227711.676553,,0,5,0,False,"['mreraser', 'cjfghk5697']",LGTM 😎 LGTM 🚀 Thank you!,Thank you!,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `trainer_utils.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
이 함수들 대부분은 라이브러리에 있는 Trainer 코드를 자세히 알아보고 싶을 때만 유용합니다.
```
표현을 다르게 해서 번역해보았습니다!"
33937,2024-10-04T01:45:22Z,2024-10-15T18:20:46Z,yijun-lee,2,2,5,65,2,2,1,[],87997.0,0,1010124.0,0,0,0,0,6227731.739007,,0,5,0,False,"['mreraser', 'cjfghk5697']","LGTM! LGTM 🚀 Small fix required for the toctree, otherwise LGTM. Thanks!","Small fix required for the toctree, otherwise LGTM. Thanks!","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `gemma2.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `gemma2.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",There should be a `local: model_doc/openai-gpt` under the Gemma2 title here I have resolved it! Thank you :) @stevhliu
33935,2024-10-04T00:02:52Z,2024-10-15T17:31:44Z,mreraser,3,0,5,47,2,1,1,[],391019.0,0,1013332.0,0,0,0,0,6230673.898063,,0,5,0,False,"['LysandreJik', 'mreraser', 'jungnerd']","Hello! @yijun-lee, @jungnerd, @cjfghk5697
May I kindly ask for your review? Thank you for your consistent reviews 🤗  LGTM! Thanks for your translation 👍🏻 Yes it's fine to me @stevhliu!LGTM, thank you! ~There is currently an issue with the CI test environment, but I will go ahead and merge once it is resolved~ 🙂 

@LysandreJik, would it be okay to merge these PRs where the failing test seems to only be related to the CI environment (see the internal Slack thread [here](https://huggingface.slack.com/archives/C021H1P1HKR/p1728636576778949) for more context)?","LGTM, thank you! ~There is currently an issue with the CI test environment, but I will go ahead and merge once it is resolved~ 🙂 

@LysandreJik, would it be okay to merge these PRs where the failing test seems to only be related to the CI environment (see the internal Slack thread [here](https://huggingface.slack.com/archives/C021H1P1HKR/p1728636576778949) for more context)?","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `vivit.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [x] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@yijun-lee, @mreraser, @jungnerd, @cjfghk5697
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)
Hello @stevhliu !  May you please review this PR? Thank you! 

Have a nice day :)",
33608,2024-09-20T06:13:29Z,2024-10-15T14:19:18Z,laurentd-lunit,5,10,11,95,11,3,3,[],331052.0,0,2189150.0,0,0,0,0,6242220.18659,,0,11,0,False,"['laurentd-lunit', 'HuggingFaceDocBuilderDev', 'zucchini-nlp']","@zucchini-nlp thanks for checking this PR!

> Would be super nice if you can get a reproducer to see why this happens, maybe a script that fails once in 10 runs?

I've encountered this issue using a large custom dataset but I haven't pinned down which sample(s) it happens to yet. I suspect it's an edge case of image dimensions that maybe causes this issue. I'll try and find some time to investigate it and make a reproducer but here this PR was really more about getting around it by simply avoiding CUDA to crash if the dimensions do not match. 

> And regarding changes, can we do same error in all llava models if you can? And I thinkg Paligemma and Qwen2-VL also use masked scatter to replace image tokens

Right, I'll check other Llava models and Paligemma and Qwen2-VL and see if I can make the same changes! @zucchini-nlp 
After doing some digging, I was able to pin down a sample where it happened within my custom dataset + custom vision backbone but it seems the issue has been fixed by https://github.com/huggingface/transformers/pull/33564. So I don't think it's needed to make a reproducer here since the bug itself has been fixed.

However, I think it could still be useful to check feature sizes and return `ValueErrror`  before doing the `masked_scatter` in case a new bug arises in the future. I added the check to all Llava architectures and found that Paligemma already had it implemented.

Let me know if there are any other changes required and if you think it's still worth merging this PR.
 @zucchini-nlp Thanks for your review again! I incorporated your changes and suggestion. Hopefully should be good to merge now.

@LysandreJik  would be good if you could check this when you get the chance, especially I had to modify some of the reference values in `test_modeling_llava.py` and `test_modeling_vip_llava` because it seems the assumed number of tokens was 1 too low: 

Using [llava processor logic](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llava/processing_llava.py#L150) to compute the expected number of image tokens:
`num_image_tokens = (height // self.patch_size) * (width // self.patch_size) + 1` 

Here we have high,width = 30 and patch_size = 2 which would make: 15*15+1 = 226 then we do minus 1 because CLS token is ignored so it should be 225 (not 224). The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33608). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. the test should have been fixed in the `main`, @laurentd-lunit can you rebase main?Hey @laurentd-lunit ! Raising an error looks good to me, as a general way to prevent errors. Would be super nice if you can get a reproducer to see why this happens, maybe a script that fails once in 10 runs?

This part can error out only if the input ids are prepared incorrectly in the first place (i.e. number of image tokens were not calculated correctly) or if you are generating without caching and for some reason your model generates more image tokens. The second case is possible only if you are tuning the model from scratch so it didn't learn what to generate yet

Please let me know if you can provide reproducers :)

And regarding changes, can we do same error in all llava models if you can? And I thinkg Paligemma and Qwen2-VL also use masked scatter to replace image tokens  I see, thanks for investigating it! Llava-next is a bit peculiar because of all those pad/unpad going on

Yeah, agreed that we can get a better error message so I'm okay with merging this PR. Can you also add same error in Qwen2-VL and Chameleon pls? And rebase main because we added tests recently, so they should run as part of CI now

Otherwise look good to me, so feel free to tag core maintainer @ LysandreJik after making changes :) Thanks for the ping! @ArthurZucker is much more comfortable with this code as he wrote a bunch of it, so his review will be much better than what mine would be, pinging him to review.

Thanks a lot for the PR :raised_hands:  Hey! For legacy paths (chameleon) that don't have the processing in the processor, sounds good! 
For the new path, I am not sure if this is a great addition especially if it breaks our support for fullgraph compile! 

WDYT? 🤗 we can wait for our benchmarks as well! ","Hey @laurentd-lunit ! Raising an error looks good to me, as a general way to prevent errors. Would be super nice if you can get a reproducer to see why this happens, maybe a script that fails once in 10 runs?

This part can error out only if the input ids are prepared incorrectly in the first place (i.e. number of image tokens were not calculated correctly) or if you are generating without caching and for some reason your model generates more image tokens. The second case is possible only if you are tuning the model from scratch so it didn't learn what to generate yet

Please let me know if you can provide reproducers :)

And regarding changes, can we do same error in all llava models if you can? And I thinkg Paligemma and Qwen2-VL also use masked scatter to replace image tokens  I see, thanks for investigating it! Llava-next is a bit peculiar because of all those pad/unpad going on

Yeah, agreed that we can get a better error message so I'm okay with merging this PR. Can you also add same error in Qwen2-VL and Chameleon pls? And rebase main because we added tests recently, so they should run as part of CI now

Otherwise look good to me, so feel free to tag core maintainer @ LysandreJik after making changes :) Thanks for the ping! @ArthurZucker is much more comfortable with this code as he wrote a bunch of it, so his review will be much better than what mine would be, pinging him to review.

Thanks a lot for the PR :raised_hands:  Hey! For legacy paths (chameleon) that don't have the processing in the processor, sounds good! 
For the new path, I am not sure if this is a great addition especially if it breaks our support for fullgraph compile! 

WDYT? 🤗 we can wait for our benchmarks as well! ","# What does this PR do?

In `LlavaNextForConditionalGeneration`, in the forward pass the following is applied:
```
special_image_mask = (
                    (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)
                )
                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)
                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)
```
It happened to me that for some edge cases the `special_image_mask` and `image_features` shapes are incompatible which leads to a `RuntimeError: CUDA error: device-side assert triggered` in the `masked_scatter` operation. 
The problem is it only happens very rarely and I couldn't pin point yet what causes the missmatch. 

Rather than fixing the issue for edge cases, in this PR I'm suggesting to first check the respective size of image tokens and image features and ensure they match before applying the `masked_scatter` operation. This allows to raise a `ValueError` instead of getting the CUDA Runtime error which is useful because one can then handle the exception as they see fit and still continue using the GPU while the CUDA Runtime error even if caught as an exception will throw the same CUDA Runtime error if any other CUDA operation is applied, in other words it can't really be handled and necessarily breaks running of the script.

In short, this PR allows nicer error handling in some edge cases in LlavaNext when GPU is used.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@zucchini-nlp 
","ultra nit: we're getting no many nested if/else so prob better for readablity

```suggestion
                if n_image_tokens != n_image_features:
                    raise ValueError(
                        f""Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}""
                    )
                special_image_mask = (
                    (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)
                )
                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)
                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)

``` Mmmm in general I don't mind, as this should help our users, but the .item() might break compile compatibility (well only full graph). 

@McPatate that's where and when we would need to see how much we are losing from this small change ! 🤗 (FYI @LysandreJik ) I don't know why we are adding this here as the processor is supposed to check this for non legacy path!  we can alwayss wrap these in `is_torchdynamo_compiling`, same was a s we wrap all warnings/logging now in generation code. So we ask users to make sure the code works w/o compilation, to see all warning etc, and then compile the code which will not show the exact reason why/where this CUDA-side error was triggered Yes, it is supposed. There was only one edge case with llava-next which uses pad/unpad technique and since we used tensors in modeling, there were minor numerical inconsistencies

Right now it should work, but in general imo it's a good idea to help users pinpoint what went wrong in their code  Not in the forward pass IMO, we are adding extra processing, .sum and .item() as seen above, which are run for every single forward pass. biggest issue for me is duplicated work!  okay that makes sense. Just 🥶 to more checks, but this one is most probably cached should be alright  The thing is these `is_compiling` are unrelated to normal users ~-> expose them to unrelated codes
 i see what you mean. Yes, the processing should maybe check this, but we cannot perform any checks before getting image hidden states. My main idea was to bring the same check we had earlier in `merge_inputs` method, so that after moving to the new logic we still can trace down bugs related to shape mismatch easily, or let users track that down

Also we won't do the sum() and item() every forward, for generation it is only for prefill stage after which we'll have image states in the cache. But anyway, if you think this is too many checks (given we now support old and new logic in VLMs for a few minor releases), I am okay with not adding it. I don't see it as a major blocker or anything, more like a nice addition for users :D Okay let's add it then 🤗 "
34073,2024-10-10T16:28:23Z,2024-10-15T13:42:07Z,SunMarc,2,0,1,9,1,2,2,[],421689.0,0,422026.0,0,0,0,0,6244450.815922,,1,1,0,False,"['SunMarc', 'muellerzr']","@ArthurZucker it's because it's the whole `TrainingArgs` object. Fairly certain we can't safetensors a general object..? (so not really much we can do about that/not sure the warning is *too* warranted) I think that's still a thing when we want to perform broadcasting:
```
torch.distributed.broadcast_object_list(object_list, src=0, group=None, device=None)[[source]](https://pytorch.org/docs/stable/_modules/torch/distributed/distributed_c10d.html#broadcast_object_list)
Broadcasts picklable objects in object_list to the whole group.

Similar to [broadcast()](https://pytorch.org/docs/stable/distributed.html#torch.distributed.broadcast), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted.
```Awesome 🤩  🦄 pickl.load is still a thing? Have not check but do we have a warning somewhere about unsafe loading? 🤗 (that happens in this function)",Awesome 🤩  🦄 pickl.load is still a thing? Have not check but do we have a warning somewhere about unsafe loading? 🤗 (that happens in this function),"# What does this PR do ? 

Fixes https://github.com/huggingface/transformers/issues/27487
This PR fixes an issue with optuna hp search when using DDP. The issue was that we didn't parsed pass the correct item to `pickle.loads` when we perform boardcasting with torch. cc @sywangyi as you were the one who integrated it ! 
The following test is passing with ddp with this PR: `test_trainer.py::TrainerHyperParameterOptunaIntegrationTest:test_hyperparameter_search`

Otherwise, we were getting the following traceback: 

```
[rank1]: Traceback (most recent call last):
[rank1]:   File ""test_local.py"", line 83, in <module>
[rank1]:     trainer.hyperparameter_search(direction=""minimize"", hp_space=hp_space, hp_name=hp_name, n_trials=4)
[rank1]:   File ""/home/marc/transformers/src/transformers/trainer.py"", line 3420, in hyperparameter_search
[rank1]:     best_run = backend_obj.run(self, n_trials, direction, **kwargs)
[rank1]:   File ""/home/marc/transformers/src/transformers/hyperparameter_search.py"", line 72, in run
[rank1]:     return run_hp_search_optuna(trainer, n_trials, direction, **kwargs)
[rank1]:   File ""/home/marc/transformers/src/transformers/integrations/integration_utils.py"", line 275, in run_hp_search_optuna
[rank1]:     args = pickle.loads(bytes(args_main_rank))
[rank1]: _pickle.UnpicklingError: pickle data was truncated
```",
34077,2024-10-10T18:38:17Z,2024-10-15T12:55:09Z,yonigozlan,1,0,3,707,3,1,1,[],247122.0,0,411412.0,0,0,0,0,6247270.992688,,0,3,0,False,['yonigozlan'],"No I hadn't seen those changes sorry about that, I have added them now.
Also added a dummy example very similar to what we're trying to do with ColPali!Overall sounds good, I applied similar changes here: https://github.com/huggingface/transformers/pull/33962/commits/a2a6a9ba1f7395f0428d80d1ec405c286881e099 which you have probably seen! 

Can you try to import them (ex: raise the error earlier, use camel case replace, no old_class_name and new_class_name for simplification etc) 

and most importantly, work with a dummy example this way we can see exactly what you are enabling. 

Hardest cases are when you inherit from CLIPMLP or CLIPTextAttention for MyModelAttention Yup nice thanks for updating! 🤗 LGTM","Overall sounds good, I applied similar changes here: https://github.com/huggingface/transformers/pull/33962/commits/a2a6a9ba1f7395f0428d80d1ec405c286881e099 which you have probably seen! 

Can you try to import them (ex: raise the error earlier, use camel case replace, no old_class_name and new_class_name for simplification etc) 

and most importantly, work with a dummy example this way we can see exactly what you are enabling. 

Hardest cases are when you inherit from CLIPMLP or CLIPTextAttention for MyModelAttention Yup nice thanks for updating! 🤗 LGTM","# What does this PR do?
Add support for inheritance from class with different suffix in modular, fixes https://github.com/huggingface/transformers/issues/33900#issuecomment-2391616600

Useful when we want to create a model for a certain task, extending another model used for a different task, as is the case for `ColPaliForRetrieval` and `PaliGemmaForConditionalGeneration` in https://github.com/huggingface/transformers/pull/33736

Not sure if that's in the spirit of modular or if it's the best way to do this though!

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #33900

## Who can review?

cc @ArthurZucker , @tonywu71 
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34066,2024-10-10T13:51:26Z,2024-10-15T12:06:20Z,subhalingamd,6,0,3,17,2,2,3,[],121.0,0,425711.0,0,0,0,0,6250185.092365,,0,3,0,False,"['subhalingamd', 'HuggingFaceDocBuilderDev', 'Rocketknight1']","I'm not sure if this change requires adding new tests and/or updating anything under `docs/`. Could you please help? I have updated the docstrings in the file. sure, i can add them. changing the pr to draft till then. @Rocketknight1 I have added a test case to `test_small_model_pt()`. Hope this works. LGTM now! Pinging @LysandreJik for core maintainer review. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34066). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Merged, thanks for the PR @subhalingamd!@subhalingamd no changes in `docs` are needed, because the docs are auto-generated from the docstrings here!

Adding a test might be useful, though - it should only be a couple of lines. Can you add it to `tests/pipelines/test_pipeline_text_classification.py`? You can either add a new test or just add a couple of lines to `test_small_model_pt()`. I'll trust you on this one @Rocketknight1 ","@subhalingamd no changes in `docs` are needed, because the docs are auto-generated from the docstrings here!

Adding a test might be useful, though - it should only be a couple of lines. Can you add it to `tests/pipelines/test_pipeline_text_classification.py`? You can either add a new test or just add a couple of lines to `test_small_model_pt()`. I'll trust you on this one @Rocketknight1 ","# What does this PR do?

For `problem_type==""regression""`, set default behaviour for `function_to_apply` to ""none"" in `TextClassificationPipeline`.

Fixes #33973

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
@Rocketknight1
",
33907,2024-10-03T10:17:01Z,2024-10-11T08:28:34Z,zucchini-nlp,5,7,6,148,11,3,2,[],1435.0,0,1043184.0,0,0,0,0,6250377.368523,,0,6,0,False,"['BenjaminBossan', 'HuggingFaceDocBuilderDev', 'Kamichanw', 'zucchini-nlp']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33907). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. When this branch can be merged? Will take a look at your comment and merge after seeing what is wrong with labels. TBH. labels should not be affected by position ids, but I'll dig into  @ArthurZucker can you take one more look pls? I added fix for `labels`. Seems one of the very first VLMs introduced it and all other simply copied from that one. I don't see any attn mask applied in LLMs and I don't think we should be doing it under the hood, without the user knowing. So I deprecated that for 2 minor releases > Thanks, there might be a test worth adding for this no? (we break that often for peft 😄 )

Maybe PEFT would be a better place to add this test. If so, LMK and I can work on something (probably based on [this code](https://github.com/huggingface/transformers/issues/33852#issuecomment-2389184690)). Since that would only test a single architecture, it wouldn't be super useful IMO but still better than nothing.Thanks, there might be a test worth adding for this no? (we break that often for peft 😄 )  Just missing info on the shift, we broke this recently (I think we never had this) would be in favor of doing less work for the users, but properly documenting ! ","Thanks, there might be a test worth adding for this no? (we break that often for peft 😄 )  Just missing info on the shift, we broke this recently (I think we never had this) would be in favor of doing less work for the users, but properly documenting ! ","# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/33852 by cropping position ids to the length of inputs and cleans up a little bit the way inputs are prepared for generation

TF equivalence tests are failing for me locally and also failing on `main`, so those are not related to this PR","The attention mask is of shape `seq_length_with_past`, so that makes sense!  that's the only thing I am curious about, we are introducing something new that we know we are gonna deprecate. Not sure this makes a lot of sense to me! Would either not add this, or just not progagate to other models than idefics  Yes, I don't know how this got introduced as none of our llms mask out labels. We can also not deprecate and stop propagating, yes. I'll take care of it when new models are added

So, in that case we remove deprecation and leave the fix for PEFT tuning > So, in that case we remove deprecation and leave the fix for PEFT tuning

exactly! We already get bashed enough for the platora of warning we produce, let's not add one more! 😉   Thanks for addressing the issue in this PR.

> So, in that case we remove deprecation and leave the fix for PEFT tuning

What fix are we talking about here? oh, that's more transformer-side fix. We are talking about attention mask, which gets extra virtual tokens with PEFT so we have to crop them off to match the shape of `labels` for CELoss Got it, thanks, I thought I might have to fix something in PEFT."
30950,2024-05-22T05:09:31Z,2024-10-04T20:28:05Z,pglorio,30,30,115,2941,18,3,0,"['New model', 'run-slow']",89643.0,0,12640027.0,0,0,0,0,6249584.610928,,0,115,0,False,"['Quentin-Anthony', 'hg0428', 'HuggingFaceDocBuilderDev', 'amazingvince', 'pglorio', 'ArthurZucker']","Just for future reference, we measured latency of a single mamba layer in Zamba and compared it to that of a single layer in Mamba, which have very similar implementations (we have some reshapings in Zamba, but they should be a non-op, and a concatenation), and found that that the mamba layer in Zamba to have the same speed in a single forward pass, but to be slower on generation.

More specifically, we instantiated these two (random) models:

```
config = ZambaConfig(num_hidden_layers=81, hidden_size=3712, n_mamba_heads=1, use_cache=True)
model_1 = ZambaForCausalLM(config).cuda()
config = MambaConfig(num_hidden_layers=81, hidden_size=3712, use_cache=True)
model_2 = MambaForCausalLM(config).cuda()
```

(here n_mamba_heads=1 corresponds to the original Mamba architecture), and use this code for generation:

```
model.eval()
input_ids = torch.randint(1000, (1, 2048)).to(device=model.device)
with torch.no_grad():
    output = model.generate(input_ids, max_new_tokens=300, return_dict_in_generate=False, output_scores=False, use_cache=True, num_beams=1, do_sample=False)
```

We found that the total time spent computing this line
https://github.com/huggingface/transformers/blob/87ec8722c936b52ea33969623810c5234bd070ee/src/transformers/models/zamba/modeling_zamba.py#L1009
is 8.1s, and for this line
https://github.com/huggingface/transformers/blob/87ec8722c936b52ea33969623810c5234bd070ee/src/transformers/models/mamba/modeling_mamba.py#L341
is 6.3s. cc @younesbelkada ! I tried running basic training script with gradient accumulation and without on this fork and am getting this error:
 File ""/home/user/transformers_zamba/src/transformers/models/zamba/modeling_zamba.py"", line 1051, in forward
    hidden_states = hidden_states + from_tf if from_tf is not None else hidden_states
RuntimeError: The size of tensor a (7424) must match the size of tensor b (3712) at non-singleton dimension 2

The from_tf is not well described in the doc strings. Not sure what is not working here.

 > I tried running basic training script with gradient accumulation and without on this fork and am getting this error:
File ""/home/user/transformers_zamba/src/transformers/models/zamba/modeling_zamba.py"", line 1051, in forward
hidden_states = hidden_states + from_tf if from_tf is not None else hidden_states
RuntimeError: The size of tensor a (7424) must match the size of tensor b (3712) at non-singleton dimension 2

> The from_tf is not well described in the doc strings. Not sure what is not working here.

Thanks for spotting this. We fixed the issue in the most recent push. Please try again and let us know if you still encounter issues.

We are adding more docstrings to explain various parts of the architecture. We will add the description below for `from_tf` around [this line](https://github.com/Zyphra/transformers_zamba/blob/0b1db69682847e82a76277f10910487ebe162d8a/src/transformers/models/zamba/modeling_zamba.py#L1016):

`from_tf is the output of shared transformer + linear layer (these layers are shown in fig. 2 in https://arxiv.org/pdf/2405.16712). from_tf is then added to the input to the mamba layer (as described in eq. (6) of https://arxiv.org/pdf/2405.16712, where y_l in that equation is from_tf).` > Thanks a lot for this PR ! I left some minor suggestions in the modeling code for general improvements
> Can you also make sure to rebase with main and make sure make fixup pass locally ? Let me know if you need any assistance!

Thank you for the thorough review!

We ran `make fixup` and `make fix-copies`. Running again `make fixup` gives this output:

```
Checking/fixing src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py src/transformers/models/zamba/configuration_zamba.py src/transformers/models/zamba/modeling_zamba.py tests/models/roc_bert/test_tokenization_roc_bert.py
All checks passed!
4 files left unchanged
python utils/custom_init_isort.py
python utils/sort_auto_mappings.py
python utils/check_doc_toc.py --fix_and_overwrite
running deps_table_update
updating src/transformers/dependency_versions_table.py
python utils/check_copies.py
Traceback (most recent call last):
  File ""/workspace/transformers_zamba/utils/check_copies.py"", line 1106, in <module>
    check_copies(args.fix_and_overwrite, args.file)
  File ""/workspace/transformers_zamba/utils/check_copies.py"", line 856, in check_copies
    raise Exception(
Exception: Found the following copy inconsistencies:
- tests/models/roc_bert/test_tokenization_roc_bert.py: copy does not match models.bert.test_tokenization_bert.BertTokenizationTest.test_is_whitespace at line 167
Run `make fix-copies` or `python utils/check_copies.py --fix_and_overwrite` to fix them.
```

It looks like `make fix-copies` is trying to correct parts of the code that are outside of our PR, and some of those fixes still fail. However, we now do not seem to get errors related to our PR.

We pushed the fixes done by `make fix-copies`. Tried training again and am now getting this:

/trainer.py"", line 3250, in training_step
    self.accelerator.backward(loss)
  File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/accelerate/accelerator.py"", line 2127, in backward
    loss.backward(**kwargs)
  File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/_tensor.py"", line 525, in backward
    torch.autograd.backward(
  File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/autograd/__init__.py"", line 267, in backward
    _engine_run_backward(
  File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/autograd/graph.py"", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/autograd/function.py"", line 301, in apply
    return user_fn(self, *args)
  File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 320, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/autograd/__init__.py"", line 267, in backward
    _engine_run_backward(
  File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/autograd/graph.py"", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to create tensor with negative dimension -274340773632: [-274340773632]
 > Tried training again and am now getting this:
> 
> /trainer.py"", line 3250, in training_step self.accelerator.backward(loss) File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/accelerate/accelerator.py"", line 2127, in backward loss.backward(**kwargs) File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/_tensor.py"", line 525, in backward torch.autograd.backward( File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/autograd/**init**.py"", line 267, in backward _engine_run_backward( File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/autograd/graph.py"", line 744, in _engine_run_backward return Variable._execution_engine.run_backward( # Calls into the C++ engine to run the backward pass File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/autograd/function.py"", line 301, in apply return user_fn(self, *args) File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 320, in backward torch.autograd.backward(outputs_with_grad, args_with_grad) File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/autograd/**init**.py"", line 267, in backward _engine_run_backward( File ""/home/user/mambaforge/envs/zamba/lib/python3.10/site-packages/torch/autograd/graph.py"", line 744, in _engine_run_backward return Variable._execution_engine.run_backward( # Calls into the C++ engine to run the backward pass RuntimeError: Trying to create tensor with negative dimension -274340773632: [-274340773632]

Hey there! We've been successfully using: https://github.com/huggingface/alignment-handbook/compare/main...Zyphra:alignment-handbook:zamba-instruct here recently to do sft of Zamba. Does your setup meaningfully differ from this? I can't seem to reproduce, can you provide us a reproducer? cc @younesbelkada should I review this or do you want to do another pass? 🤗  I am trying to extend the max context length. 
{
  ""max_position_embeddings"": 32768,
   ""rope_theta"": 192144,
}

also tried at 16k.

I tried running in your fork of alignment handbook and saw the same results. > Thanks very much for your great work on this ! I left few minor improvements to address and some file changes to revert - can you make sure to make our CI happy (by making sure `make fixup` command passes + the tests pass `pytest tests/models/zamba/` pass ) let me know if you need any help or have any question

Thank you for your help, @younesbelkada!

We believe we have addressed most of the concerns you raised; we still have two pending questions:

- `pytest tests/models/zamba/`: Pytest flags only test_initialization as failing. The specific issue arises with `x_proj_weight` and `dt_proj_weight`, where their mean is approximately 10^-2, contrary to the expected 10^-9. This discrepancy is expected, it is due to the initialization scheme using a variance of (d_input)^(-0.5), where d_input is approximately 100 in the test configuration. We implemented nn.Parameter(torch.rand(...)) for initialization of these parameters, which we verified is equivalent to the Kaiming initialization typically used for nn.Linear. It seems that `transformers` may apply additional steps for the initialization of various layer types, which might not extend to parameters such as `x_proj_weight`. We have adjusted the tolerance for these parameters to 10^-2 in the initialization test in [this line](https://github.com/Zyphra/transformers_zamba/blob/main/tests/models/zamba/test_modeling_zamba.py#L353) of the test script. Please let us know if additional steps are required.

- `make fixup`: After running it, we have this output:

```xception: Found the following copy inconsistencies:
- src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py: copy does not match models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention at line 720
- tests/models/roc_bert/test_tokenization_roc_bert.py: copy does not match models.bert.test_tokenization_bert.BertTokenizationTest.test_chinese at line 76
- tests/models/roc_bert/test_tokenization_roc_bert.py: copy does not match models.bert.test_tokenization_bert.BertTokenizationTest.test_basic_tokenizer_lower at line 85
- tests/models/roc_bert/test_tokenization_roc_bert.py: copy does not match models.bert.test_tokenization_bert.BertTokenizationTest.test_basic_tokenizer_lower_strip_accents_false at line 94
- tests/models/roc_bert/test_tokenization_roc_bert.py: copy does not match models.bert.test_tokenization_bert.BertTokenizationTest.test_basic_tokenizer_lower_strip_accents_true at line 103
- tests/models/roc_bert/test_tokenization_roc_bert.py: copy does not match models.bert.test_tokenization_bert.BertTokenizationTest.test_basic_tokenizer_lower_strip_accents_default at line 112
Run `make fix-copies` or `python utils/check_copies.py --fix_and_overwrite` to fix them.
make: *** [Makefile:38: repo-consistency] Error 1
```

all the lines are related to files outside of our PR, so we did not change those files, although indeed I do see that the CircleCI tests performed in this PR still fail. Please, let us know if further action is needed here and what would be the steps we'd need to take.

Thank you so much for your time and help! Thank you so much for your guidance. We tried to rebase our PR and ran into an error related to model generation. It looks like the rebased `GenerationMixin.generate` method instantiates Zamba's cache as a `DynamicCache` class https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L1775. This is different from `HybridMambaAttentionDynamicCache` which would be the expected class for Zamba's cache (defined here https://github.com/Zyphra/transformers_zamba/blob/main/src/transformers/models/zamba/modeling_zamba.py#L130). In the non-rebased fork, the cache is instantiated by `GenerationMixin.generate` in this line: https://github.com/Zyphra/transformers_zamba/blob/main/src/transformers/generation/utils.py#L2379, which correctly instantiates cache as `HybridMambaAttentionDynamicCache`.

For reference, these are the calls performed from `model.generate` to the instantiation of the cache object:
using the rebased fork:
```
-> output = model.generate(**tokenized_prompt, max_new_tokens=300, return_dict_in_generate=False, output_scores=False, use_cache=True, num_beams=1, do_sample=False)
  /usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py(115)decorate_context()
-> return func(*args, **kwargs)
  /workspace/transformers_zamba_rebased/src/transformers/generation/utils.py(1775)generate()
-> model_kwargs[""past_key_values""] = DynamicCache()
> /workspace/transformers_zamba_rebased/src/transformers/cache_utils.py(305)__init__()
```

and using the fork before rebasing:
```
-> output = model.generate(**tokenized_prompt, max_new_tokens=300, return_dict_in_generate=False, output_scores=False, use_cache=True, num_beams=1, do_sample=False)
  /usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py(115)decorate_context()
-> return func(*args, **kwargs)
  /workspace/transformers_zamba/src/transformers/generation/utils.py(1743)generate()
-> result = self._sample(
  /workspace/transformers_zamba/src/transformers/generation/utils.py(2379)_sample()
-> model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)  /workspace/transformers_zamba/src/transformers/models/zamba/modeling_zamba.py(1588)prepare_inputs_for_generation()
-> past_key_values = HybridMambaAttentionDynamicCache(
> /workspace/transformers_zamba/src/transformers/models/zamba/modeling_zamba.py(146)__init__()
```
Could you please let us know how we can force Zamba's cache to be `HybridMambaAttentionDynamicCache` in the rebased fork?

Thanks very much! He! Super late in coming back to you, you should be able to force it by setting `cache_class = """"` in the ZambaPreTrainedModel class! > He! Super late in coming back to you, you should be able to force it by setting cache_class = """" in the ZambaPreTrainedModel class!

Hello @ArthurZucker, thank you for the suggestion! We tried adding `cache_class = """"` to [this line](https://github.com/Zyphra/transformers_zamba/blob/cc7f7aaaf65e59aa7fbbd1322c7126c943af661b/src/transformers/models/zamba/modeling_zamba.py#L1106) but we still couldn't make generation work. As an alternative fix, we added `""zamba""` to [this line](https://github.com/Zyphra/transformers_zamba/blob/cc7f7aaaf65e59aa7fbbd1322c7126c943af661b/src/transformers/generation/utils.py#L1500), similarly to what was done with Jamba, in which case generation works fine. 

We are happy to either keep this fix or to use the one you suggested, in which case we would appreciate if you could say a few more words on how to implement it.

Meanwhile, we rebased our fork. All the local tests with `make fixup` have passed, except for a few warnings shown below which are unrelated to the updates we implemented:

```
/workspace/transformers_zamba/src/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
/workspace/transformers_zamba/utils/check_repo.py:376: UserWarning: Full repo consistency checks require all backends to be installed (with `pip install -e '.[dev]'` in the Transformers repo, the following are missing: TensorFlow, Flax. While it's probably fine as long as you didn't make any change in one of those backends modeling files, you should probably execute the command above to be on the safe side.
/workspace/transformers_zamba/src/transformers/models/deit/image_processing_deit.py:87: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  resample: PILImageResampling = PIL.Image.BICUBIC,
/workspace/transformers_zamba/src/transformers/models/chameleon/image_processing_chameleon.py:116: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  resample: PILImageResampling = PIL.Image.LANCZOS,
/workspace/transformers_zamba/src/transformers/models/efficientnet/image_processing_efficientnet.py:92: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  resample: PILImageResampling = PIL.Image.NEAREST,
```

 I see most of the CircleCI tests for this PR are still failing, please let me know if more needs to be done to fix those.

Thank you so much! I'll have a look!  Thank you so much for the extensive review @ArthurZucker, this was extremely helpful! I believe we addressed the points raised above. 

I rebased the repository and all Circle/CI tests seem to pass except this one which seems unrelated to Zamba:

```
self = Dataset({
    features: ['image', 'labels'],
    num_rows: 50
})
task = 'image-classification', id = 0

    @deprecated()
    def prepare_for_task(self, task: Union[str, TaskTemplate], id: int = 0) -> ""Dataset"":
        """"""
        Prepare a dataset for the given task by casting the dataset's [`Features`] to standardized column names and types as detailed in [`datasets.tasks`](./task_templates).
    
        Casts [`datasets.DatasetInfo.features`] according to a task-specific schema. Intended for single-use only, so all task templates are removed from [`datasets.DatasetInfo.task_templates`] after casting.
    
        Args:
            task (`Union[str, TaskTemplate]`):
                The task to prepare the dataset for during training and evaluation. If `str`, supported tasks include:
    
                - `""text-classification""`
                - `""question-answering""`
    
                If [`TaskTemplate`], must be one of the task templates in [`datasets.tasks`](./task_templates).
            id (`int`, defaults to `0`):
                The id required to unambiguously identify the task template when multiple task templates of the same type are supported.
        """"""
        # TODO(lewtun): Add support for casting nested features like answers.text and answers.answer_start in SQuAD
        if isinstance(task, str):
            tasks = [template.task for template in (self.info.task_templates or [])]
            compatible_templates = [template for template in (self.info.task_templates or []) if template.task == task]
            if not compatible_templates:
>               raise ValueError(
                    f""Task {task} is not compatible with this dataset! Available tasks: {list(unique_values(tasks))}""
                )
E               ValueError: Task image-classification is not compatible with this dataset! Available tasks: []

/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2814: ValueError
```

Please let us know if more needs to be done. Thank you so much again!
 Just merged and fixed a minor merge issue. Should be good to review from our end @ArthurZucker since the test failures don't appear related to our changes:

**tests_torch**:
```
FAILED tests/models/dpt/test_modeling_dpt_auto_backbone.py::DPTModelTest::test_attention_outputs - AttributeError: 'NoneType' object has no attribute 'shape'
FAILED tests/models/dpt/test_modeling_dpt_auto_backbone.py::DPTModelTest::test_retain_grad_hidden_states_attentions - AttributeError: 'NoneType' object has no attribute 'retain_grad'
```

**examples_torch**:
```
/usr/local/lib/python3.10/site-packages/albumentations/augmentations/blur/functional.py:9: in <module>
    from albucore.utils import clipped, maybe_process_in_chunks, preserve_channel_dim
E   ImportError: cannot import name 'preserve_channel_dim' from 'albucore.utils' (/usr/local/lib/python3.10/site-packages/albucore/utils.py)
``` @ArthurZucker -- Paolo and I have responded or addressed your comments. We just merged again. We're ready to kick this back to you. We're getting close!

The zamba tests are passing. There appears to be an unrelated error in `tests_torch` with paligemma:

```
FAILED tests/models/paligemma/test_modeling_paligemma.py::PaliGemmaForConditionalGenerationModelTest::test_static_cache_matches_dynamic - AssertionError: False is not true
``` Just merged again. Test failures again don't appear related to us?

### tests_hub:
```
FAILED tests/trainer/test_trainer.py::TrainerIntegrationWithHubTester::test_push_to_hub_in_organization - huggingface_hub.errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://hub-ci.huggingface.co/valid_org/test-trainer-org.git/info/lfs/objects/batch (Request ID: Root=1-66f46931-2271680b5d9f004f31f064c1;855ff866-65d6-4d2c-9b19-a1823475f1fc)

Cannot read properties of undefined (reading 'storageClass') (internal error hidden in production)
FAILED tests/trainer/test_trainer.py::TrainerIntegrationWithHubTester::test_push_to_hub_tags - huggingface_hub.errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://hub-ci.huggingface.co/__DUMMY_TRANSFORMERS_USER__/test-trainer-tags.git/info/lfs/objects/batch (Request ID: Root=1-66f46933-59f3b6205417227376459499;ff4fe7f7-ebbe-4c51-9719-a4b7e825918a)

Cannot read properties of undefined (reading 'storageClass') (internal error hidden in production)
FAILED tests/trainer/test_trainer.py::TrainerIntegrationWithHubTester::test_push_to_hub_with_revision - huggingface_hub.errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://hub-ci.huggingface.co/__DUMMY_TRANSFORMERS_USER__/test-trainer-revision.git/info/lfs/objects/batch (Request ID: Root=1-66f46933-53e3df2636b2308d6961e76b;a80b6a39-d5e8-4c88-817b-393c5581fc7f)

Cannot read properties of undefined (reading 'storageClass') (internal error hidden in production)
FAILED tests/trainer/test_trainer.py::TrainerIntegrationWithHubTester::test_push_to_hub_with_saves_each_epoch - AssertionError: no logs of level WARNING or higher triggered on root
======= 4 failed, 34 passed, 14 skipped, 17 warnings in 63.98s (0:01:03) =======
```

### tests_torch:
This is a timeout when loading the zamba-7B-v1 model. I don't get the issue on my local setup so I think it's an issue with the CI hardware not keeping up. Do we need to increase the timeout for a model of this size, or should this just be ignored?

```
self = <ssl.SSLSocket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>
len = 188416, buffer = <memory at 0x7fe605054880>

    def read(self, len=1024, buffer=None):
        """"""Read up to LEN bytes and return them.
        Return zero-length string on EOF.""""""
    
        self._checkClosed()
        if self._sslobj is None:
            raise ValueError(""Read on closed or unwrapped SSL socket."")
        try:
            if buffer is not None:
>               return self._sslobj.read(len, buffer)
E               Failed: Timeout >120.0s

/usr/local/lib/python3.10/ssl.py:1163: Failed
```

### tests_generate:
Looks related to the llama3.2 release today. I was able to get the same error with the latest `huggingface:main`, so this is also not due to our changes:
```
FAILED tests/models/mllama/test_modeling_mllama.py::MllamaForConditionalGenerationModelTest::test_new_cache_format_0 - RuntimeError: The size of tensor a (1808) must match the size of tensor b (904) at non-singleton dimension 3
``` Yep don't worry about unrelated ones! Merged again. Now the only failing CI test is the 7B loading timeout. Still not reproducible on my local machine, so it's likely a CI hardware issue (either remove the 7B or increase the timeout?):

```
self = <ssl.SSLSocket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>
len = 6365184, buffer = <memory at 0x7f87589ec580>

    def read(self, len=1024, buffer=None):
        """"""Read up to LEN bytes and return them.
        Return zero-length string on EOF.""""""
    
        self._checkClosed()
        if self._sslobj is None:
            raise ValueError(""Read on closed or unwrapped SSL socket."")
        try:
            if buffer is not None:
>               return self._sslobj.read(len, buffer)
E               Failed: Timeout >120.0s

/usr/local/lib/python3.10/ssl.py:1163: Failed
``` Yeah 7B tests should be wrapped with `@slow` as we only run fast tests on the CIs! @ArthurZucker yes, we were able to run the slow tests in https://github.com/Zyphra/transformers_zamba/blob/d0c1bc102b6f9b5e078a53b477e96e7729703762/tests/models/zamba/test_modeling_zamba.py (`test_simple_generate` and `test_simple_batched_generate_with_padding`) and they passed. We didn't run the flash attention-related tests (`test_flash_attn_2_fp32_ln`, `test_flash_attn_2_generate_padding_right`, `test_flash_attn_2_generate_use_cache`, `test_flash_attn_2_inference_equivalence_right_padding`) as these do not apply. @ArthurZucker -- Green CI 👀  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_30950). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). Does this include support for Zamba2? @hg0428 thanks for asking! Support for Zamba2 will be added in a follow-up PR. Meanwhile, you can install Zyphra's local transformers as described in the Zamba2's [model card](https://huggingface.co/Zyphra/Zamba2-2.7B-instruct). > @hg0428 thanks for asking! Support for Zamba2 will be added in a follow-up PR. Meanwhile, you can install Zyphra's local transformers as described in the Zamba2's [model card](https://huggingface.co/Zyphra/Zamba2-2.7B-instruct).

Unfortunately, that does not work on my device. Zamba2 transformers runs on mamba_ssm, which requires an NVIDIA GPU. I have Apple Silicon. See my issue: https://github.com/Zyphra/transformers_zamba2/issues/3 Just waiting for https://github.com/huggingface/transformers/actions/runs/11116137341/job/30897696145?pr=30950#step:12:64 to be fixed! (related to accelerate and auto device, good that we have this test!) Hi @arthur, thank you again for reviewing.

The test mentioned above `test_multi_gpu_data_parallel_forward` now passes. We had to change some of the shared layers logic for it to work. Previously, `self.mamba_layers` and `self.linear_layers` were both `nn.ModuleList` objects and `self.layers` was not, which prevented most of the layers from being scattered across devices. Now only `self.layers` is `nn.ModuleList` and everything seems to work.

Additionally, we updated all the model's checkpoints on the hub since this involved changing some of the weight keys related to the shared layers. Separately, given we updated the checkpoints, we also swapped `up`<->`gate` in the MLP weight keys as well as in the [forward pass](https://github.com/huggingface/transformers/blob/1504774123b7d56b64334593f58220f14acbf4a8/src/transformers/models/zamba/modeling_zamba.py#L860) so [this issue](https://github.com/huggingface/transformers/pull/30950#discussion_r1770715099) is now addressed.

All tests related to zamba appear to pass. Thank you!Thanks a lot for this PR ! I left some minor suggestions in the modeling code for general improvements
Can you also make sure to rebase with main and make sure `make fixup` pass locally ? Let me know if you need any assistance!  Thanks very much for your great work on this ! I left few minor improvements to address and some file changes to revert - can you make sure to make our CI happy (by making sure `make fixup` command passes + the tests pass `pytest tests/models/zamba/` pass ) let me know if you need any help or have any question  Thanks a lot for iterating ! I left one minor suggestion, can you also merge your branch with upstream main branch? This should make the CI happy and tests should be green  Thanks a lot, great comments about what is different should help everyone! 
- ZambaMixer == MambaMixer? 
- Let's update the decoder layers to make our life easier and the code more readable ","Thanks a lot for this PR ! I left some minor suggestions in the modeling code for general improvements
Can you also make sure to rebase with main and make sure `make fixup` pass locally ? Let me know if you need any assistance!  Thanks very much for your great work on this ! I left few minor improvements to address and some file changes to revert - can you make sure to make our CI happy (by making sure `make fixup` command passes + the tests pass `pytest tests/models/zamba/` pass ) let me know if you need any help or have any question  Thanks a lot for iterating ! I left one minor suggestion, can you also merge your branch with upstream main branch? This should make the CI happy and tests should be green  Thanks a lot, great comments about what is different should help everyone! 
- ZambaMixer == MambaMixer? 
- Let's update the decoder layers to make our life easier and the code more readable ","# What does this PR do?

Please include support for Zamba architecture created by Zyphra Technologies.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [X] Did you write any new necessary tests?


## Who can review?

@ArthurZucker @younesbelkada","```suggestion
### Prerequities
``` Is this true? I don't think we do support that argument in `from_pretrained` currently can't this be an attribute instead of a property method? ```suggestion
``` `update`,  `reorder_cache`, `get_seq_length` and the two last methods seem copied from jamba - could you add a `# Copied from` statement in these methods? ```suggestion
``` Actually they look pretty much the same, why this can't be copied from? Same question as above Same comment as above, this can be copied from no? What this argument is used for? Do you think it can be removed ? ```suggestion
```
If it's adapted from we don't need to put this statement Same here ```suggestion
``` I see the difference is here Can't you use `self.layer_idx`, similarly as Jamba: https://github.com/huggingface/transformers/blob/6739e1d261f80caec34b8c8ac7a030907a4f75a2/src/transformers/models/jamba/modeling_jamba.py#L355 ? see my comment above about relying on `self.layer_idx` instead, what do you think? Same comment about `layer_id_num` same comment about about `layer_idx` being an attribute of the class  Instead of passing it can you concatenate it before passing it to the decoder layer? it is unclear to me what `from_tf` is used for Yes, we tested this. `use_mamba_kernels` is a parameter in `ZambaConfig` that defaults to `True`. We tested that passing `use_mamba_kernels=False` in `from_pretrained` sets the forward pass to the one using the pure Python implementation. Yes, we changed the implementation and made this an attribute (and introduced the method `_layers_block_type` to construct the attribute). We updated this, thank you! The input dimension of this attention layer is twice `config.hidden_size` (the input here is the concatenation of `x_orig` with the output of the previous mamba layer, hence the extra factor of 2). Additionally, some intermediate dimensions also have this factor of 2: `self.head_dim = 2 * self.hidden_size // self.num_heads`.

We added this explanation in the comment above `ZambaAttention` class. Please let us know if this requires further edits. We added more details on the changes done from Mistral's implementation, as in the previous reply. Same as the reply above, we detailed the implementation differences in the code. We cannot remove `from_tf` as this is summed to `hidden_states` in the non-residual stream of the mamba block (which is then fed into the mamba layer). We added a comment to explain the role of `from_tf` around [this line](https://github.com/huggingface/transformers/blob/aa576ac536e983f5f42a5fa885c4d82736f736d7/src/transformers/models/zamba/modeling_zamba.py#L1047). We cannot implement the layer number as an attribute of `ZambaAttention`, unfortunately. This is because the instantiation of this block is used in multiple layers (equivalently, it is a shared layer), as shown in fig. 2 of [this paper](https://arxiv.org/pdf/2405.16712). Therefore, we found it convenient to update the layer number through the forward pass.  Please see the reply above. Please see the reply above."
33757,2024-09-27T10:09:06Z,2024-10-15T09:27:54Z,RUFFY-369,20,0,18,105,4,4,3,"['Vision', 'SDPA', 'run-slow']",899.0,0,1553151.0,0,0,0,0,6259287.411906,,0,18,0,False,"['ArthurZucker', 'RUFFY-369', 'HuggingFaceDocBuilderDev', 'amyeroberts']","@amyeroberts All green 🟢 :smiley:  Hi @amyeroberts , I think in the `ViViT` model, originally `interpolate_pos_encoding` method [here](https://github.com/huggingface/transformers/blob/4196590aa0fe6ed735b68e126c2125a5d806acb7/src/transformers/models/vivit/modeling_vivit.py#L110) hasn't been correctly implemented and has just been copied from `ViT` because I made a commit in this current PR regarding `patch_size` [here](https://github.com/huggingface/transformers/pull/33757/commits/9c46bf2d4df7137a686dc8b55a58ee4280fc1c61) as `patch_size` itself wasn't declared in the method. I gave benefit of doubt to the work done during the port of this model and just fixed it. But when I tested the slow tests with `RUN_SLOW`, `test_inference_interpolate_pos_encoding` failed because the implementation hasn't been done properly. So, should I raise an issue separately or fix it here.

One thing can be that apart from the `test_inference_interpolate_pos_encoding` slow test rest can tested by `[run_slow] vivit` commit message and I will push the training and inference benchmark too just in some time. And when this PR gets merged, I will open a separate issue for `test_inference_interpolate_pos_encoding` test fix.

Or the other thing can be that I fix it here in this PR.

Please suggest what works well for you?! > Hi @amyeroberts , I think in the `ViViT` model, originally `interpolate_pos_encoding` method [here](https://github.com/huggingface/transformers/blob/4196590aa0fe6ed735b68e126c2125a5d806acb7/src/transformers/models/vivit/modeling_vivit.py#L110) hasn't been correctly implemented and has just been copied from `ViT` because I made a commit in this current PR regarding `patch_size` [here](https://github.com/huggingface/transformers/pull/33757/commits/9c46bf2d4df7137a686dc8b55a58ee4280fc1c61) as `patch_size` itself wasn't declared in the method. I gave benefit of doubt to the work done during the port of this model and just fixed it. But when I tested the slow tests with `RUN_SLOW`, `test_inference_interpolate_pos_encoding` failed because the implementation hasn't been done properly. So, should I raise an issue separately or fix it here.
> 
> One thing can be that apart from the `test_inference_interpolate_pos_encoding` slow test rest can tested by `[run_slow] vivit` commit message and I will push the training and inference benchmark too just in some time. And when this PR gets merged, I will open a separate issue for `test_inference_interpolate_pos_encoding` test fix.
> 
> Or the other thing can be that I fix it here in this PR.
> 
> Please suggest what works well for you?!

Update:  [This](https://github.com/huggingface/transformers/pull/33757/commits/9c46bf2d4df7137a686dc8b55a58ee4280fc1c61) and [this commit](https://github.com/huggingface/transformers/pull/33757/commits/50b9f1a54bcee7a8a4e8f1ff37a69424e54e7cd3) was enough to fix the interpolate method and the only thing required right now is a nit in `test_inference_interpolate_pos_encoding` for it to pass, basically a correction in the input being passed. Right now, I'm keeping that nit change in hold because I need a confirmation from your side that to fix `test_inference_interpolate_pos_encoding` failure in this issue or simply open a separate issue.

And regarding this PR, 

> Thanks for adding!
> 
> Two things before merge:
> 
> * Running the slow tests for the model. Could you push an empty commit with the message `[run_slow] vivit`?
> * Adding info and benchmarks to the model's doc page. A good example is [here](https://huggingface.co/docs/transformers/v4.44.2/en/model_doc/gpt_neox#using-scaled-dot-product-attention-sdpa). Benchmarking scripts from @fxmarty for [training](https://gist.github.com/fxmarty/7e75cc3942d6974e4849093ebea0a331) and [inference](https://gist.github.com/fxmarty/5113e4304fbdd38c9c3702ce44683f6a).

Both of the things asked has been completed and pushed. So, this PR is ready for merge. Hi @amyeroberts , I just created a separate PR where `interpolate_pos_encoding` for ViViT has been fixed here #33815 , fixing the slow test with it. 
Also, your suggestions have already been addressed for this PR so, its ready to be merged.

Please review both of them when you get the time.
Thank you :smile:  @amyeroberts Can you please add `run_slow` label here as well so that the empty commit can work.
Thanks @RUFFY-369 You should be able to add the label yourself, I think. Could you try? It would be useful to know if external collaborators can or can't add this, as we'd like to make this so that contributors can do as much as possible independently  > @RUFFY-369 You should be able to add the label yourself, I think. Could you try? It would be useful to know if external collaborators can or can't add this, as we'd like to make this so that contributors can do as much as possible independently


@amyeroberts No, I can't. On my side I don't have any gear/settings button near label:
![Screenshot from 2024-09-30 18-23-20](https://github.com/user-attachments/assets/7cf2d4df-d54a-4e75-9ce1-7205c9f30140)

Usually if I can add the label, I would see something like this:

![Screenshot from 2024-09-30 18-24-06](https://github.com/user-attachments/assets/c97ba843-0587-4ff8-b87c-0d0b53a90191)

So, yes, I guess external collaborators can't add this
 @RUFFY-369 OK, thanks for checking! @ydshieh This means the message for the slow model tests bot might need to be changed as contributors can't add the label as instructed @RUFFY-369 I've added the label. Could you push another `[run-slow] vivit` commit to trigger the tests?  Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). 
> @RUFFY-369 I've added the label. Could you push another `[run-slow] vivit` commit to trigger the tests?

@amyeroberts Done :+1: 
 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33757). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Failing tests are unrelated @RUFFY-369 Could you rebase? A fix has been merged in  > @RUFFY-369 Could you rebase? A fix has been merged in

@amyeroberts Done @amyeroberts Fallback has been added for `output_attentions`

Cheers! All green, slow tests are left to trigger
If a maintainer can trigger them before merge
cc @amyeroberts  soft ping @amyeroberts 
Thank you soft ping @ArthurZucker 
Thank you :smile:  🤗 Thanks for adding! 

Two things before merge: 
* Running the slow tests for the model. Could you push an empty commit with the message `[run_slow] vivit`?
* Adding info and benchmarks to the model's doc page. A good example is [here](https://huggingface.co/docs/transformers/v4.44.2/en/model_doc/gpt_neox#using-scaled-dot-product-attention-sdpa). Benchmarking scripts from @fxmarty for [training](https://gist.github.com/fxmarty/7e75cc3942d6974e4849093ebea0a331) and [inference](https://gist.github.com/fxmarty/5113e4304fbdd38c9c3702ce44683f6a).  looks good to me, appreciate the effort @RUFFY-369  Thanks for adding @RUFFY-369! 

The only thing missing is a fallback for when output_attentions are passed e.g. [like here](https://github.com/huggingface/transformers/blob/e7c8af7f338eab42bd6615f77da5b69094b4836e/src/transformers/models/distilbert/modeling_distilbert.py#L364)


cc @qubvel  Thanks, looks good to me! cc @ArthurZucker for final review LGTM thanks for the details documentation update!","Thanks for adding! 

Two things before merge: 
* Running the slow tests for the model. Could you push an empty commit with the message `[run_slow] vivit`?
* Adding info and benchmarks to the model's doc page. A good example is [here](https://huggingface.co/docs/transformers/v4.44.2/en/model_doc/gpt_neox#using-scaled-dot-product-attention-sdpa). Benchmarking scripts from @fxmarty for [training](https://gist.github.com/fxmarty/7e75cc3942d6974e4849093ebea0a331) and [inference](https://gist.github.com/fxmarty/5113e4304fbdd38c9c3702ce44683f6a).  looks good to me, appreciate the effort @RUFFY-369  Thanks for adding @RUFFY-369! 

The only thing missing is a fallback for when output_attentions are passed e.g. [like here](https://github.com/huggingface/transformers/blob/e7c8af7f338eab42bd6615f77da5b69094b4836e/src/transformers/models/distilbert/modeling_distilbert.py#L364)


cc @qubvel  Thanks, looks good to me! cc @ArthurZucker for final review LGTM thanks for the details documentation update!","# What does this PR do?

Towards #28005 

Adds sdpa for the ViViT model.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@amyeroberts 
",
34150,2024-10-14T09:10:17Z,2024-10-15T08:45:22Z,gary149,0,0,1,6,1,1,1,[],,0,84908.0,0,0,0,0,6262259.525586,,0,1,0,False,[],"Nice, love it!","Nice, love it!",,
34062,2024-10-10T11:08:41Z,2024-10-15T09:17:14Z,zucchini-nlp,1,4,10,502,10,3,2,[],362186.0,0,425314.0,0,0,0,0,6260349.583145,,0,10,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34062). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM, thanks for debugging these tests! LGTM, except for those two `continue` replacements.

(tests 💛 )","LGTM, thanks for debugging these tests! LGTM, except for those two `continue` replacements.

(tests 💛 )","# What does this PR do?

Enables GenerationTesterMixin for Idefics models. Tests are passing for me locally.

There are a few PRs with idefics already and they need to be reviewed/merged in the following order: https://github.com/huggingface/transformers/pull/33907 -> https://github.com/huggingface/transformers/pull/34043 -> current PR","ouuu very nice!  We need a continue here :P

With a skip, we skip for all other models in `self.all_generative_model_classes`, and we might have encoder-decoder and decoder-only models mixed up together in that iterator Likewise, this also needs to be a continue hmm oke, didn't think we could have cases with mixed model types. Was just confused the test was passing for one model while not other, and took me a while to figure out hehe"
34095,2024-10-11T15:38:48Z,2024-10-12T15:45:52Z,gante,4,1,5,134,1,2,1,[],3561.0,0,320093.0,0,0,0,0,6262965.682931,,2,5,0,False,"['SunMarc', 'ringohoffman', 'gante']","@SunMarc this should help with FSDP + `generate` 🤗  This fixes the error I was seeing here:

* https://github.com/huggingface/transformers/pull/33483#issuecomment-2350480749

Thank you so much! @ArthurZucker I don't think this is being tested! 

@SunMarc -- I couldn't find any related test, but multigpu tests have a more elaborated setup, so I could be missing something. Can you confirm?

Meanwhile, I'm merging since this PR unblocks users. If there is no test, I'll open a follow-up PR :)  > @SunMarc -- I couldn't find any related test, but multigpu tests have a more elaborated setup, so I could be missing something. Can you confirm?

I'm not aware of any tests related to multi-gpu and generate with sync_gpus=True. I will have a look at this since we also need to add them for deepspeed and fdsp  ! cc @muellerzr The decorelation between prepare input for generation and the modeling is very nice. 
I don't know how well we test this, if the slow CIs were crying or not, but if yes, then it's already tested and Good to go!","The decorelation between prepare input for generation and the modeling is very nice. 
I don't know how well we test this, if the slow CIs were crying or not, but if yes, then it's already tested and Good to go!","# What does this PR do?

Step 5 in #32685 
Fixes #32885 
Fixes #32603 
Fixes #32641

Modern LLMs, i.e. LLMs that support our cache classes, currently fail when the input has a `batch size > 1` and `synced_gpus = True`. 

On `main`, this is what happens with `synced_gpus`
1. `cache_position` stops being updated when generation finishes in a given device, causing cache indexing errors on that device (the cache continues growing because we keep doing dummy forward passes)
2. if we continue updating `cache_position`, then slicing `input_ids` gets out of bounds for the dummy computations (we stop updating `input_ids`, so it stops growing)

This PR makes the changes to enable generation with the behavior above. 

💛 Please note that, because of the efforts in #32685, updating model input preparation requires an update in a single function, as opposed to an update per model 💛 

_________________________________________________________________

Test script (call with 2+ GPUs) that fails before this PR (from [this comment](https://github.com/huggingface/transformers/issues/32885#issuecomment-2313149520)):
```py
import transformers
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

def run(rank, size):
    # dist.initialize_dist('gpu')

    name = 'meta-llama/Meta-Llama-3-8B-Instruct'
    tokenizer = transformers.AutoTokenizer.from_pretrained(name)
    pad_token_id = tokenizer.eos_token_id
    model = transformers.AutoModelForCausalLM.from_pretrained(name)

    # rank = dist.get_global_rank()

    model.to(f'cuda:{rank}')

    if rank == 0:
        content = 'Write one short sentence.'
    else:
        content = 'Write one long paragraph.'

    messages = [
        {
            'role': 'user',
            'content': content,
        }
    ]

    tokenized_messages = tokenizer.apply_chat_template(messages, return_tensors='pt')

    padded_messages = torch.cat(
        [
            torch.LongTensor((4096 - 20) * [pad_token_id]),
            tokenized_messages[0],  # [seq]
        ],
        dim=0,
    )
    padded_messages = padded_messages.unsqueeze(0)
    padded_messages = padded_messages.to(f'cuda:{rank}')
    attention_mask = ~(padded_messages == pad_token_id)
    attention_mask = attention_mask.to(f'cuda:{rank}')
    output = model.generate(input_ids=padded_messages, attention_mask=attention_mask, synced_gpus=True, max_new_tokens=200)

    print(tokenizer.decode(output[0]))

def init_process(rank, size, fn, backend='gloo'):
    """""" Initialize the distributed environment. """"""
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '29500'
    dist.init_process_group(backend, rank=rank, world_size=size)
    fn(rank, size)

if __name__ == ""__main__"":
    size = 2
    processes = []
    mp.set_start_method(""spawn"")
    for rank in range(size):
        p = mp.Process(target=init_process, args=(rank, size, run))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()
```",Simplifies logic in assisted generation: see the new `is_first_iteration` variable and its uses :)
33657,2024-09-23T07:50:35Z,2024-09-25T08:39:10Z,jmamou,7,2,8,32,4,2,2,[],16826.0,0,1825779.0,0,0,0,0,6340572.845184,,1,8,0,False,"['HuggingFaceDocBuilderDev', 'gante', 'jmamou']","Need to modify it.
We should set the default values only for the generation_config of the assistant model.
Currently, ConfidenceCriteria is added to any model that has generation_config.assistant_confidence_threshold > 0.   I added `is_assistant` flag.
Fixed! I'm consistently getting a 30% speedup over the previous default, well done @jmamou 💛  @ArthurZucker it is not a breaking change in the sense that the API doesn't change and the model output doesn't change. Only the run time gets upgraded 🤗 

In any case, updated the PR name in case something goes wrong (cc @jmamou we add an emoji to potentially breaking PRs) @jmamou would you kindly rebase the PR? I suspect it fixes the red CI :)  (I hope you don't mind -- I've rebased the PR, in the hopes this PR makes the cut for our next release which will be very soon 🤗 ) The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33657). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM 👍 

I'm going to run a few benchmarks on my end to confirm it also improves the base performance on my test cases. I'll tag a core maintainer to review and approve if all goes well 🤗  LGTM but changing the default is breaking Let's update the PR with the 🔴 ","LGTM 👍 

I'm going to run a few benchmarks on my end to confirm it also improves the base performance on my test cases. I'll tag a core maintainer to review and approve if all goes well 🤗  LGTM but changing the default is breaking Let's update the PR with the 🔴 ","# What does this PR do?

Following https://github.com/huggingface/transformers/pull/33258#issuecomment-2361897163

Setting default behavior of assisted decoding: the number of speculative tokens per assistant model iteration is dynamically determined according to the confidence in the assistant model prediction.

By default, 
```
num_assistant_tokens=20 
num_assistant_tokens_schedule=""constant"" 
assistant_confidence_threshold=0.4
```

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? 
https://github.com/huggingface/transformers/pull/33258#issuecomment-2361897163
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
@gante ","```suggestion
            generation_config.is_assistant
```
`generation_config.is_assistant is not None and generation_config.is_assistant` can be simplified as the latter implies the former we are breaking the default here! "
31098,2024-05-29T04:28:17Z,2024-09-24T09:40:56Z,jiqing-feng,30,30,66,536,20,5,4,['Quantization'],2604892.0,0,11953381.0,1,0,0,0,6333910.201251,,0,66,0,False,"['Titus-von-Koeller', 'SunMarc', 'HuggingFaceDocBuilderDev', 'github-actions[bot]', 'jiqing-feng']","This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.

Please note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored. Hi @younesbelkada @SunMarc @ArthurZucker . I think this PR is ready to be reviewed, as the bitsandbytes already gives the installation guide for CPU. See [here](https://github.com/TimDettmers/bitsandbytes?tab=readme-ov-file#alpha-testers-wanted-multi-backend-refactor-amd-gpu--intel-cpugpu-specific-bnb-backend-implementations). cc @Titus-von-Koeller  @jiqing-feng, we decided this likely still needs some work. I'll look into it more this week. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_31098). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Refer to [1243](https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1243). I updated the check method when cuda is not available, please review it. Thx! @SunMarc 

 @Titus-von-Koeller please help to check if it is enough for multi-backend-refactor branch. 
 Ok, I started looking into this a bit on the Intel machine that we have access to and based on the code of this PR, I'm still getting other errors when running the tests, disabling the skipping of tests due to CUDA having been required before.

Basically almost all the tests fail with the same below error and none of them pass.

```
______________________________________________________ Bnb4BitGPT2Test.test_rwkv_4bit _______________________________________________________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_rwkv_4bit>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map=""auto""
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map=""auto"")

tests/quantization/bnb/test_4bit.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3834: in from_pretrained
    hf_quantizer.validate_environment(device_map=device_map)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7f0219252070>, args = ()
kwargs = {'device_map': OrderedDict([('', 'cpu')])}, device_map_without_lm_head = {'': 'cpu'}

    def validate_environment(self, *args, **kwargs):
        if not is_accelerate_available():
            raise ImportError(""Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install accelerate`"")
        if not is_bitsandbytes_available():
            raise ImportError(
                ""Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`""
            )
        # if not torch.cuda.is_available():
        #    import bitsandbytes as bnb
    
        #    if not getattr(bnb, ""is_multi_backend_refactor_preview"", False):
        #        raise RuntimeError(
        #            ""Current bitsandbytes only support cuda, please switch to multi_backend_refactor to support multi backends.""
        #        )
    
        if kwargs.get(""from_tf"", False) or kwargs.get(""from_flax"", False):
            raise ValueError(
                ""Converting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make""
                "" sure the weights are in PyTorch format.""
            )
    
        device_map = kwargs.get(""device_map"", None)
        if (
            device_map is not None
            and isinstance(device_map, dict)
            and not self.quantization_config.llm_int8_enable_fp32_cpu_offload
        ):
            device_map_without_lm_head = {
                key: device_map[key] for key in device_map.keys() if key not in self.modules_to_not_convert
            }
            if ""cpu"" in device_map_without_lm_head.values() or ""disk"" in device_map_without_lm_head.values():
>               raise ValueError(
                    ""Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the ""
                    ""quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules ""
                    ""in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to ""
                    ""`from_pretrained`. Check ""
                    ""https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu ""
                    ""for more details. ""
                )
E               ValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details.

src/transformers/quantizers/quantizer_bnb_4bit.py:91: ValueError
``` > Ok, I started looking into this a bit on the Intel machine that we have access to and based on the code of this PR, I'm still getting other errors when running the tests, disabling the skipping of tests due to CUDA having been required before.
> 
> Basically almost all the tests fail with the same below error and none of them pass.

Could you try to pass `None` or `cpu` into `device_map` since we are testing it on CPU? I suppose device_map=""auto"" is for cuda, am I right? @SunMarc  Yeah, I'm working on a solution.

To see if there are further issues when running the [test script from the original Intel 4bit PR](https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1206#issuecomment-2126386067) or when running the Transformer bnb-related integration test suite, I modified it temporarily like this:

```
 90             if ""disk"" in device_map_without_lm_head.values():
 91             #if ""cpu"" in device_map_without_lm_head.values() or ""disk"" in device_map_without_lm_head.values():
```

My idea is to check if BNB is multi-backend enabled and in that case skip all the GPU related checks and allow CPU as well.

@SunMarc In this case, do you think it's save to sth like:

```
if (""cpu"" in device_map_without_lm_head.values() or bnb_is_multibackend_enabled) or ""disk"" in device_map_without_lm_head.values():
```

?

Currently, for the `tests/quantization/bnb/test_4bit.py` I'm still getting a bunch of failures, but most are related to the model being moved to `cuda(0)` or NotImplementedError, for which I'm working on a fix and is not related to the Intel implementation in BNB.

However, there's one failing test that caught my attention and that I couldn't understand the root cause of, yet.. Could you please help me figure out what's going on with that one, @jiqing-feng ? See below:

```
______________________________________________ Pipeline4BitTest.test_pipeline _______________________________________________
                                                                                                                             
self = <bnb.test_4bit.Pipeline4BitTest testMethod=test_pipeline>                                                             
                                                                                                                             
    def test_pipeline(self):                                  
        r""""""                                                                                                                 
        The aim of this test is to verify that the mixed 4bit is compatible with `pipeline` from transformers. Since         
        we used pipline for inference speed benchmarking we want to make sure that this feature does not break anything      
        on pipline.                                                                                                          
        """"""                                                                                                                  
        # self._clear_cuda_cache()                                                                                           
        self.pipe = pipeline(                                                                                                
            ""text-generation"",                                                                                               
            model=self.model_name,                                                                                           
            model_kwargs={""device_map"": ""auto"", ""load_in_4bit"": True, ""torch_dtype"": torch.float16},                         
            max_new_tokens=self.MAX_NEW_TOKENS,                                                                              
        )                                                                                                                    
                                                                                                                             
        # Real second forward pass                                                                                           
>       pipeline_output = self.pipe(self.input_text)                                                                         
                                                                                                                             
tests/quantization/bnb/test_4bit.py:462:                                                                                     
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src/transformers/pipelines/text_generation.py:262: in __call__                                                               
    return super().__call__(text_inputs, **kwargs)                                                                           
src/transformers/pipelines/base.py:1254: in __call__                                                                         
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)                                    
src/transformers/pipelines/base.py:1261: in run_single                                                                       
    model_outputs = self.forward(model_inputs, **forward_params)                                                             
src/transformers/pipelines/base.py:1161: in forward                                                                          
    model_outputs = self._forward(model_inputs, **forward_params)  
src/transformers/pipelines/text_generation.py:351: in _forward                                         (11 results) [443/925]
    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)          
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:115: in decorate_context                 
    return func(*args, **kwargs)                                                                                             
src/transformers/generation/utils.py:1969: in generate                                                                       
    result = self._sample(                                                                                                   
src/transformers/generation/utils.py:2912: in _sample                                                                        
    outputs = self(**model_inputs, return_dict=True)                                                                         
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl              
    return self._call_impl(*args, **kwargs)                                                                                  
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl                      
    return forward_call(*args, **kwargs)                                                                                     
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward                             
    output = module._old_forward(*args, **kwargs)                                                                            
src/transformers/models/bloom/modeling_bloom.py:848: in forward                                                              
    transformer_outputs = self.transformer(                                                                                  
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl              
    return self._call_impl(*args, **kwargs)                                                                                  
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl                      
    return forward_call(*args, **kwargs)                                                                                     
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward                             
    output = module._old_forward(*args, **kwargs)                                                                            
src/transformers/models/bloom/modeling_bloom.py:712: in forward                                                              
    outputs = block(                                                                                                         
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl              
    return self._call_impl(*args, **kwargs)                   
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl                      
    return forward_call(*args, **kwargs)                                                                                     
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward                             
    output = module._old_forward(*args, **kwargs)                                                                            
src/transformers/models/bloom/modeling_bloom.py:400: in forward                                                              
    attn_outputs = self.self_attention(                                                                                      
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl              
    return self._call_impl(*args, **kwargs)                                                                                  
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl                      
    return forward_call(*args, **kwargs)                                                                                     
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward                             
    output = module._old_forward(*args, **kwargs)                                                                            
src/transformers/models/bloom/modeling_bloom.py:251: in forward                                                              
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]                             
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl              
    return self._call_impl(*args, **kwargs)                                                                                  
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl                      
    return forward_call(*args, **kwargs)                                                                                     
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward                             
    output = module._old_forward(*args, **kwargs)                                                                            
../bnb/bitsandbytes/nn/modules.py:475: in forward                                                                            
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)                                
../bnb/bitsandbytes/autograd/_functions.py:586: in matmul_4bit                                                               
    out = F.gemv_4bit(A, B.t(), out, state=quant_state)                                                                      
../bnb/bitsandbytes/functional.py:1506: in gemv_4bit                                                                         
    return backends[A.device.type].gemv_4bit(                                                                                
../bnb/bitsandbytes/backends/cpu.py:171: in gemv_4bit                                                                        
    return gemm_4bit_impl(A, B, out, transposed_A, transposed_B, state)                                                      
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
                                                                                                                             
A = tensor([[[ 0.0030,  0.2297,  0.1515,  ..., -0.7241, -0.2993, -0.6665],                                                   
         [-0.8872, -0.4702,  0.6714,  ...,  0.2...62,  0.1533,  ..., -0.0767, -0.4731,  0.0826],                             
         [-0.1199,  0.2257,  0.4138,  ...,  0.1516,  0.3250, -0.5029]]])                                                     
B = tensor([231, 102, 109,  ...,  44, 100, 203], dtype=torch.uint8), out = None, transposed_A = False, transposed_B = False  
state = <bitsandbytes.utils.QuantState object at 0x7f6d1f90d760>                                                             
                                                                                                                             
    def gemm_4bit_impl(                                                                                                      
        A: torch.Tensor,                                                                                                     
        B: torch.Tensor,                                                                                                     
        out: Optional[torch.Tensor] = None,                                                                                  
        transposed_A=False,                                                                                                  
        transposed_B=False,                                                                                                  
        state: QuantState = None,                                                                                            
    ) -> torch.Tensor:                                                                                                       
        """"""                                                                                                                  
        Matrix-matrix multiplication with 4-bit quantization.                                                                
                                                                                                                             
        Parameters                                                                                                           
        ----------                                                                                                           
        A : torch.Tensor
            The first input tensor. Usually the activation tensor.
        B : torch.Tensor
            The second input tensor. Usually the weight tensor.
        out : torch.Tensor
            The output tensor.
        transposed_A : bool
            Whether A is transposed
        transposed_B : bool
            Whether B is transposed
        state : QuantState
            Contains quantization info, such as blocksize and dtype
     
        Returns
        -------
        torch.Tensor:
            GEMM output tensor.
        """"""
        if ipex_cpu and _ipex_cpu_version_prereq(2, 3) and hasattr(state, ""op_context""):
            assert state.op_context is not None
            output = torch.ops.torch_ipex.ipex_woq_linear(A, state.op_context.get_data_handle())
        else:
            dqB = dequantize_4bit_impl(B, state, blocksize=state.blocksize)
>           output = torch.matmul(A, dqB)
E           RuntimeError: expected m1 and m2 to have the same dtype, but got: float != c10::Half

../bnb/bitsandbytes/backends/cpu_xpu_common.py:527: RuntimeError
``` Hi @Titus-von-Koeller , as we have fixed the issue [1285](https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1285), do you have any updates? Hey @jiqing-feng,

Yes, I'm actively working on it and currently struggling with these remaining failing tests. Do any of these catch your eye, could you help fixing them?

```
=============================================================================================== FAILURES ================================================================================================
_____________________________________________________________________________ Bnb4BitTest.test_generate_quality_dequantize ______________________________________________________________________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_generate_quality_dequantize>

    def test_generate_quality_dequantize(self):
        r""""""
        Test that loading the model and unquantize it produce correct results
        """"""
        bnb_config = BitsAndBytesConfig(load_in_4bit=True)
    
        model_4bit = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map=""auto""
        )
    
        model_4bit.dequantize()
    
        encoded_input = self.tokenizer(self.input_text, return_tensors=""pt"")
>       output_sequences = model_4bit.generate(input_ids=encoded_input[""input_ids""].to(self.device), max_new_tokens=10)

tests/quantization/bnb/test_4bit.py:285: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:115: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1969: in generate
    result = self._sample(
src/transformers/generation/utils.py:2912: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = Linear(in_features=2048, out_features=6144, bias=True)
input = tensor([[[ 0.0030,  0.2297,  0.1515,  ..., -0.7241, -0.2993, -0.6665],
         [-0.8872, -0.4702,  0.6714,  ...,  0.2...-0.4731,  0.0826],
         [-0.1199,  0.2257,  0.4138,  ...,  0.1516,  0.3250, -0.5029]]],
       dtype=torch.float16)

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x2048 and 6144x2048)

../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/linear.py:116: RuntimeError
___________________________________________________________________________________ Bnb4BitTestTraining.test_training ___________________________________________________________________________________

self = <bnb.test_4bit.Bnb4BitTestTraining testMethod=test_training>

    def test_training(self):
        if version.parse(importlib.metadata.version(""bitsandbytes"")) < version.parse(""0.37.0""):
            self.skipTest(reason=""This test requires bitsandbytes >= 0.37.0"")
    
        # Step 1: freeze all parameters
        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True)
    
>       self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})

tests/quantization/bnb/test_4bit.py:544: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 512, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=512, out_features=50272, bias=False)
)
name = 'hf_device_map'

    def __getattr__(self, name: str) -> Any:
        if '_parameters' in self.__dict__:
            _parameters = self.__dict__['_parameters']
            if name in _parameters:
                return _parameters[name]
        if '_buffers' in self.__dict__:
            _buffers = self.__dict__['_buffers']
            if name in _buffers:
                return _buffers[name]
        if '_modules' in self.__dict__:
            modules = self.__dict__['_modules']
            if name in modules:
                return modules[name]
>       raise AttributeError(f""'{type(self).__name__}' object has no attribute '{name}'"")
E       AttributeError: 'OPTForCausalLM' object has no attribute 'hf_device_map'

../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1709: AttributeError
_______________________________________________________________________________ Bnb4BitGPT2Test.test_fp32_4bit_conversion _______________________________________________________________________________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_fp32_4bit_conversion>

    def test_fp32_4bit_conversion(self):
        r""""""
        Test whether it is possible to mix both `4bit` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        """"""
        model = AutoModelForSeq2SeqLM.from_pretrained(""google-t5/t5-small"", load_in_4bit=True, device_map=""auto"")
>       self.assertTrue(model.decoder.block[0].layer[2].DenseReluDense.wo.weight.dtype == torch.float32)
E       AssertionError: False is not true

tests/quantization/bnb/test_4bit.py:334: AssertionError
___________________________________________________________________________ Bnb4BitGPT2Test.test_generate_quality_dequantize ____________________________________________________________________________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_generate_quality_dequantize>

    def test_generate_quality_dequantize(self):
        r""""""
        Test that loading the model and unquantize it produce correct results
        """"""
        bnb_config = BitsAndBytesConfig(load_in_4bit=True)
    
        model_4bit = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map=""auto""
        )
    
        model_4bit.dequantize()
    
        encoded_input = self.tokenizer(self.input_text, return_tensors=""pt"")
>       output_sequences = model_4bit.generate(input_ids=encoded_input[""input_ids""].to(self.device), max_new_tokens=10)

tests/quantization/bnb/test_4bit.py:285: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:115: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1969: in generate
    result = self._sample(
src/transformers/generation/utils.py:2912: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1532: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1541: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = Linear(in_features=1600, out_features=4800, bias=True)
input = tensor([[[ 0.1334, -0.0541, -0.0432,  ...,  0.0113,  0.1115,  0.0245],
         [-0.1196,  0.0271, -0.1114,  ...,  0.0...-0.3340,  0.0841],
         [-0.1744, -0.0175, -0.0184,  ..., -0.5654, -0.4402, -0.0274]]],
       dtype=torch.float16)

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x1600 and 4800x1600)

../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/linear.py:116: RuntimeError
``` Particularly the tests with

```
RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x1600 and 4800x1600)
```

seem to me that they might be related to the Intel backend logic. Ok, so I just pushed my changes with which I ran all the tests. Find attached also the 8-bit related test results. There are a bunch of failures, essentially three types (just search the logs for `E   `):

1. `RuntimeError: self and mat2 must have the same dtype, but got Half and BFloat16`
2. `AttributeError: 'NoneType' object has no attribute 'device'`
3. `AttributeError: 'OPTForCausalLM' object has no attribute 'hf_device_map'`

No idea about the root causes yet.

@jiqing-feng I saw you also changed the HF 8bit quantizer. Not sure to what extent these tests should be working [based on the Intel 8bit BNB PR](https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1178) (cc @Xia-Weiwen) and which parts show an issue on the BNB side and which might be related to further fixes needed in the tests on the Transformer side. Would extremely appreciate your input on that.

See [transf_multi-backend_8bit-tests.log](https://github.com/user-attachments/files/16433845/transf_multi-backend_8bit-tests.log).

Do the other changes I introduced seem sensible to you? Would be happy to hear your opinion.

cc @SunMarc do you have an idea of how error type 3 could come to be? looks more like sth on the Transformer side to me.. > RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x1600 and 4800x1600)
> seem to me that they might be related to the Intel backend logic.

This [PR](https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1300) could fix this error  @jiqing-feng Unfortunately, it doesn't, see the [transf_multi-backend_4bit-tests.log](https://github.com/user-attachments/files/16440618/transf_multi-backend_4bit-tests.log):

```
❯ rg -o 'E\s\s+(.*)' ~/Downloads/transf_multi-backend_4bit-tests.log -r '$1' | sort | uniq -c | sort -rn

   4 RuntimeError: mat1 and mat2 shapes cannot be multiplied (7x768 and 3072x768)
   3 RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x2048 and 6144x2048)
   2 RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x1600 and 4800x1600)
   2 RuntimeError: mat1 and mat2 shapes cannot be multiplied (12x512 and 2048x512)
   1 AttributeError: 'OPTForCausalLM' object has no attribute 'hf_device_map'
   1 AssertionError: False is not true
```

You can validate yourself with the following commands:
```
RUN_SLOW=1 pytest tests/quantization/bnb/test_4bit.py -rsx -v
RUN_SLOW=1 pytest tests/quantization/bnb/test_mixed_int8.py -rsx -v
```

Since it's practical to have this overview of failed tests, I'm also pasting the current status of failing tests for the 8-bit tests:
```
❯ rg -o 'E\s\s+(.*)' ~/Downloads/transf_multi-backend_8bit-tests.log -r '$1' | sort | uniq -c | sort -rn

   9 AttributeError: 'NoneType' object has no attribute 'device'
   2 RuntimeError: self and mat2 must have the same dtype, but got Half and BFloat16
   1 AttributeError: 'OPTForCausalLM' object has no attribute 'hf_device_map'
```

Who could help me in figuring out how to get the 8-bit tests working from the Intel side? Would that be @Xia-Weiwen I invited him to the new #bitsandbytes-intel-collab Slack channel. Could you communicate internally with him that he accepts the invite and/or let me know who else I should invite? 1. For `self and mat2 must have the same dtype, but got Half and BFloat16`: The 4bit has been fixed by [1285](https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1285). The 8bit comes from [here](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/bitsandbytes.py#L334-L364). I think we should pass the original dtype so we can dequant the model's type to the original dtype. WDYT? I fixed it by this [commit](https://github.com/huggingface/transformers/pull/31098/commits/48578048d5a5bf619ea9d18f57ae4d04c5085b17), please let me know your opinion.(https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1285)
2. For `mat1 and mat2 shapes cannot be multiplied`: The 4bit has been fixed by [1300](https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1300). 
3. For `'OPTForCausalLM' object has no attribute 'hf_device_map'`: I suppose the CPU model doesn't have `device_map` or `hf_device_map`, we need to discuss with the transformers maintainer whether we should enable device_map in the CPU model, or we should disable `device_map` check in CPU model. And we cannot pass `device_map='auto'` in CPU models.
4. For `'NoneType' object has no attribute 'device'`: I cannot produce the error if I set `device_map=""cpu""`. > Who could help me in figuring out how to get the 8-bit tests working from the Intel side? Would that be @Xia-Weiwen I invited him to the new #bitsandbytes-intel-collab Slack channel. Could you communicate internally with him that he accepts the invite and/or let me know who else I should invite?

I have communicated with him and he should be in the group.
 Hey @SunMarc 🤗 

> For 'OPTForCausalLM' object has no attribute 'hf_device_map': I suppose the CPU model doesn't have device_map or hf_device_map, we need to discuss with the transformers maintainer whether we should enable device_map in the CPU model, or we should disable device_map check in CPU model. And we cannot pass device_map='auto' in CPU models.

Do you have an opinion on this or do you have a suggestion whom of our colleagues we could pull in to help answer this question? @jiqing-feng

Current status of open failing tests:
```
4-bit Test Error Summary:
      4 RuntimeError: mat1 and mat2 shapes cannot be multiplied (7x768 and 3072x768)
      1 AttributeError: 'OPTForCausalLM' object has no attribute 'hf_device_map'

8-bit Test Error Summary:
      9 AttributeError: 'NoneType' object has no attribute 'device'
      1 AttributeError: 'OPTForCausalLM' object has no attribute 'hf_device_map'
```


> 1. For `self and mat2 must have the same dtype, but got Half and BFloat16` [...]

YES, these are all fixed, thanks 🤗 !

> 2. For `mat1 and mat2 shapes cannot be multiplied`: The 4bit has been fixed by [1300](https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1300).

That PR fixed most of them, but not all:

all fixed
```
3x RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x2048 and 6144x2048)
2x RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x1600 and 4800x1600)
2x RuntimeError: mat1 and mat2 shapes cannot be multiplied (12x512 and 2048x512)
```

remaining in 4-bit, even with current fix:
```
4x RuntimeError: mat1 and mat2 shapes cannot be multiplied (7x768 and 3072x768)
```

> 3. For `'OPTForCausalLM' object has no attribute 'hf_device_map'`:

Agreed, also not an expert on this, let's see what Marc says.

> 4. For `'NoneType' object has no attribute 'device'`: I cannot produce the error if I set `device_map=""cpu""`.

I tried replacing `device_map=""auto""` in `transformers/src/transformers/quantizers/quantizer_bnb_8bit.py` with `device_map=""auto"" if torch.cuda.is_available() else ""cpu"",` but that didn't change the test results regarding NoneType at all:

`9x AttributeError: 'NoneType' object has no attribute 'device'` for the 8 bit test before or after the replacement. Is that what you meant?

If it's sth else, please try changing the tests accordingly and see for yourself if the tests pass or not. On the Intel dev VM that you also have access to I created a command `integration` that you can run from any directory and that will output the test logs to stdout and also to file in the ~/src/workbench/multi-backend/logs dir. Please feel free to work there and share the results here. The below style of summary will be written out as `summary.log` in the just-mentioned directory. Hi @Titus-von-Koeller 

For 4bit matmul dim mismatch: Fixed by [1301](https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1301)

For 8bit None type error: Fixed by [1303](https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1303) Ok, all tests passing now, but the below:

```
=================================== FAILURES ===================================
______________________ Bnb4BitTestTraining.test_training _______________________

self = <bnb.test_4bit.Bnb4BitTestTraining testMethod=test_training>

    def test_training(self):
        if version.parse(importlib.metadata.version(""bitsandbytes"")) < version.parse(""0.37.0""):
            self.skipTest(reason=""This test requires bitsandbytes >= 0.37.0"")
    
        # Step 1: freeze all parameters
        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True)
    
>       self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})

tests/quantization/bnb/test_4bit.py:523: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 512, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=512, out_features=50272, bias=False)
)
name = 'hf_device_map'

    def __getattr__(self, name: str) -> Any:
        if '_parameters' in self.__dict__:
            _parameters = self.__dict__['_parameters']
            if name in _parameters:
                return _parameters[name]
        if '_buffers' in self.__dict__:
            _buffers = self.__dict__['_buffers']
            if name in _buffers:
                return _buffers[name]
        if '_modules' in self.__dict__:
            modules = self.__dict__['_modules']
            if name in modules:
                return modules[name]
>       raise AttributeError(f""'{type(self).__name__}' object has no attribute '{name}'"")
E       AttributeError: 'OPTForCausalLM' object has no attribute 'hf_device_map'

../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1709: AttributeError


_____________________ MixedInt8TestTraining.test_training ______________________

self = <bnb.test_mixed_int8.MixedInt8TestTraining testMethod=test_training>

    def test_training(self):
        if version.parse(importlib.metadata.version(""bitsandbytes"")) < version.parse(""0.37.0""):
            self.skipTest(reason=""This test requires bitsandbytes>=0.37.0"")
    
        # Step 1: freeze all parameters
        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)
    
>       self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})

tests/quantization/bnb/test_mixed_int8.py:858: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 512, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=512, out_features=50272, bias=False)
)
name = 'hf_device_map'

    def __getattr__(self, name: str) -> Any:
        if '_parameters' in self.__dict__:
            _parameters = self.__dict__['_parameters']
            if name in _parameters:
                return _parameters[name]
        if '_buffers' in self.__dict__:
            _buffers = self.__dict__['_buffers']
            if name in _buffers:
                return _buffers[name]
        if '_modules' in self.__dict__:
            modules = self.__dict__['_modules']
            if name in modules:
                return modules[name]
>       raise AttributeError(f""'{type(self).__name__}' object has no attribute '{name}'"")
E       AttributeError: 'OPTForCausalLM' object has no attribute 'hf_device_map'

../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1709: AttributeError
``` Ok, so with this change I could avoid the device map error:

```diff
-        self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})
+        if torch.cuda.is_available():
+            self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})
+        else:
+            self.assertTrue(all(param.device.type == ""cpu"" for param in model.parameters()))
```
It seems for GPU it triggers Accelerate and therefore has `hf_device_map` and for CPU it doesn't so you need to do another check..

However, this brought to light two more failures:

```
4-bit Test Error Summary:
      1 RuntimeError: expected m1 and m2 to have the same dtype, but got: c10::Half != float

8-bit Test Error Summary:
      1 RuntimeError: expected m1 and m2 to have the same dtype, but got: c10::Half != float
```

[Find the detailed test failure logs here](https://github.com/bitsandbytes-foundation/workbench/tree/main/multi-backend/logs)

Reproduce with:
```bash
export RUN_SLOW=1 && pytest tests/quantization/bnb/test_4bit.py::Bnb4BitTestTraining::test_training -rsx -v && pytest tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestTraining::test_training
```

All other tests are passing now. Hi @Titus-von-Koeller 

For `RuntimeError: expected m1 and m2 to have the same dtype, but got: c10::Half != float`. The error comes from the default type. The default type of opt model is float16 which is the same as cuda bnb dtype, but bfloat16 in CPU. In that case, the error will be in cuda if you replace the model with a bf16 model like [mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2). 

So we can fix the model dtype to fp32 by replacing `model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)` to `model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, torch_dtype=torch.float32)`. Then, the error will disappear.
 x-posting here what I already wrote on Slack. cc @SunMarc Do you have any idea what might be going on here?

---

In the BNB integration tests, which I modified in the PR for Intel CPU, I encountered an error in the test `Bnb4BitTestTraining::test_training` with the line `model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)`. The error was as follows:

```
self = Linear(in_features=1024, out_features=16, bias=False)
input = tensor([[[ 0.0041, -0.0679, -0.0397,  ...,  0.0058,  0.0221, -0.0017],
         [-0.0280,  0.0693,  0.0341,  ..., -0.0...-0.0358,  0.0205],
         [-0.0345,  0.0459,  0.0049,  ..., -0.0307,  0.0067, -0.0451]]],
       dtype=torch.float16)

def forward(self, input: Tensor) -> Tensor:
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected m1 and m2 to have the same dtype, but got: c10::Half != float
```

When using `model = AutoModelForCausalLM.from_pretrained(..., torch_dtype=torch.bfloat16 if not torch.cuda.is_available() else None)`, I received the following error:

```
self = Linear(in_features=1024, out_features=16, bias=False)
input = tensor([[[ 0.0032, -0.0679, -0.0405,  ...,  0.0045,  0.0244, -0.0016],
         [-0.0276,  0.0708,  0.0344,  ..., -0.0...0.0322,  0.0205],
         [-0.0339,  0.0447,  0.0045,  ..., -0.0334,  0.0092, -0.0452]]],
       dtype=torch.bfloat16)

def forward(self, input: Tensor) -> Tensor:
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected m1 and m2 to have the same dtype, but got: c10::BFloat16 != float
```

However, when setting `torch_dtype=torch.float32`, there is no error. I find this behavior very weird.

The strange thing is that the failing test becomes passing if I remove [the torch.amp line](https://github.com/huggingface/transformers/blob/458b0cd2c544cdd6c700f9b0c21077c889bcee6c/tests/quantization/bnb/test_4bit.py#L523)

You can see that the input is actually changed and not the weight in the test error output `input = tensor(...., dtype=torch.bfloat16)` and the dtype of the weight is visible in the RuntimeError. Hi @Titus-von-Koeller . I just recognized we use the wrong autocast for CPU, the correct usage should be `with torch.autocast(device_type):` which the `device_type` should be `""cpu""` or `""cuda""`. The failed CI is not related to my changes. Ok, I think we're through with this PR from our side, could you please review @SunMarc?

We're looking forward to merge this, so we can announce the alpha release.

This is where we stand regarding integration test suite execution ([see script](https://github.com/bitsandbytes-foundation/workbench/blob/main/multi-backend/run_quant_tests.sh)) on the 4 different backends ([logs here](https://github.com/bitsandbytes-foundation/workbench/tree/main/multi-backend/logs); including CUDA for reference and checking that we didn't break anything via the test refactor):

---

- CUDA: no tests failing, neither with bnb `main` nor `multi-backend-refactor` branches
- Intel CPU: all tests pass!
- ROCm:
  - 4-bit: all tests pass!
  - 8-bit:
    - 20 passed
    - 24 Exception: cublasLt ran into an error! (needs fixing in upcoming ROCm release)
    - 1 AssertionError: False is not true (needs fixing on Python side, communicated to AMD team)
- XPU: WIP, however integration test execution itself now successfully executes; failing tests need further implementation work on BNB XPU Backend side.

The failing tests in XPU and ROCm are not related to the Transformer code base and are therefore not a hold-up in merging this PR. They need fixing/implementing on the ROCm and BNB side. Also, we're already catching ""Not Implemented"" errors and skipping those tests, but for the ROCm exceptions this wasn't yet possible as they will need to be made more specific; which is currently being worked on by AMD on the BNB side.

---

In conclusion, in my view at this time there's no apparent reason that would keep us from merging, other than now needing a thorough look by you and fixing whatever comes up in the review :)

Let us know what you think, thanks 🤗 

cc @jiqing-feng @faaany @pnunna93 @matthewdouglas  P.S. I'm getting some errors when trying to fix the failing CI checks, they seem to come from elsewhere and the merge from `main` didn't fix it.

How would you advise to proceed? For now I didn't commit the changed files:

```
❯ make style; make quality
ruff check examples tests src utils setup.py conftest.py --fix --exclude """"
ruff format examples tests src utils setup.py conftest.py --exclude """"
22 files reformatted, 3042 files left unchanged
make autogenerate_code
make[1]: Entering directory '/home/sdp/src/transformers'
running deps_table_update
updating src/transformers/dependency_versions_table.py
make[1]: Leaving directory '/home/sdp/src/transformers'
make extra_style_checks
make[1]: Entering directory '/home/sdp/src/transformers'
python utils/custom_init_isort.py
python utils/sort_auto_mappings.py
python utils/check_doc_toc.py --fix_and_overwrite
make[1]: Leaving directory '/home/sdp/src/transformers'
/home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/sdp/src/transformers/src/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Traceback (most recent call last):
  File ""/home/sdp/src/transformers/src/transformers/utils/import_utils.py"", line 1636, in _get_module
    return importlib.import_module(""."" + module_name, self.__name__)
  File ""/home/sdp/.condax/mamba/envs/bnb/lib/python3.10/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module
  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
  File ""/home/sdp/src/transformers/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py"", line 30, in <module>
    import torchaudio.compliance.kaldi as ta_kaldi
  File ""/home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torchaudio/__init__.py"", line 1, in <module>
    from . import (  # noqa: F401
  File ""/home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torchaudio/_extension/__init__.py"", line 45, in <module>
    _load_lib(""libtorchaudio"")
  File ""/home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torchaudio/_extension/utils.py"", line 64, in _load_lib
    torch.ops.load_library(path)
  File ""/home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_ops.py"", line 1295, in load_library
    ctypes.CDLL(path)
  File ""/home/sdp/.condax/mamba/envs/bnb/lib/python3.10/ctypes/__init__.py"", line 374, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: /home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZNK5torch8autograd4Node4nameB5cxx11Ev

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""<frozen importlib._bootstrap>"", line 1073, in _handle_fromlist
  File ""<frozen importlib._bootstrap>"", line 1075, in _handle_fromlist
  File ""/home/sdp/src/transformers/src/transformers/utils/import_utils.py"", line 1627, in __getattr__
    value = getattr(module, name)
  File ""/home/sdp/src/transformers/src/transformers/utils/import_utils.py"", line 1626, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File ""/home/sdp/src/transformers/src/transformers/utils/import_utils.py"", line 1638, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.audio_spectrogram_transformer.feature_extraction_audio_spectrogram_transformer because of the following error (look up to see its traceback):
/home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZNK5torch8autograd4Node4nameB5cxx11Ev
🚨 import failed, this means you introduced unprotected imports! 🚨
make: *** [Makefile:53: quality] Error 1

❯ git status
On branch bnb_cpu
Your branch is up to date with 'jiqing-feng/bnb_cpu'.

Changes not staged for commit:
  (use ""git add <file>..."" to update what will be committed)
  (use ""git restore <file>..."" to discard changes in working directory)
        modified:   examples/legacy/benchmarking/plot_csv_file.py
        modified:   examples/tensorflow/benchmarking/plot_csv_file.py
        modified:   src/transformers/generation/stopping_criteria.py
        modified:   src/transformers/models/big_bird/modeling_big_bird.py
        modified:   src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
        modified:   src/transformers/models/convbert/modeling_convbert.py
        modified:   src/transformers/models/donut/convert_donut_to_pytorch.py
        modified:   src/transformers/models/esm/openfold_utils/feats.py
        modified:   src/transformers/models/esm/openfold_utils/tensor_utils.py
        modified:   src/transformers/models/longt5/convert_longt5x_checkpoint_to_flax.py
        modified:   src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py
        modified:   src/transformers/models/nougat/convert_nougat_to_hf.py
        modified:   src/transformers/models/swin/convert_swin_simmim_to_pytorch.py
        modified:   src/transformers/models/swin/convert_swin_timm_to_pytorch.py
        modified:   src/transformers/models/swin2sr/convert_swin2sr_original_to_pytorch.py
        modified:   src/transformers/models/swinv2/convert_swinv2_timm_to_pytorch.py
        modified:   src/transformers/models/t5/convert_t5x_checkpoint_to_flax.py
        modified:   src/transformers/models/umt5/convert_umt5_checkpoint_to_pytorch.py
        modified:   src/transformers/quantizers/base.py
        modified:   src/transformers/testing_utils.py
        modified:   src/transformers/tokenization_utils.py
        modified:   src/transformers/utils/fx.py
        modified:   utils/check_copies.py
``` Just freshly rebased onto `main`. Hi @SunMarc . I have fixed all your comments, please take a review. Thx!
cc @Titus-von-Koeller Nice job @jiqing-feng @Titus-von-Koeller and @matthewdouglas on this PR ! This looks very good ! Thanks for making the tests device agnostic also ! I left a few comments  Super nice! This needs a bit of documentation! 
1. FMI but can e included in the doc: is it faster to quantize on CPU vs on the remote device
2. What is the use-case for CPU quantization
3. Can we cover the cost of quantizing by asynch sending to device orsomething like that?  the only thing missing is a small sentence about this new feature in the doc as I believe it's written in quite a few places that only GPU is supported Nice ! You can merge the PR @Titus-von-Koeller whenever you are ready !  Gave my thoughts on a few issues. Can we rebase this PR on main also ? There are unrelated commits in the diff","Nice job @jiqing-feng @Titus-von-Koeller and @matthewdouglas on this PR ! This looks very good ! Thanks for making the tests device agnostic also ! I left a few comments  Super nice! This needs a bit of documentation! 
1. FMI but can e included in the doc: is it faster to quantize on CPU vs on the remote device
2. What is the use-case for CPU quantization
3. Can we cover the cost of quantizing by asynch sending to device orsomething like that?  the only thing missing is a small sentence about this new feature in the doc as I believe it's written in quite a few places that only GPU is supported Nice ! You can merge the PR @Titus-von-Koeller whenever you are ready !  Gave my thoughts on a few issues. Can we rebase this PR on main also ? There are unrelated commits in the diff","Refer to [1227](https://github.com/TimDettmers/bitsandbytes/pull/1227), and [1178](https://github.com/TimDettmers/bitsandbytes/pull/1178), [1206](https://github.com/TimDettmers/bitsandbytes/pull/1206). The bitsandbytes now support CPU backend, so we can remove the cuda restriction in transformers.","This comment becomes stale with the code change.

```suggestion
``` Somewhat unrelated to this PR, but: why is this the case? 🤔  In BNB the quantization happens when the tensor is move `.to(device)`, so far `cuda`... Does that answer your question? ➕  The thing with cuda being needed is that the [quantization happens on the fly when tensors are moved to the target device, so far that was only cuda, now it's also cpu](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/08597844023a5c59e9b5d5dbeafbac4174fae5cc/bitsandbytes/nn/modules.py#L330-L331) Not sure about it. Why is this needed ? The dtype output should have been saved by the `QuantState`. Is it because this is needed for cpu since we can also load in fp32 / bf16 and most quantized model were quantized from the fp16 version ? Let's keep them separate 
```suggestion
        if not is_accelerate_available():
``` Nice ! Just a small suggestion
```suggestion
        bnb_multibackend_is_enabled = ""multi_backend"" in getattr(bnb, ""features"", set())
``` ```suggestion
            if not bnb_multibackend_is_enabled:
``` I would even add a link to the instruction in order to activate the `multi-backend-refactor` preview Nice behavior. This way, we won't have any breaking change from users who didn't specify the device when loading the model.  When `bnb_is_multibackend_enabled` is activate, should we check that the user has one of the backend supported (intel,rocm,cuda), else we return an error ? This might be useful for mac users same comments for the 8bit version  if this keeps increasing, we might need to create a separate function to get the device count (see accelerate case: https://github.com/huggingface/accelerate/blob/1a6af0bd6dc125db287bbe7cf8577a45ebe252ec/src/accelerate/utils/modeling.py#L934 )  Is this fixed ? Can we safely import bnb on the latest release if cuda is not available + not on the multi-bakend refactor branch ? One way to fix this would be to do try except block and redirect them to the multi-backend refactor branch if they encounter this issue.  Yes, exactly. The default data type is bf16 in CPU BNB. WDYT? @Titus-von-Koeller  @Titus-von-Koeller  Users need to decide backend in installation. See [here](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend). need to change for 8bit too 
```suggestion
        if not is_accelerate_available():
``` ```suggestion
            err_msg = f""None of the available devices `available_devices = {available_devices or None}` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {bnb_supported_devices_with_info}`. 
            Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend""

``` ```suggestion
        log_msg = ""CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. 
        Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend""
``` We shouldn't add that here. It should be taken care of in the dispatch function itself. If you don't want to modify the `dispatch_model` (requires to update accelerate), then you can the bnb +cpu condition just above of where we use `dispatch_model`.    ```suggestion
The model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, `pip install bitsandbytes` and make sure to have access to a CUDA compatible GPU device. If you want to run bitsandbytes on CPU or other devices, please follow the [instruction](https://huggingface.co/docs/bitsandbytes/main/en/installation). Simply change the snippet above with:
``` Same here?
```suggestion
The model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, `pip install bitsandbytes`. If you want to run bitsandbytes on CPU or other devices, please follow [these installation instructions](https://huggingface.co/docs/bitsandbytes/main/en/installation). Simply change the snippet above with:
``` This feels a bit out of place in this file – I'd probably put it in `integrations.bitsandbytes`? There are multiple versions of this idiom around this PR now: this `hasattr(bnb, ""features"") and ""multi_backend"" in bnb.features` and `""multi_backend"" in getattr(bnb, ""features"", set())` multiple times elsewhere.

Feels like these should be a single

```python
def is_bitsandbytes_multi_backend_available() -> bool:
    import bitsandbytes as bnb
    return ""multi_backend"" in getattr(bnb, ""features"", set())
```
 Will this be considered a part of the public `transformers.utils` API in the future? I think importing `bitsandbytes` would fail if Torch isn't available, so the check for Torch is likely moot?
```suggestion
        _bitsandbytes_available and validate_bnb_backend_availability(raise_exception=False)
```"
33982,2024-10-05T19:12:48Z,2024-10-09T21:51:19Z,AhmedAlmaghz,5,0,1,2322,13,2,2,[],506.0,0,743829.0,0,0,0,0,6344791.893145,,0,1,0,False,"['AhmedAlmaghz', 'abodacs']","مرحبا أخي عبدالله @abodacs 
لقد حللت التعارض بطلب السحب هذا
ارجو عمل approval لطلب السحب فقد جمعت طلبات السحب السابقة هنا وقمت بإغلاقها جميعا
تحياتي  وهذا هي المهام التالية للمراجعة

# DEVELOPER GUIDES

- [ ]  Use fast tokenizers from 🤗 Tokenizers #33034
- [ ]  Run inference with multilingual models #33048
- [ ]  Use model-specific APIs #33030
- [ ]  Share a custom model #33031
- [ ]  Templates for chat models #33026
- [ ]  Trainer #33080
- [ ]  Run training on Amazon SageMaker #33071
- [ ]  Export to ONNX #33072
- [ ]  Export to TFLite #33077
- [ ]  Export to TorchScript #33079
- [ ]  Benchmarks #33023
- [ ]  Notebooks with examples #33049
- [ ]  Community resources #33027
- [ ]  Troubleshoot #33017
- [ ]  Interoperability with GGUF files #33037


تحياتي وتقديري لك أخي @abodacs  @stevhliu Could you please review and merge this PR? Thank you, @stevhliu and @abodacs! 
Excellent work! سﻻم عليكم  @AhmedAlmaghz 
بدأت العمل على مرجع للمصطلحات المسنخدمه للترجمة ﻷنى ﻻحظت اعتماد أكثر من معنى لنفس الكلمه
ملحوظة جارى العمل عليه لكنى فضلت مشاركته معك من البداية

https://docs.google.com/spreadsheets/d/1WyVvc1PMaFkpnlsO6NkYT6Sl_5XUXoXN1GC98LTMHd4/edit?usp=sharingNice work all around!",Nice work all around!,"Add Translate docs into Arabic - section files CONCEPTUAL GUIDES 

---------------------------------------------------------------------------------------
# CONCEPTUAL GUIDES

- [x]  Philosophy #33064
- [x]  Glossary #33038
- [x]  What 🤗 Transformers can do #33073
- [x]  How 🤗 Transformers solve tasks #33074
- [x]  The Transformer model family #33047
- [x]  Summary of the tokenizers #33078
- [x]  Attention #33021
- [x]  Padding and truncation #33050
- [x]  BERTology #33024
- [x]  Perplexity of fixed-length models #33063
- [x]  Pipelines for webserver inference #33066
- [x]  Model training anatomy #33045
- [x] Getting the most out of LLMs #33043

",
33156,2024-08-28T07:04:12Z,2024-10-14T09:58:45Z,ArthurZucker,1,3,3,9,1,3,0,[],1326.0,0,4071275.0,0,0,0,0,6344261.903459,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33156). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?
Fixes #32628",Do we want to merge PRs that add new dockerfiles (and even build them on our side?) Yeah I think we can! cc @LysandreJik we don't have any for transformers yet Sure!
34045,2024-10-09T15:59:09Z,2024-10-14T06:53:32Z,vasqu,2,6,8,511,19,2,1,[],81394.0,0,409656.0,0,0,0,0,6344985.595422,,0,8,0,False,"['vasqu', 'ArthurZucker']",@ArthurZucker Changed it to top-level copied from now. Lmk if I should change something else. That's it! Merging 🤗 LGTM in general! would be nice to have a single # Copied from at the top of the class (either # Ignore copy or just don't copy from llama for one of them!) ,LGTM in general! would be nice to have a single # Copied from at the top of the class (either # Ignore copy or just don't copy from llama for one of them!) ,"# What does this PR do?
Adds question answering to mistral, mixtral, qwen2, qwen2moe. Either we take every model due to the copy statements or we need to ignore it in the copied checks. Based on #29168 but using copied from instead.

Motivation: We have a benchmark paper at https://github.com/LSX-UniWue/SuperGLEBer which uses the transformers QnA models for simplicity but due to it not being available in main, it's manually patched in. Would be great to see it getting into main!

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #28908


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@LysandreJik @ArthurZucker ","Copying from each one individually due to llama having a wrong base model prefix which already led/leads to issues in the past: #30381. Currently, it makes copying force to include the base model prefix (ref. [here](https://github.com/huggingface/transformers/blob/a84c413773cdfdba58b35194c5ba51e2ccb2ca39/src/transformers/models/llama/modeling_llama.py#L1368)) if using top-level copied from.  So it's more of a stylistic choice: individual copies vs. include (unnecessary) base model prefix. If #34061 gets merged, we can top-level copy from llama without any problems. You can also use `# Ignore copy` on the single place where the copy does not match! Ah ok, perfect I'll change it later and ping you when ready ;) `base_model_prefix = ""model""` is due to the llama stuff I mentioned, otherwise the classes have different structures and copied from will fail in an error."
34069,2024-10-10T14:42:46Z,2024-10-11T13:41:47Z,zucchini-nlp,6,0,2,3,2,1,1,[],1305.0,0,323669.0,0,0,0,0,6349155.90443,,0,2,0,False,"['LysandreJik', 'gante', 'zucchini-nlp']","I see, this is not a very nice workaround to force certain tokens from being generated. Actually the test for mllama would not be flaky just by changing the check in prepare-inputs, as I did in this PR

We can get rid of flakiness for now, but take a note for future that fixing it in a proper way is very important. Interestingly passing logits processot makes other tests flaky.... So yeah, that wouldn't work   @ArthurZucker can you merge pls? The tests is failing because we have unused attribute, but we can't remove it for BC. Also I want all VLMs have such an attr by default, like `bos_token` etc.

For the default one, I'll open another PR to see if everyone agrees @zucchini-nlp we can't force-merge, this failing check would start showing up in all PRs :P In this case we have to fix the underlying issue or add an exception to the script that checks it :) (lmk if you'd like pointers) @gante okey, let's do the second option as part of this PR then :)

Added image token id as always allowed, so that all VLMs can have it even if not used directly in modeling. I believe this is useful already in tests + may be useful in other cases in the future @LysandreJik FYI -- merged this PR, as it is blocking other PRs :) Thanks for doing so!I don't like this approach 😅 

The root issue is in the models, which should never generate certain tokens (as opposed to being in a poorly parameterized test). Merging this PR makes the test pass, but not only doesn't fix the issue, but also makes testing more complex. The problem will just become bigger as we add more VLMs.

Given that `@is_flaky()` also fixes the failing test, but adds a bigger sense of urgency (we know that we shouldn't have this tag in the long run) and doesn't make tests more complex, I'm heavily biased towards it.

The exception would be if ALL tests have the same problem. In that case, I'd agree merging tests as is, but we would have to put the underlying fix at a very high priority.  LGTM is this fixes the issue :)","I don't like this approach 😅 

The root issue is in the models, which should never generate certain tokens (as opposed to being in a poorly parameterized test). Merging this PR makes the test pass, but not only doesn't fix the issue, but also makes testing more complex. The problem will just become bigger as we add more VLMs.

Given that `@is_flaky()` also fixes the failing test, but adds a bigger sense of urgency (we know that we shouldn't have this tag in the long run) and doesn't make tests more complex, I'm heavily biased towards it.

The exception would be if ALL tests have the same problem. In that case, I'd agree merging tests as is, but we would have to put the underlying fix at a very high priority.  LGTM is this fixes the issue :)","# What does this PR do?

Fixes flakiness in some generation tests for VLMs. We have to pass the `bad_ids` in all tests that call generate with tiny models.

Plus for Mllama we don't have to check the generation stage that way anymore, since we decided not to support muti-turn conversations.",
34153,2024-10-14T09:35:51Z,2024-10-14T09:40:39Z,LysandreJik,0,0,2,14,2,1,1,[],,0,290.0,0,0,0,0,6345349.937883,,0,2,0,False,[],"WOw! Thanks for adding this, this major security threat is will no longer be exploited","WOw! Thanks for adding this, this major security threat is will no longer be exploited",,
34106,2024-10-12T05:39:35Z,2024-10-14T08:30:35Z,dmgcsilva,0,0,1,2,1,2,2,[],,0,183060.0,0,0,0,0,6349557.528159,,1,1,0,False,[],Thanks!,Thanks!,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes an incorrect variable usage in an error message in processing_mllama.py


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@amyeroberts
@qubvel

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34148,2024-10-14T04:13:04Z,2024-10-14T08:15:25Z,PengWeixuan,0,0,1,2,1,1,1,[],,0,14541.0,0,0,0,0,6350467.839011,,0,1,0,False,[],Thanks!,Thanks!,"

# What does this PR do?

Fixes typo: ""If you want you tokenizer...""->""If you want your tokenizer....""


## Before submitting
- [ √] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34094,2024-10-11T15:32:08Z,2024-10-14T08:22:49Z,VladOS95-cyber,1,0,2,69,3,2,2,[],70.0,0,233441.0,0,0,0,0,6350023.946456,,0,2,0,False,['VladOS95-cyber'],"Hi @SunMarc! This PR is ready for review, please, take a look.Nice ! Thanks !  Thanks @VladOS95-cyber!",Nice ! Thanks !  Thanks @VladOS95-cyber!,"# What does this PR do?
Add GGUF support for Starcoder2
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Link: https://github.com/huggingface/transformers/issues/33260
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Regarding the task @SunMarc @LysandreJik @ArthurZucker.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34100,2024-10-11T17:49:59Z,2024-10-11T17:52:06Z,McPatate,0,0,1,68,1,1,1,[],,0,129.0,0,0,0,0,6575066.782838,,0,1,0,False,[],Nice one!,Nice one!,"In the panels' queries, we use a `branch` variable to filter for the appropriate results.

I'm failing to understand why it sometimes works with single quotes, other times doesn't, then changes it's mind again. If the error happens again, I'll investigate deeper. For now the fix works.

Note: I'm only persisting the dashboard's json content and the change has already been made in Grafana.",
33258,2024-09-02T13:21:03Z,2024-09-11T12:22:28Z,jmamou,4,5,21,57,6,2,2,['Generation'],515817.0,0,3607646.0,0,0,0,0,6353285.879438,,0,21,0,False,"['gante', 'jmamou']","> Thank you for opening the PR @jmamou 🤗 The idea in the paper is really cool, much better than my scrappy heuristic! 🔥
> 
> I've added a few nits in the PR review that should be trivial to solve.
> 
> A proposal for a follow-up PR: Different flavors of speculative decoding/assisted generation are (mostly) invisible to the user other than execution speed. According to your paper, DISCO should be faster than the current default. Do you think you can find a single default value for `assistant_confidence_threshold` such that it beats the current default in most situations? If so, I'd be more that happy to make DISCO the default 🤗

Thanks @gante for your feedback. 

Concerning the follow-up PR, I have run experiments with vicuna-13b/vicuna-68m as target/assistant on the datasets Alpaca and CNN-DM with do_sample True and False. The values of `assistant_confidence_threshold=0.4` and `num_assistant_tokens=20` are consistently almost optimal.  > Thank you for iterating! To make our CI happy, run `make fixup` and push the changes :)

sure!
I am done, except for the test ""examples_tensorflow"" that does not seem to be related to my code.  > Thank you for iterating! To make our CI happy, run `make fixup` and push the changes :)

sure!
I am done, except for the test ""examples_tensorflow"" that does not seem to be related to my code.  > The values of assistant_confidence_threshold=0.4 and num_assistant_tokens=20 are consistently almost optimal.

@jmamou feel free to open a PR to update the defaults, I'd be glad to accept it! 🤗 Thank you for opening the PR @jmamou 🤗  The idea in the paper is really cool, much better than my scrappy heuristic! 🔥 

I've added a few nits in the PR review that should be trivial to solve.

A proposal for a follow-up PR: Different flavors of speculative decoding/assisted generation are (mostly) invisible to the user other than execution speed. According to your paper, DISCO should be faster than the current default. Do you think you can find a single default value for `assistant_confidence_threshold` such that it beats the current default in most situations? If so, I'd be more that happy to make DISCO the default 🤗  Thank you for iterating! To make our CI happy, run `make fixup` and push the changes :) Thank you for the contribution! The failing test is unrelated to this PR and should now be fixed on `main`","Thank you for opening the PR @jmamou 🤗  The idea in the paper is really cool, much better than my scrappy heuristic! 🔥 

I've added a few nits in the PR review that should be trivial to solve.

A proposal for a follow-up PR: Different flavors of speculative decoding/assisted generation are (mostly) invisible to the user other than execution speed. According to your paper, DISCO should be faster than the current default. Do you think you can find a single default value for `assistant_confidence_threshold` such that it beats the current default in most situations? If so, I'd be more that happy to make DISCO the default 🤗  Thank you for iterating! To make our CI happy, run `make fixup` and push the changes :) Thank you for the contribution! The failing test is unrelated to this PR and should now be fixed on `main`","# What does this PR do?

This PR adds support to dynamic number of _speculative tokens_ in order to accelerate speculative decoding.
It is an unsupervised version of the dynamic speculation lookahead from [Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large Language Models](https://arxiv.org/abs/2405.04304).
 
We add the argument `assistant_confidence_threshold` to the generation configuration of the model. It is a confidence threshold for the assistant model. If the assistant model's confidence in its prediction for the current token is lower than this threshold, the assistant model stops the current token generation iteration, even if the number of _speculative tokens_ (defined by `num_assistant_tokens`) is not yet reached.

## Before submitting
- [X ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ V] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ V] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ V] Did you write any new necessary tests?

## Who can review?

@gante @amyeroberts","This should be removed, it might cause problems to future commits (unlikely, but possible :D) ```suggestion
        self.assistant_confidence_threshold = kwargs.pop(""assistant_confidence_threshold"", None)
```
Let's default to `None` -- it's easier for us to check whether a parameter is set 🤗 

(needs change in the docstring as well) Missing: docstring (most importantly, explaining what it should do and that is related to the method proposed in your paper) Feel free to add a reference to your paper in this docstring 🤗 It will be useful in the long run for everyone, to track the origin of the technique! ```suggestion
        assistant_confidence_threshold (`float`, *optional*):
```
defaults to `None` is the default, which we prefer to leave as implicit :)"
33766,2024-09-27T15:57:15Z,2024-09-30T17:08:48Z,aroun-coumar,2,0,1,23,1,3,3,[],39229.0,0,1427503.0,0,0,0,0,6364056.896341,,0,1,0,False,"['aroun-coumar', 'zucchini-nlp']","[#33776](https://github.com/huggingface/transformers/pull/33776)

I noticed another PR is raised on the very same issue later

For reference to avoid conflicts if any @aroun-coumar oh right, looks good to me whichever of the PRs is merged 🤗 Works perfectly. Thanks for fixing! 

As the changes here are exactly the same as those in #33776 - there's no need to run the slow tests here as well. ","Works perfectly. Thanks for fixing! 

As the changes here are exactly the same as those in #33776 - there's no need to run the slow tests here as well. ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #33763 #33752

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@zucchini-nlp 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34070,2024-10-10T14:50:56Z,2024-10-11T15:41:50Z,ydshieh,0,1,6,3,1,2,1,['run-slow'],,0,89455.0,0,0,0,0,6582885.970829,,1,6,0,False,[],Thanks for flaggin,Thanks for flaggin,"# What does this PR do?

This test `test_dola_decoding_sample` failed for `LlavaNextVideo` and affect many subsequential tests for that model",@gante told me it's ok to skip
34071,2024-10-10T15:30:02Z,2024-10-11T13:24:01Z,ydshieh,0,5,7,9,2,3,1,['run-slow'],,0,83090.0,0,0,0,0,6586905.012274,,1,7,0,False,[],"Thanks, cc @gante and @zucchini-nlp ","Thanks, cc @gante and @zucchini-nlp ","# What does this PR do?

See comment in the change","just a thought, but can it is be because we need to `self.asserts(ValueError)` and not `RuntimeError`? Similar test for video/image_inputs are not causing massive failures But this test itself is still passing, so what the code gives (expected) `RuntimeError`. Not sure if we can adjust the test to use `ValueError`.

I can give it a try without any `asserts` to see if other tests are affected. I think the desired error (here `RuntimeError`) from such input (`remove some images from inputs`) causing the cdua in a bad state (I tried without assert it). Not sure what would be best approach if we want such test case. But before we have an idea, just skip it 😅  oke, thanks! Will have it noted for such tests in the future @zucchini-nlp Running that test in a subprocess could avoid the issue


https://github.com/huggingface/transformers/compare/main...try_sub

But the necessary change is not super trivial "
33896,2024-10-02T16:54:05Z,2024-10-11T16:03:29Z,McPatate,2,6,43,2719,5,2,2,[],1409.0,0,774566.0,0,0,0,0,6581586.022165,,0,43,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33896). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Compile complains IDK why, will handle, rest mr @McPatate 🚀  Awesome, let's ship a first version","🚀  Awesome, let's ship a first version","Based on a discussion with @LysandreJik & @ArthurZucker, the goal of this PR is to improve transformers' benchmark system.

This is a WIP, for the moment the infrastructure required to make things work is not ready. Will update the PR description when it is the case.

","```suggestion
        torch.compiler.reset() # reset compiler
``` We should also use  (to make sure we are not sampling):
```python
from transformers import GenerationConfig
gen_config = GenerationConfig(do_sample=False, top_p =1, temperature=1)
model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, generation_config = gen_config)
```
 okay, we are more computing the full generation here ! let's keep this . You can copy past the entire `run_benchmark` and use this: https://gist.github.com/ArthurZucker/5dc54a3fb443e979fac437e5df7c800b 
which is the proper forward calling!  ```suggestion
        past_key_values = StaticCache(model.config, batch_size=batch_size, device=device)
``` ```suggestion
        logger.info(f""generated: {tokenizer.batch_decode(output.cpu().tolist())}"")
``` ```suggestion
        logger.info(f""generated: {tokenizer.batch_decode(output.cpu().tolist())}"")
```"
34074,2024-10-10T16:58:51Z,2024-10-11T13:35:55Z,ducha-aiki,1,0,1,1,1,2,2,[],68508.0,0,74224.0,0,0,0,0,6590444.052927,,1,1,0,False,['ducha-aiki'],"Thanks! @qubvel could you please approve test workflows run? 
LGTM, thanks for the fix Thx!","LGTM, thanks for the fix Thx!","I don't have a reproduction code (because that often happens after like day of training), but sometimes training Mask2Former gives me an error:

```python3
rank0]:   File ""/opt/conda/lib/python3.10/site-packages/transformers/models/mask2former/modeling_mask2former.py"", line 478, in forward
[rank0]:     assigned_indices: Tuple[np.array] = linear_sum_assignment(cost_matrix.cpu())
[rank0]: ValueError: matrix contains invalid numeric entries
```
This PR fixes that.

@amyeroberts, @qubvel

",
34010,2024-10-07T15:22:02Z,2024-10-11T12:59:23Z,LysandreJik,4,0,4,67,2,2,1,[],1474.0,0,337044.0,0,0,0,0,6592633.177343,,0,4,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'abidlabs']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34010). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Yes thanks for flagging @osanseviero. Although you'd need to add `gradio_client` as a dependency, it would be much cleaner (and more future-proof) to use the Gradio Client. The Client is pretty lightweight: https://github.com/gradio-app/gradio/blob/main/client/python/requirements.txt, so lmk if you'd like to go down that approach and I can open up a PR Unfortunately we're extremely strict in the required dependencies for `transformers` so we won't be able to add `gradio_client` at this time And in terms of future-proof, I mostly need to stay more up to date with the gradio updates to the `safetensors/convert` Space so as to not miss an update! Thanks cc @abidlabs @pngwn 

(should this be using `gradio_client` for simplicity? Looks simpler! 🤗 ","cc @abidlabs @pngwn 

(should this be using `gradio_client` for simplicity? Looks simpler! 🤗 ","The usage of the conversion Space was not working since it was upgraded to use gradio 4.36.

This is fixed and using the new approach used in Gradio.

Also a test should have been raised but wasn't visible as it was in a subTest; it should now be visible.",
34090,2024-10-11T12:23:15Z,2024-10-11T13:06:15Z,Wauplin,3,0,1,30,2,1,1,[],1798.0,0,2583.0,0,0,0,0,6592221.194262,,2,1,0,False,"['Wauplin', 'ydshieh']","If the PR doesn't exist yet, but specified (like `refs/pr/1`), it would still fail right? Is this the behavior we want to keep (and if some doc mentions this?) > If the PR doesn't exist yet, but specified (like refs/pr/1), it would still fail right? Is this the behavior we want to keep (and if some doc mentions this?)

It would fail yes and I do think this is the correct behavior to have. If the user provides an unknown PR ref, it's preferable to let them know rather than trying to guess what would be the best solution for them (I also do think this should be the case for missing branches as well but it would introduce a breaking change) Feel free to merge if it's ok. Failing CI doesn't seem to be related (only the docs is failing)Thanks @Wauplin!",Thanks @Wauplin!,"This PR fixes `PushToHubMixin` when trying to push a component to a PR revision.

In `.push_to_hub`, if a revision is passed, it currently tries to create a branch with that name. To be honest I don't think this should be the role of `transformers` to try to auto-create a branch if the user asked to push to it. It can lead to misleading behavior where a branch is autocreated even if the user made a typo in the revision name. But we won't change that at least for the sake of not introducing a breaking change.

This PR introduces a fix when the passed revision is in fact a PR ref (`""refs/pr/1""`). In that case, we should not try to create a branch otherwise a HTTP 400 Bad request: `Invalid reference for a branch: refs/pr/1` is raised. I've added a test following the same structure as existing tests.

---
Context: this issue has been reported by a user that wants to push both a model and a tokenizer to the same PR. So what they want to do is:
1. create an empty PR
2. push the model
3. push the tokenizer

Which is currently not possible because of the issue above.",
34088,2024-10-11T10:36:43Z,2024-10-11T12:43:03Z,ylacombe,0,0,2,13,2,2,2,[],,0,7580.0,0,0,0,0,6593618.020563,,0,2,0,False,[],🙏  Thanks,🙏  Thanks,"# What does this PR do?

I've made sure the expected values were the right ones here, in the same set-up than the slow tests! 

cc @ArthurZucker @ydshieh ",
34042,2024-10-09T14:31:18Z,2024-10-11T10:51:50Z,gante,1,0,1,54,11,1,1,[],1617.0,0,159632.0,0,0,0,0,6600291.094355,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34042). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Sounds fair to me! Thanks @gante,Sounds fair to me! Thanks @gante,"# What does this PR do?

Some of our `forward` numerical checks compare `logits` against expected values. #33902 removes the upcast scheduled for deprecation in `forward` (`logits = logits.float()`), resulting in significant memory savings.

However, our tests were expecting the cast. This PR is a fix for (some of) those tests. e.g. two slow tests in `llama` are fixed as a result of this PR.

(text search -> `EXPECTED_SLICE`, used in tests that compare `logits` against hardcoded values -> fix whenever applicable )",
33912,2024-10-03T12:39:05Z,2024-10-11T12:38:35Z,LysandreJik,1,0,3,60,5,1,1,[],1419.0,0,691172.0,0,0,0,0,6593884.176506,,0,3,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33912). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.thanks !
","thanks !
",Closes https://github.com/huggingface/transformers/issues/33853,
34084,2024-10-11T08:38:48Z,2024-10-11T08:53:12Z,ydshieh,0,1,2,31,1,2,1,[],,0,866.0,0,0,0,0,6607409.119202,,0,2,0,False,[],"Makes sense! I'm not sure I understand the details exactly, but this seems ok to me! Thanks :)","Makes sense! I'm not sure I understand the details exactly, but this seems ok to me! Thanks :)","# What does this PR do?

Similar to #33849 where glegendre01 update the runner names, this PR does the same by some mapping of user (team members) inputs to the actual runner names.","We still keep the original inputs: single/multi + t4/a10, but map them to the runner names"
33990,2024-10-06T17:47:46Z,2024-10-10T09:58:26Z,djmarti,4,0,1,49,2,1,1,[],50582.0,0,384246.0,0,0,0,0,6623091.208136,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'djmarti', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33990). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks for reviewing the code, @ArthurZucker. I am not sure what the next step is for me. The `ci/circleci: tests_hub` check is failing due to some errors unrelated to this PR:
```
HTTPError: 401 Client Error: Unauthorized for url: https://hub-ci.huggingface.co/Tomlim/myt5-base/resolve/main/tokenizer_config.json
```
Should I rebase from main and repush? All good I was waiting for the other CIs to run! 
 Thanks @djmarti 🤗 LGTM",LGTM,"argparse converts hyphens into underscores before assignment (e.g., an option passed as `--long-option` will be stored under `long_option`), so there is no need to pass options as literal attributes, as in `--long_option` (with an underscore instead of a hyphen). This commit ensures that the original `argparse`'s  behavior is respected by `parse_args_into_dataclasses` as well.

Fixes [Issue 33933](https://github.com/huggingface/transformers/issues/33933).

# What does this PR do?

Given a dataclass
```python
from dataclasses import dataclass

@dataclass
class Config:
    item_type: str
    item_length: int
```
`HfArgumentParser.parse_args_into_dataclasses` should correctly parse parameters like `--item-type` or `--item-length` (with words separated by *hyphens* instead of *underscores*). Previous versions were not able to parse them:
```python
from transformers import HfArgumentParser
parser = HfArgumentParser([Config])
parser.parse_args_into_dataclasses(args=[""--item-type"", ""my_type"", ""--item-length"", ""23""])
```
would throw an error because the parser was expecting `--item_type` and `--item_length`. That's not ideal.

### Motivation

1. Consistency with `argparse`. [`argparse` converts hyphens into underscores before assignment](https://docs.python.org/dev/library/argparse.html#dest):
   >  If no long option strings were supplied, dest will be derived from the first short option string by stripping the initial - character. Any internal - characters will be converted to _ characters to make sure the string is a valid attribute name. 
2. It is standard convention to use hyphens in long-option parameters (e.g., `--file-type`, not `--file_type`).


- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
",
33707,2024-09-26T00:49:28Z,2024-10-11T08:16:32Z,guangy10,29,30,5,168,3,3,1,['run-slow'],85075.0,0,1322824.0,0,0,0,0,6609612.171222,,0,5,0,False,"['guangy10', 'LysandreJik', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'ydshieh']","Why am I start getting `fetch_tests - Unauthorized` error on all the PRs I'm creating? This seems to be happening more often, @ydshieh would you know what might be happening? Otherwise I'll reach out to the CircleCI team, this is hindering work on the repo I've asked the CircleCI team @guangy10, very sorry for the inconvenience. Might be related to 

> Allow CI could be run on private forked repositories (e.g. new model additions) (#33594)

But not happening on all external contributor's PRs. Strange Could you first try ..?

https://support.circleci.com/hc/en-us/articles/360048210711-How-to-Refresh-User-Permissions



 Another thing to check

[If you're following the fork instead of the upstream repo](https://support.circleci.com/hc/en-us/articles/360008097173-Troubleshooting-why-pull-requests-are-not-triggering-jobs-on-my-organization)

```
A user who submits a pull request to your repository from a fork, but no pipeline is triggered with the pull request. This can happen when the user is following the project fork on their personal account rather than the project itself on CircleCI.

This will cause the jobs to trigger under the user's personal account. If the user is following a fork of the repository on CircleCI, we will only build on that fork and not the parent, so the parent’s PR will not get status updates. 

In these cases, the user unfollows their fork of the project on CircleCI. This will trigger their jobs to run under the organization when they submit pull requests. Those users can optionally follow the source project if they wish to see the pipelines.
``` The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33707). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > Could you first try ..?
> 
> https://support.circleci.com/hc/en-us/articles/360048210711-How-to-Refresh-User-Permissions

Weird. Used to work fine on my old PRs. Tried ""Refresh Permission"". Let's see if it can be unblocked. 

@ydshieh  It's getting worse after re-fresh the permissions. check_circleci_user starts failing. > Another thing to check
> 
> [If you're following the fork instead of the upstream repo](https://support.circleci.com/hc/en-us/articles/360008097173-Troubleshooting-why-pull-requests-are-not-triggering-jobs-on-my-organization)
> 
> ```
> A user who submits a pull request to your repository from a fork, but no pipeline is triggered with the pull request. This can happen when the user is following the project fork on their personal account rather than the project itself on CircleCI.
> 
> This will cause the jobs to trigger under the user's personal account. If the user is following a fork of the repository on CircleCI, we will only build on that fork and not the parent, so the parent’s PR will not get status updates. 
> 
> In these cases, the user unfollows their fork of the project on CircleCI. This will trigger their jobs to run under the organization when they submit pull requests. Those users can optionally follow the source project if they wish to see the pipelines.
> ```

""In these cases, the user unfollows their fork of the project on CircleCI. "" I have no idea how to unfollow my fork on CircleCI. I don't even see if I'm following transformers on CircleCI. I created a CircleCI account with exact email and linked to my github, and I can't see any project I'm following.. https://support.circleci.com/hc/en-us/articles/360008097173-Troubleshooting-why-pull-requests-are-not-triggering-jobs-on-my-organization
> so please be sure ""Build forked pull requests"" is enabled in Project Settings > Advanced

@ydshieh Their Wiki is so bad. I can't find this setting from anywhere. Hmm, weird CircleCI issue ...

Could you check a last time:

top-left (like in the image): organization : switch to your own org, follow `transformers` there.
Optionally: Then switch to `huggingface` (if you can find it) and follow `transformers` there

If still not working, I could try to push a commit.

<img width=""1258"" alt=""Screenshot 2024-09-28 115104"" src=""https://github.com/user-attachments/assets/a2e5cc56-cb0a-44e3-9c20-6c5abaa3a0c3"">
 I push a commit to trigger the CI jobs. It runs now. Got to say I have no idea what is wrong on the CircleCI side with the permission issue as other external contributors' PRs don't have it. Hi @ydshieh thank you so much for debugging this issue together with me. Per the message from CircleCI support team, it seems like the `TRANSFORMERS_CONTEXT` in the `.circleci/config.yml` is causing the permission issue, and I was suggested to remove it from the `config.yml`. It seems like it's newly added in #33594 last week. Still I'm not sure why it only affects my PRs but works for other users, but hope this message can give you more pointer to help resolve this issue on my fork and PRs. Thank you.


![Screenshot 2024-09-30 at 11 42 35 AM](https://github.com/user-attachments/assets/4fd40324-e962-40a9-aa50-d4e3cd10eac8)

 Since @ydshieh pushed a commit to trigger the CI and all CIs are green. Can I get a review on this PR? @amyeroberts @qubvel  Hi @guangy10 Thank you for the message and contacting CircleCI team on your side too. I also think it is from #33594 from the beginning of seeing this issue. But as you mentioned, only a few (external) contributors face this issue, and the answer from CircleCI attached above is somehow confusing (i.e. it doesn't explain what **actually** causes it).

I will create a new personal github account and see if I can reproduce and come up with a solution.

 @guangy10 Could you go https://app.circleci.com/home/ and see what you get there? 🙏 

And https://app.circleci.com/projects/project-dashboard/github/guangy10/ Well I am able to reproduce now 

https://github.com/huggingface/transformers/pull/33850 Opened a PR #33866 for this CircleCI issue > Opened a PR #33866 for this CircleCI issue

@ydshieh Thanks a lot. Really appreciate the quick fix! Let me rebase on top of the fix Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). > * Add the `run-slow` label to the PR

@ydshieh I don't see there is a way for me to add `run-slow` label to the PR. Is there one? Are you able to see what is shown below?

<img width=""991"" alt=""Screenshot 2024-10-01 200227"" src=""https://github.com/user-attachments/assets/9abebb94-45da-4e3a-a9a1-d42f63ce96c9"">
 > Are you able to see what is shown below?
> 
> <img alt=""Screenshot 2024-10-01 200227"" width=""991"" src=""https://private-user-images.githubusercontent.com/2521628/372594758-9abebb94-45da-4e3a-a9a1-d42f63ce96c9.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjc4MDY4NjUsIm5iZiI6MTcyNzgwNjU2NSwicGF0aCI6Ii8yNTIxNjI4LzM3MjU5NDc1OC05YWJlYmI5NC00NWRhLTRlM2EtYTlhMS1kNDJmNjNjZTk2YzkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTAwMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEwMDFUMTgxNjA1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGQ2YmNiY2M0OGFjZDMxNDRkNDE3ODc3NWYzNjQ4MjQyNTRjZjc1NTA4MjJkY2NiZWIxZDkyY2NkODEyZDhmMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.9fIh9YuMAi5q5TFtbUDUNBU3TS0Ji22ybkKnbouN-AI"">

No, that's why I'm asking if the message is asking me to do so or the repo maintainers. Also wondering if we have a something like `pytorchbot` that can do labeling for any user w/o having write permission to the repo.
 
![Screenshot 2024-10-01 at 11 29 21 AM](https://github.com/user-attachments/assets/ab8d7112-5f2d-4dc6-a31e-f81e3a50b754)
 Thanks for the feedback. We will improve the stuffs. For now I just added it manually here. @ArthurZucker @qubvel could you help review for this PR? Once it's merged we can add such integration tests for all executorch-compatible models, not only test the exportability but also the generate. Don't forget to push an (empty) commit with message: `[run_slow] gemma, gemma2`, thank you Comments addressed.
Does it require a 2nd reviewer in order to merge? Nope, just waiting for the CIs right now!  Debug the `gemma2` export issue and update the required `torch` version for it.Thanks! 🤗 asked a question for general direction as I am wondering if there is a way for us to improve our generate make it a bit more compatible !  Thanks 🤗 ",Thanks! 🤗 asked a question for general direction as I am wondering if there is a way for us to improve our generate make it a bit more compatible !  Thanks 🤗 ,"# What does this PR do?

Adding `generate` support for exported model.
Adding `gemma2-2b` to `ExecuTorch` with tests.
Adding an integration test for `gemma-2b` that we've enabled already. 

### Additional Test in `ExecuTorch`
Running `gemma2-2b` E2E:
```
cmake-out/examples/models/llama2/llama_main --tokenizer_path=tokenizer_gemma2.bin --model_path=gemma2.pte --prompt=""My name is""
I 00:00:00.001356 executorch:cpuinfo_utils.cpp:62] Reading file /sys/devices/soc0/image_version
I 00:00:00.001425 executorch:cpuinfo_utils.cpp:78] Failed to open midr file /sys/devices/soc0/image_version
I 00:00:00.001431 executorch:cpuinfo_utils.cpp:158] Number of efficient cores 4
I 00:00:00.001434 executorch:main.cpp:65] Resetting threadpool with num threads = 6
I 00:00:00.005564 executorch:runner.cpp:65] Creating LLaMa runner: model_path=gemma2.pte, tokenizer_path=tokenizer_gemma2.bin
E 00:00:03.701808 executorch:tiktoken.cpp:79] invalid tiktoken line:
I 00:00:03.701845 executorch:runner.cpp:88] Failed to load tokenizer_gemma2.bin as a Tiktoken artifact, trying BPE tokenizer
I 00:00:03.767485 executorch:runner.cpp:94] Reading metadata from model
I 00:00:03.767512 executorch:runner.cpp:119] Metadata: get_vocab_size = 256000
I 00:00:03.767515 executorch:runner.cpp:119] Metadata: get_bos_id = 2
I 00:00:03.767517 executorch:runner.cpp:117] Methond use_sdpa_with_kv_cache not found, using the default value 0
I 00:00:03.767518 executorch:runner.cpp:119] Metadata: use_sdpa_with_kv_cache = 0
I 00:00:03.767520 executorch:runner.cpp:119] Metadata: get_n_eos = 1
I 00:00:03.767521 executorch:runner.cpp:117] Methond append_eos_to_prompt not found, using the default value 0
I 00:00:03.767522 executorch:runner.cpp:119] Metadata: append_eos_to_prompt = 0
I 00:00:03.767524 executorch:runner.cpp:119] Metadata: get_max_seq_len = 123
I 00:00:03.767525 executorch:runner.cpp:117] Methond enable_dynamic_shape not found, using the default value 0
I 00:00:03.767527 executorch:runner.cpp:119] Metadata: enable_dynamic_shape = 0
I 00:00:03.767529 executorch:runner.cpp:119] Metadata: use_kv_cache = 1
I 00:00:03.767575 executorch:runner.cpp:119] Metadata: get_n_bos = 1
I 00:00:03.767604 executorch:runner.cpp:167] RSS after loading model: 0.000000 MiB (0 if unsupported)
I 00:00:04.408489 executorch:runner.cpp:234] RSS after prompt prefill: 0.000000 MiB (0 if unsupported)
My name is Lale and I love to play with my dolls. I started to play with dolls at the age of two years old. My favorite activity is dancing. I would like to help people. I would like to travel to Spain. I like to be a vet. I love to help people. I would like to travel. I would like to be. I love to travel. I would like to be. I like to travel. I would like to be. I love to travel. I would like to be. I love to travel. I would like to be. I love to travel
I 00:00:23.188418 executorch:runner.cpp:246] RSS after finishing text generation: 0.000000 MiB (0 if unsupported)
PyTorchObserver {""prompt_tokens"":4,""generated_tokens"":118,""model_load_start_ms"":1727310065089,""model_load_end_ms"":1727310068851,""inference_start_ms"":1727310068851,""inference_end_ms"":1727310088272,""prompt_eval_end_ms"":1727310069492,""first_token_ms"":1727310069492,""aggregate_sampling_time_ms"":180,""SCALING_FACTOR_UNITS_PER_SECOND"":1000}
I 00:00:23.188436 executorch:stats.h:84] 	Prompt Tokens: 4    Generated Tokens: 118
I 00:00:23.188438 executorch:stats.h:90] 	Model Load Time:		3.762000 (seconds)
I 00:00:23.188440 executorch:stats.h:100] 	Total inference time:		19.421000 (seconds)		 Rate: 	6.075897 (tokens/second)
I 00:00:23.188442 executorch:stats.h:108] 		Prompt evaluation:	0.641000 (seconds)		 Rate: 	6.240250 (tokens/second)
I 00:00:23.188444 executorch:stats.h:119] 		Generated 118 tokens:	18.780000 (seconds)		 Rate: 	6.283280 (tokens/second)
I 00:00:23.188446 executorch:stats.h:127] 	Time to first generated token:	0.641000 (seconds)
I 00:00:23.188480 executorch:stats.h:134] 	Sampling time over 122 tokens:	0.180000 (seconds)
```


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. #33709
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker
@gante
@amyeroberts
@qubvel 
","Quick question on your vivison here, this would mean we have to maintain two versions of `generate`, do you think we could ""avoid"" that if you pass self.key_cache and self.value_cache to the original generate function? 🤗  to revert!  I think the goal of the `generate` here is slightly different. This simple generate function is created for validating the correctness of the exported program, so it's used only in tests to guard on the exported program for each enabled model. Typically users won't directly take the exported program into product for inference/generate as it's just an intermediate representation to unlock other possibilities, that is, users should either further lower the exported program with ExecuTorch for running on-device (better perf) or switch to use the compiled program for training or server-side inference. I think it's better to keep this PR clean to still focus on model enablement using this simple generate. I could try follow up in a separate PR to see if the original `generate` function can be reused. The signature of the exported program is different than the eager model, so I guess we may need to create an adaptor for it other than passing the cache object to the `generate`. Can you point to me which `generate` is the original one? Aha, forget to change this back. Good catch! ```suggestion
        This generate function is not here to replace the original generate, but a helper to make sure we can test the exported model! Support for `generate` is potentially planed! 
        Generate a sequence of tokens using the model.
``` Of course: https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L1704 is it supported to infer with a different prompt / batch of prompt? The model is exported with batch size = 1, which is typical for on-device use-case. So we're not testing with batch prompts. @ArthurZucker I see a test failure may be related to this. I copied this over from another model with additional texts added, feel free to truncate it to a shorter message and push a new commit Ah yeah won't have time to update them tonight! But yeah we can merge otherwise!  @ArthurZucker Let me know if there is anything I can do to merge this PR Sorry lost track was wondering if you could update the expected values, ortherwise we can merge and @ydshieh will take care of them!  I will check this afternoon and merge gemma: all passing

gemma2: `test_export_static_cache`

I am getting the following (the output is the same as the prompt), and I am not sure that is expected.

```
(Pdb) ep_generated_text
['Hello I am doing']
(Pdb) ep_generated_ids
tensor([[   2, 4521,  590, 1144, 3900,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0]])

```

 For gemma, it could generate 

```
(Pdb) ep_generated_text
['Hello I am doing a project on the 1990s and I need to know what the most popular music was in the 1990s. I have looked on the internet and I have found']

``` @ydshieh thank you for helping on merging this PR. Are you running the test on CPU or GPU? Actually the export test is expected to run on CPU only, however, the entire `Gemma2IntegrationTest` is decorated `@require_torch_gpu`. To verify the correctness locally, if you comment out `@require_torch_gpu`, then run `RUN_SLOW=1 pytest tests/models/gemma2/test_modeling_gemma2.py -k test_export_static_cache`, it shows
```
tests/models/gemma2/test_modeling_gemma2.py::Gemma2IntegrationTest::test_export_static_cache huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.98it/s]
PASSED
======================================================== 1 passed, 388 deselected, 4 warnings in 25.10s ========================================================
```
 @guangy10 Thanks for the information. Maybe it's also a hardware (we are running on T4 GPU). Two quick question:

> expected to run on CPU only

is it because you get the expected results from running on CPU and put it in the test, or there are some other reasons?

> regarding not generate anything on GPU (here T4)

Do you think it's just unfortunately predict EOS or padding. Maybe I can change to some other prompt. In this case, it's just our side.
 @ydshieh Yes, happy to explain more. The exported program is an intermediate artifact that should not be directly used for product or anything. Instead users typically should take this intermediate artifact and further lower it to ExecuTorch which will covers the hardware delegates, op fusion, memory planning, etc. So the starting point (i.e. export) is always on CPU. This is different from the torch compiled artifacts. So the reason I'm adding a `generate` util in this PR is to validate the correctness of the exported program and ensure it can be further consumed in the downstream ExecuTorch stack. This is clearly documented in the util API itself. So for this test the perf doesn't matter, only correctness maters, so we don't expect to run this test on GPU. And it may produce weird output even if it may work in some cases. I thought it may be okay to run the test on a machine configured with GPU because the `device=cpu` is already hard-coded in the test itself, maybe I'm wrong. I'm wondering if there is a tag to mark this test cpu-only?  OK, so it's on CPU with `device=cpu` even if the host has GPU. Not sure why it didn't get the same value as the specified expected value on our CI runner (despite it is running on CPU), but I will merge and check internally.

~~I might update the value so it will match on our runner though.~~

Thanks for explaining.

  Thank you @ydshieh ! Once this PR is merged I can start scaling the model coverage to other models utilizing the same util function added in this PR. Could you share your torch version ..?  I finally get it pass (on our runner) by changing to

> attn_implementation = None

No idea why `sdpa` always get the original prompt as output ( I tried 3 differ prompts).
 I will push a commit and merge tomorrow morning. > Could you share your torch version ..?

Mine is a dev version `2.6.0.dev20241007`. What is the torch version where the test fails?

>  attn_implementation = None

It doesn't look correct. If we set it to `None`, what is the default attention being used? If I recall correctly, SPDA is the only attention impl that supports StaticCache. Yeah, torch version seem to be the issue. Let me dig into it and will update here shortly @ydshieh Okay, I can confirm that exporting `gemma2` model will require `torch==2.5.0` in order to work correctly.
I also verified that running the test on `torch==2.4.1` or `torch==2.0.0` will get the original prompt as output.

Here are the detailed package info:
```
pip list | grep torch
executorch                         0.5.0a0+f8cec53
executorchcoreml                   0.0.1
torch                              2.5.0
torchaudio                         2.5.0
torchvision                        0.20.0
```

`RUN_SLOW=1 pytest tests/models/gemma2/test_modeling_gemma2.py -k test_export_static_cache -v`
```
=========================================================================================== test session starts ===========================================================================================
platform darwin -- Python 3.10.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/guangyang/miniconda3/envs/executorch/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/Users/guangyang/transformers/.hypothesis/examples')
rootdir: /Users/guangyang/transformers, configfile: pyproject.toml
plugins: cov-4.1.0, anyio-4.4.0, xdist-3.3.1, hypothesis-6.84.2
collected 389 items / 388 deselected / 1 selected

tests/models/gemma2/test_modeling_gemma2.py::Gemma2IntegrationTest::test_export_static_cache PASSED                                                                                                 [100%]
```
BTW, the model do require `torchvision >= 0.19.0`, will fail with lower version. It's already implied by requiring torch>=2.5.0 I updated the PR to bump up the required `torch` version to `2.5.0`. 

`torch==2.5.0` is not available publicly until Oct 17, 2024. We can either merge this PR as-is, the `test_export_static_cache` for `gemma2` will be skipped in the CI until `torch 2.5.0` is available in `transformers`. Or we can defer `gemma2` enablement until `torch 2.5.0` is available in `transformers` by splitting the `gemma2` related code out of this PR and merge the rest. Personally I would prefer former, but let me know how you want to proceed. cc: @ArthurZucker @ydshieh 


 Great! We are still using python 3.8 and torch 2.4.1. I plan to switch to python 3.9 at the end of October. Before that, python 2.5 won't be available in the system. We will think about that.

We can merge this PR as it is. Thank you for checking (it works indeed).

> If we set it to None, what is the default attention being used?

Just the native implementation of attention (in the modeling files) using torch operators.

> If I recall correctly, SPDA is the only attention impl that supports StaticCache.

The native implementation of attention also works with `StaticCache` I believe.
"
34036,2024-10-09T10:45:01Z,2024-10-10T15:33:46Z,eaidova,3,0,1,8,2,1,1,[],90600.0,0,148866.0,0,0,0,0,6624638.132165,,0,1,0,False,"['eaidova', 'LysandreJik', 'Rocketknight1']","Seems legitimate! cc @Rocketknight1  Yes, looks good to me too! @eaidova do you have some sample code that used to trigger the issue, but is fixed with this PR? > Yes, looks good to me too! @eaidova do you have some sample code that used to trigger the issue, but is fixed with this PR?

I found this issue running optimum-cli tool, but I think reproducing may be simplified as:

```
from transformers import AutoFeatureExtractor

try:
    fe = AutoFeatureExtractor.from_pretrained(""openbmb/MiniCPM-V-2_6"", trust_remote_code=True)
except Exception:
   pass

````

as you can see I explicitly set trust_remote_code=True, but without this fix I'll see dialog with ask to decide to use remote code Y/N.

Besides that for AutoProcessor and AutoTokenizer classes, in this place there is providing trust_remote_code parameter : 
https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/processing_auto.py#L299


Yes, confirmed the issue and the fix looks good. Thank you!","Yes, confirmed the issue and the fix looks good. Thank you!","# What does this PR do?

Found issue during usage https://github.com/huggingface/optimum/blob/main/optimum/utils/save_utils.py#L27 with remote_code model, which does not have feature extractor clss,  we see message about providing trust_remote_code argument, however it is already provided.  This message may confuse users and lead to some hanging in code where there is no interactive mode until answer timeout reached.

",
34037,2024-10-09T11:01:31Z,2024-10-10T12:44:05Z,abuelnasr0,0,0,1,7,2,3,3,[],,0,130564.0,0,0,0,0,6641950.235042,,0,1,0,False,[],"LGTM, thank you for having a look at that skip 💛  Nice! 🤗 ","LGTM, thank you for having a look at that skip 💛  Nice! 🤗 ","A follow-up for https://github.com/huggingface/transformers/pull/33950 and https://github.com/huggingface/transformers/pull/33325

This fixes reformer test_resize_tokens_embeddings. We should check if the eigenvalues are complex also to check if the covariance is positive definite.
c.c. @gante Thaks for the other fixes!
and c.c. @ArthurZucker ",
34051,2024-10-09T21:12:16Z,2024-10-10T16:43:27Z,MekkCyber,2,3,3,3,1,3,1,[],55653.0,0,70313.0,0,0,0,0,6665557.261614,,0,3,0,False,"['ArthurZucker', 'MekkCyber']",LGTM! @SunMarc Good finding! Nice job finding this ! ,Good finding! Nice job finding this ! ,"# What does this PR do?

This update addresses a small issue in the modular converter. The relative path used in `AUTO_GENERATED_MESSAGE` is generated using a regex that assumes all models reside in the `src/transformers` directory. This approach fails when handling examples located in the `examples/` folder.

The fix is simple : the regex is modified to make sure `examples/` is also taken into account with `src/transformers`

If it's not needed, feel free to disregard.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

cc @ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
                    f""(.*){os.sep}modular_.*.py"", os.path.abspath(modular_file)
```
could you try something like this, this way we also have the correct path for `examples` ( a bit more general!) I used `rf""(src{os.sep}transformers{os.sep}.*|examples{os.sep}.*)""` as a pattern to make it a bit more general, but keep the relative path instead of the absolute path No worries"
34067,2024-10-10T14:21:16Z,2024-10-10T15:32:12Z,qubvel,0,0,1,11,1,1,1,[],,0,4256.0,0,0,0,0,6669874.94592,,0,1,0,False,[],Thanks for the fix!,Thanks for the fix!,"# What does this PR do?

Update Blip2 `is_pipeline_test_to_skip` method signature to fix test failure.
https://app.circleci.com/pipelines/github/huggingface/transformers/107299/workflows/7171a45b-75d9-462e-a77c-ca7abd1cab78/jobs/1423992 

cc @Rocketknight1 ",
33925,2024-10-03T16:32:47Z,2024-10-10T15:31:23Z,ylacombe,1,0,3,8,1,1,1,[],1522.0,0,601116.0,0,0,0,0,6669924.308566,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33925). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"Fix two ASR pipeline tests! 
Both didn't pass before and pass now.

Note that the other failing tests are mostly related to failing word timestamps Whisper generation, that we're working to fix with @eustlb

cc @ydshieh and @ArthurZucker 
",
33725,2024-09-26T13:44:41Z,2024-10-03T14:15:37Z,BenjaminBossan,7,5,2,69,2,2,1,[],1459.0,0,1211806.0,0,0,0,0,6674121.414291,,0,2,0,False,"['BenjaminBossan', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'sayakpaul']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33725). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker could you give this a look?  not sure what's going on with the tests, rebasing should help ! @ArthurZucker I still get `fetch_tests - Failed` from `setup_and_quality`. Any idea what could be the cause? I ran `ruff` on the touched files. I had to force push as the rebasing you did went a bit wrong! Should be good now! 🤗  > I had to force push as the rebasing you did went a bit wrong! Should be good now! 🤗

Oops, sorry about that. Anything that needs to be done from my side for this PR? Nope, I was just waiting for the cis! ",,"# What does this PR do?

PEFT added support for `low_cpu_mem_usage=True` when loading adapters in https://github.com/huggingface/peft/pull/1961. This feature is now available when installing PEFT v0.13.0. With this PR, this option is also supported when loading PEFT adapters directly into transformers models.

Additionally, with this PR, https://github.com/huggingface/diffusers/pull/9510 will be unblocked, which implements this option in diffusers.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?","LGTM, not sure we need all the kwargs as you are not taking them from the kwargs of this function 😉  Not sure if I get you, do you mean this should be merged with `adapter_kwargs`? That wouldn't work, as these are `kwargs` used for a different purpose. 

Or that using a dict for a single argument is overkill? > Or that using a dict for a single argument is overkill?
this 🤗  I found this more readable and extensible than having:

```python
if low_cpu_mem_usage:
    inject_adapter_in_model(peft_config, self, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
else:
    inject_adapter_in_model(peft_config, self, adapter_name)
```

(same with the `set_peft_model_state_dict` call)

If you want me to change it to this instead or have an alternative idea, let me know and I'll change it. no worries"
33571,2024-09-18T16:55:19Z,2024-09-19T18:14:06Z,yonigozlan,2,10,2,227,5,3,2,[],1625.0,0,1891022.0,0,0,0,0,6674668.208397,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'MnCSSJ4x']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33571). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hey. Thanks for taking this up and continuing on my PR and completing it. I was caught up with a lot of work and wasn't able to fix issues after the last iteration. Beautiful! Thanks a lot :) Thanks for adding! 

Main comment is just about the default for `tokenize_newline_separately`","Beautiful! Thanks a lot :) Thanks for adding! 

Main comment is just about the default for `tokenize_newline_separately`","# What does this PR do?
Adds uniformized processors kwargs following https://github.com/huggingface/transformers/issues/31911 for Paligemma.
Continuation of https://github.com/huggingface/transformers/pull/32377 .
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@molbap @amyeroberts 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","nits (2024 + line break)
```suggestion
# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import shutil
``` So the only difference with the common test is the length value, right? wonder if we can parameterize this to reduce loc Yes this seems to be a recurrent issue with this test. I'm trying to make it more generalizable in this PR https://github.com/huggingface/transformers/pull/33479 left a quick review over there too! btw if you want to take a look at https://github.com/huggingface/transformers/pull/31198 (older one, but I updated it), I think it would very much benefit from the one you just linked ```suggestion
``` If #[31198](https://github.com/huggingface/transformers/pull/31198) is merged in first - then this should be imported from processing_utils cc @molbap as we might need a PR to clean this up for other processors after 31198 merge  Shouldn't this be in the text_kwargs defaults too?  yes! I'll open a PR to do a batch replace towards
```python
from processing_utils import Unpack
``` This wasn't used anywhere so I removed it. Same with `do_thumbnail`, `do_align_long_axis`, `do_rescale`. Do you think we should handle them still for BC even if they had no impact before?  ah, ok, no in that case let's remove :)"
33731,2024-09-26T15:10:20Z,2024-10-10T13:28:00Z,MekkCyber,2,2,5,11,2,4,3,[],1433.0,0,1203460.0,1,0,0,0,6677329.600853,,0,5,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33731). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. cc @MekkCyber  let's update the PR since we merged this PR : https://github.com/huggingface/accelerate/pull/3150#event-14575790028can you elaborate more about the changes you made in the code? LGTM as we discussed offline ! Don't forget to fix the CI with `make style` Let's merge if it unblocks, and Zach feel free to comment when back if it's not up to your standards Some direction on where to go from here: Nice and simple ! Let's fix the merge conflits and we are good to merge ! ","can you elaborate more about the changes you made in the code? LGTM as we discussed offline ! Don't forget to fix the CI with `make style` Let's merge if it unblocks, and Zach feel free to comment when back if it's not up to your standards Some direction on where to go from here: Nice and simple ! Let's fix the merge conflits and we are good to merge ! ","# What does this PR do?
This PR fixes the problem related to the data_seed being unused. It was removed during a refactor of how samplers used to work.

Fixes https://github.com/huggingface/transformers/issues/31818 https://github.com/huggingface/trl/issues/1810

You need to use this accelerate PR : https://github.com/huggingface/accelerate/pull/3130

For example in this code : 
```
from datasets import load_dataset
from transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer, set_seed
import torch

set_seed(125)

tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
model = AutoModelForSequenceClassification.from_pretrained(""bert-base-uncased"", num_labels=2)

dataset = load_dataset(""glue"", ""mrpc"", split=""train"")

def tokenize_function(example):
    return tokenizer(example[""sentence1""], example[""sentence2""], truncation=True)

dataset = dataset.map(tokenize_function, batched=True)
SEED = 126
DATA_SEED = 762
training_args = TrainingArguments(f""test-trainer_dataseed_{DATA_SEED}_seed_{SEED}"", max_steps=400, logging_steps=40, seed=SEED, data_seed=DATA_SEED)
trainer = Trainer(
    model,
    training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
)

trainer.train()
```
When we set the seed using `set_seed`, if the data sampler's seed is chosen using `torch.random.initial_seed()`, it will remain deterministic even when the `data_seed` value changes. To address this, the PR checks whether a generator using the `data_seed` is already available. If it exists, we use the initial `data_seed` provided by the user. If not, we randomly select one. This ensures that even when `set_seed` is used, the randomness of the data sampler remains unaffected when different `data_seed` values are used.","make sure to update this when we fix the issue on accelerate With the new version what should essentially happen is:

Instead we should modify/tweak the `create_accelerator_and_postprocess` func, and change this chunk of it https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L4860-L4866

With something like:

```python
        if is_accelerate_available(""0.28.0""):
            dataloader_config = DataLoaderConfiguration(
                split_batches=accelerator_config.pop(""split_batches""),
                dispatch_batches=accelerator_config.pop(""dispatch_batches""),
                even_batches=accelerator_config.pop(""even_batches""),
                use_seedable_sampler=accelerator_config.pop(""use_seedable_sampler""),
            )
            if is_accelerate_available(""1.1.0""):
                dataloader_config.data_seed = self.args.data_seed # <- this
```
And then in `TrainingArguments.__post_init__` check if a `data_seed` was passed/not the default. If so, do a check for the accelerate version and raise a `NotImplementedError()` telling users to upgrade their accelerate version."
33961,2024-10-04T18:18:39Z,2024-10-10T13:22:41Z,mgoin,3,1,2,13,1,4,3,[],6874.0,0,500642.0,1,0,0,0,6677649.549093,,0,2,0,False,"['mgoin', 'HuggingFaceDocBuilderDev', 'ArthurZucker']","Ping us when ready! 🤗  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33961). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @ArthurZucker we are ready to go, thanks!LGTM! Thanks!  🤗 ",LGTM! Thanks!  🤗 ,"Fix some unfinished sections

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes issues in the https://huggingface.co/docs/transformers/en/quantization/compressed_tensors docs


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
4. `pack-quantized` ([sample](https://huggingface.co/nm-testing/tinyllama-w4a16-compressed-hf-quantizer)): INT4 or INT8 weight-quantized models, packed into INT32. For INT4, the weights have an INT4 range but are stored as INT8 and then packed into INT32.
```
The original link redirects to a 404 page."
33953,2024-10-04T17:21:33Z,2024-10-10T12:39:33Z,htahboub,5,0,2,2,1,2,2,[],1550.0,0,501481.0,0,0,0,0,6680237.719364,,0,2,0,False,"['htahboub', 'qubvel', 'HuggingFaceDocBuilderDev', 'ArthurZucker']","cc @zucchini-nlp if you have bandwidth The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33953). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @zucchini-nlp Ah I see, agreed in that case. And I agree that changing the fallback from `None` to the default dtype would be a lot cleaner, so I'll make that change to `modeling_utils.py` then and revert the current proposed fix. Hey @zucchini-nlp, since this is a pretty easy fix for the bug, I suppose we should either merge now or close this PR since the change is already included in your big PR. So, if you think your PR is about to be merged soon, please let me know so we can just close this, or if it's gonna take a while longer, then I think we'd be better off merging this as soon as possible because the warning with Qwen can be a bit confusing to people encountering it. Thanks! Thanks 🤗 Thanks for the PR!

I don't think the proposed fix is a good way as it would prompt us to pass a ""dtype"" when init the backbone in all composite models (a lot of code change) and I haven't seen us using `config.dtype`. Usually the dtype is set when loading the model with `torch.get_default_dtype()`. 

I have a huuge PR to make attn implementation API more consistent with what is expected for composite models in https://github.com/huggingface/transformers/pull/32238. That should fix the issue with warning, but I am also okay to merge separately the PR related to `dtypes`. My personal preference for fix would be to tweak `modeling_utils.py` file to get the default dtype (as fallback) when loading from config Oh I didn't see you made the changes already. Yes, the linked PR will take a while as it touches so many models and opened a can of worms. 

LGTM, agree that we should merge this. After the cpre maintainer approves, we'll merge it :)","Thanks for the PR!

I don't think the proposed fix is a good way as it would prompt us to pass a ""dtype"" when init the backbone in all composite models (a lot of code change) and I haven't seen us using `config.dtype`. Usually the dtype is set when loading the model with `torch.get_default_dtype()`. 

I have a huuge PR to make attn implementation API more consistent with what is expected for composite models in https://github.com/huggingface/transformers/pull/32238. That should fix the issue with warning, but I am also okay to merge separately the PR related to `dtypes`. My personal preference for fix would be to tweak `modeling_utils.py` file to get the default dtype (as fallback) when loading from config Oh I didn't see you made the changes already. Yes, the linked PR will take a while as it touches so many models and opened a can of worms. 

LGTM, agree that we should merge this. After the cpre maintainer approves, we'll merge it :)","Fixing a bug that led to the warning

```You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour```

every time this model was initialized with `attn_implementation=""flash_attention_2""`, even with the dtype specified.

@amyeroberts, @qubvel",
33383,2024-09-09T08:57:01Z,2024-10-10T12:41:53Z,danielkorat,18,30,38,450,4,3,1,['run-slow'],104409.0,0,2691892.0,0,0,0,0,6680099.030786,,1,38,0,False,"['jmamou', 'HuggingFaceDocBuilderDev', 'danielkorat', 'gante', 'ArthurZucker']","Note:
The usage can be streamlined by creating a new config class e.g. `SpeculativeConfig`, which initializes the tokenizers and has good default values for the lookbehinds:

```python
sd_config = SpeculativeConfig(model_name_or_path=model_name_or_path, assistant_name_or_path=assistant_name_or_path)

out = model.generate(input_ids, do_sample=False, max_new_tokens=max_new_tokens, sd_config=sd_config)
``` Quick question before I dive into the code: I see @jmamou as a co-author, is this PR related to DISCO (and, therefore, to #33258 ?) > Quick question before I dive into the code: I see @jmamou as a co-author, is this PR related to DISCO (and, therefore, to #33258 ?)

no ... it is orthogonal Hi @gante 
Our team greatly values your SD and AG implementations in HuggingFace! 😃 
We developed this feature because we wanted to solve one of the main pain points of Speculative Decoding, which does not support model pairs from different families.
Our feature is highly desirable by the community, since it supports acceleration of models which were not supported until now. As an example, our feature is requested in vLLM: [link](https://github.com/vllm-project/vllm/issues/7252). @gante 

We even got a significant speedup using a 9 billion target model (`google/gemma-2-9b`):
| target | assistant | dataset | task | speedup |
|----------------------|---------------------|---------------------------|---------------------------|---------------------------|
| `codellama/CodeLlama-13b-Instruct-hf` | `bigcode/tiny_starcoder_py` | `openai/humaneval` | code generation | **1.96x** |
| `microsoft/Phi-3-medium-128k-instruct` | `Qwen/Qwen2-0.5B-Instruct`  | `tau/scrolls`   | long-context summarization | **1.42x** |
| `google/gemma-2-9b` | `double7/vicuna-68m`  | `cnn_dailymail`   | summarization | **1.70x** |

 @danielkorat @jmamou apologies for the delayed review, bugfixes in the pipeline 👼 I had a quick look at the PR and it looks really really cool, thank you for proposing this technique and opening the PR 💛 

My immediate comment goes in the same direction as a comment added above, about having a ~`SpeculativeConfig`~ `AssistantConfig` to parameterize the added technique, living inside `GenerationConfig`. I am suggesting naming it after assisted generation simply because of consistency throughout the library. Let's please add it, and avoid adding new arguments to `generate` 🤗 This has a few immediate advantages:
1. simpler `generate` signature (advanced users will look into `GenerationConfig`, basic users will appreciate not having to read more flags)
2. we gain the ability to serialize a `GenerationConfig` with a `AssistantConfig` to automatically enable your technique
3. In a future PR, I am thinking of moving all flags related to assisted generation there, to add some organization to the large number of flags :)

Let me know if I can help in any way beyond reviewing. I've placed this PR on my top priorities :) I think I will have to work next on `torch.compile` + assisted generation, to squeeze the most of these speedups 😈  @danielkorat ping me when it's ready for a review 🤗  @gante I have addressed your comments, feel free to review. 😃
Regarding `AssistantConfig`: I implemented it as you suggested, but it causes a huge slowdown in performance.
I suspect that since it holds two tokenizer objects, and lives inside `GenerationConfig`, it causes this overhead. I'm still trying to mitigate this. I'm open to any suggestion / solution.

 @gante 
Update:
I managed to find a workaround for the slowdown bug. In `AssistedCandidateGeneratorDifferentTokenizers.get_candidates()`, I remove `assistant_config` from `assistant_generation_kwargs['generation_config']`, before calling `self.assistant_model.generate()`:

```python
        # 2. Forecast next N tokens using the assistant model.
        assistant_generation_kwargs = {
            self.input_ids_key: assistant_input_ids,
            ""min_new_tokens"": min_new_tokens,
            ""max_new_tokens"": max_new_tokens,
            ""generation_config"": self.generation_config,
            ""logits_processor"": self.logits_processor,
        }

        self.assistant_kwargs.pop(""attention_mask"", None)

        assistant_config = self.generation_config.__dict__.pop(""assistant_config"", None)
        assistant_output = self.assistant_model.generate(**assistant_generation_kwargs, **self.assistant_kwargs)
        self.generation_config.assistant_config = assistant_config
```

 This mitigates the issue. Please advise if you think there's a better solution. @gante 
Another workaround is to pass only one additional parameter, `assistant_tokenizer`, directly to `generate()` using `kwargs`. Passing `tokenizer` to `generate()` through `kwargs` is already implemented:

https://github.com/huggingface/transformers/blob/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src/transformers/generation/utils.py#L1803

WDYT? @gante 
After running some tests, this line is causing the slowdown mentioned above:

https://github.com/huggingface/transformers/blob/f5247aca0151de9b53096772d3ba0516ee1b896f/src/transformers/generation/candidate_generator.py#L158

The line above is super slow since it deep-copies two Tokenizer objects living inside `generation_config.assistant_config`.

A proper solution could be implementing something like this for `GenerationConfig`:

```python
from copy import deepcopy

class Foo:
    def __init__(self, content, linked_to):
        self.content = content
        self.linked_to = linked_to

    def __deepcopy__(self, memo):
        # create a copy with self.linked_to *not copied*, just referenced.
        return Foo(deepcopy(self.content, memo), self.linked_to)
```

where `generation_config.assistant_config` won't get deep-copied. Hi @danielkorat 👋 I got it right from the comments above, passing the assistant tokenizer separately (like we pass the assistant model) is an option -- let's go with it :)

The deep copy is a hard requirement, to prevent side-effects -- we want to make adjustments to the object inside `generate`, but we don't want to modify the original object.

 Thanks @gante, applied your suggestions 😃 

Ready to merge (failing tests are unrelated to this PR)

 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33383). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. (the broken CI should be fixed by #33950 , let's wait for it to be merged to then rebase :) ) hi @gante @ArthurZucker 
is there an ETA for this merge? 
Thanks Sorry for the delay, merging! 

Great work, congrats 🔥 A few comments and suggestions 🤗 

There is a question I want you folks to check first, before all others. I've tagged it with `❗ ❗`  Aside from the tokenizer placement in the generation config, LGTM 🤗  Perfect! Thank you for iterating 🤗  Very nice addition! Needs a tad bit more documentation!","A few comments and suggestions 🤗 

There is a question I want you folks to check first, before all others. I've tagged it with `❗ ❗`  Aside from the tokenizer placement in the generation config, LGTM 🤗  Perfect! Thank you for iterating 🤗  Very nice addition! Needs a tad bit more documentation!","# What does this PR do?
Co-Authors: @mosheber, @jmamou, @orenpereg

Assisted Generation (AG; i.e. Speculative Decoding) can accelerate inference speed by using a small assistant model in tandem with the original (target) model. However, it only supports assistant models that use the same tokenizer as the target model. This significantly limits the usability of AG, as only few models have such corresponding assistant models.
This PR alleviates that requirement, allowing AG execution with any assistant model regardless of its tokenizer.
Thus, AG can support any target model (!), allowing acceleration of a wide range of models that was not possible until now.

For example, our initial experiments show the following speedups:

| target | assistant | dataset | task | speedup |
|----------------------|---------------------|---------------------------|---------------------------|---------------------------|
| `codellama/CodeLlama-13b-Instruct-hf` | `bigcode/tiny_starcoder_py` | `openai/humaneval` | code generation | **2.06x** |
| `microsoft/Phi-3-medium-128k-instruct` | `Qwen/Qwen2-0.5B-Instruct`  | `tau/scrolls`   | long-context summarization | **1.63x** |
| `google/gemma-2-9b` | `double7/vicuna-68m`  | `cnn_dailymail`   | summarization | **1.90x** |

Our method works by decoding the speculated tokens and re-encoding them using the target tokenizer, and then performing an alignment procedure, to account for inter-tokenizer discrepancies.

Usage example:
```python
out = model.generate(input_ids, do_sample=False, assistant_model=assistant_model, 
                                  assistant_tokenizer=assistant_tokenizer, tokenizer=target_tokenizer
)
```

where `target_tokenizer` and `assistant_tokenizer` are the respective tokenizers; `target_lookbehind` and `assistant_lookbehind` are optional parameters which define the window size to consider for solving inter-tokenizer discrepancies after every generation step of the assistant model. 

Experimental setup:
1 x A6000 GPU
100 examples per dataset

@gante

CC @lewtun

## Before submitting
- [x] <s>This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).</s>
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] <s>Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link to it if that's the case.</s>
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

@gante (all others)
CC @lewtun 

 -->
","Can we move these 3 functions (`_get_tokens_diag`,  `_get_longest_diag_index`, and `_get_longest_diag_dict`) inside `AssistedCandidateGeneratorDifferentTokenizers`? 

They would be static methods, but in terms of code organization it will help us quickly identify where they are needed :) In `transformers` we prefer descriptive arguments and variable names, even if they are somewhat verbose :) Clear names -> easier to understand the code -> easier to contribute to `transformers`

`src` -> `source_tokenizer`
`dest` -> `destination_tokenizer`

In other functions, some variable names are also non-descriptive, such as `some`. We should explain somewhere in the docstring what ""look behind"" means for `AssistedCandidateGeneratorDifferentTokenizers` ```suggestion
        self.prev_tokens = None
        self.prev_assistant_ids = None
```
(nit: let's initialize all attributes) This branch is reached when `self.prev_tokens is not None and self.prev_target_ids.shape[1] <= self.target_lookbehind`. In other words, when the original input prompt was so small that `self.prev_target_ids` got populated with a sequence of tokens shorter than `self.target_lookbehind` in the first `get_candidates` call.

The logic here is similar to the branch in `if self.prev_tokens is None`, in the sense that we call `self.convert_token_ids(input_ids, **convert_kwargs)` on both branches to get `assistant_input_ids`. As such, the run time should be similar.

To simplify the code, would it make sense to remove this branch and change the first condition from
```py
if self.prev_tokens is None:
```
to
```py
if self.prev_tokens is None or self.prev_target_ids.shape[1] <= self.target_lookbehind:
```
?

___________________________

If you dislike the suggestion above: shouldn't `self.prev_assistant_ids` be updated in this branch too? (i.e. `self.prev_assistant_ids = assistant_input_ids` before assigning `new_cur_len`)



 Either here or in the class docstring: let's add a short description on why we need to check previous tokens to correctly rebuild the assistant input tokens. New users will appreciate and learn from it 🤗  Is there some way we can create a robust strategy for `target_lookbehind`, without needing it to be a user-defined variable? Most of the times, fewer variables = better UX

From what I'm gathering, this variable is used to ensure nothing gets lost in translation between the two models. Could we define something like `target_lookbehind = min(20, (input_ids.shape[1] - self.prev_target_ids.shape[1]) * 2)`?

The same comment applies to `assistant_lookbehind` ❗ ❗ 

Sanity check: you have confirmed that the runtime with this logic is faster than simply re-encoding the whole `input_ids`?

If it is just a tiny bit faster, like 5%, then I'd prefer having the simpler reencoding code path exclusively -- it's much simpler to maintain and document

(I'll likely have a few comments to make this section more beginner-friendly, but I'm holding them until we confirm this part :D) We can move this block (except `min_new_tokens = ...`) to the top of `get_candidates`, so we can hit this return more quickly 😎  Same question as above: is this code block faster than a naive re-encoding? In this (corner) case, the assistant model is about to return fewer tokens than it received as input. In the main generation loop, we simply want the main model to ignore the candidate sequences and generate 1 new token.

We can achieve this if we do `return input_ids, None`, as opposed to appending dummy tokens for the main model to reject 🤗 This would be the same as the return above I think we can avoid these changes: you are passing `stopping_criteria` to reach its `max_length` attribute.

If I'm not mistaken, the assistant model's `generation_config` should the same value for `max_length` sure Yes, we have tested both options and this one is significantly faster.
Natuarlly, as the number of new tokens increases it becomes even more significant. good catch, thanks ! great In one test we did, it was 15% faster. And that test used `cnn_dailymail` which does not even have long prompts.

Re-encoding (decode+encode) the whole `input_ids` (which increase in size at at each step) at every assistant generation step incurs a significant overhead. I will add:
""Since re-encoding the tokens may result in token discrepancies, we need to define a look behind which will mark the start index for the searching the matching overlap between the 2 tokenizers, so the new tokens include the correct prompt suffix."" awesome, thank you for confirming

(going to leave this comment unresolved for future reference) agreed Same answer as above Trying to fix it to 20 and testing it  @gante Currently both lookbehind are fixed to 10 and there are no issues when running the experiments in the [PR description](https://github.com/huggingface/transformers/pull/33383#issue-2513354628) (50 examples each).
So they are not parameterized anymore.
The only parameters now are the target and assistant tokenizers. Let's try to find a solution where the tokenizers are not inside the generation config. Even if it means extra kwargs in `generate` (we should avoid complex objects in the config instances) Can we modify this message to suggest using your technique? (i.e. to pass the assistant tokenizer as well 🤗 ) Nit: can we rename it to something like `TestAssistedCandidateGeneratorDifferentTokenizers` (so we know all tests here are related to `AssistedCandidateGeneratorDifferentTokenizers`)? Nit: let's move this to the integration tests above (`GenerationIntegrationTests`). We place all `generate` integration tests there 🤗  let's go with full naming here please inp -> input_matrix same here ```suggestion
        text = source_tokenizer.batch_decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
```
not sure you want to remove this! "
34039,2024-10-09T12:02:21Z,2024-10-10T12:38:14Z,Rocketknight1,1,2,3,35,3,3,2,[],1936.0,0,88555.0,0,0,0,0,6680316.661444,,0,3,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34039). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks @Rocketknight1! Awesome! Thanks for the comment regarding topk, I was confused :grin: ","Thanks @Rocketknight1! Awesome! Thanks for the comment regarding topk, I was confused :grin: ","This PR synchronizes the QuestionAnsweringPipeline with the Hub spec for the `question-answering` task. There's a significant change here: The old pipeline had special handling for `SQuAD` inputs. In fact, you could argue it was primarily a `SQuAD` benchmarking pipeline! This PR deprecates those inputs to focus on the `question` and `context` inputs instead.

Note for reviewers: You can't see it in the diff, but `topk` has already been renamed to `top_k` in the pipeline code, with a deprecation warning. As a result, only docstring changes are needed for that arg.",These should be actionable and specify which arguments should be passed instead Done!
34044,2024-10-09T15:15:48Z,2024-10-10T11:42:19Z,VladOS95-cyber,2,0,3,97,5,2,2,[],40.0,0,73591.0,0,0,0,0,6683674.760895,,0,3,0,False,['VladOS95-cyber'],"Hi @SunMarc! This PR is ready for review, please, take a look. > Nice and simple! Thanks @VladOS95-cyber!

@SunMarc @LysandreJik You are welcome!Thanks for your work as always !  Nice and simple! Thanks @VladOS95-cyber!",Thanks for your work as always !  Nice and simple! Thanks @VladOS95-cyber!,"# What does this PR do?
Add GGUF support for GPT2
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Link: https://github.com/huggingface/transformers/issues/33260
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Regarding the task @SunMarc @LysandreJik @ArthurZucker.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34049,2024-10-09T19:07:43Z,2024-10-10T11:04:07Z,qubvel,0,2,2,6,1,2,1,[],,0,57384.0,0,0,0,0,6685967.17106,,0,2,0,False,[],Thanks! cc @Rocketknight1 for knowledge (please ping him for anything pipeline-related),Thanks! cc @Rocketknight1 for knowledge (please ping him for anything pipeline-related),"# What does this PR do?

Following 
 - https://github.com/huggingface/transformers/pull/32514

1. Fix failed pipeline test for Pix2Struct
2. Fix tests that were wrongly annotated as skipped, while they were not skipped actually
","`Pix2Struct` model has `Processor` object in tiny model json, and that led to an error raised for `Pix2Struct` and filed test. Error raise is not required here because we take the processor from `PROCESSOR_MAPPING_NAMES`, we can just skip this Processor It looks like it was a wrong merge with `main`, some tests are marked as skipped while they were not actually skipped"
33298,2024-09-04T12:37:05Z,2024-10-10T09:49:34Z,avishaiElmakies,30,20,57,325,6,3,0,['run-slow'],517772.0,0,3101487.0,0,0,0,0,6689303.416663,,0,57,0,False,"['amyeroberts', 'HuggingFaceDocBuilderDev', 'Rocketknight1', 'ArthurZucker', 'vasqu', 'avishaiElmakies']","@amyeroberts fixed the comments. would love to understand what to do with the tests that are failing @avishaiElmakies Similar to [my comment](https://github.com/huggingface/transformers/pull/33403#issuecomment-2340236947) for DinoV2 -- other PRs can be a good reference for how to fix. There you will see that many of the tests enforce the test model to use eager attention. This will resolve the TF/flax equivalence issues. 

For the sdpa equivalence tests, I'm not sure. As the flash attention implementation follows llama, following llama's tests for sdpa seems reasonable cc @ArthurZucker here who might now more about this  Hi @amyeroberts. looked at the faiiling test

changed the test ```test_eager_matches_sdpa_generate``` to match the one in ```llama```, fixed ```test_pt_tf_model_equivalence```.

the tests in the file ```test_modelling_tf_opt.py```. ```test_xla_generate_contrastive``` and ```test_xla_generate_slow```. don't seem related to this PR. they also fail in main.

the only tests which I would love some guidance on are the ```test_eager_matches_sdpa_inference```. generating seems to work the same with both implementations. from what I understand and after looking at it the behavior for both implementations is different in the inference test. as stated by #32086. fixing this might require a refactor, to make sure behaviors match.

would love some guidance. 

EDIT: I noticed when running the tests using your setup, it fails on ```test_eager_matches_sdpa_generate```. but when running the test on my machine it passes. 
![image](https://github.com/user-attachments/assets/fed31e90-9b11-4191-95b6-0a31ebf972e5)
 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33298). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. The code looks eerily similar to bart's implementation, it might be more beneficial to let it ""inherit"" from bart via `# Copied from ...` statements and adjust it to fit in with barts uses of attention masks etc instead of manually figuring stuff out 👀 

Hence, it's also more appropriate to look into encoder-decoder models like bart instead of llama who are decoder only regarding the attention implementation. Ok #17437 seems to be the cause why the copied from doesn't exist in the first place. Is there a way to just ignore a few lines since it seems a bit too much to me to remove the whole copied from then 👀  @vasqu I can take inspiration from Bart instead of llama,if that helps. What do you think?  @avishaiElmakies I think we can closely follow bart, maybe even leverage a `Copied from` for the SDPA attention module. It also makes sense to follow bart regarding the attention mask creation then. It's more tested and makes our life easier :) For the generation, they can be flaky so I'd opt (unintended word play) for up to a 1b ish model for the test. 125M is still relatively small for todays standards.  @vasqu OK, I will look into changing it more to a Bart inspiration.

I will also try to use use a larger model, i think opt has 125M, 350M and 1.3B
Do you want the 1.3B? I also hope I have the resources to test with the larger models 😅 Hopefully, the 350M suffices no need to go further than 1.3b tho. For the larger models, we can leverage the slow runs if that's necessary ;)  @vasqu OK, thanks for the guidance! I will work on this and update when I'm Done @vasqu I changed the test to 350M.

I also think i found the bug thanks to bart, the 5 tests are now passing (at least on my machine) @avishaiElmakies great to hear! Let's wait until Amy has time to review but I'll take a closer look in a bit :)  Thx LGTM, the failure on CI is not related to this PR. @amyeroberts would love a review, a slow-run test and hopefully a merge 🫡 @amyeroberts is there some code to run those benchmarks?

EDIT: nvm think i found it. @amyeroberts  ran benchmarks

used code from here https://github.com/huggingface/transformers/pull/31031

local resources - (L40S-45GB, PyTorch 2.4.0, OS Debian GNU/Linux 11

### Training 

|    batch_size |    seq_len |  Time per batch (eager - s)   |    Time per batch (sdpa - s) |  Speedup (%)   |  Eager peak mem (MB)   |    sdpa peak mem (MB) |  Mem saving (%)   |
|--------------:|-----------:|:------------------------------|-----------------------------:|:---------------|:-----------------------|----------------------:|:------------------|
|             1 |        128 | 0.047                         |                        0.037 | 26.360         | 1474.611               |               1474.32 | 0.019             |
|             1 |        256 | 0.046                         |                        0.037 | 24.335         | 1498.541               |               1499.49 | -0.063            |
|             1 |        512 | 0.046                         |                        0.037 | 24.959         | 1973.544               |               1551.35 | 27.215            |
|             1 |       1024 | 0.062                         |                        0.038 | 65.135         | 4867.113               |               1698.35 | 186.578           |
|             1 |       2048 | 0.230                         |                        0.039 | 483.933        | 15662.224              |               2715.75 | 476.718           |
|             2 |        128 | 0.045                         |                        0.037 | 20.455         | 1498.164               |               1499.49 | -0.089            |
|             2 |        256 | 0.046                         |                        0.037 | 24.027         | 1569.367               |               1551.35 | 1.161             |
|             2 |        512 | 0.045                         |                        0.037 | 20.965         | 3257.074               |               1698.35 | 91.778            |
|             2 |       1024 | 0.122                         |                        0.038 | 225.958        | 9054.405               |               2715.75 | 233.403           |
|             2 |       2048 | 0.464                         |                        0.067 | 593.646        | 30572.058              |               4750.55 | 543.548           |
|             4 |        128 | 0.045                         |                        0.037 | 21.918         | 1549.448               |               1551.35 | -0.123            |
|             4 |        256 | 0.044                         |                        0.038 | 18.084         | 2451.768               |               1698.35 | 44.361            |
|             4 |        512 | 0.069                         |                        0.037 | 84.421         | 5833.180               |               2715.75 | 114.791           |
|             4 |       1024 | 0.262                         |                        0.062 | 319.475        | 17427.842              |               4750.55 | 266.860           |
|             4 |       2048 | OOM                           |                        0.062 | Eager OOM      | OOM                    |               4750.55 | Eager OOM         |
|             8 |        128 | 0.044                         |                        0.037 | 18.436         | 2049.115               |               1697.78 | 20.694            |
|             8 |        256 | 0.048                         |                        0.036 | 32.887         | 4222.567               |               2715.75 | 55.484            |
|             8 |        512 | 0.153                         |                        0.06  | 154.862        | 10985.391              |               4750.55 | 131.245           |
|             8 |       1024 | 0.526                         |                        0.122 | 330.697        | 34175.763              |               8821.18 | 287.428           |
|             8 |       2048 | OOM                           |                        0.122 | Eager OOM      | OOM                    |               8821.18 | Eager OOM         |

### Inference

|    batch_size |    seq_len |    Per token latency eager (ms) |    Per token latency SDPA (ms) |    Speedup (%) |    Mem eager (MB) |    Mem BT (MB) |    Mem saved (%) |
|--------------:|-----------:|--------------------------------:|-------------------------------:|---------------:|------------------:|---------------:|-----------------:|
|             1 |        128 |                          11.634 |                          8.647 |         34.546 |           717.676 |        717.674 |            0     |
|             1 |        256 |                          11.593 |                          8.86  |         30.851 |           742.852 |        742.845 |            0.001 |
|             1 |        512 |                          11.515 |                          8.816 |         30.614 |           798.232 |        799.593 |           -0.17  |
|             1 |       1024 |                          11.556 |                          8.915 |         29.628 |           917.265 |        895.538 |            2.426 |
|             2 |        128 |                          12.724 |                         11.002 |         15.659 |           762.434 |        762.431 |            0     |
|             2 |        256 |                          12.704 |                         11.063 |         14.83  |           816.809 |        816.733 |            0.009 |
|             2 |        512 |                          12.757 |                         10.947 |         16.535 |           917.383 |        918.339 |           -0.104 |
|             2 |       1024 |                          13.018 |                         11.018 |         18.147 |          1162.65  |       1114.81  |            4.291 |
|             4 |        128 |                          12.739 |                         10.959 |         16.243 |           856.335 |        856.483 |           -0.017 |
|             4 |        256 |                          12.718 |                         10.837 |         17.355 |           957.298 |        957.674 |           -0.039 |
|             4 |        512 |                          12.813 |                         10.822 |         18.393 |          1158.44  |       1158.45  |           -0.001 |
|             4 |       1024 |                          13.416 |                         11.06  |         21.301 |          1653.42  |       1557.19  |            6.18  |
|             8 |        128 |                          12.763 |                         10.891 |         17.193 |          1036.13  |       1036.51  |           -0.036 |
|             8 |        256 |                          12.89  |                         11.104 |         16.085 |          1236.98  |       1236.87  |            0.01  |
|             8 |        512 |                          13.327 |                         10.939 |         21.836 |          1642.29  |       1641.78  |            0.031 |
|             8 |       1024 |                          15.181 |                         11.175 |         35.848 |          2634.98  |       2443.35  |            7.843 |


updated model card opt.md as well

 @avishaiElmakies Can you make the `[slow-run] opt` commit; it's needed to start slow runs. Great benchmarks btw!

Ci errors are not related. @vasqu commited.

Yeah, the benchmarks look great. I was not expecting this much of an improvement when training  slow runs cc @amyeroberts  @amyeroberts it seems it have skipped the tests, do you know what happend?  @avishaiElmakies I think it's because the message in the commit message is `[slow-run]` rather than `[run-slow]`. Could you try again with `[run-slow] opt`?  @amyeroberts sorry about that, commited with `[run-slow] opt` The failures seem unrelated to me tbh. XLA should be tf stuff, no? @amyeroberts  @vasqu Yes, it should be TF stuff, although the TF files are touched in this PR so they should be passing too. Could you confirm if the tests are passing on main or not? 

cc @Rocketknight1 @ArthurZucker  @amyeroberts at the time those tests were failing in main as well Confirmed that `test_xla_generate_contrastive` and `test_xla_generate_slow` are failing on `main` for me in TF - I think it's fine to just add skips to those tests in this PR for now, since it's not related to this PR. Passing locally on my machine with a GPU ([tf_opt_logs.txt](https://github.com/user-attachments/files/17232562/tf_opt_logs.txt)) and not with a CPU ([tf_opt_logs_cpu.txt](https://github.com/user-attachments/files/17232603/tf_opt_logs_cpu.txt)).

Seems like a CPU issue then? FYI @Rocketknight1 
cc @amyeroberts  Yep, we can add the wrapper `require_accelerator`! Thanks for working on adding this! Overall LGTM, needs a small check on the head mask too (eager fall back) and possibly the encoder attention mask hasn't been handled yet. Thanks for adding this! 

Two things before merge: 
* OPT is a popular model, so we should add some benchmarking numbers and an example of how to use on the model's doc page. Here's a good example: https://huggingface.co/docs/transformers/v4.44.2/en/model_doc/gpt_neox#using-scaled-dot-product-attention-sdpa
* Rebasing on main should resolve the failing tests ","Thanks for working on adding this! Overall LGTM, needs a small check on the head mask too (eager fall back) and possibly the encoder attention mask hasn't been handled yet. Thanks for adding this! 

Two things before merge: 
* OPT is a popular model, so we should add some benchmarking numbers and an example of how to use on the model's doc page. Here's a good example: https://huggingface.co/docs/transformers/v4.44.2/en/model_doc/gpt_neox#using-scaled-dot-product-attention-sdpa
* Rebasing on main should resolve the failing tests ","adds SDPA to OPT model

impl inspired by ```gemma2``` and ```llama```.

part of #28005.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
 I think @amyeroberts @fxmarty 

some notes:

- I did some refactoring to the model code. created a function ```_update_key_and_values```. the code was used by all 3 attention implementations. ```self._shape``` to ```_shape```. and moved masks logic to ```self._update_casual_mask```.
- I created a test that makes sure the generate of eager and SDPA are equivalent. the test is similar to the one ```llama```
- I seem to fail 3 implementations of common tests ```test_eager_matches_sdpa_inference_0_float16```, ```test_eager_matches_sdpa_inference_1_bfloat16```, ```test_eager_matches_sdpa_inference_2_float32```. I took inspiration from gemma, which seems to ignore those tests as well. should i ignore them as well? I think it is related also related to #32086 since my code is similar to that. Also seems the affect equivalence with flax/tf beacuse the default will become sdpa. 

would love some feedback!","I agree in general this is better as a function, but this breaks the pattern with other models so it's better to just leave as-is  I understand the motivation here (and think it's a good refactor) but like moving out `_shape` it means it diverges from other model patterns, which will make future changes more difficult, so we should leave as-is  You can leave the scaling to SDPA and leave out the scale mul on the query states. Since it's a scalar it doesn't matter when we apply the scaling:
`(QK^T) * scale == (Q * scale)K^T` (there should be barely any differences)

That's why I'd prefer following bart as it's doing the same stuff as OPT except I'm overlooking smthn. While you are correct about the math, it seems this way is the only way to make eager and sdpa equivalent. When I let sdpa handle scalling they were not equivalent  Interesting, but it might be also HW dependent in how much influence it does have. Not opposed to keeping it as you have it for now. I'd be more in favor of using `# Copied from ...` here tbh (as long as it doesn't break anything). It also needs a check on `layer_head_mask`, i.e., check if it is not None.

See ref:
https://github.com/huggingface/transformers/blob/78b2929c0554b79e0489b451ce4ece14d265ead2/src/transformers/models/bart/modeling_bart.py#L427-L440 Are we missing the handling of the cross-attention mask expansion (the encoder attention mask)? 

See for reference bart again:
https://github.com/huggingface/transformers/blob/78b2929c0554b79e0489b451ce4ece14d265ead2/src/transformers/models/bart/modeling_bart.py#L1286-L1321

I think we could just copy past it from bart directly, wdyt. Would need a check on `head_mask` to be None, else we would need to fall back to the eager mask. I didn't use copy because the other attn implementations are not really copy. the sdpa is also not a perfect copy as well there was no implementation for encoder mask(or encoder logic at all for that matter) in opt as far as I am aware. It is only used as a decoder. I took the old implementation and refactored it k k Ah ok, makes sense. If there was an option to disable copying for a few lines, it'd favor that more. For now, I think it's ok to leave it like this. Thx for clarifying, wasn't aware of this. Still a bit weird that the attention implementation mostly follows bart then, but not that it matters. Maybe a last nit: Is it necessary to keep the original attention mask over here? Yeah, it was also weird to me, there was a refactor I tried to do, but I was told to revert it so it would still stay similar 😢 the embedding layer takes the attention mask as input, there is another PR I have waiting for some feedback to change some of the logic there

https://github.com/huggingface/transformers/pull/33121#pullrequestreview-2283357050 Alright, makes sense. Just showed my unfamiliarity with opt as a model ^^  nit - formatting here is different from standard library patterns and elsewhere where the same warning is used 

```suggestion
            logger.warning_once(
                ""OPTModel is using SDPA attention, which currently does not support output_attentions=True.""
                'failing back to eager attention. remove warning using attn_implementation=""eager"".'
            )

```"
33818,2024-09-30T13:08:07Z,2024-10-09T18:55:07Z,yijun-lee,2,2,6,417,2,2,1,[],136995.0,0,798420.0,0,0,0,0,6744109.393559,,0,6,0,False,"['HuggingFaceDocBuilderDev', 'cjfghk5697']","
LGTM😎 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33818). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Other than this, LGMT! Thanks, just one minor formatting thing and then we can merge! 🙂  👍 ","Other than this, LGMT! Thanks, just one minor formatting thing and then we can merge! 🙂  👍 ","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `generation_utils.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
위 코드는 `(generation_output.sequences, generation_output.scores)` 튜플을 반환합니다.
``` Oops, indentation looks off here

```suggestion
[[autodoc]] CacheConfig
    - update

[[autodoc]] QuantizedCacheConfig
    - validate
```"
33586,2024-09-19T10:59:22Z,2024-10-10T09:50:39Z,zucchini-nlp,7,23,10,976,13,4,2,[],1936.0,0,1810277.0,0,0,0,0,6690377.548505,,0,10,0,False,"['HuggingFaceDocBuilderDev', 'Cyrilvallez', 'zucchini-nlp']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33586). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Done, I think this PR is ready to be reviewed/merged.

Depending on which PR is merged first, this or the linked one, I will rebase and apply necessary changes. Then I'll add the TODO comment for moving `target_length` logic to `_prepare_attention_mask` Rebased main and updated accordingly by moving `prepare_causal_mask` to XXXModel. Also, noticed Phi3Moe was added while PR was in progress and it is same as Phi3, so I propagated changes there too

Will be merging tomorrow if no comment remain :) Hey @zucchini-nlp, while working on https://github.com/huggingface/transformers/pull/33619 I had issues with the 4d masks and just found this PR - however, it is not only an issue for Phi3! For what I could see, the following models have the exact same issue (`AttentionMaskConverter._ignore_causal_mask_sdpa()` does not check for the `sliding_window` resulting in wrong masks, and neither do `_prepare_4d_causal_attention_mask_with_cache_position()`). 
- Mimi
- Mixtral
- PhiMoe
- Qwen2
- Qwen2Moe
- Qwen2VL
- Starcoder2

Let me know if you can fix it or if you want me to jump on it. @Cyrilvallez oh I see, didn't know we had more models that support sliding window. I can propagate changes to other models, sure :) Yes! I think I listed them all but you can maybe double-check so that all of them get correctly fixed 🤗 @ArthurZucker done! I had to change tests for Qwen2 models because otherwise we won't get same results for long padded input as for the base input. Applying sliding mask results in minor differencesThank you for opening the PR with the fix 💪 A few questions and comments awesome test! Thanks, let's try to abstract a tad bit, as we generally would avoid differentiating cache classes in the modeling! LGTM, but the changes will likely fail because of a recently merged PR (things need to be moved, see comment) ping me when merge this way I can put it in a patch! Thanks, good for the test modifications, if run-slow were good let's go! 🔥 ","Thank you for opening the PR with the fix 💪 A few questions and comments awesome test! Thanks, let's try to abstract a tad bit, as we generally would avoid differentiating cache classes in the modeling! LGTM, but the changes will likely fail because of a recently merged PR (things need to be moved, see comment) ping me when merge this way I can put it in a patch! Thanks, good for the test modifications, if run-slow were good let's go! 🔥 ","# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/32945. The reason is that Phi3 prev prepared 4D attn with sliding window while the new `updte_causal_mask` didn't take that into account. This PR fixes it and adds a test","This was needed in case the input prompt is longer than sliding window, so we have to crop attention mask to the max possible length before masking out pad tokens can we do it the other way around, having phi-3 copying from mistral? we have many models copying from mistral, and none from phi-3 (and thus we would we could centralize changes more easily) If this function is the same as in (`prepare_inputs_for_generation`) phi-3, let's add a copy statement there (in phi-3) 🤗  Two comments and a nit:
1. I think this is missing an assignment, i.e. `exclude_mask = exclude_mask.bitwise_or...`
2. `bitwise_or_(` -> `bitwise_or(`

nit: `tensor_a |= tensor_b` is more readable than `tensor_a = tensor_a.bitwise_or(tensor_b)` :D  we now have a significant code block to determine `target_length`, but `target_length` is not used directly in `_update_causal_mask`

suggestion to avoid this disconnection:
1. make `SlidingWindowCache` store the `config.sliding_window` it receives at init time
2. move the logic that computes `target_length` inside `_prepare_4d_causal_attention_mask_with_cache_position`
3. `_prepare_4d_causal_attention_mask_with_cache_position` no longer receives `target_length` nor `config`, as they can be retrieved from `past_key_values` (same comment as above here) A comment explaining what `exclude_mask` is doing would be nice to quickly understand this logic

I'm assuming it further zeroes out values in `causal_mask` that should be ignored because of the sliding window. But, at a first glance, I'm not understanding how (more descriptive variable names and storing intermediary tensors with good variable names might help with readbility) Tests 💛 

missing: a small comment at the top, explaining what this test should be testing (I'm assuming it tests the ability of the model to handle large sequence lengths with a sliding window, and that it is a regression test) hehe, this was blindly copied from mistral. We can do it with ""|"" sign, but the current one is in-place so seems assignment is redundant a bit For the target length, maybe we merge https://github.com/huggingface/transformers/pull/32421/ first where the max length should be easily accessible through cache no matter cache type. We earlier discussed returning max cache shape, and sliding window has a max capacity for cache class

And then yeah, we can move that part to `_prepare_4d_causal_attention_mask_with_cache_position` but I think we'd better move it in all classes for general consistency I completely forgot that functions ending in `_` are the original operation but in-place!

(operations like `+=`, `*=`, ... are also in place and, in my opinion, more readable :p ) Agreed with the plan -- shall we leave a TODO for us in this PR, linking to your comment? Mmmm I don't think we can crop the attention mask for `eager`  Nit: Ig this print is to be removed ;) For eager we never go beyond cache length because the `target_length` is `attention mask.shape[1]` whenever attention mask is present. So cropping shouldn't yield any difference

Same for static cache, where we expect cache shape to be always more than attention mask cant do that, phi3 has special treatment for long-short scales in RoPE, it was added recently and will be here after I rebase main  yes, I'll add a TODO for us so we can make the required change in all models at once, to not mix different updates in one PR :) ah okay my bad then!  this LGTM! thanks for the thorough test  I don't understand why we need to do this: if there is a sliding WindowCache it was init from the `config` and thus has a correct `sliding_window`. 

Then, `SlidingWindowCache. get_max_length` should take `sequence_length` as input to return the max and avoid having these checks here WDYT?  Yes, it should but the PR for getting `max_length` on slidingWindow cache is on its way and not merged yet. 

So, to update you on our discussions with @gante which are currently in different PR comments: some cache classes now do not have a `max_length` (e.g. SlidingWindow). As commented in the code, sliding window technically has no max length and goes on a rolling basis. But in transformers what we want to check is the ""maximum capacity of cache instance"", independently of how cache handles new tokens going beyond that capacity.

So, in a different PR I changed naming to `get_max_cache_shape` which is more straightforward and added `get_max_cache_shape` for Sliding Window cache. We'll do a simple deprecation cycle, as we did for static cache's ""max_batch_size"". Until the linked PR is merged, I am copying this piece of code from `mistral` and using it in `phi3`. I have it noted and will handle it depending which one gets merged first :)  alright sounds good. I just don't want us to add too much complexity to the code! 🤗  Because of #33677, this function is part of the model class -- I think you will have to move the diff there, otherwise tests may fail on `main`

(see the diff in that PR for `Llama`, it should be similar to the changes you need to do here)"
33572,2024-09-18T17:16:19Z,2024-10-09T18:54:38Z,Jwaminju,3,19,10,137,2,5,2,[],1411117.0,0,1820299.0,0,0,0,0,6744139.980332,,0,10,0,False,"['Jwaminju', '4N3MONE']","Hello, @stevhliu May you please review this PR?
Thanks in advacne! 리뷰 완료하였습니다. 특별하게 고칠 사항 없는 것 같습니다. 고생하셨습니다~ > Awesome, thank you! Just need to fix the toctree 🤗 

Great catch! Thanks!번역 작업하느라 수고하셨습니다! 리뷰를 늦게 해드려서 죄송해요🙏
문장 조금씩 다듬어봤는데 확인 부탁드립니다:) LGTM! 섬세한 번역 감사드립니다! Awesome, thank you! Just need to fix the toctree 🤗 ","번역 작업하느라 수고하셨습니다! 리뷰를 늦게 해드려서 죄송해요🙏
문장 조금씩 다듬어봤는데 확인 부탁드립니다:) LGTM! 섬세한 번역 감사드립니다! Awesome, thank you! Just need to fix the toctree 🤗 ","# What does this PR do?

Translated the `main_classes/callback.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang 


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)
@stevhliu May you please review this PR?","주석 부분도 번역이 필요할 것 같습니다! 주석 부분도 번역이 필요할 것 같습니다!(2) 감사합니다! 오! 놓칠 수 있었던 부분인데, 감사합니다!  ```suggestion
콜백은 PyTorch [`Trainer`]의 반복 훈련 동작을 사용자 정의할 수 있는 객체입니다
``` ```suggestion
(이 기능은 TensorFlow에서는 아직 구현되지 않았습니다). 콜백은 반복 훈련의 상태를
``` ```suggestion
콜백은 [`TrainerControl`] 객체를 반환하는 것 외에는 반복 훈련에서 어떤 것도 변경할 수 없는
``` ```suggestion
""읽기 전용"" 코드 조각입니다. 반복 훈련에 변경이 필요한 사용자 정의 작업이 필요한 경우, 
``` ```suggestion
[`TrainerState`]를 통해 접근할 수 있으며, [`TrainerControl`]을 통해 반복 훈련에서 일부 
``` ```suggestion
검사하여 (진행 상황 보고, TensorBoard 또는 기타 머신 러닝 플랫폼에 로그 남기기 등) 
``` ```suggestion
결정(예: 조기 종료)을 내릴 수 있습니다.
``` ```suggestion
  ([`TrainingArguments`]를 통해 tqdm을 비활성화하면 첫 번째 콜백이 사용되고, 그렇지 않으면 두 번째가 사용됩니다).
```
번역을 살짝 고쳐보았습니다! ```suggestion
패키지가 설치되어 있지만 해당 통합 기능을 사용하고 싶지 않다면, `TrainingArguments.report_to`를 사용하고자 하는 통합 기능 목록으로 변경할 수 있습니다 (예: `[""azure_ml"", ""wandb""]`).
``` ```suggestion
(이 기능은 TensorFlow에서는 아직 구현되지 않았습니다). 콜백은 반복 학습의 상태를
```
훈련 -> 학습으로 변경했습니다. ```suggestion
콜백은 PyTorch [`Trainer`]의 반복 학습 동작을 사용자 정의할 수 있는 객체입니다
```
훈련 -> 학습으로 변경했습니다. ```suggestion
콜백은 [`TrainerControl`] 객체를 반환하는 것 외에는 반복 학습에서 어떤 것도 변경할 수 없는
``` ```suggestion
""읽기 전용"" 코드 조각입니다. 반복 학습에 변경이 필요한 사용자 정의 작업이 필요한 경우, 
``` ```suggestion
[`TrainerState`]를 통해 접근할 수 있으며, [`TrainerControl`]을 통해 반복 학습에서 일부 
``` ```suggestion
    - local: main_classes/callback
```"
33777,2024-09-27T18:25:11Z,2024-10-09T18:20:01Z,yijun-lee,3,2,7,60,2,2,1,[],654836.0,0,1036491.0,0,0,0,0,6746216.523175,,0,7,0,False,"['yijun-lee', 'mreraser', 'HuggingFaceDocBuilderDev']","LGTM ! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33777). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I have resolved it! Thank you :) @stevhliuNice, thank you! We can merge once the conflict is resolved :)","Nice, thank you! We can merge once the conflict is resolved :)","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `text_generation.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
각 프레임워크에는 해당하는 `GenerationMixin` 클래스에서 구현된 텍스트 생성을 위한 generate 메소드가 있습니다:
```
glossary에 맞게 바꿨습니다! ```suggestion
사용하는 프레임워크에 상관없이, generate 메소드는 [`~generation.GenerationConfig`] 클래스 인스턴스로 매개변수화 할 수 있습니다. generate 메소드의 동작을 제어하는 모든 생성 매개변수 목록을 확인하려면 이 클래스를 참조하세요. 
```"
33589,2024-09-19T11:36:17Z,2024-10-09T18:15:25Z,fabxoe,1,11,7,78,2,5,1,[],1678324.0,0,1751948.0,1,0,0,0,6746493.920638,,0,7,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33589). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: docs suggestions Thanks! Just one question about the consistency of translations, but otherwise, we should be able to merge! fix: resolve suggestions Thanks again! 👍 ","fix: docs suggestions Thanks! Just one question about the consistency of translations, but otherwise, we should be able to merge! fix: resolve suggestions Thanks again! 👍 ","# What does this PR do?

Translated the `model_doc/patchtst.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [ ] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
## 개요[[overview]]
``` ```suggestion
이 모델은 [namctin](https://huggingface.co/namctin), [gsinthong](https://huggingface.co/gsinthong), [diepi](https://huggingface.co/diepi), [vijaye12](https://huggingface.co/vijaye12), [wmgifford](https://huggingface.co/wmgifford), [kashif](https://huggingface.co/kashif)에 의해 기여 되었습니다. 원본코드는 [이곳](https://github.com/yuqinie98/PatchTST)에서 확인할 수 있습니다.
``` ```suggestion
## 사용 팁[[usage-tips]]
``` ```suggestion
    우리의 채널 독립적 패치 시계열 트랜스포머(PatchTST)는 최신 트랜스포머 기반 모델들과 비교했을 때 장기 예측 정확도를 크게 향상시킬 수 있습니다. 또한 모델을 자기지도 사전 훈련 작업에 적용하여, 대규모 데이터셋에 대한 지도 학습을 능가하는 아주 뛰어난 미세 조정 성능을 달성했습니다. 한 데이터셋에서 마스크된 사전 훈련 표현을 다른 데이터셋으로 전이하는 것도 최고 수준의 예측 정확도(SOTA)를 산출했습니다.*
``` Glossary를 참고하여 수정했습니다. ```suggestion
The PatchTST 모델은 Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam가 저술한 [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/abs/2211.14730)라는 논문에서 소개되었습니다.
```
논문 제목을 번역하지 않고 그대로 넣어보았습니다. ```suggestion
*우리는 다변량 시계열 예측과 자기지도 표현 학습을 위한 효율적인 트랜스포머 기반 모델 설계를 제안합니다. 이는 두 가지 주요 구성 요소를 기반으로 합니다.*

*(i) 시계열을 하위 시리즈 수준의 패치로 분할하여 트랜스포머의 입력 토큰으로 사용 (ii) 각 채널이 모든 시리즈에 걸쳐 동일한 임베딩과 트랜스포머 가중치를 공유하는 단일 단변량 시계열을 포함하는 채널 독립성*

*패칭 설계는 자연스럽게 세 가지 이점을 가집니다.
    - 지역적 의미 정보가 임베딩에 유지됩니다.
    - 동일한 룩백 윈도우에 대해 어텐션 맵의 계산과 메모리 사용량이 제곱으로 감소합니다.
    - 모델이 더 긴 과거를 참조할 수 있습니다.*

*우리의 채널 독립적 패치 시계열 트랜스포머(PatchTST)는 최신 트랜스포머 기반 모델들과 비교했을 때 장기 예측 정확도를 크게 향상시킬 수 있습니다. 또한 모델을 자기지도 사전 훈련 작업에 적용하여, 대규모 데이터셋에 대해 지도 학습시키는 것을 능가하는 뛰어난 미세 조정 성능을 달성했습니다. 한 데이터셋에서 마스크된 사전 훈련 표현을 다른 데이터셋으로 전이하는 것도 최고 수준의 예측 정확도(SOTA)를 산출했습니다.*
```
italic(기울임체) 적용으로 줄바꿈이 오적용되어 수정해보았습니다. 줄바꿈을 적용하되, 항목별로 이해가 쉽게 4문단으로 나누었습니다. 원문은 전체 내용이 한줄로 들어가 있긴합니다. 아래는 참고용 마크다운 미리보기 스크린샷입니다.

<img width=""618"" alt=""image"" src=""https://github.com/user-attachments/assets/3c6edbdb-2f85-47bf-af4b-aa6b572cbf0d""> ```suggestion
이 모델은 [namctin](https://huggingface.co/namctin), [gsinthong](https://huggingface.co/gsinthong), [diepi](https://huggingface.co/diepi), [vijaye12](https://huggingface.co/vijaye12), [wmgifford](https://huggingface.co/wmgifford), [kashif](https://huggingface.co/kashif)가 기여했습니다. 원본 코드는 [here](https://github.com/yuqinie98/PatchTST)에서 찾을 수 있습니다.
```
오기입된 원문 수정했습니다! ```suggestion
이 모델은 시계열 분류와 시계열 회귀에도 사용될 수 있습니다. 각각 [`PatchTSTForClassification`]와 [`PatchTSTForRegression`] 클래스를 참조하세요.
``` I noticed in other PRs, the `Overview` is translated to `개요`. Can we also do that here to keep it consistent (same with `Usage tips` and `Resources`)? ```suggestion
## 자료[[resources]]
```"
33954,2024-10-04T17:24:33Z,2024-10-09T18:14:43Z,fabxoe,2,6,6,70,2,3,1,[],78576.0,0,435010.0,1,0,0,0,6746537.236022,,0,6,0,False,"['ahnjj', 'HuggingFaceDocBuilderDev']","다른분들이 올려주신 리뷰외에 LGTM입니다!!! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33954). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.glossary 참고하여 수정하였습니다:) Thank you, just need to resolve the merge conflict!","glossary 참고하여 수정하였습니다:) Thank you, just need to resolve the merge conflict!","# What does this PR do?

Translated the `main_classes/data_collator.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
# 데이터 콜레이터(Data Collator)[[data-collator]]
```
 ```suggestion
데이터 콜레이터는 데이터셋 요소들의 리스트를 입력으로 사용하여 배치를 형성하는 객체입니다. 이러한 요소들은 `train_dataset` 또는 `eval_dataset의` 요소들과 동일한 타입 입니다. 배치를 구성하기 위해, 데이터 수집기는 (패딩과 같은) 일부 처리를 적용할 수 있습니다. [`DataCollatorForLanguageModeling`]과 같은 일부 콜레이터는 형성된 배치에 (무작위 마스킹과 같은) 일부 무작위 데이터 증강도 적용합니다. 사용 예시는 [예제 스크립트](../examples)나 [예제 노트북](../notebooks)에서 찾을 수 있습니다.
``` ```suggestion
## 기본 데이터 콜레이터[[transformers.default_data_collator]]
``` 이번에 수집기라고 작성한 것은, 가짜연구소 아카데미 시기 때 철화님이 구글시트에 데이터 수집기라는 단어로 glossary에 남겨서 이렇게 해봤어요.
<img width=""890"" alt=""image"" src=""https://github.com/user-attachments/assets/a8cc6f27-2e67-4da0-b254-41c5297d8288"">
 ```suggestion
      title: 데이터 콜레이터
``` ```suggestion
데이터 콜레이터는 데이터셋 요소들의 리스트를 입력으로 사용하여 배치를 형성하는 객체입니다. 이러한 요소들은 `train_dataset` 또는 `eval_dataset의` 요소들과 동일한 타입 입니다. 배치를 구성하기 위해, 데이터 콜레이터는 (패딩과 같은) 일부 처리를 적용할 수 있습니다. [`DataCollatorForLanguageModeling`]과 같은 일부 콜레이터는 형성된 배치에 (무작위 마스킹과 같은) 일부 무작위 데이터 증강도 적용합니다. 사용 예시는 [예제 스크립트](../examples)나 [예제 노트북](../notebooks)에서 찾을 수 있습니다.
```"
33808,2024-09-30T08:34:03Z,2024-10-09T17:50:03Z,yijun-lee,6,5,6,91,2,4,1,[],9908.0,0,810960.0,0,0,0,0,6748017.701942,,0,6,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'stevhliu', 'yijun-lee', 'cjfghk5697']","LGTM 💯  LGTM! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33808). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I have resolved it! Thank you :) @stevhliu

 Oops, need to resolve again with all the Korean translations being merged today! 🚀  I have resolved it! Thank you :) @stevhliuLGTM, thanks (just need to resolve merge conflict)!","LGTM, thanks (just need to resolve merge conflict)!","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `modeling_utils.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
## PyTorch 헬퍼(helper) 함수 [[transformers.apply_chunking_to_forward]]
```
뭔가 도우미라는 말이 살짝 어색한 것 같아서 다음과 같이 제안드립니다! 아 확인해보니 glossary에 도우미 함수로 적혀 있군요, 이 코멘트는 무시해도 될 것 같습니다 @mreraser 제 생각에는 helper는 헬퍼로 남기는게 나을거 같습니다. 2024 glossary에 search helper, helper function에서 모두 헬퍼라고 적혀있어요! @cjfghk5697 오 그런가요? 그렇다면 말씀하시는 대로 하는게 좋은 것 같습니다! 좀 애매한 부분도 있으니, 제안해주신대로 헬퍼(helper)로 진행하도록 하겠습니다! :)"
33569,2024-09-18T16:31:34Z,2024-10-09T17:44:29Z,fabxoe,1,6,5,58,2,3,1,[],1746972.0,0,1818775.0,0,0,0,0,6748352.170937,,0,5,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33569). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: docs suggestions fix: resolve suggestions Thanks! We can merge once @fabxoe's comments have been addressed 🙂  👍 ,fix: docs suggestions fix: resolve suggestions Thanks! We can merge once @fabxoe's comments have been addressed 🙂  👍 ,"# What does this PR do?

Translated the `model_doc/graphormer.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
## 오버뷰[[overview]]
``` ```suggestion
## 개요[[overview]]
``` ```suggestion
이 모델은 [clefourrier](https://huggingface.co/clefourrier)가 기여했습니다. 원본 코드는 [이곳](https://github.com/microsoft/Graphormer)에서 확인할 수 있습니다.
``` ```suggestion
*트랜스포머 아키텍처는 자연어 처리와 컴퓨터 비전 등 많은 분야에서 지배적인 선택을 받고 있는 아키텍처 입니다. 그러나 그래프 수준 예측 리더보드 상에서는 주류 GNN 변형모델들에 비해 경쟁력 있는 성능을 달성하지 못했습니다. 따라서 트랜스포머가 그래프 표현 학습에서 어떻게 잘 수행될 수 있을지는 여전히 미스터리였습니다. 본 논문에서는 Graphormer를 제시함으로써 이 미스터리를 해결합니다. Graphormer는 표준 트랜스포머 아키텍처를 기반으로 구축되었으며, 특히 최근의 OpenGraphBenchmark Large-Scale Challenge(OGB-LSC)의 광범위한 그래프 표현 학습 작업에서 탁월한 결과를 얻을 수 있었습니다. 그래프에서 트랜스포머를 활용하는데 핵심은 그래프의 구조적 정보를 모델에 효과적으로 인코딩하는 것입니다. 이를 위해 우리는 Graphormer가 그래프 구조 데이터를 더 잘 모델링할 수 있도록 돕는 몇 가지 간단하면서도 효과적인 구조적 인코딩 방법을 제안합니다. 또한, 우리는 Graphormer의 표현을 수학적으로 특성화하고, 그래프의 구조적 정보를 인코딩하는 우리의 방식으로 많은 인기 있는 GNN 변형모델들이 Graphormer의 특수한 경우로 포함될 수 있음을 보여줍니다.*
``` ```suggestion
## 사용 팁[[usage-tips]]
``` ```suggestion
      title: 그래프 모델
```"
33585,2024-09-19T10:21:37Z,2024-10-09T17:41:07Z,fabxoe,1,8,6,59,2,4,1,[],1682733.0,0,1754370.0,1,0,0,0,6748555.675182,,0,6,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33585). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: docs suggestions fix: resolve suggestions Thank you, just need to resolve the merge conflict!","fix: docs suggestions fix: resolve suggestions Thank you, just need to resolve the merge conflict!","# What does this PR do?

Translated the `model_doc/informer.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
## 개요[[overview]]
``` ```suggestion
## 자료[[resources]]
``` 
```suggestion
논문 초록:
``` 
```suggestion
*실제로 많은 응용프로그램에서는 장기 시퀀스 시계열 예측(LSTF)을 필요로 합니다. LSTF는 출력 - 입력 간 정확한 장기 의존성 결합도를 포착해내는 높은 예측 능력을 모델에 요구합니다. 최근 연구들은 예측 능력을 향상시킬 수 있는 트랜스포머의 잠재력을 보여주고 있습니다. 그러나, 트랜스포머를 LSTF에 직접 적용하지 못하도록 막는 몇 심각한 문제점들이 있습니다. 예로, 이차 시간 복잡도, 높은 메모리 사용량, 인코더-디코더 아키텍처의 본질적 한계를 들 수 있습니다. 이러한 문제를 해결하기 위해 LSTF를 위한 효율적인 트랜스포머 기반 모델인 Informer를 설계했습니다.
``` 
```suggestion
(ii) 셀프 어텐션 증류는 계단식 레이어 입력을 반으로 줄여 지배적인 어텐션을 강조하고 극단적으로 긴 입력 시퀀스를 효율적으로 처리합니다. 
``` 
```suggestion
(iii) 생성 스타일 디코더는 개념적으로 단순하지만 장기 시계열 시퀀스를 단계별 방식이 아닌 한 번의 전방 연산으로 예측하여 장기 시퀀스 예측의 추론 속도를 크게 향상시킵니다. 4개의 대규모 데이터셋에 걸친 광범위한 실험은 Informer가 기존 방법들을 크게 능가하며 LSTF 문제에 새로운 해결책을 제공함을 보여줍니다.*
``` 
```suggestion
초심자분들께 도움이 될 Hugging Face와 community 공식 자료 목록(🌎로 표시됨) 입니다. 이 커뮤니티에 속할 자료를 제출하고 싶으시다면, PR(Pull Request)를 열어주세요. 리뷰 해드리겠습니다! 자료는 기존 자료를 복제하는 대신 새로운 내용을 담고 있어야 합니다.
``` ```suggestion
이 모델은 [elisim](https://huggingface.co/elisim)와 [kashif](https://huggingface.co/kashif)가 기여했습니다.
```"
33596,2024-09-19T15:34:09Z,2024-10-09T17:40:48Z,fabxoe,1,11,7,66,2,5,1,[],1664027.0,0,1735599.0,1,0,0,0,6748575.138822,,0,7,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33596). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.전반적으로 깔끔하게 번역을 잘해주셔서 많이 배울 수 있었습니다 😊😊 자연스럽게 다듬을 수 있는 몇 가지 포인트와 오타 부분만 수정 제안드려요! fix: docs suggestions fix: resolve suggestions fix: resolve suggestions fix: resolve suggestions Thanks much, just need to resolve the merge conflict! 🤗 ","전반적으로 깔끔하게 번역을 잘해주셔서 많이 배울 수 있었습니다 😊😊 자연스럽게 다듬을 수 있는 몇 가지 포인트와 오타 부분만 수정 제안드려요! fix: docs suggestions fix: resolve suggestions fix: resolve suggestions fix: resolve suggestions Thanks much, just need to resolve the merge conflict! 🤗 ","# What does this PR do?

Translated the `model_doc/time_series_transformer.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
# 시계열 트랜스포머[[time-series-transformer]]
``` ```suggestion
## 사용 팁[[usage-tips]]
``` 
```suggestion
# 시계열 트랜스포머 [[time-series-transformer]]
``` 
```suggestion
## 활용 팁 [[usage-tips]]
``` 
```suggestion
## 자료 [[resources]]
``` 
```suggestion
## 개요[[overview]]
``` 
```suggestion
초심자에게 도움이 될 Hugging Face와 커뮤니티 자료 목록(🌎로 표시됨) 입니다. 이곳에 포함될 자료를 제출하고 싶으시다면 PR(Pull Request)를 열어주세요. 리뷰 해드리겠습니다! 자료는 기존 자료를 복제하는 대신 새로운 내용을 담고 있어야 합니다.
``` ```suggestion
- 다른 라이브러리의 모델들과 마찬가지로, [`TimeSeriesTransformerModel`]은 상단에 헤드가 없는 기본적인 트랜스포머 입니다. [`TimeSeriesTransformerForPrediction`]은 상단에 분포 헤드를 추가하여 시계열 예측에 사용할 수 있습니다. 이 모델은 이른바 확률적 예측 모델이며, 포인트 예측 모델이 아닙니다. 즉 샘플링할 수 있는 분포를 학습하며, 값을 직접 출력 하지는 않습니다.
``` ```suggestion
- [`TimeSeriesTransformerForPrediction`]은 두개의 블록으로 구성되어 있습니다. 인코더는 `context_length`의  시계열 값을 입력(`past_values`라고 부름)으로 받아들이며, 디코더는 미래의 `prediction_length`만큼 시계열 값을 예측합니다(`future_values`라고 부름). 학습중에는 모델에 `past_values` 와 `future_values`쌍을 모델에 제공해야 합니다.
```
called 번역 과정에서 삭제가 안된 것 같아 제거하였습니다. ```suggestion
    예시: 특정 시계열 값이 8월 11일에 기록되었다면, [11, 8]을 시간 특성 벡터로 사용할 수 있습니다 (11은 ""월의 일"", 8은 ""연도의 월"").
```
시계열 데이터 관련 맥락에서 얻어지다 -> 기록되어지다가 더 적합할 것 같아 제안드립니다. ```suggestion
- 다은 라이브러리의 모델들과 마찬가지로, [`TimeSeriesTransformerModel`]은 상단에 헤드가 없는 기본적인 트랜스포머입니다. [`TimeSeriesTransformerForPrediction`]은 상단에 분포 헤드를 추가하여 시계열 예측에 사용할 수 있습니다. 이 모델은 이른바 확률적 예측 모델이며, 포인트 예측 모델이 아닙니다. 즉 샘플링할 수 있는 분포를 학습하며, 값을 직접 출력 하지는 않습니다.
```"
33597,2024-09-19T17:03:40Z,2024-10-09T17:40:36Z,fabxoe,1,7,4,53,2,3,1,[],1658647.0,0,1730217.0,0,0,0,0,6748586.482431,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33597). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: docs suggestions Thanks! I think the English version of some of the text needs to be removed :) 🤗 ,fix: docs suggestions Thanks! I think the English version of some of the text needs to be removed :) 🤗 ,"# What does this PR do?

Translated the `model_doc/trajectory_transformer.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
## 개요[[overview]]
``` ```suggestion
## 사용 팁[[usage-tips]]
``` Should this be removed? ```suggestion
``` ```suggestion
``` ```suggestion
``` @stevhliu yes, I deleted that."
33606,2024-09-20T02:57:57Z,2024-10-09T17:40:06Z,fabxoe,1,10,7,72,2,5,1,[],1623039.0,0,1694529.0,1,0,0,0,6748619.046608,,0,7,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33606). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.양질의 번역을 다수 해주셔서 많이 배우고 갑니다🤗👍 fix: docs suggestion Thanks, we can merge once the conflict is resolved! 🤗 ","양질의 번역을 다수 해주셔서 많이 배우고 갑니다🤗👍 fix: docs suggestion Thanks, we can merge once the conflict is resolved! 🤗 ","# What does this PR do?

Translated the `main_classes/model.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
기본 클래스 [`PreTrainedModel`], [`TFPreTrainedModel`], [`FlaxPreTrainedModel`]는 로컬 파일과 디렉토리로부터 모델을 로드하고 저장하거나 또는 (허깅페이스 AWS S3 리포지토리로부터 다운로드된) 라이브러리에서 제공하는 사전 훈련된 모델 설정을 로드하고 저장하는 것을 지원하는 기본 메소드를 구현하였습니다.   
``` ```suggestion
- 모델의 어텐션 헤드를 가지치기 합니다.
``` ```suggestion
사용자 정의 모델은 초고속 초기화가 특정 모델에 적용될 수 있는지 여부를 결정하는 `_supports_assign_param_buffer`도 포함해야 합니다.
```
superfast init도 번역을 하면 좋을 것 같아 제안드립니다!😊 ```suggestion
기본 클래스 [`PreTrainedModel`], [`TFPreTrainedModel`], [`FlaxPreTrainedModel`]는 로컬 파일 및 디렉토리에서 모델을 로드하고 저장하거나,
 또는 허깅페이스 AWS S3 리포지토리에서 다운로드한 라이브러리에서 제공하는 사전훈련 모델 설정값으로부터 모델을 로드하고 저장하는 기본 메소드를 구현하였습니다.   
```
가독성을 높이려 표현을 바꿔보았습니다 ! ```suggestion
[`PreTrainedModel`]과 [`TFPreTrainedModel`]은 모든 모델들에 공통적으로 지원되는 몇 메소드를 구현하였습니다:
``` ```suggestion
`test_save_and_load_from_pretrained` 실패 시, 모델이 `_supports_assign_param_buffer`를 필요로 하는지 확인하세요.
``` ```suggestion
각 모델에 공통인 다른 메소드들은 다음의 클래스에서 정의됩니다. 
``` 표현을 바꾸었습니다. ```suggestion
사용자 정의 모델은 초고속 초기화(superfast init)가 특정 모델에 적용될 수 있는지 여부를 결정하는 `_supports_assign_param_buffer`도 포함해야 합니다.
``` ```suggestion
[`PreTrainedModel`]과 [`TFPreTrainedModel`]은 또한 모든 모델들을 공통적으로 지원하는 메소드 여러개를 구현하였습니다:
```"
33629,2024-09-20T16:30:40Z,2024-10-09T17:39:54Z,fabxoe,1,13,7,113,2,5,1,[],1574192.0,0,1645754.0,2,0,0,0,6748631.569169,,0,7,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33629). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.번역을 꼼꼼하게 잘해주셔서 간단한 오타와 띄어쓰기만 리뷰 드립니다 🤗 fix: docs suggestions Thanks, just need to resolve the merge conflict!","번역을 꼼꼼하게 잘해주셔서 간단한 오타와 띄어쓰기만 리뷰 드립니다 🤗 fix: docs suggestions Thanks, just need to resolve the merge conflict!","# What does this PR do?

Translated the `model_doc/mamba2.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
맘바2 모델은 Tri Dao, Albert Gu가 제안한 [트랜스포머는 SSM이다: 구조화된 상태 공간 이중성을 통한 일반화된 모델과 효율적인 알고리즘](https://arxiv.org/abs/2405.21060)라는 논문에서 소개되었습니다. 맘바2는 맘바1과 유사한 상태 공간 모델로, 단순화된 아키텍처에서 더 나은 성능을 보입니다.
``` ```suggestion
이 모델은 `torch_forward`와 `cuda_kernels_forward`라는 두 가지 다른 전방 패스를 가집니다. `cuda_kernels_forward`는 환경에서 cuda 커널을 찾으면 이를 사용하며, prefill에서는 더 느립니다. 즉, 높은 CPU 오버헤드로 인해 ""웜업 실행""이 필요하기 때문입니다. 관련 내용은 [이곳](https://github.com/state-spaces/mamba/issues/389#issuecomment-2171755306)과 [이곳](https://github.com/state-spaces/mamba/issues/355#issuecomment-2147597457)을 참고하세요. 
``` ```suggestion
이로인해 맘바2 커널의 재구현과 함께 배치 생성 및 캐시된 생성에서 약간의 차이가 예상됩니다. 또한 cuda 커널 또는 tourch forward가 제공하는 결과가 약간 다를 것으로 예상됩니다. SSM 알고리즘은 텐서 수축에 크게 의존하는데, 이는 matmul과 동등하지만 연산 순서가 약간 다르며, 이로 인해 더 작은 정밀도에서 차이가 더 커집니다.
``` ```suggestion
이 버전은 맘바2 구현을 지원해야 하며, 특히 Mistral AI의 [Mamba-2 codestral](https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1)을 지원합니다. 특히, mamba 2 codestral은 8개의 `groups`로 출시되었는데, 이는 어텐션 기반 모델의 KV 헤드 수와 유사하다고 판단 가능합니다.
``` ```suggestion
이로 인해 맘바2 커널의 재구현과 함께 배치 생성 및 캐시된 생성에서 약간의 차이가 예상됩니다. 또한 cuda 커널 또는 tourch forward의 결과가 약간 다를 것으로 예상됩니다. SSM 알고리즘은 텐서 수축에 크게 의존하는데, 이는 matmul과 동등하나, 연산 순서에 약간의 차이가 있으며, 이로 인해 더 작은 정밀도에서 그 차이가 더 커집니다.
``` ```suggestion
## 개요[[overview]]
``` ```suggestion
컴파일 없이는 `torch_forward` 구현이 3~4배 빠릅니다. 또한, 이 모델에는 위치 임베딩이 없지만 `attention_mask`와 배치 생성의 경우 두 곳에서 은닉 상태(hidden state)를 마스킹하는 특정 로직이 있습니다. 관련 내용은 [이곳](https://github.com/state-spaces/mamba/issues/66#issuecomment-1863563829)을 참고하세요. 
``` ```suggestion
또 다른 참고사항으로, 패딩 토큰에 해당하는 은닉 상태(hidden state)의 종료는 두 곳에서 이루어지며 주로 왼쪽 패딩으로 테스트되었습니다. 오른쪽 패딩은 노이즈를 전파하므로 만족스러운 결과를 보장하지 않습니다. `tokenizer.padding_side = ""left""`를 사용하면 올바른 패딩 방향을 사용할 수 있습니다.
``` ```suggestion
맘바2 모델은 Tri Dao, Albert Gu가 제안한 [[Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060)라는 논문에서 소개되었습니다. 맘바2는 맘바1과 유사한 상태 공간 모델로, 단순화된 아키텍처에서 더 나은 성능을 보입니다.
```
논문명은 원제목대로 두는 것 같아 변경하였습니다! ```suggestion
이로인해 맘바2 커널의 재구현과 함께 배치 생성 및 캐시된 생성에서 약간의 차이가 예상됩니다. 또한 cuda 커널 또는 torch forward가 제공하는 결과가 약간 다를 것으로 예상됩니다. SSM 알고리즘은 텐서 수축에 크게 의존하는데, 이는 matmul과 동등하지만 연산 순서가 약간 다르며, 이로 인해 더 작은 정밀도에서 차이가 더 커집니다.
``` ```suggestion
*트랜스포머는 언어 모델링에서 딥러닝 성공의 주요 아키텍처였지만, 맘바와 같은 상태 공간 모델(SSM)이 최근 소규모 혹은 중간 규모에서 트랜스포머와 대등하거나 더 나은 성능을 보이는 것으로 나타났습니다. 우리는 이러한 모델 계열들이 실제로 매우 밀접하게 연관되어 있음을 파악했습니다. 그리고 구조화된 준분리(semiseparable) 행렬 중 연구가 잘 이루어진 클래스의 다양한 분해를 통해 연결된 SSM과 어텐션 변형 사이의 풍부한 이론적 연결 프레임워크를 개발했습니다. 상태 공간 이중성(SSD) 프레임워크를 통해 맘바1의 선택적 SSM을 개선한 새로운 아키텍처를 설계할 수 있었고, 이는 언어 모델링에서 트랜스포머와 경쟁력을 유지하면서 속도는 2~8배 빠릅니다.*
```
가독성을 높이기위해 문장을 나눴습니다! ```suggestion
이로인해 맘바2 커널의 재구현과 함께 배치 생성 및 캐시된 생성에서 약간의 차이가 날 것으로 예상됩니다. 또한 cuda 커널 또는 tourch forward의 결과도 약간 다를 것으로 예상됩니다. SSM 알고리즘은 텐서 수축에 크게 의존하는데, 이는 matmul과 동등하지만 연산 순서가 약간 다르며, 이로 인해 더 작은 정밀도에서 차이가 더 커집니다.
``` ```suggestion
*트랜스포머는 언어 모델링에서 딥러닝 성공의 주요 아키텍처였지만, 맘바와 같은 상태 공간 모델(SSM)이 최근 소규모 혹은 중간 규모에서 트랜스포머와 대등하거나 더 나은 성능을 보이는 것으로 나타났습니다. 우리는 이러한 모델 계열들이 실제로 매우 밀접하게 연관되어 있음을 파악했습니다. 그리고 구조화된 준분리(semiseparable) 행렬 중 연구가 잘 이루어진 클래스의 다양한 분해를 통해 연결된 SSM과 어텐션 변형 사이의 풍부한 이론적 연결 프레임워크를 개발했습니다. 상태 공간 이중성(SSD) 프레임워크를 통해 맘바1의 선택적 SSM을 개선한 새로운 아키텍처를 설계할 수 있었고, 트랜스포머와 경쟁력을 유지하면서도 속도는 2~8배 더 빠른 성능을 냅니다.*
```"
33955,2024-10-04T17:40:48Z,2024-10-09T17:34:01Z,fabxoe,1,1,3,31,2,2,1,[],360403.0,0,431593.0,0,0,0,0,6748984.583201,,0,3,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33955). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks, just need to resolve the merge conflict :)","Thanks, just need to resolve the merge conflict :)","# What does this PR do?

Translated the `main_classes/keras_callbacks.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [x] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
케라스로 트랜스포머 모델을 학습할 때, 일반적인 작업을 자동화하기 위한 라이브러리 전용 콜백들을 사용 할 수 있습니다.
```
자연스러운 표현을 위해 바꿨습니다!"
33967,2024-10-05T03:13:17Z,2024-10-09T17:33:34Z,fabxoe,1,4,6,156,2,3,1,[],326107.0,0,397218.0,0,0,0,0,6749012.297895,,0,6,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33967). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.번역 잘해주셔서 사소한 부분만 리뷰 드립니다! 감사합니다🤗 Thanks, just need to resolve the merge conflict!","번역 잘해주셔서 사소한 부분만 리뷰 드립니다! 감사합니다🤗 Thanks, just need to resolve the merge conflict!","# What does this PR do?

Translated the `model_doc/deberta.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
DeBERTa를 시작하는 데 도움이 되는 Hugging Face와 community 자료 목록(🌎로 표시됨) 입니다. 여기에 포함될 자료를 제출하고 싶으시다면 PR(Pull Request)를 열어주세요. 리뷰해 드리겠습니다! 자료는 기존 자료를 복제하는 대신 새로운 내용을 담고 있어야 합니다.
``` ```suggestion
[DeBERTa](https://huggingface.co/DeBERTa) 모델의 텐서플로 2.0 구현은 [kamalkraj](https://huggingface.co/kamalkraj)에 의해 기여되었습니다. 원본 코드는 [이곳](https://github.com/microsoft/DeBERTa)에서 확인하실 수 있습니다.
``` ```suggestion
- DeBERTa와 [머신러닝으로 한층 향상된 고객 서비스](https://huggingface.co/blog/supercharge-customer-service-with-machine-learning) 에 대한 블로그 포스트.
``` ```suggestion
## 개요[[overview]]
```"
33893,2024-10-02T14:58:34Z,2024-10-09T17:33:14Z,fabxoe,1,30,25,221,2,4,1,[],542970.0,0,614080.0,1,0,0,0,6749034.366468,,0,25,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33893). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: suggestions fix: resolve suggestions fix: resolve suggestions Thanks! Just one suggestion, otherwise LGTM 👍  👍 ","fix: suggestions fix: resolve suggestions fix: resolve suggestions Thanks! Just one suggestion, otherwise LGTM 👍  👍 ","# What does this PR do?

Translated the `model_doc/bart.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
## BartConfig[[transformers.BartConfig]]
``` ```suggestion
## BartTokenizer[[transformers.BartTokenizer]]
``` ```suggestion
## BartTokenizerFast[[transformers.BartTokenizerFast]]
``` ```suggestion
## BartModel[[transformers.BartModel]]
``` ```suggestion
## BartForConditionalGeneration[[transformers.BartForConditionalGeneration]]
``` ```suggestion
## BartForSequenceClassification[[transformers.BartForSequenceClassification]]
``` ```suggestion
## BartForQuestionAnswering[[transformers.BartForQuestionAnswering]]
``` ```suggestion
## BartForCausalLM[[transformers.BartForCausalLM]]
``` ```suggestion
## TFBartModel[[transformers.TFBartModel]]
``` ```suggestion
## TFBartForConditionalGeneration[[transformers.TFBartForConditionalGeneration]]
``` ```suggestion
## TFBartForSequenceClassification[[transformers.TFBartForSequenceClassification]]
``` ```suggestion
## FlaxBartModel[[transformers.FlaxBartModel]]
``` ```suggestion
## FlaxBartForConditionalGeneration[[transformers.FlaxBartForConditionalGeneration]]
``` ```suggestion
## FlaxBartForSequenceClassification[[transformers.FlaxBartForSequenceClassification]]
``` ```suggestion
## FlaxBartForQuestionAnswering[[transformers.FlaxBartForQuestionAnswering]]
``` ```suggestion
## FlaxBartForCausalLM[[transformers.FlaxBartForCausalLM]]
``` ```suggestion
## 개요 [[overview]]
``` ```suggestion
추가 자료:
``` ```suggestion
- [Trainer 클래스를 사용하여 두 가지 언어로 요약하기 위한 BART 미세 조정](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)하는 방법에 대한 노트북. 🌎
```
더 자연스러운 표현으로 바꾸었습니다 :) ```suggestion
  [`~BartTokenizer.encode`]를 사용합니다.
```
다른 부분들은 높임체(?)로 쓰셔서 통일했습니다! ```suggestion

```
아래줄에 번역 해두신 것 같아 삭제되어야할 것 같습니다. ```suggestion
```
아래줄에 번역 해두신 것 같아 삭제되어야할 것 같습니다. ```suggestion
```
아래줄에 번역 해두신 것 같아 삭제되어야할 것 같습니다. ```suggestion
- 인코더와 디코더가 있는 seq2seq 모델입니다. 인코더에는 손상된 토큰이(corrupted tokens) 입력되고, 디코더에는 원래 토큰이 입력됩니다(단, 일반적인 트랜스포머 디코더처럼 미래 단어를 숨기는 마스크가 있습니다). 사전 훈련 작업에서 인코더에 적용되는 변환들의 구성은 다음과 같습니다:
``` ```suggestion
- Bart는 시퀀스 분류에 `token_type_ids`를 사용하지 않습니다. 적절하게 나누기 위해서 [`BartTokenizer`]나
``` ```suggestion
- [`BartModel`]의 정방향 전달은 `decoder_input_ids`가 전달되지 않으면 `decoder_input_ids`를 자동으로 생성할 것입니다. 이는 다른 일부 모델링 API와 다른 점입니다. 이 기능의 일반적인 사용 사례는 마스크 채우기(mask filling)입니다.
``` ```suggestion
  모델 예측은 `forced_bos_token_id=0`일 때 기존 구현과 동일하게 작동하도록 의도되었습니다. 하지만, [`fairseq.encode`]에 전달하는 문자열이 공백으로 시작할 때만 이 기능이 작동합니다.
``` ```suggestion
- 이 [논문](https://arxiv.org/abs/2010.13002)은 [증류된 체크포인트](https://huggingface.co/models?search=distilbart)에 대해 설명합니다.
``` ```suggestion
## 자료[[resources]]
``` I think this should be two separate items here.
```suggestion
- [`BartModel`]의 정방향 전달은 `decoder_input_ids`가 전달되지 않으면 `decoder_input_ids`를 자동으로 생성할 것입니다. 이는 다른 일부 모델링 API와 다른 점입니다. 이 기능의 일반적인 사용 사례는 마스크 채우기(mask filling)입니다.
-  모델 예측은 `forced_bos_token_id=0`일 때 기존 구현과 동일하게 작동하도록 의도되었습니다. 하지만, [`fairseq.encode`]에 전달하는 문자열이 공백으로 시작할 때만 이 기능이 작동합니다.
```"
33410,2024-09-10T15:41:09Z,2024-10-09T15:51:41Z,MekkCyber,2,30,27,746,11,5,3,[],2098.0,0,2506232.0,0,0,0,0,6755127.911918,,0,27,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33410). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Can you have a look @dacorvo on the BitLinear class ? I would love to have your insights ! Thanks for your work @MekkCyber ! This looks pretty good ! I've left a few comments. Could you also add some tests and update the documentation about this new quantizer + fix CI ?  Nice job for adding this @MekkCyber ! Just a few nits ! Could you also update the quantization overview and create a page for bitnet ? I would link to the nanotron PR for users who wants to fine-tune their model and link a script to perform the conversion to the right format (quantization + packing). This way, users will be able to load their 1.58 models with this quantizer.  Thanks for adding this new quantizer ! Excited to see how this quantizer will evolve in the future ! Just a few nits.  🚀 let's make sure we link the ressources for training, a gist? or a link to a repo!  Thanks for iterating ! Can you fix the merge conflits ? Also, in a previous PR, we changed the `is_serializable` method. It's not a property anymore, so you need to change this also here. Thanks !  IMO missing one small test and good to go!  Thanks for iterating ! Merging ! ","Thanks for your work @MekkCyber ! This looks pretty good ! I've left a few comments. Could you also add some tests and update the documentation about this new quantizer + fix CI ?  Nice job for adding this @MekkCyber ! Just a few nits ! Could you also update the quantization overview and create a page for bitnet ? I would link to the nanotron PR for users who wants to fine-tune their model and link a script to perform the conversion to the right format (quantization + packing). This way, users will be able to load their 1.58 models with this quantizer.  Thanks for adding this new quantizer ! Excited to see how this quantizer will evolve in the future ! Just a few nits.  🚀 let's make sure we link the ressources for training, a gist? or a link to a repo!  Thanks for iterating ! Can you fix the merge conflits ? Also, in a previous PR, we changed the `is_serializable` method. It's not a property anymore, so you need to change this also here. Thanks !  IMO missing one small test and good to go!  Thanks for iterating ! Merging ! ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->
This pull request introduces a new quantization method: [BitNet](https://arxiv.org/pdf/2310.11453) quantization at 1.58 bits. It enables users to load and utilize quantized & packed models with ternary weights directly in Transformers, providing out-of-the-box inference.
<!-- Remove if not applicable -->



## Who can review?

@SunMarc 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","could you describe with an example the unpacking process ? Also even though it is not needed in this integration, could you add the pack_weights function that you used to pack the weights ?  To put under `is_torch_available` ```suggestion
``` Let's force the user to pass a value for the bias 
```suggestion
    def __init__(self, in_features: int, out_features: int, bias: bool, input_bits: int = 8,
                 device=None, dtype=None, config=None):
``` Not used, can it be removed ?  Also why are we setting it  `torch.bfloat16` ? This shouldn't be a big issue as in transformers, when we load the model, we don't really care about the dtype during the initialization.  Add a quick description of the activation quant (sym/asym/per-channel/per-tensor) Is this a requirement to unpack in bf16 ?  You already casted to the right dtype in `unpack_weights`. Also, I think that we need to pass the dtype of `weight_scale` to cast to the right dtype (that can be float32/float16 or bfloat16) depending on how the model should be loaded. Of course, you can enforce the dtype is not set in the quantizer. This is what awq, bnb or gptq do for example.
```suggestion
``` Just a preference, both works 
```suggestion
        return result.to(torch.int8), s
``` Let's cast to the right dtype, which is not necessarily bf16 Let's just call it Bitlinear
```suggestion
class BitLinear(nn.Module):

    def __init__(self, in_features: int, out_features: int, bias: bool = False, input_bits: int = 8,
``` This should also run on cpu no since we are just using compile ?  to update according to how we fix the above comment te remove if we don't support in the end Yes it runs on cpu, but it's slow I changed the code so that the dtype used is the same we use in loading the model, but the models were trained on bfloat16 precision for the scales, so bfloat16 or float32 are required to load the model since the scales are very sensitive and they contain a lot of information  Why do we need to upcast to bf16 ?  did you check if the modules size were correctly computed if we set this ?  Yes seems to be working fine nit: for clarity I would have put this line immediately under line 46 as they are both related to the conversion from [-1, 0, 1]/int8 to [0, 1, 2]/uint8.   This code reminds me of something ... ;-). 
https://github.com/huggingface/optimum-quanto/blob/f62c887731cfc4800f930ba55c3da0262f10f84e/optimum/quanto/tensor/qbits/packed.py#L24
If you don't plan to support the MPS device, then you can inline the `<<` operation in the loop. Consider defining a constant for the number of values per item and use it in the whole file. Quantizing per-tensor (i.e. using a single scale value for the whole tensor) at that level of quantization will greatly reduce the precision. Are the quantized weights supposed to be fine-tuned ? Note: this does not produce ternary outputs for bits=2 (but outputs in [-2, 1, 0, 1]). per-channel is a bit misleading here, since by convention 'channel' usually denotes the last dimension, and one would expect the quantized output to have one scale per 'channel', i.e. slice along the last dimension where here you obtain one scale for each slice along the first dimension (typically the tokens in language models). This could be done more efficiently as:

```python
out = input / (si * sw)
``` This is prone to overflows in the matmul accumulator: when unpacking you basically recreate a tensor of ternary values expressed in full precision, i.e. not only does it occupy a large chunk of device memory, but is also expressed in a range ([-1, 1]) that is usually larger than the one of the original float16 weights.
You should therefore apply the weight scale immediately to come back into the expected computation range. Same thing for the activations.

In other words: apply the scales before the computation, and not afterwards. ```suggestion
            raise ImportError(""Loading a BitNet quantized model requires accelerate (`pip install accelerate`)"")
``` Ternary ?"
34028,2024-10-08T15:03:54Z,2024-10-09T14:45:06Z,muellerzr,1,0,1,3,1,2,2,[],1642.0,0,85274.0,0,0,0,0,6759122.34299,,0,1,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34028). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM! Thanks!  Seems fair, thanks @muellerzr!","LGTM! Thanks!  Seems fair, thanks @muellerzr!","# What does this PR do?

https://github.com/huggingface/transformers/pull/32385 missed that we need `@require_vision` for the new processor tests. We don't install PIL in the accelerate test suite, which flagged this requirement. 

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@LysandreJik ",
32514,2024-08-07T21:01:52Z,2024-10-09T15:46:11Z,qubvel,15,30,37,1551,91,4,1,[],1158.0,2,5424260.0,0,0,0,0,6755458.583527,,0,37,0,False,"['qubvel', 'yonigozlan', 'LysandreJik', 'HuggingFaceDocBuilderDev', 'Rocketknight1', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32514). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @qubvel . IIRC, that PR is more about ""make pipeline able to accept loading processor"", but no currently existing pipeline code is changed to use processor yet (even if we specify to load it), right?  @ydshieh yes, you are right, the [following PR](https://github.com/huggingface/transformers/pull/32490) will add processor to the ZeroShotObjectDetection pipeline. I decided to split it to simplify the review a bit. Hi @qubvel I have a general question. The process class could load also a single tokenizer, image processor or feature extractor (if not all components are on a Hub repository). For example,

```python
from transformers import AutoProcessor
AutoProcessor.from_pretrained(""bert-base-uncased"")
```
will turn out to be a tokenizer instead of an instance of `ProcessorMixin`.

Therefore, in `src/transformers/pipelines/__init__.py`, this part

```python
       # Instantiate processor if needed
        if isinstance(processor, (str, tuple)):
            processor = AutoProcessor.from_pretrained(processor, _from_pipeline=task, **hub_kwargs, **model_kwargs)
```
should have some guard to make sure we really get a `ProcessorMixin` instance.

WDYT?

I will share my thoughts regarding the testing part this afternoon, somehow related to the same consideration.



 ## continuation of my previous comment

Once we start to have the code using `processor`, and if we support it for some currently existing pipelines, it should support:
- either not use the processor at all (i.e. currently implementation): for backward compatibility
- or completely use the processor only (but need to verify the results are the same)

## for testing

Similar consideration to the above. We should test 2 cases:

- only load a list of tokenizer, image processor and/or feature extractor: test case 1
- only load a single processor: test case 2 > I did it because in the original tiny-models JSON file we do not have separate lists for image processors and feature extractors, I found they are stored in the same list (even processors are sometimes in this list).

Yes, you are correct. It is because I treated them (image processor / feature exactor) equally, and later I decided to make `processor` join into this pipeline testing

I see better now why you need this change. Could you, at least for now, simply use the information from the summary file `processor_classes` field, and by looking at training part to decide to assign them to one of the  `tokenizer_names`, `feature_extractor_names`, `processor_names` etc.?
``
 @ydshieh Thanks for reviewing, I addressed the comments, please have a look!

> Once we start to have the code using processor, and if we support it for some currently existing pipelines, it should support:
> - either not use the processor at all (i.e. currently implementation): for backward compatibility
> - or completely use the processor only (but need to verify the results are the same)

This is what I tried to achieve with granular control for each pipeline class by adding `_load_*` attributes 
https://github.com/huggingface/transformers/pull/32514#discussion_r1711361484 @ydshieh could you please have a look once again? Hi @qubvel @ydshieh ! Just wondering if anything is blocking this PR from getting merged, as it is needed for the image-text-to-text pipeline. Thanks! Hi @yonigozlan, just a lack of time 🙂 I will address comments this week, and hopefully it can be merged then! cc @Rocketknight1 as well (please always ping Matt for pipelines :hugs:) Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). @Rocketknight1 can you please review the PR, if that aligns with plans for `pipeline` refactoring/standardization I'm sorry, I missed that ping somehow! Reviewing now. @LysandreJik thanks for reviewing, I addressed the comment: made clearer docs and removed the warning. Please have a look whenever you have bandwidth.

P.S. The test failure looks unrelated.The main changes for pipeline code:
 - Added loading of `processor` (previously only tokenizer, image_processor and feature_extractor)
 - Added Pipeline class args to control loading
 
 The main changes 
 - Refactored test signatures to support `processor` pass
 - Changed the way how class names are obtained for processors of the testing tiny models in a pipeline. Instead of getting them from JSON file, the MAPPINGs are used.
 
 I highlighted these changes below, please see the comments
 
cc @ydshieh for initial review Some comments :-) Just a few nits but overall good!

Let me know if my comments are clear 🙏  Just finished the review, with a few comments.

Overall summary for core maintainers:
- This is a nice PR! It's clean, and although it seems very large, almost all of the changes are boilerplate adding kwargs and fixing spelling mistakes in a lot of files.
- The actual changes you need to look at are `__init__.py`, `base.py` and `test_pipeline_mixin.py`
- I made some comments, but they're nits and shouldn't affect any of the PR logic.

cc @lysandrejik for core maintainer review! This looks good! I'm mostly concerned about having good and clear documentation and removing unnecessary warnings.

Pipelines are the very first transformers object many users interact with -> ensuring that they're aware of the info they need, without being hammered by warnigns should be an absolute goal of ours.

Thanks for working on this important change!","The main changes for pipeline code:
 - Added loading of `processor` (previously only tokenizer, image_processor and feature_extractor)
 - Added Pipeline class args to control loading
 
 The main changes 
 - Refactored test signatures to support `processor` pass
 - Changed the way how class names are obtained for processors of the testing tiny models in a pipeline. Instead of getting them from JSON file, the MAPPINGs are used.
 
 I highlighted these changes below, please see the comments
 
cc @ydshieh for initial review Some comments :-) Just a few nits but overall good!

Let me know if my comments are clear 🙏  Just finished the review, with a few comments.

Overall summary for core maintainers:
- This is a nice PR! It's clean, and although it seems very large, almost all of the changes are boilerplate adding kwargs and fixing spelling mistakes in a lot of files.
- The actual changes you need to look at are `__init__.py`, `base.py` and `test_pipeline_mixin.py`
- I made some comments, but they're nits and shouldn't affect any of the PR logic.

cc @lysandrejik for core maintainer review! This looks good! I'm mostly concerned about having good and clear documentation and removing unnecessary warnings.

Pipelines are the very first transformers object many users interact with -> ensuring that they're aware of the info they need, without being hammered by warnigns should be an absolute goal of ours.

Thanks for working on this important change!","# What does this PR do?

Add `processor` to the `pipeline` and refactor tests a bit to support it. Related to 
 - #32490
 - #32059 

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","For backward compatibility, we can control with `Pipeline` class if we need to load specific processors/tokenizers. For example, for zero-shot object detection, we will need to load only the `processor`, and do not need to load `image_processor` and tokenizer separately. Other legacy pipelines might load only tokenizer and image_processor, even if they have processor class. granular control for loading, see comment in the code This one is skipped on the main but was not skipped on this branch, so I added a skip rule here Llama no longer raises exception for long generation, so I added it to exclude list in the test. This test was skipped on main. Previously, tokenizer, image processor and feature extractor class names were taken from JSON file of tiny models, however, there is no `*Processor` class there. Now MAPPING lists are used to get class names. The order of loading is a bit changed in this test, now the model is loaded first to check if it exists, otherwise, the test is skipped. > This one is skipped on the main

Could you explain a bit what this means? Do you mean, in the job run's result, it shows it is skipped (on the main branch)? I am not in favor of using `MAPPING` for anything except `processor`. 


The pipeline testing is highly tighten to the tiny model on the Hub, and we should use the actual classes that are used on the Hub repositories (to avoid any surprising).

For `processor`, it's OK to use `MAPPING`, but we can also change the json file's structure for the long term.
 should not skip here. should not skip here this should have arguments `image_processor` and `feature_extractor` we should make sure `processor` is an instance of `ProcessorMixin`, otherwise reset it to `None` can you provide more details, please? this function is used only here and not using image_processor or feature_extractor (I updated the function signature accordingly)

 should we? Class is not checked for feature extractor or image processor, I guess we might want to load anything that might be loaded with AutoProcessor class while it keeps the pipeline working. I did it because in the original tiny-models JSON file we do not have separate lists for image processors and feature extractors, I found they are stored in the same list (even processors are sometimes in this list). Yeah, it's more a design instead of what is actually happening so far.

The picture is that, a pipeline test is involved pipeline class (task), model, tokenizer/image processor/feature extractor/processor, and all of these **could** determine what we want to perform. (although currently only model and tokenizer appears in the body) Yes, this test was not running on the main branch. One of the problem of current tests: we are testing a set of cases, and if one of the cases (e.g. the first one) falls under skipTest(...) the rest test cases are not running. Probably it's better to return case status, and then aggregate statuses in the main test function running on multiple cases. > Class is not checked for feature extractor or image processor

This is because they are not a composite, and are not design to fallback to load partially.

For example, this works
```
from transformers import AutoProcessor
AutoProcessor.from_pretrained(""bert-base-uncased"")
```
but this fails
```
from transformers import AutoImageProcessor
AutoImageProcessor.from_pretrained(""bert-base-uncased"")
```
and same
```
from transformers import AutoTokenizer
AutoTokenizer.from_pretrained(""google/vit-base-patch16-224"")
``` A simple example to reproduce, this test will be skipped

```python
import unittest

class TestCases(unittest.TestCase):

    def test_multiple_cases(self):
        for i in range(10):
            self.run_test(i)

    def run_test(self, i):
        if i == 0:
            self.skipTest(""Skip this test"")
        elif i == 5:
            raise ValueError(""This test failed"")
``` It is designed the same way on the `main` now, according to the comment some instances might not be loaded due to optional dependencies
https://github.com/huggingface/transformers/blob/af638c4afea81ac251f1ae63497789be79e0baaa/tests/test_pipeline_mixin.py#L256-L267

 Thanks for the clarification, I understand your point. 
While this is a design decision, I'm not entirely comfortable with it. The function isn't widely used or overridden in subclasses, it's only used once in the codebase. Not using arguments might cause confusion about what's actually happening in the function and could give the false impression that the image processor and tokenizer are being validated by it, but they are not. We can always add additional arguments to the function if they become necessary. Addressed in https://github.com/huggingface/transformers/pull/32514/commits/8e3eb2b49f4972923c2adda145769bb712296d2c Got it, addressed in https://github.com/huggingface/transformers/pull/32514/commits/2f561479290970fa8beaf668b504df39a2608f4f Modified in https://github.com/huggingface/transformers/pull/32514/commits/b6d3bbc09378fbefbbf4fccacfcb56f64d2298b1

according to @ydshieh suggestion to use mapping only for `Processor` Currently on `main`, this `run_pipeline_test` only take a single `tokenizer_name` and a **single** `processor_name`, and try to load them. So if a specified `processor_name` could not be loaded, we can skip it.

However, here in this PR, we try to **collect** the processors and put them into the dictionary `processors`. So logically it's better not to skip the test during this collection.

(But in practice, this may not produce any difference) This looks like not the correct docstring? works for me.

(FYI: unused arguments exist in `class Pipeline.__init__` and I agree with your above point)

 the first condition is not necessary at this point. if we don't add `None` later in this method (while it's empty), then in `run_model_pipeline_tests`, 

> test_cases = [ ....]

will be empty if `tokenizer_names` is empty or `image_processor_names` is empty or ... (etc).

But we should allow the test case to exist even if only one of them present: for example, a text only model or a vision only model."
34000,2024-10-07T07:45:54Z,2024-10-09T12:02:56Z,zucchini-nlp,1,5,4,145,4,2,1,[],187808.0,0,189552.0,0,0,0,0,6767525.087163,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34000). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Cool to see lots of tests not skipped anymore!,Cool to see lots of tests not skipped anymore!,"# What does this PR do?

As per title, fixes skipped tests in mllama and the QuantoQuantizedCache with new optimum-quanto library
","whisper is encoder decoder and supports quantized cache let's add as docstring of this func!  weird, cross attention cache cannot be obtained in next steps correctly because we won't have all tokens in `self.key_cache`. Will check out thanks verified that Whisper doesn't support it and didn't set the flag to `True`. If we set the flag, tests will fail same way as for Mllama ok got it thanks! 🤗 "
33950,2024-10-04T15:45:59Z,2024-10-09T10:23:50Z,abuelnasr0,17,5,13,60,6,2,2,[],15384.0,0,416628.0,0,0,0,0,6770845.716224,,0,13,0,False,"['HuggingFaceDocBuilderDev', 'gante', 'ArthurZucker', 'ydshieh', 'abuelnasr0']","Ah could you update `tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py::RecurrentGemmaModelTest::test_resize_tokens_embeddings ` as welll?  Ah and `tests/models/git/test_modeling_git.py::GitModelTest::test_resize_tokens_embeddings - AssertionError: Padding_idx must be within num_embeddings` as well  See here 😅 https://app.circleci.com/pipelines/github/huggingface/transformers/106584/workflows/539a0425-e7e1-4a82-aedb-d7134aa524ba/jobs/1415704 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33950). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker I have addressed mobilebert test, GitModeltest, and recurrent_gemma test.

recurrent_gemma test failed because an outlier was sampled, so I multiplied covariance by 1e-9 instead of 1e-5. 
Git model tests failed because the config was overwritten after the first resizing test and used to initialize the model again, so I created a new copy for the new model initialization.  I feel bad about those tests' failures actually, I wanted to deliver good code but tests didn't help me. 😅

The outlier of recurrent_gemma got sampled only after merging the code haha.

And I am not sure if other tests were actually skipped before merging. It's weird.
Do you know why the test failures appeared after merging? mobilebert and GitModeltest? Yep! They are not part of the important model, the test fetcher seems to badly behave! It should have found out the whole dependencies! 

No worries, we are the ones who set you up for failure in that case! 

cc @ydshieh if you can have a look at the reasons why this was not fetched when you have time!  > Yep! They are not part of the important model, the test fetcher seems to badly behave! It should have found out the whole dependencies!
> 
> No worries, we are the ones who set you up for failure in that case!
> 
> cc @ydshieh if you can have a look at the reasons why this was not fetched when you have time!

Could one of you provide a link of the previous (PR) job run page where you believe there is something being missed by the test fetcher? Yes! https://app.circleci.com/pipelines/github/huggingface/transformers/106469/workflows/2ce53b8a-9a18-4e16-9b94-fd3b786c0916/jobs/1414187 is all green, but a few hours later: https://app.circleci.com/pipelines/github/huggingface/transformers/106724/workflows/fb68022e-2d64-44e9-91a5-f86c18f73cdd/jobs/1417162 or any other test torch on main would fail! In the mean time @abuelnasr0 could you commit with a message `test_all` 🤗  This way all models should be ran!  <img width=""1048"" alt=""image"" src=""https://github.com/user-attachments/assets/9866b094-3777-450d-a7fe-ecac56378a7f"">
A few failing tests still!  ``` RUN_SLOW=1 pytest  -n 4 tests/models/*/test_modeling_* -k test_resize_tokens_embeddings``` to filter! I hope you don't mind @abuelnasr0 -- this PR is blocking other PRs, so I'm taking care of the rest of the fixes 🤗  (@ArthurZucker -- all tests in `pytest tests/models/ -k test_resize_tokens_embeddings` were green) @gante No problem. That is completely fine.
I am sorry for blocking other PRs! Thanks for the fixes! @gante thanks for fixing it!SG to me, why were the tests skipped?  thanks 🤗 ","SG to me, why were the tests skipped?  thanks 🤗 ","Fixes the failures introduced by #33325

The tests failed with mobilebert because of a missing transposing for the `old_lm_head`. This PR fixes that. I have tried the two failed tests locally.
It's weird that all tests passed before merging. EDIT: I see now, some tests were skipped

I have also changed the logic when the covariance matrix is not positive definite, just initialize the new embeddings with the mean if covariance is not positive definite.

c.c. @ArthurZucker

","could have reduce the strictness of the test as well, not sure what's best?  what was this failing?  The pad_toke_id in the config is 98 for the GitModel. This results in an error in the embedding layer because it's higher than the vocab_size
This error appeared in the GitModel after fixing the error which was caused by overwriting the configuration. It was my first choice to do that but the test is pretty loose. with atol = 1e-3 and rtol = 1e-1.
The Recerrunt gemma tests failed with rtol=2.155319929122925 and atol=0.001309454208239913
Making the rtol higher than 2 is not the best choice IMO, so I decided to reduce the covariance, given that the most important thing is to initialize the added embeddings with the mean, the added noise from the covariance is not a big deal.

WDYT? yep good decision!"
33793,2024-09-29T14:34:09Z,2024-10-09T10:16:13Z,VladOS95-cyber,6,9,3,112,4,3,2,[],177.0,0,848524.0,0,0,0,0,6775260.397006,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'VladOS95-cyber']","Hi @SunMarc! This PR is ready for review, please, take a look. Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). Hey @SunMarc! I resolved all comments, you could take a look again > Thanks for iterating ! Could you also add a test to check the weights of each layer for the fp16 model, to see if we get the same model ? We recently merged the gguf falcon model and he added a nice test for that. Also there are a few merge conflits, thanks !

Hey @SunMarc! Sure, it is done The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33793). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hi @LysandreJik! Just a kind reminder to take a look on this PR!Thanks for adding @VladOS95-cyber as always ! Left a few comments Thanks for iterating ! Could you also add a test to check the weights of each layer for the fp16 model, to see if we get the same model ? We recently merged the gguf falcon model and he added a nice test for that. Also there are a few merge conflits, thanks !  LGTM ! Left a question to better understand Thanks @VladOS95-cyber, this seems to have passed through the net. Looks great to me!","Thanks for adding @VladOS95-cyber as always ! Left a few comments Thanks for iterating ! Could you also add a test to check the weights of each layer for the fp16 model, to see if we get the same model ? We recently merged the gguf falcon model and he added a nice test for that. Also there are a few merge conflits, thanks !  LGTM ! Left a question to better understand Thanks @VladOS95-cyber, this seems to have passed through the net. Looks great to me!","# What does this PR do?
Add GGUF support for StableLM
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Link: https://github.com/huggingface/transformers/issues/33260
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
Regarding the task @SunMarc @LysandreJik @ArthurZucker .

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","why did you add that ?  just like bloom, can you also add a fp16 test to see if we get the same result with the transformers model and the converted gguf model ?  Because of **tokenization_utils_fast.py (L120-123)**. If we do not explicitly assign vocab_file and merges_file params just like that, `kwargs.get(""vocab_file"")` returns `None` and `gguf_param = load_gguf_checkpoint(kwargs.get(""vocab_file""))` will not work. The same things we did for Bloom and Qwen Yes, agree, I just added I'm not use to understand why we need to use the original model config ? Since we are using the fp16 model, it shouldn't be quantized.  @SunMarc, because even in fp16 model we use config from gguf file and it is quite different compared with original model config. For example, `use_parallel_residual = True` in quantazied one, while in original model it is false. If it is true, the model skips creating post_attention_layernorm. And `use_qkv_bias = False` in quantized one, and True in original model. If it is false, it skips using q, k, v biases. That's why we explicitly use original model config in order to have all weights and biases correctly created for fair comparison.  Thanks for the explanation ! This makes sense to me. The issue is on the conversion script from hf to gguf then ? If one specify fp16, it shoudn't modify `use_parallel_residual` and `use_qkv_bias`.   So, I suppose yes and no, use_parallel_residual = True is specified in conversion script by default `self.gguf_writer.add_parallel_residual(hparams[""use_parallel_residual""] if ""use_parallel_residual"" in hparams else True)`, `use_qkv_bias` is not specified at all, so by default it is False according to `StableLmConfig` implementation. Oh thx ! That will be very helpful for the next models we add ! "
33870,2024-10-01T16:25:16Z,2024-10-09T11:15:49Z,gante,3,17,15,1692,68,4,3,[],14.0,0,672636.0,0,0,0,0,6771681.482046,,0,15,0,False,"['HuggingFaceDocBuilderDev', 'gante']","Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33870). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. before merging, ran locally:
- `py.test tests/models/ -k greedy` -- all PT tests passing
- `RUN_SLOW=1 py.test tests/models/gpt2/test_modeling_gpt2.py` 
- `RUN_SLOW=1 py.test tests/models/llama/test_modeling_llama.py` (there are two failing tests, but they come from `main`)This is up to mark working efficiently Wow, so much code killed, thanks! Okay good for me, let's fix generate tests if related","This is up to mark working efficiently Wow, so much code killed, thanks! Okay good for me, let's fix generate tests if related","# What does this PR do?

Part of step 6 in #32685
Follow-up to #33677 

This PR:
1. revises `GenerationMixin.prepare_inputs_for_generation` so as to handle models WITHOUT the `Cache` refactor, prepare `token_type_ids`, and forward arbitrary kwargs
2. because of 1., we can remove this function from most decoder-only LLMs 🧹🤗🧹 All decoder-only LLMs were checked
3. added a comment on each overwrite occurring in decoder-only LLMs, for our future selves 

_________________________________
✅ slow tests were ran on `llama` and `gpt2`","Not all models expect this one. We now inspect the signature to determine whether we need to generate them on the fly these are moved to `kwargs`. We now forward `kwargs` to the model inputs :) Just curious: does that mean blenderbot cannot generate from inputs embeds and it cannot be fixed? I see many models touched here didn't pass further inputs embeds, so that mean after this PR all of them will support generation from embeddings. So interesting to see why this model failed I think this was marked flaky for VLMs in one of the other PRs >  I see many models touched here didn't pass further inputs embeds, so that mean after this PR all of them will support generation from embeddings.

Precisely! Many models will get this feature for free as part of these deletions 💛 

> Just curious: does that mean blenderbot cannot generate from inputs embeds and it cannot be fixed?

No clue, I didn't dive deeper :) Failed in `inputs_embeds` tests -> pasted this comment. I don't think these combos of model/feature are worth the dive, so I left this low-information (but better than nothing) note  With this PR, it becomes a failure all the times 👀 I have no idea why (didn't dive) super sad, i started diving a while ago and that seems related to paligemma's weird masking for prefix/suffix. I'll see if I can get time to spot the bug Actually the test was just flaky! I've added flakiness protection to the failing test and deleted a few more cases :) (this test calls `generate`) quick Q, how fast is this / is it slowing down generation? 
- we can store the inspect result if needed otherwise!  nice seen in other PRs, that it needed to be sliced to seq_length no? -seq_len:  not sure this is super efficient TBH! It's not too bad, but can be improved, yes. On my machine, this adds 0.024ms per generated token (small, but not negligible). If we cache the `inspect.signature`, we reduce it by 100x.

We actually make several `inspect.signature(foward)` calls in `generate` and other bits of the codebase, I think it makes sense to store the inspect as a cached model property (e.g. `model.forward_signature`). WDYT? If you agree, I'll open a follow-up PR with this change

For completeness, script to measure the impact of caching this call:
```py
import time
import inspect
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(""distilgpt2"")

# Fresh inspect
all_times = []
for _ in range(1000):
    start = time.time()
    ""position_ids"" in set(inspect.signature(model.forward).parameters.keys())
    all_times.append(time.time() - start)
print(sum(all_times) / len(all_times))

# Cached inspect
signature_keys = set(inspect.signature(model.forward).parameters.keys())
all_times = []
for _ in range(1000):
    start = time.time()
    ""position_ids"" in signature_keys
    all_times.append(time.time() - start)
print(sum(all_times) / len(all_times))
``` Yes, slicing happens in the code block after this one. That code block abstracts slicing to other input names (e.g. `token_type_ids` needs to be sliced exactly like `position_ids` -- and we can add more to this list as needed 🤗 ) Its run time is negligible, even if `kwargs` contains a handful of entries (usually it will only contain one or two). At most 0.001 ms per call :P 

On the plus side, this code block will allow us to generalize this function to VLMs 😉 I think that's worth the super small cost.

```py
import time
import torch

all_times = []
for _ in range(1000):

    model_inputs = {str(i): i for i in range(10)}
    kwargs = {'a': 1, 'b': 2, 'c': torch.zeros((100, 100)), ""0"": 12, ""1"": 3546}

    start = time.time()
    for key, value in kwargs.items():
        if key not in model_inputs:
            model_inputs[key] = value
    all_times.append(time.time() - start)
print(sum(all_times) / len(all_times))
``` makes sense"
33902,2024-10-02T22:37:31Z,2024-10-04T06:21:12Z,ringohoffman,6,0,2,141,18,1,1,[],42484.0,0,550704.0,0,0,0,0,6784879.70245,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'Cyrilvallez', 'KyleMylonakisProtopia']","Hey @ringohoffman, I'm curious about the batch size you used to notice such a difference with the `float()` only, given that `num_logits_to_keep=1` by default? The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33902). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > batch size you used to notice such

I have observed these severe spikes with batch size 1. At this point I always just comment out that piece of code manually given it does not matter for my workflows at all.  Humm, are you using `generate`? If not, are you making sure to pass `num_logits_to_keep=1` to the `forward`? By default it is 0 if not using `generate`. Because a batch size of 1 would result in a tensor of shape `(1, 1, 128k)` for Llama 3.1, which is always low independently of the dtype. We observe these spikes during training and generation, but we are using a different loss than the LM loss, so we don't require full precision during training.  I see, makes sense then, as long as you don't pass `labels` 🤗🚀🚀thanks for being so prompt in fixing! 🤗",🚀🚀thanks for being so prompt in fixing! 🤗,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Follow up to:

* https://github.com/huggingface/transformers/pull/31292
* https://github.com/huggingface/transformers/issues/30860

given that 4.45 has been released and 4.46 is next

Llama 3.1 8B FSDP2 peak inference memory usage with `float()` (18.5GiB):

<img width=""1728"" alt=""float()"" src=""https://github.com/user-attachments/assets/e2aacbd3-f03e-4468-9b0b-33c49572b21e"">

Llama 3.1 8B FSDP2 peak inference memory usage without `float()` (10.6GiB):

<img width=""1728"" alt=""no_float()"" src=""https://github.com/user-attachments/assets/8797e42c-b9b1-40b5-81e5-1e452c3c8917"">

In my environment, ~43% reduction in peak memory usage.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->

@Cyrilvallez @gante @ArthurZucker ",
34006,2024-10-07T13:30:33Z,2024-10-09T07:21:46Z,ArthurZucker,1,0,2,7,1,1,1,[],1519.0,0,150675.0,0,0,0,0,6785727.410828,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34006). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?
Also adds support for full PR links",
33772,2024-09-27T16:52:40Z,2024-10-09T01:30:41Z,yijun-lee,2,8,9,98,2,3,1,[],891172.0,0,981481.0,0,0,0,0,6806794.753995,,0,9,0,False,"['yijun-lee', 'HuggingFaceDocBuilderDev']","I have resolved it! Thank you :) @stevhliu  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33772). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice, thanks for your contribution! May need to fix this in the toctree title","Nice, thanks for your contribution! May need to fix this in the toctree title","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `modular_transformers.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
`transformers`는 opinionated(자기 의견이 강한) 프레임워크이며, 우리의 철학은 다음의 [개념 가이드](./philosophy)에 정의되어 있습니다.
```
opinionated 라는 원문의 내용을 살리는 것이 좋을 것 같아 위와 같이 제안드립니다. ```suggestion
마지막으로, 이 방식은 우리가 줄이고자 하는 상당한 오버헤드를 모델 기여 과정에 추가하게 됩니다. 이 접근 방식은 종종 모델 기여에 모델링 코드(~1,000줄), 프로세서(~500줄), 테스트, 문서 등을 추가해야 합니다. 모델 기여 PR은 대부분 3,000~5,000줄 이상의 코드를 추가하며, 이 중 많은 부분이 boilerpate 코드입니다.
```
원문을 살려서 번역해봤습니다. @mreraser boilerpate 오타난 거 같습니다. boilerplate로 수정 가능할까요?! ```suggestion
모델 사용자는 단일 파일 인터페이스를 임포트하고 사용하게 되므로, 여기에는 변화가 없을 것입니다. 이를 통해 간단한 기여를 가능하게 하면서도 우리의 철학을 유지하는 양쪽의 장점을 결합하고자 합니다.
```
한번 자연스럽게 바꿔봤습니다! ```suggestion
따라서 이는 `# Copied from` 마커의 대체품이며, 이전에 기여된 모델은 앞으로 몇 달 내에 새로운 모듈식 트랜스포머 형식으로 전환될 예정입니다.
``` ```suggestion
“linter”는 상속 구조를 풀어서 모듈화된 파일로부터 모든 단일 파일을 생성하며, Python 사용자들에게는 그 과정이 보이지 않도록 동작합니다. 현재 linter는 **단일** 수준의 상속만을 평탄화합니다.
``` ```suggestion
    title: `transformers`에서의 모듈성
``` Ah sorry, I think we just can't have any other characters in the title here! 😅 

```suggestion
    title: transformers에서의 모듈성
```"
33804,2024-09-30T06:54:19Z,2024-10-09T01:19:37Z,yijun-lee,4,0,4,51,2,1,1,[],16054.0,0,757519.0,0,0,0,0,6807458.832347,,0,4,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'cjfghk5697', 'yijun-lee']","LGTM 💯  LGTM🎯 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33804). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I have resolved it! Thank you :) @stevhliu LGTM, just need to fix the merge conflict :)","LGTM, just need to fix the merge conflict :)","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `image_processing_utils.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",
33607,2024-09-20T05:01:24Z,2024-10-09T01:19:21Z,4N3MONE,1,30,10,320,2,3,0,[],1627600.0,0,1628277.0,0,0,0,0,6807477.045339,,0,10,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33607). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.번역하느라 정말 고생 많으셨습니다!
리뷰를 늦게 달아드려서 정말 죄송해요...🙏

크게 변경한 부분은 없고, 번역 살짝 수정한 부분이랑 anchor 반영해뒀습니다.
근데 이 문서가 anchor가 많아보여서, 확인해보고 반영하면 좋을 것 같습니다...!","번역하느라 정말 고생 많으셨습니다!
리뷰를 늦게 달아드려서 정말 죄송해요...🙏

크게 변경한 부분은 없고, 번역 살짝 수정한 부분이랑 anchor 반영해뒀습니다.
근데 이 문서가 anchor가 많아보여서, 확인해보고 반영하면 좋을 것 같습니다...!","# What does this PR do?

Translated the `<your_file>.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

@junejae, @Jwaminju, @010kim, @boyunJang 


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?

## Who can review? (Final)
May you please review this PR? @stevhliu.","```suggestion
labels = torch.tensor([1]).unsqueeze(0)  # 배치 크기 1
``` ```suggestion
여기서부터는 두 가지 이상의 모델 유형에서 사용되는 일반 모델 출력을 다룹니다. 구체적인 출력 유형은 해당 모델 페이지에 문서화되어 있습니다.
``` ```suggestion
하지만 항상 그런 것은 아닙니다. 일부 모델은 마지막 은닉 상태가 반환될 때 정규화를 적용하거나 다른 후속 프로세스를 적용합니다.
```
glossary를 반영했습니다! ```suggestion
# 모델 출력[[model-outputs]]
```
anchor 추가했습니다 ```suggestion

```
중복된 텍스트가 있어서 제거했습니다. ```suggestion
`outputs` 객체는 [`~modeling_outputs.SequenceClassifierOutput`]입니다.
```
중복된 문구 제거했습니다. ```suggestion
일반적으로 사용할 때와 동일하게 각 속성들에 접근할 수 있으며, 모델이 해당 속성을 반환하지 않은 경우 `None`이 반환됩니다. 예시에서는 `outputs.loss`는 모델에서 계산한 손실이고 `outputs.attentions`는 `None`입니다.
```
조금 더 자연스럽게 번역해봤습니다. ```suggestion
예시에서는 `loss`와 `logits`라는 두 개의 요소가 있습니다. 그러므로,
``` ```suggestion
예시에는 `loss`와 `logits`라는 두 개의 키가 있습니다.
``` ```suggestion
## ModelOutput[[transformers.utils.ModelOutput]]
```
anchor 추가했습니다.
아래에도 비슷한 부분들이 많은데, anchor 반영하게 된다면 나머지도 적용시키는 게 좋을 것 같아요...! ```suggestion
## BaseModelOutput[[transformers.BaseModelOutput]]
``` ```suggestion
## BaseModelOutputWithPooling[[transformers.modeling_outputs.BaseModelOutputWithPooling]]
``` ```suggestion
## BaseModelOutputWithCrossAttentions[[transformers.modeling_outputs.BaseModelOutputWithCrossAttentions]]
``` ```suggestion
## BaseModelOutputWithPoolingAndCrossAttentions[[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions]]
``` ```suggestion
## BaseModelOutputWithPast[[transformers.modeling_outputs.BaseModelOutputWithPast]]
``` ```suggestion
## BaseModelOutputWithPastAndCrossAttentions[[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions]]
``` ```suggestion
## Seq2SeqModelOutput[[transformers.modeling_outputs.Seq2SeqModelOutput]]
``` ```suggestion
## CausalLMOutput[[transformers.modeling_outputs.CausalLMOutput]]
``` ```suggestion
## CausalLMOutputWithCrossAttentions[[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions]]
``` ```suggestion
## CausalLMOutputWithPast[[transformers.modeling_outputs.CausalLMOutputWithPast]]
``` ```suggestion
## MaskedLMOutput[[transformers.modeling_outputs.MaskedLMOutput]]
``` ```suggestion
## Seq2SeqLMOutput[[transformers.modeling_outputs.Seq2SeqLMOutput]]
``` ```suggestion
## NextSentencePredictorOutput[[transformers.modeling_outputs.NextSentencePredictorOutput]]
``` ```suggestion
## SequenceClassifierOutput[[transformers.modeling_outputs.SequenceClassifierOutput]]
``` ```suggestion
## Seq2SeqSequenceClassifierOutput[[transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput]]
``` ```suggestion
## MultipleChoiceModelOutput[[transformers.modeling_outputs.MultipleChoiceModelOutput]]
``` ```suggestion
## TokenClassifierOutput[[transformers.modeling_outputs.TokenClassifierOutput]] ```suggestion
## QuestionAnsweringModelOutput[[transformers.modeling_outputs.QuestionAnsweringModelOutput]]
``` ```suggestion
## Seq2SeqQuestionAnsweringModelOutput[[transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput]]
``` ```suggestion
## Seq2SeqSpectrogramOutput[[transformers.modeling_outputs.Seq2SeqSpectrogramOutput]]
```"
33515,2024-09-16T17:11:10Z,2024-10-09T00:59:31Z,cjfghk5697,1,4,4,140,2,5,1,[],1928058.0,0,1928901.0,0,0,0,0,6808667.119721,,0,4,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33515). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM! Thanks, LGTM!","LGTM! Thanks, LGTM!","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `blip.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR? ","```suggestion
      - local: model_doc/blip
```
blip의 위치가 modeldoc이 아니라 model_doc 폴더에 있어 위와 같이 수정하는 것이 올바를 것 같습니다.
 ```suggestion
*비전-언어 사전 학습(Vision-Language Pre-training, VLP)은 다양한 비전-언어 작업의 성능을 크게 향상시켰습니다. 하지만, 대부분의 기존 사전 학습 모델들은 이해 기반 작업이나 생성 기반 작업 중 하나에서만 뛰어난 성능을 발휘합니다. 또한 성능 향상은 주로 웹에서 수집한 노이즈가 많은 이미지-텍스트 쌍으로 데이터셋의 규모를 키우는 방식으로 이루어졌는데, 이는 최적의 지도 학습 방식이라고 보기 어렵습니다. 본 논문에서는 BLIP이라는 새로운 VLP 프레임워크를 제안합니다. 이 프레임워크는 비전-언어 이해 및 생성 작업 모두에 유연하게 적용될 수 있습니다. BLIP는 캡셔너가 합성 캡션을 생성하고 필터가 노이즈 캡션을 제거하는 부트스트래핑 방법을 통해 웹 데이터의 노이즈를 효과적으로 활용합니다. 우리는 이미지-텍스트 검색(Recall@1에서 +2.7%), 이미지 캡셔닝(CIDEr에서 +2.8%), 그리고 VQA(VQA 점수에서 +1.6%)와 같은 다양한 비전-언어 작업에서 최신 성과를 달성했습니다. 또한 BLIP은 제로샷 방식으로 비디오-언어 작업에 직접 전이될 때도 강력한 일반화 능력을 보여줍니다. 이 논문의 코드, 모델, 데이터셋은 공개되었습니다.*
``` 일부 문장을 수정해보았습니다! 감사합니다!"
33773,2024-09-27T17:43:15Z,2024-10-09T00:57:51Z,yijun-lee,3,1,4,113,2,2,1,[],657549.0,0,976477.0,0,0,0,0,6808767.027002,,0,4,0,False,"['yijun-lee', 'mreraser', 'HuggingFaceDocBuilderDev']",LGTM 👍  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33773). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I have resolved it! Thank you :) @stevhliu Thanks! This should be ready to merge as soon as you've incorporated @cjfghk5697 feedback 😄 ,Thanks! This should be ready to merge as soon as you've incorporated @cjfghk5697 feedback 😄 ,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `biogpt.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
- 이 모델은 `past_key_values`(PyTorch 용)를 입력으로 받을 수 있는데, 이는 이전에 계산된 키/값 어텐션 쌍입니다. 이 값을 사용하면 텍스트 생성 중 이미 계산된 값을 다시 계산하지 않도록 할 수 있습니다. PyTorch에서 `past_key_values` 인수는 BioGptForCausalLM.forward() 메소드에서 자세히 설명되어 있습니다.
```"
33803,2024-09-30T04:29:20Z,2024-10-09T00:57:17Z,yijun-lee,4,1,5,54,2,2,1,[],24846.0,0,764877.0,0,0,0,0,6808803.227279,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'cjfghk5697', 'yijun-lee']",LGTM 💯  LGTM😎 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33803). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I have resolved it! Thank you :) @stevhliu Thank you (just need to resolve the merge conflict)!,Thank you (just need to resolve the merge conflict)!,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `file_utils.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
## 기타 유틸리티 [[transformers.utils._LazyModule]]
```
기타 유틸리티 정도는 원문을 쓰지 않아도 이해할 수 있지 않을까 하는 마음에 제안드립니다."
33801,2024-09-30T03:56:28Z,2024-10-09T00:57:33Z,yijun-lee,3,5,7,153,2,3,1,[],27812.0,0,766865.0,0,0,0,0,6808787.287786,,0,7,0,False,"['yijun-lee', 'mreraser', 'HuggingFaceDocBuilderDev']","review finished 🚀  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33801). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I have resolved it! Thank you :) @stevhliu One minor nit, otherwise thanks for the translation!","One minor nit, otherwise thanks for the translation!","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `openai-gpt.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
*자연어 이해는 텍스트 함의, 질문 응답, 의미 유사성 평가, 문서 분류와 같은 다양한 작업을 포함합니다. 비록 대규모의 레이블이 없는 텍스트 말뭉치가 풍부하기는 하지만, 이러한 특정 작업에 대한 학습을 위한 레이블된 데이터는 부족하여 판별적으로 학습된 모델이 적절하게 성능을 발휘하기 어렵습니다. 우리는 다양한 레이블이 없는 텍스트 말뭉치에 대한 언어 모델의 생성적 사전 학습을 수행하고, 각 특정 과제에 대한 판별적 미세 조정을 수행함으로써 이러한 과제에서 큰 성과를 달성할 수 있음을 보여줍니다. 이전 접근 방식과 달리, 우리는 모델 아키텍처에 최소한의 변화를 요구하면서 효과적인 전이를 달성하기 위해 미세 조정 중에 과제 인식 입력 변환(task-aware input transformation)을 사용합니다. 우리는 자연어 이해를 위한 다양한 벤치마크에서 우리의 접근 방식의 효과를 입증합니다. 우리의 general task-agnostic 모델은 각 과제에 특별히 설계된 아키텍처를 사용하는 판별적으로 학습된 모델보다 뛰어나며, 연구된 12개 과제 중 9개 부문에서 최첨단 성능을 크게 향상시킵니다.*
```
원문 표현을 쓰는 것이 이해에 더 적절한 것 같아 제안드립니다. ```suggestion
*자연어 이해는 텍스트 함의, 질문 응답, 의미 유사성 평가, 문서 분류와 같은 다양한 작업을 포함합니다. 비록 대규모의 레이블이 없는 텍스트 말뭉치가 풍부하기는 하지만, 이러한 특정 작업에 대한 학습을 위한 레이블된 데이터는 부족하여 판별적으로 학습된 모델이 적절하게 성능을 발휘하기 어렵습니다. 우리는 다양한 레이블이 없는 텍스트 말뭉치에 대한 언어 모델의 생성적 사전 학습을 수행하고, 각 특정 과제에 대한 판별적 미세 조정을 수행함으로써 이러한 과제에서 큰 성과를 달성할 수 있음을 보여줍니다. 이전 접근 방식과 달리, 우리는 모델 아키텍처에 최소한의 변화를 요구하면서 효과적인 전이를 달성하기 위해 미세 조정 중에 과제 인식 입력 변환(task-aware input transformation)을 사용합니다. 우리는 자연어 이해를 위한 다양한 벤치마크에서 우리의 접근 방식의 효과를 입증합니다. 우리의 일반적인 과제 불가지론적 모델은 각 과제에 특별히 설계된 아키텍처를 사용하는 판별적으로 학습된 모델보다 뛰어나며, 연구된 12개 과제 중 9개 부문에서 최첨단 성능(state of the art)을 크게 향상시킵니다.*
```
이해에 도움이 되고자 다음과 같이 제안합니다. ```suggestion
[Write With Transformer](https://transformer.huggingface.co/doc/gpt)는 Hugging Face가 만든 웹 애플리케이션으로, 여러 모델의 생성 능력을 보여주며 그중에는 GPT도 포함되어 있습니다.
``` 좀더 연결해봤습니다! This should be indented"
33510,2024-09-16T15:20:44Z,2024-10-09T00:57:03Z,mreraser,3,3,12,103,2,2,1,[],1595153.0,0,1935379.0,1,0,0,0,6808818.353316,,0,12,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'cjfghk5697']",LGTM The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33510). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thank you for your review @stevhliu ! I resolved the conflicts 😄 LGTM (just need to resolve merge conflict)!,LGTM (just need to resolve merge conflict)!,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `swin.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [x] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@yijun-lee, @mreraser, @jungnerd, @cjfghk5697
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
Hello @stevhliu !  May you please review this PR? Thank you! 

Have a nice day :)
","```suggestion
*이 논문은 Swin Transformer라는 새로운 비전 트랜스포머를 소개합니다. 이 모델은 컴퓨터 비전에서 범용 백본(backbone)으로 사용될 수 있습니다. 트랜스포머를 언어에서 비전으로 적용할 때의 어려움은 두 분야 간의 차이에서 비롯되는데, 예를 들어 시각적 객체의 크기가 크게 변동하며, 이미지의 픽셀 해상도가 텍스트의 단어에 비해 매우 높다는 점이 있습니다. 이러한 차이를 해결하기 위해, 우리는 'Shifted Windows'를 이용해 표현을 계산하는 계층적 트랜스포머를 제안합니다. Shifted Windows 방식은 겹치지 않는 로컬 윈도우에서 self-attention 계산을 제한하여 효율성을 높이는 동시에 윈도우 간 연결을 가능하게 합니다. 이 계층적 구조는 다양한 크기의 패턴을 모델링할 수 있는 유연성을 제공하며, 이미지 크기에 비례한 선형 계산 복잡성을 가지고 있습니다. Swin Transformer의 이러한 특징들은 이미지 분류(Imagenet-1K에서 87.3의 top-1 정확도) 및 객체 검출(COCO test-dev에서 58.7의 박스 AP, 51.1의 마스크 AP)과 같은 밀집 예측 작업, 의미적 분할(ADE20K val에서 53.5의 mIoU)과 같은 광범위한 비전 작업에 적합합니다. 이 모델은 COCO에서 이전 최고 성능을 박스 AP에서 +2.7, 마스크 AP에서 +2.6, ADE20K에서 mIoU에서 +3.2를 초과하는 성과를 보여주며, 트랜스포머 기반 모델이 비전 백본으로서의 잠재력을 입증했습니다. 계층적 설계와 Shifted Windows 방식은 순수 MLP 아키텍처에도 유리하게 작용합니다.* 
``` 보다 자연스러운 문장을 위해 조사를 추가했습니다 :) 이외에는 완벽한 것 같습니다! 수고 많으셨습니다 👍 "
33813,2024-09-30T08:51:56Z,2024-10-09T00:56:31Z,yijun-lee,4,0,4,47,2,1,1,[],8615.0,0,749075.0,0,0,0,0,6808851.128162,,0,4,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'cjfghk5697', 'yijun-lee']","LGTM 💯  LGTM🫡 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33813). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I have resolved it! Thank you :) @stevhliu

👍 (just need to fix merge conflict) ",👍 (just need to fix merge conflict) ,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `tokenization_utils.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `tokenization_utils.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",
33601,2024-09-19T17:47:14Z,2024-10-09T00:15:46Z,fabxoe,1,11,8,54,2,4,1,[],1656056.0,0,1664913.0,2,0,0,0,6811295.531048,,0,8,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33601). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.다른 트랜스포머 문서에서는 Transformers를 따로 번역하지 않았던 것으로 기억해서 수정해보았습니다. 💯 ,다른 트랜스포머 문서에서는 Transformers를 따로 번역하지 않았던 것으로 기억해서 수정해보았습니다. 💯 ,"# What does this PR do?

Translated the `main_classes/onnx.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
각 ONNX 설정은 다양한 유형의 토폴로지나 작업의 모델을 익스포트 해주는 _features_ 세트와 관련이 있습니다.
```
export가 glossary에 안올라와 있어서, 임의로 익스포트로 바꿔보았습니다!  ```suggestion
* 인코더 기반 모델은 [`~onnx.config.OnnxConfig`]을 상속받습니다.
``` ```suggestion
* 디코더 기반 모델은 [`~onnx.config.OnnxConfigWithPast`]을 상속받습니다.
``` ```suggestion
* 인코더-디코더 기반 모델은 [`~onnx.config.OnnxSeq2SeqConfigWithPast`]을 상속받습니다.
``` ```suggestion
익스포트하고자하는 모델 아키텍처 유형에 따라 상속받아야 할 세 가지 추상 클래스를 제공합니다:
``` ```suggestion
🤗 트랜스포머는 `transformers.onnx` 패키지를 제공하며, 이 패키지는 설정 객체를 활용하여 모델 체크포인트를 ONNX 그래프로 변환할 수 있게 합니다.
``` ```suggestion
# 🤗 Transformers 모델을 ONNX로 내보내기[[exporting--transformers-models-to-onnx]]
``` ```suggestion
🤗 Transformers는 `transformers.onnx` 패키지를 제공하며, 패키지를 통해 설정 객체를 활용함으로써 모델 체크포인트를 ONNX 그래프로 변환할 수 있게 합니다.
``` ```suggestion
🤗 Transformers에 대한 자세한 내용은 [이 가이드](../serialization)를 참조하세요.
``` ```suggestion
내보내려는(export) 모델 아키텍처의 유형에 따라 상속받아야 할 세 가지 추상 클래스를 제공합니다:
``` ```suggestion
각 ONNX 설정은 다양한 유형의 토폴로지나 작업에 대해 모델을 내보낼 수 있게(exporting) 해주는 _features_ 세트와 연관되어 있습니다.
```"
33968,2024-10-05T04:38:49Z,2024-10-09T00:15:33Z,fabxoe,1,6,5,151,2,3,1,[],320938.0,0,329804.0,0,0,0,0,6811310.691127,,0,5,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33968). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.간단한 리뷰 남겨보았습니다! 좋은 번역 감사드립니다🤗 👍 ,간단한 리뷰 남겨보았습니다! 좋은 번역 감사드립니다🤗 👍 ,"# What does this PR do?

Translated the `model_doc/deberta-v2.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
*사전 학습된 신경망 언어 모델의 최근 발전은 많은 자연어 처리(NLP) 작업의 성능을 크게 향상시켰습니다. 본 논문에서는 두 가지 새로운 기술을 사용하여 BERT와 RoBERTa 모델을 개선한 새로운 모델 구조인 DeBERTa를 제안합니다. 첫 번째는 분리된 어텐션 메커니즘으로, 각 단어가 내용과 위치를 각각 인코딩하는 두 개의 벡터로 표현되며, 단어들 간의 어텐션 가중치는 내용과 상대적 위치에 대한 분리된 행렬을 사용하여 계산됩니다. 두 번째로, 모델 사전 학습을 위해 마스킹된 토큰을 예측하는 출력 소프트맥스 층을 대체하는 향상된 마스크 디코더가 사용됩니다. 우리는 이 두 가지 기술이 모델 사전 학습의 효율성과 다운스트림 작업(모델을 특정 태스크에 적용하는 것)의 성능을 크게 향상시킨다는 것을 보여줍니다. RoBERTa-Large와 비교했을 때, 절반의 학습 데이터로 학습된 DeBERTa 모델은 광범위한 NLP 작업에서 일관되게 더 나은 성능을 보여주며, MNLI에서 +0.9%(90.2% vs 91.1%), SQuAD v2.0에서 +2.3%(88.4% vs 90.7%), RACE에서 +3.6%(83.2% vs 86.8%)의 성능 향상을 달성했습니다. DeBERTa 코드와 사전 학습된 모델은 https://github.com/microsoft/DeBERTa 에서 공개될 예정입니다.*
```

다운스트림이라는 표현에 대해 독자의 입장에서 좀 더 이해할 수 있게 간단한 설명을 병치하면 좋을 것 같아 제안드립니다😊 ```suggestion
- **nGiE(n그램 기반 입력 인코딩)** DeBERTa-v2 모델은 입력 토큰들의 지역적 의존성을 더 잘 학습하기 위해 첫 번째 트랜스포머 층과 함께 추가적인 합성곱 층을 사용합니다.
``` ```suggestion
[DeBERTa](https://huggingface.co/DeBERTa) 모델의 텐서플로 2.0 구현은 [kamalkraj](https://huggingface.co/kamalkraj)에 의해 기여되었습니다. 원본 코드는 [이곳](https://github.com/microsoft/DeBERTa)에서 확인하실 수 있습니다.
``` ```suggestion
## 개요
``` ```suggestion
- **nGiE[n그램 유도(Induced) 입력 인코딩]** DeBERTa-v2 모델은 입력 토큰들의 지역적 의존성을 더 잘 학습하기 위해 첫 번째 트랜스포머 층과 함께 추가적인 합성곱 층을 사용합니다.
``` ```suggestion
## 자료
```"
33951,2024-10-04T16:46:34Z,2024-10-09T00:14:42Z,fabxoe,1,8,7,127,2,3,1,[],363702.0,0,372488.0,1,0,0,0,6811362.508542,,0,7,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33951). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.좋은 번역 감사합니다:) fix: resolve suggestions Nice, thank you! 🤗 ","좋은 번역 감사합니다:) fix: resolve suggestions Nice, thank you! 🤗 ","# What does this PR do?

Translated the `model_doc/dbrx.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
`generate()` 메소드는 DBRX를 사용하여 텍스트를 생성하는 데 사용될 수 있습니다. 표준 어텐션 구현뿐만 아니라, 플래시 어텐션과 PyTorch의 스케일된 내적 어텐션(Scaled Dot-Product Attention)을 사용하여 생성할 수 있습니다. 후자의 두 어텐션 구현 방식은 처리 속도를 크게 높여줍니다.
```
점곱 어텐션을 내적 어텐션으로 바꾸어보았습니다. 
 ```suggestion
`pip install flash-attn`를 통해 플래시 어텐션을 설치했다면, 더 빠른 생성이 가능합니다. (플래시 어텐션에 대한 HuggingFace 문서는 [이곳](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2)에서 확인할 수 있습니다.)
```
링크 부분의 괄호 위치를 추가했고, 어순을 조금 바꾸어 보았습니다. ```suggestion
PyTorch의 스케일된 내적 어텐션을 사용하여도 더 빠른 생성이 가능합니다. (스케일된 내적 어텐션에 대한 HuggingFace 문서는 [이곳](https://huggingface.co/docs/transformers/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)에서 확인할 수 있습니다.)
``` ```suggestion
이 모델은 [eitan-turok](https://huggingface.co/eitanturok)와 [abhi-db](https://huggingface.co/abhi-db)가 기여했습니다. 원본 코드는 [이곳](https://github.com/databricks/dbrx-instruct)에서 찾을 수 있지만, 최신 버전이 아닐 수 있습니다.
```
띄어쓰기 및 능동 표현으로 고쳐보았습니다. ```suggestion
## 개요[[overview]]
``` ```suggestion
DBRX는 [트랜스포머 기반의](https://www.isattentionallyouneed.com/) 다음 토큰을 예측하는 디코더 전용 LLM 모델입니다.
``` ```suggestion
`pip install flash-attn`를 통해 플래시 어텐션을 설치하면, 더 빠른 생성이 가능합니다. (플래시 어텐션에 대한 HuggingFace 문서는 [이곳](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2)에서 확인할 수 있습니다.)
``` ```suggestion
`generate()` 메소드는 DBRX를 사용하여 텍스트를 생성하는 데 사용될 수 있습니다. 표준 어텐션 구현, 플래시 어텐션, PyTorch의 스케일된 내적 어텐션(Scaled Dot-Product Attention)을 사용하여 생성할 수 있습니다. 후자의 두 어텐션 구현 방식은 처리 속도를 크게 높여줍니다.
```"
33885,2024-10-02T12:11:06Z,2024-10-09T00:14:25Z,fabxoe,1,3,4,141,2,4,1,[],552999.0,0,561799.0,0,0,0,0,6811379.759197,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33885). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: docs suggestion Thanks! 🤗 ,fix: docs suggestion Thanks! 🤗 ,"# What does this PR do?

Translated the `model_doc/cohere.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
## 개요[[overview]]
``` ```suggestion
bitsandbytes 라이브러리를 이용해서 4bit 양자화된 모델 로딩
``` ```suggestion
## 사용 팁[[usage-tips]]
```"
33648,2024-09-21T15:38:08Z,2024-10-09T00:14:12Z,fabxoe,1,20,6,235,2,5,1,[],1490983.0,0,1499764.0,1,0,0,0,6811394.187117,,0,6,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33648). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.전체적으로 번역이 잘 되어 있어 글이 잘 읽히네요 ㅎㅎ 띄어쓰기 부분만 간단하게 수정 요청드립니다 😊 fix: docs suggestions LGTM, thank you!","전체적으로 번역이 잘 되어 있어 글이 잘 읽히네요 ㅎㅎ 띄어쓰기 부분만 간단하게 수정 요청드립니다 😊 fix: docs suggestions LGTM, thank you!","# What does this PR do?

Translated the `model_doc/mistral.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
미스트랄은 Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed가 작성한 [이 블로그 포스트](https://mistral.ai/news/announcing-mistral-7b/)에서 소개되었습니다.
``` ```suggestion
미스트랄-7B는 [mistral.ai](https://mistral.ai/)에서 출시한 첫 번째 대규모 언어 모델(LLM)입니다.
``` ```suggestion
- 바이트 폴백(Byte-fallback) BPE 토크나이저: 문자들이 절대 어휘 목록 외의 토큰으로 매핑되지 않도록 보장합니다.
``` ```suggestion
지시 조정 모델은 입력이 올바른 형식으로 준비되도록 [채팅 템플릿](../chat_templating)을 적용해야 합니다.
```
문장 구조를 조금 더 간결하게 수정해보았습니다. ```suggestion
## 개요[[overview]]
``` ```suggestion
*미스트랄 AI팀은 현존하는 언어 모델 중 크기 대비 가장 강력한 미스트랄7B를 출시하게 되어 자랑스럽습니다.*
```
더 자연스러운 것 같아 조금 변경했습니다! ```suggestion
- GQA(Grouped Query Attention): 더 빠른 추론과 더 작은 캐시 크기를 구현할 수 있습니다.
``` ```suggestion
- 지시 조정 모델인 [미스트랄-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)은 지도 미세 조정(SFT)과 직접 선호도 최적화(DPO)를 사용한 채팅에 최적화된 기본 모델입니다.
``` ```suggestion
하드웨어와 플래시 어텐션2의 호환여부를 확인하세요. 이에 대한 자세한 내용은 [플래시 어텐션 저장소](https://github.com/Dao-AILab/flash-attention)의 공식 문서에서 확인할 수 있습니다. 또한 모델을 반정밀도(예: `torch.float16`)로 불러와야합니다. ```suggestion
- 2024년에 Hugging Face 도구를 사용해 LLM을 미세 조정하는 방법에 대한 [블로그 포스트](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl). 🌎
``` ```suggestion
- GQA(Grouped Query Attention): 더 빠른 추론이 가능하고 더 작은 크기의 캐시를 사용합니다.
``` 의역했습니다 ```suggestion
- 바이트 폴백(Byte-fallback) BPE 토크나이저: 문자들이 out of vocabulary token으로 절대 매핑되지 않습니다.
``` out of vocabulary tokens > OOV token을 의미합니다. 그로서리에 예시가 없고, 해석 용례도 찾지 못했습니다. 해석하면 어색해져 영문 표기 제안합니다. ```suggestion
플래시 어텐션2와 하드웨어가 호환하는지 확인이 필요합니다. 이에 대한 자세한 내용은 [플래시 어텐션 저장소](https://github.com/Dao-AILab/flash-attention)의 공식 문서에서 확인할 수 있습니다. 또한 모델을 반정밀도(예: `torch.float16`)로 불러와야 합니다.
``` ```suggestion
이 모델은 [Younes Belkada](https://huggingface.co/ybelkada)와 [Arthur Zucker](https://huggingface.co/ArthurZ)가 기여했습니다.
``` ```suggestion
미스트랄을 시작하는 데 도움이 되는 Hugging Face와 community 자료 목록(🌎로 표시됨) 입니다. 여기에 포함될 자료를 제출하고 싶으시다면 PR(Pull Request)를 열어주세요. 리뷰해 드리겠습니다! 자료는 기존 자료를 복제하는 대신 새로운 내용을 담고 있어야 합니다.
``` ```suggestion
- Hugging Face의 [정렬(Alignment) 핸드북](https://github.com/huggingface/alignment-handbook)에는 미스트랄-7B를 사용한 지도형 미세 조정(SFT) 및 직접 선호 최적화(DPO)를 수행하기 위한 스크립트와 레시피가 포함되어 있습니다. 여기에는 단일 GPU에서 QLoRa 및 다중 GPU를 사용한 전체 미세 조정을 위한 스크립트가 포함되어 있습니다.
```  [Alignment Handbook]은 SFT와 DPO로 alignment 방법을 설명하는 쿡북 같습니다. ```suggestion
## 사용 팁[[usage-tips]]
```"
33635,2024-09-20T19:24:55Z,2024-10-09T00:13:58Z,fabxoe,1,11,9,82,2,5,1,[],1563813.0,0,1572543.0,1,0,0,0,6811408.947407,,0,9,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33635). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.간단하게 몇 가지 수정사항 제안드려봅니다! 꼼꼼하게 번역해주셔서 많이 배우고 가요🤗 fix: docs suggestions Thanks for the translation!,간단하게 몇 가지 수정사항 제안드려봅니다! 꼼꼼하게 번역해주셔서 많이 배우고 가요🤗 fix: docs suggestions Thanks for the translation!,"# What does this PR do?

Translated the `model_doc/llama3.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
*오늘, 광범위한 사용을 위해 이용 가능한 라마의 차세대 모델인 메타 라마3의 첫 두 모델을 공유하게 되어 기쁩니다. 이번 출시는 8B와 70B 매개변수를 가진 사전 훈련 및 지시 미세 조정된 언어 모델을 특징으로 하며, 광범위한 사용 사례를 지원할 수 있습니다. 라마의 이 차세대 모델은 다양한 산업 벤치마크에서 최첨단의 성능을 보여주며, 개선된 추론 능력을 포함한 새로운 기능을 제공합니다. 우리는 이것들이 단연코 해당 클래스에서 최고의 오픈 소스 모델이라고 믿습니다. 오랜 개방적 접근 방식을 유지하며, 우리는 라마3를 커뮤니티에 맡기고 있습니다. 애플리케이션에서 개발자 도구, 평가, 추론 최적화 등에 이르기까지 AI 스택 전반에 걸친 다음 혁신의 물결을 촉발하길 희망합니다. 여러분이 무엇을 만들지 기대하며 여러분의 피드백을 고대합니다.*
```
""커뮤니티의 손에""라는 표현이 조금 어색해서 수정해보았습니다. ```suggestion
    이 스크립트를 실행시키려면 모델 전체를 float16 정밀도로 호스팅할 수 있는 충분한 메인메모리가 필요하다는 점을 유의하세요. 가장 큰 버전이 여러 체크포인트로 나뉘어 있더라도, 각 체크포인트가 모델의 가중치 일부를 포함하고 있기 때문에 이를 모두 RAM에 로드해야 합니다. 75B 모델을 예로 들면 대략 145GB의 RAM이 필요합니다. 
```
앞 부분에서 RAM이라고 표기되어서 램 -> RAM으로 수정해보았습니다. 오버뷰 -> 개요 ```suggestion
*오늘, 범용적으로 이용 가능한 라마의 차세대 모델, 메타 라마3의 첫 두 모델을 공유하게 되어 기쁩니다. 이번 출시의 특징으로는, 범용적인 사용 사례들을 지원하는 매개변수 8B와 70B를 가진 사전 훈련 및 지시 미세 조정된 언어 모델에 있습니다. 라마의 이 차세대 모델은 다양한 산업 벤치마크에서 최첨단의 성능을 보여주며, 개선된 추론 능력을 포함한 새로운 기능을 제공합니다. 우리는 이 모델이 해당 클래스에서 최고의 오픈 소스 모델이라 자부합니다. 오랜 기간 이어온 우리의 개방적인 방식을 지지하며, 라마3를 커뮤니티의 손에 맡기고 있습니다. 애플리케이션에서 개발자 도구, 평가, 추론 최적화 등에 이르기까지 AI 스택 전반에 걸친 차세대 혁신의 물결을 일으키길 희망합니다. 여러분이 무엇을 만들지 기대하며 여러분들의 피드백을 기대합니다.*
``` ```suggestion
 `model = AutoModelForCausalLM.from_pretrained(""path"", torch_dtype = ""auto"")`를 사용하여 모델을 초기화할 때, 온라인 가중치의 `dtype`는 `torch_dtype=""auto""`를 사용하지 않는 한 대부분 무관합니다. 그 이유는 모델이 먼저 다운로드되고(온라인 체크포인트의 `dtype`를 사용), 그 다음 `torch`의 `dtype`으로 변환되어(`torch.float32`가 됨), 마지막으로 config에 `torch_dtype`이 제공된 경우 가중치가 사용되기 때문입니다.
```
좀 더 읽기 쉽게 바꿔보았습니다>! ```suggestion
## 자료[[resources]]
``` ```suggetstion
이미 [라마2](./llama2) 문서 페이지에서는 수 많은 유익한 자료들이  제공되고 있으며, 이곳에 라마3에 대한 새로운 자료를 더해주실 컨트리뷰터들을 초대합니다! 🤗
``` ```suggestion
## 사용 팁[[usage-tips]]
``` ```suggestion
## 개요[[overview]]
``` ```suggestion
*오늘, 광범위한 사용을 위해 이용 가능한 라마의 차세대 모델인 메타 라마3의 첫 두 모델을 공유하게 되어 기쁩니다. 이번 출시는 8B와 70B 매개변수를 가진 사전 훈련 및 지시 미세 조정된 언어 모델을 특징으로 하며, 광범위한 사용 사례를 지원할 수 있습니다. 라마의 이 차세대 모델은 다양한 산업 벤치마크에서 최첨단의 성능을 보여주며, 개선된 추론 능력을 포함한 새로운 기능을 제공합니다. 우리는 이것들이 단연코 해당 클래스에서 최고의 오픈 소스 모델이라고 믿습니다. 오랜 개방적 접근 방식을 지지하며, 우리는 라마3를 커뮤니티 기여자들에게 맡기고 있습니다. 애플리케이션에서 개발자 도구, 평가, 추론 최적화 등에 이르기까지 AI 스택 전반에 걸친 다음 혁신의 물결을 촉발하길 희망합니다. 여러분이 무엇을 만들지 기대하며 여러분의 피드백을 고대합니다.*
``` ```suggestion
[라마2](./llama2) 문서 페이지에서는 이미 수 많은 멋지고 유익한 자료들을 제공하고 있습니다. 이곳에 라마3에 대한 새로운 자료를 더해주실 컨트리뷰터들을 초대합니다! 🤗
```"
33612,2024-09-20T08:01:06Z,2024-10-09T00:13:25Z,fabxoe,1,8,7,82,2,3,1,[],1604855.0,0,1613539.0,1,0,0,0,6811442.213457,,0,7,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33612). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice, thanks! :)","Nice, thanks! :)","# What does this PR do?

Translated the `model_doc/paligemma.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","오버뷰 -> 개요 ```suggestion
PaliGemma 모델은 구글이 제안한 [PaliGemma – Google의 최첨단 오픈 비전 언어 모델](https://huggingface.co/blog/paligemma)에서 소개 되었습니다. PaliGemma는 [SigLIP](siglip) 비전 인코더와 [Gemma](gemma) 언어 인코더로 구성된 3B 규모의 비전-언어 모델로, 두 인코더가 멀티모달 선형 프로젝션으로 연결되어 있습니다. 이 모델은 이미지를 고정된 수의 VIT토큰으로 분할하고 이를 선택적 프롬프트 앞에 추가 하며, 모든 이미지 토큰과 입력 텍스트 토큰에 대해 전체 블록 어텐션을 사용하는 특징을 가지고 있습니다.
```
projection이 그로서리에 없어서,, 투영으로 안하고 프로젝션으로 표기했습니다. ```suggestion
PaliGemma의 추론 수행
``` ```suggestion
- 모델에 필요한 이미지, 텍스트 및 선택적 레이블을 준비하는데 `PaliGemmaProcessor`를 사용할 수 있습니다. PaliGemma 모델을 미세 조정할 때는, 프로세서에 `suffix`인자를 전달하여 다음 처럼 모델의 `labels`를 생성할 수 있습니다:
``` 리소스 -> 자료 ```suggestion
## 개요[[overview]]
``` ```suggestion
## 자료[[resources]]
``` ```suggestion
## 사용 팁[[usage-tips]]
```"
33610,2024-09-20T06:57:22Z,2024-10-09T00:13:07Z,fabxoe,1,16,18,322,2,4,1,[],1608693.0,0,1617345.0,1,0,0,0,6811461.651886,,0,18,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33610). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: docs suggestions LGTM! 🤗 ,fix: docs suggestions LGTM! 🤗 ,"# What does this PR do?

Translated the `model_doc/clip.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
## 개요[[overview]]
``` ```suggestion
*최신 컴퓨터 비전 시스템은 미리 정해진 고정된 객체 카테고리 집합을 예측하도록 훈련됩니다. 이러한 제한된 형태의 지도는 다른 시각적 개념을 지정하기 위해 추가적인 라벨링된 데이터가 필요하므로 그들의 일반성과 사용성을 제한합니다. 이미지 원시 텍스트에서 직접 학습하는 것은 훨씬 더 광범위한 지도 소스를 활용하는 유망한 대안입니다. 우리는 이미지와 캡션을 맞추는 간단한 사전 학습 작업이, 인터넷에서 수집한 4억 쌍의 이미지-텍스트 데이터셋에서 SOTA 수준의 이미지 표현을 처음부터 효율적이고 확장 가능하게 학습하는 방법임을 보여줍니다. 사전 훈련 후, 자연어는 학습된 시각적 개념을 참조하거나 새로운 개념을 설명하는 데 사용되어 모델의 하위 작업으로의 제로샷 전이를 가능하게 합니다. 우리는 OCR, 비디오 내 행동 인식, 지리적 위치 파악, 그리고 많은 종류의 세밀한 객체 분류 등 30개 이상의 다양한 기존 컴퓨터 비전 데이터셋에 대한 벤치마킹을 통해 이 접근 방식의 성능을 연구합니다. 이 모델은 대부분의 작업에 대해 의미 있게 전이되며, 종종 데이터셋별 훈련 없이도 완전 지도 학습 기준선과 경쟁력 있는 성능을 보입니다. 예를 들어, 우리는 ImageNet에서 원래 ResNet-50의 정확도를 제로샷으로 일치시키는데, 이는 ResNet-50이 훈련된 128만 개의 훈련 예제를 전혀 사용할 필요가 없었습니다. 코드 및 사전 훈련된 모델 가중치는 이 https URL에서 공개합니다.*
``` ```suggestion
**이미지 검색**
``` ```suggestion
**설명 가능성**
``` ```suggestion
- 어떻게 사전학습된 CLIP모델을 이미지 캡셔닝을 위한 빔서치 추론에 활용하는지에 대한 [노트북](https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing)
``` ```suggestion
## 자료[[resources]]
``` ```suggestion
### 플래시 어텐션과 스케일된 점곱 어텐션(SDPA)으로 인해 예상되는 속도향상[[expected-speedups-with-flash-attention-and-sdpa]]
``` ```suggestion
먼저 최신버전의 플래시 어텐션2를 설치합니다.
``` ```suggestion
플래시 어텐션2와 호환되는 하드웨어를 가지고 있는지 확인하세요. 이에 대한 자세한 내용은 flash-attn 리포지토리의 공식문서에서 확인할 수 있습니다. 또한 모델을 반정밀도(`torch.float16`)로 로드하는 것을 잊지 마세요.
``` ```suggestion
## 사용 팁과 예시[[usage-tips-and-example]]
``` ```suggestion
### 스케일된 점곱 어텐션 (Scaled dot-product Attention(SDPA)) 사용하기[[using-scaled-dot-product-attention-sdpa]]
``` ```suggestion
### 플래시 어텐션과 스케일된 내적 어텐션(SDPA)으로 인해 예상되는 속도향상[[expected-speedups-with-flash-attention-and-sdpa]]
``` ```suggestion
### 스케일된 내적 어텐션 (Scaled dot-product Attention(SDPA)) 사용하기[[using-scaled-dot-product-attention-sdpa]]
``` ```suggestion
파이토치는 `torch.nn.functional`의 일부로 네이티브 스케일된 내적 어텐션(SPDA) 연산자를 포함하고 있습니다. 이 함수는 입력과 사용 중인 하드웨어에 따라 적용될 수 있는 여러 구현을 포함합니다. 자세한 정보는 [공식문서](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)나 [GPU 추론](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention) 페이지를 참조하세요.
``` ```suggestion
*최신 컴퓨터 비전 시스템은 미리 정해진 고정된 객체 카테고리 집합을 예측하도록 훈련됩니다. 이러한 제한된 형태의 지도는 다른 시각적 개념을 지정하기 위해 추가적인 라벨링된 데이터가 필요하므로 그 일반성과 사용성을 제한합니다. 이미지 원시 텍스트에서 직접 학습하는 것은 훨씬 더 광범위한 지도 소스를 활용하는 아주 좋은 대안입니다. 이미지와 캡션을 맞추는 간단한 사전 학습 작업이, 인터넷에서 수집한 4억 쌍의 이미지-텍스트 데이터셋에서 SOTA 수준의 이미지 표현을 처음부터 효율적이고 확장 가능하게 학습하는 방법임을 확인할 수 있습니다. 사전 훈련 후, 자연어는 학습된 시각적 개념을 참조하거나 새로운 개념을 설명하는 데 사용되어 모델의 하위 작업으로의 제로샷 전이를 가능하게 합니다. 해당 논문에서는 OCR, 비디오 내 행동 인식, 지리적 위치 파악, 그리고 많은 종류의 세밀한 객체 분류 등 30개 이상의 다양한 기존 컴퓨터 비전 데이터셋에 대한 벤치마킹을 통해 이 접근 방식의 성능을 연구합니다. 이 모델은 대부분의 작업에 대해 의미 있게 전이되며, 종종 데이터셋별 훈련 없이도 완전 지도 학습 기준선과 경쟁력 있는 성능을 보입니다. 예를 들어, ImageNet에서 원래 ResNet-50의 정확도를 제로샷으로 일치시키는데, 이는 ResNet-50이 훈련된 128만 개의 훈련 예제를 전혀 사용할 필요가 없었습니다. 코드 및 사전 훈련된 모델 가중치는 이 https URL에서 공개합니다.*
``` ```suggestion
CLIP은 멀티모달 비전 밒 언어 모델입니다. 이미지-텍스트 유사도 계산과 제로샷 이미지 분류에 사용될 수 있습니다. CLIP은 ViT와 유사한 트랜스포머를 사용하여 시각적 특징을 추출하고, 인과적 언어 모델을 사용하여 텍스트 특징을 추출합니다. 그 후 텍스트와 시각적 특징 모두 동일한 차원의 잠재(latent) 공간으로 투영됩니다. 투영된 이미지와 텍스트 특징 사이의 내적이 유사도 점수로 사용됩니다.
```"
33587,2024-09-19T11:07:41Z,2024-10-09T00:11:48Z,fabxoe,1,5,6,97,2,3,1,[],1680002.0,0,1688648.0,0,0,0,0,6811540.218214,,0,6,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33587). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: docs suggestions fix: resolve suggestions 👍 ,fix: docs suggestions fix: resolve suggestions 👍 ,"# What does this PR do?

Translated the `model_doc/patchtsmixer.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
## 사용 예[[usage-example]]
``` ```suggestion
## 사용 팁[[usage-tips]]
``` ```suggestion
## 자료[[resources]]
``` ```suggestion
## 개요[[overview]]
``` ```suggestion
[wmgifford](https://huggingface.co/wmgifford), [kashif](https://huggingface.co/kashif)가 기여했습니다.
```"
33574,2024-09-18T17:42:17Z,2024-10-09T00:11:19Z,fabxoe,1,3,4,54,2,2,1,[],1742742.0,0,1751342.0,0,0,0,0,6811570.365534,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33574). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: resolve suggestions Thanks for the translation!,fix: resolve suggestions Thanks for the translation!,"# What does this PR do?

Translated the `model_doc/autoformer.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [ ] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave-->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
시작하는 데 도움이 되는 Hugging Face와 community 자료 목록(🌎로 표시됨) 입니다. 여기에 포함될 자료를 제출하고 싶으시다면 PR(Pull Request)를 열어주세요. 리뷰 해드리겠습니다! 자료는 기존 자료를 복제하는 대신 새로운 내용을 담고 있어야 합니다.
``` ```suggestion
## 개요[[overview]]
``` ```suggestion
## 자료[[resources]]
```"
33626,2024-09-20T15:12:23Z,2024-10-09T00:11:11Z,fabxoe,1,8,4,107,2,4,1,[],1578953.0,0,1587528.0,1,0,0,0,6811579.784931,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33626). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: docs suggestions 💯 ,fix: docs suggestions 💯 ,"# What does this PR do?

Translated the `model_doc/mamba.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
## 개요[[overview]]
``` ```suggestion
논문 초록:
``` ```suggestion
*현재 딥러닝에서 흥미로운 응용 프로그램을 구동하는 대부분의 기초 모델들은 거의 보편적으로 트랜스포머 아키텍처와 그 핵심 어텐션 모듈을 기반으로 합니다. 선형 어텐션, 게이트된 컨볼루션과 순환 모델, 구조화된 상태 공간 모델(SSM) 등 많은 준이차시간(subquadratic-time) 아키텍처가 긴 시퀀스에 대한 트랜스포머의 계산 비효율성을 해결하기 위해 개발되었지만, 언어와 같은 중요한 양식에서는 어텐션만큼 성능을 내지 못했습니다. 우리는 이러한 모델의 주요 약점이 내용 기반 추론을 수행하지 못한다는 점임을 알고 몇 가지를 개선했습니다. 첫째, SSM 매개변수를 입력의 함수로 만드는 것만으로도 이산 양식에 대한 약점을 해결할 수 있어, 현재 토큰에 따라 시퀀스 길이 차원을 따라 정보를 선택적으로 전파하거나 잊을 수 있게 합니다. 둘째, 이러한 변경으로 효율적인 컨볼루션을 사용할 수 없게 되었지만, 우리는 순환 모드에서 하드웨어를 인식하는 병렬 알고리즘을 설계했습니다. 우리는 이러한 선택적 SSM을 어텐션이나 MLP 블록도 없는 단순화된 종단간 신경망 아키텍처인 맘바에 통합시켰습니다. 맘바는 빠른 추론(트랜스포머보다 5배 높은 처리량)과 시퀀스 길이에 대한 선형 확장성을 누리며, 백만 길이 시퀀스까지 실제 데이터에서 성능이 향상됩니다. 일반적인 시퀀스 모델 백본으로서 맘바는 언어, 오디오, 유전체학과 같은 여러 양식에서 최첨단 성능을 달성합니다. 언어 모델링에서 우리의 맘바-3B 모델은 같은 크기의 트랜스포머를 능가하고 두 배 크기의 트랜스포머와 맞먹는 성능을 보이며, 사전 훈련과 다운스트림 평가 모두에서 성능을 나타납니다.*
``` ```suggestion
- 맘바는 고전적인 트랜스포머와 견줄 만한 새로운 `상태 공간 모델` 아키텍처입니다. 이는 구조화된 상태 공간 모델의 발전 선상에 있으며, [플래시어텐션](https://github.com/Dao-AILab/flash-attention)의 정신을 따르는 효율적인 하드웨어 인식 설계와 구현을 특징으로 합니다.
``` ```suggestion
- 단순구현의 성능을 빠르게 향상시키는 기여를 환영합니다.🤗
``` ```suggestion
*현재 딥러닝의 대부분의 흥미로운 응용 프로그램을 구동하는 기초 모델들은 거의 보편적으로 트랜스포머 아키텍처와 그 핵심 어텐션 모듈을 기반으로 합니다. 선형 어텐션, 게이트된 컨볼루션과 순환 모델, 구조화된 상태 공간 모델(SSM) 등 많은 준이차시간(subquadratic-time) 아키텍처가 긴 시퀀스에 대한 트랜스포머의 계산 비효율성을 해결하기 위해 개발되었지만, 언어와 같은 중요한 양식에서는 어텐션만큼 성능을 내지 못했습니다. 우리는 이러한 모델의 주요 약점이 내용 기반 추론을 수행하지 못한다는 점을 식별하고 몇 가지 개선을 했습니다. 첫째, SSM 매개변수를 입력의 함수로 만드는 것만으로도 이산 모달리티(discrete modalities)의 약점을 해결할 수 있어, 현재 토큰에 따라 시퀀스 길이 차원을 따라 정보를 선택적으로 전파하거나 잊을 수 있게 합니다. 둘째, 이러한 변경으로 효율적인 컨볼루션을 사용할 수 없게 되었지만, 우리는 순환 모드에서 하드웨어를 인식하는 병렬 알고리즘을 설계했습니다. 우리는 이러한 선택적 SSM(Selective State Spaces Model)을 어텐션이나 MLP 블록도 없는 단순화된 종단간 신경망 아키텍처인 맘바에 통합시켰습니다. 맘바는 빠른 추론(트랜스포머보다 5배 높은 처리량)과 시퀀스 길이에 대한 선형 확장성을 누리며, 백만 길이 시퀀스까지 실제 데이터에서 성능이 향상됩니다. 일반적인 시퀀스 모델 백본으로서 맘바는 언어, 오디오, 유전체학과 같은 여러 양식에서 최첨단 성능을 달성합니다. 언어 모델링에서 우리의 맘바-3B 모델은 같은 크기의 트랜스포머를 능가하고 두 배 크기의 트랜스포머와 맞먹는 성능을 보이며, 이는 사전 훈련과 다운스트림 평가 모두에서 나타납니다.*
``` ```suggestion
*현재 딥러닝에서 흥미로운 응용 프로그램을 구동하는 대부분의 기초 모델들은 거의 보편적으로 트랜스포머 아키텍처와 그 핵심 어텐션 모듈을 기반으로 합니다. 선형 어텐션, 게이트된 컨볼루션과 순환 모델, 구조화된 상태 공간 모델(SSM) 등 많은 준이차시간(subquadratic-time) 아키텍처가 긴 시퀀스에 대한 트랜스포머의 계산 비효율성을 해결하기 위해 개발되었지만, 언어와 같은 중요한 양식에서는 어텐션만큼 성능을 내지 못했습니다. 우리는 이러한 모델의 주요 약점이 내용 기반 추론을 수행하지 못한다는 점임을 알고 몇 가지를 개선했습니다. 첫째, SSM 매개변수를 입력의 함수로 만드는 것만으로도 이산 모달리티(discrete modalities)의 약점을 해결할 수 있어, 현재 토큰에 따라 시퀀스 길이 차원을 따라 정보를 선택적으로 전파하거나 잊을 수 있게 합니다. 둘째, 이러한 변경으로 효율적인 컨볼루션을 사용할 수 없게 되었지만, 우리는 순환 모드에서 하드웨어를 인식하는 병렬 알고리즘을 설계했습니다. 우리는 이러한 선택적 SSM을 어텐션이나 MLP 블록도 없는 단순화된 종단간 신경망 아키텍처인 맘바에 통합시켰습니다. 맘바는 빠른 추론(트랜스포머보다 5배 높은 처리량)과 시퀀스 길이에 대한 선형 확장성을 누리며, 백만 길이 시퀀스까지 실제 데이터에서 성능이 향상됩니다. 일반적인 시퀀스 모델 백본으로서 맘바는 언어, 오디오, 유전체학과 같은 여러 양식에서 최첨단 성능을 달성합니다. 언어 모델링에서 우리의 맘바-3B 모델은 같은 크기의 트랜스포머를 능가하고 두 배 크기의 트랜스포머와 맞먹는 성능을 보이며, 사전 훈련과 다운스트림 평가 모두에서 성능을 나타납니다.*
``` ```suggestion
- cuda 커널을 최적화하는 방향 보다는, 단순하지만 모든 장치에서 실행가능하도록하는 방향인 '단순구현'의 성능을 빠르게 향상시키는 기여를 더 환영하고 있습니다. 🤗
```"
33952,2024-10-04T17:07:48Z,2024-10-09T00:11:02Z,fabxoe,1,0,3,32,2,1,1,[],362326.0,0,370994.0,0,0,0,0,6811589.343517,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33952). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thank you!,Thank you!,"# What does this PR do?

Translated the `main_classes/configuration.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
",
33959,2024-10-04T17:55:42Z,2024-10-09T00:10:41Z,fabxoe,1,3,5,75,2,2,1,[],358495.0,0,368100.0,1,0,0,0,6811609.579464,,0,5,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33959). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice, thank you!","Nice, thank you!","# What does this PR do?

Translated the `main_classes/quantization.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
```
아래 번역 해두신것 같아 원문부분 삭제했습니다~! ```suggestion
```
아래 번역 해두신것 같아 원문부분 삭제했습니다~! ```suggestion
 모델을 양자화하는 방법은 이  [양자화](../quantization) 가이드를 통해 배울 수 있습니다.
```
좀더 가독성 있어보이게 바꾸었습니다!"
33989,2024-10-06T15:19:09Z,2024-10-09T00:10:26Z,chhaewxn,2,0,3,104,2,1,1,[],12804.0,0,204678.0,0,0,0,0,6811625.776931,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'fabxoe']",LGTM! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33989). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thank you!,Thank you!,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `rag.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
",
33894,2024-10-02T15:02:04Z,2024-10-09T00:08:06Z,ahnjj,1,3,6,82,2,2,1,[],541662.0,0,551162.0,0,0,0,0,6811767.452125,,0,6,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33894). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks, LGTM! 👍 ","Thanks, LGTM! 👍 ","# What does this PR do?

Translated the `gpt_neox_japanese.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [ ] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR? ","```suggestion
이 모델은 Google의 [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) 연구 권장 사항을 따르며, 트랜스포머 블록에서 편향 파라미터를 제거하여 모델 성능을 향상시켰습니다. 자세한 내용은 [이 기사](https://medium.com/ml-abeja/training-a-better-gpt-2-93b157662ae4)를 참조하세요.
``` ```suggestion
일본어를 위한 자동회귀 언어 모델인 GPT-NeoX-Japanese를 소개합니다. 이 모델은 [https://github.com/EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox)에서 학습되었습니다. 일본어는 많은 어휘와 히라가나, 가타카나, 한자의 조합으로 이루어진 독특한 언어입니다. 이러한 일본어의 독특한 구조를 해결하기 위해 [특수 서브워드 토크나이저](https://github.com/tanreinama/Japanese-BPEEncoder_V2)를 사용했습니다. 이 유용한 토크나이저를 오픈소스로 제공해 준 *tanreinama*에게 매우 감사드립니다.
``` ```suggestion
모델 개발은 [ABEJA, Inc.](https://www.abejainc.com/)의 [신야 오타니](https://github.com/SO0529), [타카요시 마카베](https://github.com/spider-man-tm), [안주 아로라](https://github.com/Anuj040), [쿄 하토리](https://github.com/go5paopao)에 의해 주도되었습니다. 이 모델 개발 활동에 대한 자세한 내용은 [여기](https://tech-blog.abeja.asia/entry/abeja-gpt-project-202207)를 참조하세요.
```"
33891,2024-10-02T14:41:05Z,2024-10-09T00:07:14Z,ahnjj,1,3,4,73,2,3,1,[],542969.0,0,552369.0,0,0,0,0,6811819.803965,,0,4,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33891). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: docs suggetion LGTM!,fix: docs suggetion LGTM!,"# What does this PR do?

Translated the `bertweet.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [ ] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
 @stevhliu May you please review this PR? ","```suggestion
BERTweet 모델은 Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen에 의해 [BERTweet: A pre-trained language model for English Tweets](https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf) 에서 제안되었습니다.
``` 논문명이라 영문 추천합니다 ```suggestion
이 모델은 [dqnguyen](https://huggingface.co/dqnguyen)님이 기여했습니다. 원본 코드는 [여기](https://github.com/VinAIResearch/BERTweet)에서 확인할 수 있습니다.
```
제가 알기로는 2중 높임은 문법적으로 사용하지 않아야 합니다.(""~님이 ~했다"". ""~가 ~하셨다."") "
33775,2024-09-27T18:06:14Z,2024-10-09T00:06:57Z,yijun-lee,2,0,3,43,2,2,1,[],656129.0,0,972043.0,0,0,0,0,6811837.7911,,0,3,0,False,"['mreraser', 'HuggingFaceDocBuilderDev']",LGTM 👍  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33775). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM! Thank you! 🤗 ,LGTM! Thank you! 🤗 ,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `feature_extractor.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",
33665,2024-09-23T16:01:31Z,2024-10-03T13:59:01Z,ylacombe,12,0,13,51,4,2,1,['run-slow'],2937.0,0,1319120.0,0,0,0,0,6817844.514419,,0,13,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'ylacombe', 'leng-yue', 'ydshieh']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33665). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. LMK when you want me to review @ylacombe (please fix the tests and the conflicts first :blush:) Hey @LysandreJik, could you review now ? It seems that the failing check is unrelated to Whisper ! cc @ArthurZucker  for a quick look ! cc @ydshieh , there's still this error when doing slow tests: 
 ```
File ""/usr/lib/python3.8/runpy.py"", line 194 in _run_module_as_main
Bus error (core dumped)
```

 hi @ylacombe let me merge #33849 and try to rebase this PR on top of main and let's see. Could you rebase and trigger again with `[run-slow] whisper`? It's working thanks ! The list of failing tests are as expected, merging ! Great work! I noticed there's a small issue in this PR, especially when `prompt_ids` is specified—the model always returns a timestamp of 0.0.

To fix this quickly, you can update https://github.com/huggingface/transformers/blob/38f9f10dd9240619ea17fb6c7acb51b3bc592232/src/transformers/models/whisper/generation_whisper.py#L191 to:
```python
cross_attentions.append(torch.cat([x[i] for x in generate_outputs.cross_attentions], dim=2)[:, :, num_input_ids-3:, :])
``` Hey @leng-yue, thanks for your message!
While there's maybe something wrong with how cross-attentions weights are computed, I'm not sure that your proposed solution works. 
Could you provide a code snippet and maybe an explanation of the proposed solution if you have time?
Thanks! Basically `num_input_ids` is `prefix_tokens` (timestamps, language, etc) + `prompt_ids`, therefore, we want to strip the`prompt_ids`, which is `num_input_ids` - 3.I'm not totally up to date with the changes in `generation_whisper` so I'll trust you with them; if you want a double check feel free to cc @ArthurZucker but if you're confident you can ahead and merge. LGTM, let's just run slow before we merge? 🤗 ","I'm not totally up to date with the changes in `generation_whisper` so I'll trust you with them; if you want a double check feel free to cc @ArthurZucker but if you're confident you can ahead and merge. LGTM, let's just run slow before we merge? 🤗 ","# What does this PR do?

There's a lot of pending failing tests with Whisper. This PR addresses some issues:

1. #31683 and #31770 mentioned a out-of-range word level timestamps. This happens because `decoder_inputs_ids` were once `forced_input_ids`. This had an impact on the `beam_indices`. 

`beam_indices` has a length of `decoder_input_ids + potentially_generated_ids` but doesn't take into account `decoder_input_ids` when keeping track of the indices. In other words `beam_indices[0]` is really the beam indice of the first generated token, instead of `decoder_input_ids[0]`.

2. The Flash-Attention 2 attention mask was causing an issue

3. The remaining work is done on the modeling tests. Note that some of these tests were failing because of straightforward reasons - e.g the output was a dict -  and are actually still failing, but their reasons for failing are not straightforward anymore. Debugging will be easier though.


**Note:** With #33450 and this, we're down from 29 failing tests to 17
  ",
34021,2024-10-08T06:54:12Z,2024-10-08T20:43:31Z,benglewis,3,3,3,9,1,3,2,[],11316.0,0,49759.0,0,0,0,0,6824045.196888,,0,3,0,False,"['LysandreJik', 'benglewis', 'HuggingFaceDocBuilderDev']","Could you also run the code quality tool to ensure that the code quality passes? You can install them with the following, from the root of your clone:
```
pip install -e "".[quality]""
```
And then run them with:
```
make fixup
``` The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34021). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @LysandreJik I ran `make fixup` following that install command and installing a few other pip dependencies that seemed to actually be necessary to run it and everything is fine following the fix I applied after @muellerzr 's comment.Thanks! Thanks! Looked into the quality check and left a suggestion for I believe the fix",Thanks! Thanks! Looked into the quality check and left a suggestion for I believe the fix,"Fix type annotations for `trainer_seq2seq.py`'s `__init__` call.

@ylacombe @eustlb
@muellerzr @SunMarc

Apologies if I tagged irrelevant people, this is my first PR. I also see some other type annotation issues, e.g. `overloads` that are missing. If this kind of PR will be considered, I would be happy to contribute other fixes too!","As it's only here when type checking, please put it in between quotes so that it's known at runtime:

```suggestion
        train_dataset: Optional[Union[Dataset, ""IterableDataset"", ""datasets.Dataset""]] = None,
``` Thanks! The quality check is for this chunk here. I think it'd be fine if this was outside `TYPE_CHECKING` Thank you 🙌 "
34014,2024-10-07T16:36:17Z,2024-10-08T13:57:12Z,SunMarc,2,0,2,1,1,2,1,[],1462.0,0,76857.0,0,0,0,0,6848422.916305,,0,2,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34014). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > I think it is decoder_config that needs to be removed, default_config is actually used.

Indeed thanks ! I think it is `decoder_config` that needs to be removed, `default_config` is actually used. Ok!","I think it is `decoder_config` that needs to be removed, `default_config` is actually used. Ok!","# What does this PR do?
I've merged this [PR](https://github.com/huggingface/transformers/pull/33934) a little to fast, we need to remove a line as stated [here](https://github.com/huggingface/transformers/pull/33934#discussion_r1790157995). ",
34011,2024-10-07T15:29:23Z,2024-10-08T13:56:05Z,SunMarc,2,0,1,21,2,1,1,[],1477.0,0,80804.0,0,0,0,0,6848490.514807,,0,1,0,False,"['SunMarc', 'HuggingFaceDocBuilderDev']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34011). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Yes, that why ;)Was this caught in the quantization CIs ? 🤗 
","Was this caught in the quantization CIs ? 🤗 
","# What does this PR do ? 
This PR fixes a couple of issues (mainly versioning) from the ipex backend that was added to awq quantizer. We were importing the kernel but it is only available with awq from source. ",
34012,2024-10-07T16:02:05Z,2024-10-08T13:15:40Z,SunMarc,1,2,3,4,1,3,2,[],1447.0,0,76417.0,0,0,0,0,6850916.351889,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34012). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Great with a super tiny question. Thanks for fixing !,Great with a super tiny question. Thanks for fixing !,"# What does this PR do ?
This PR fixes a typing issue that this [PR](https://github.com/huggingface/transformers/pull/33940) introduced. 
TLDR: we can't do dict[str,dict] with python 3.8 cc @ydshieh 

Traceback received: 

```
Traceback (most recent call last):
  File ""/home/marc/transformers/src/transformers/utils/import_utils.py"", line 1780, in _get_module
    return importlib.import_module(""."" + module_name, self.__name__)
  File ""/usr/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 848, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/marc/transformers/src/transformers/models/__init__.py"", line 15, in <module>
    from . import (
  File ""/home/marc/transformers/src/transformers/models/mt5/__init__.py"", line 36, in <module>
    from ..t5.tokenization_t5_fast import T5TokenizerFast
  File ""/home/marc/transformers/src/transformers/models/t5/tokenization_t5_fast.py"", line 23, in <module>
    from ...tokenization_utils_fast import PreTrainedTokenizerFast
  File ""/home/marc/transformers/src/transformers/tokenization_utils_fast.py"", line 34, in <module>
    from .modeling_gguf_pytorch_utils import load_gguf_checkpoint
  File ""/home/marc/transformers/src/transformers/modeling_gguf_pytorch_utils.py"", line 245, in <module>
    weights: np.ndarray, parsed_parameters: dict[str, dict], name: str, tensor_key_mapping: dict
TypeError: 'type' object is not subscriptable

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""test_gemma_save.py"", line 1, in <module>
    from transformers import AutoModelForCausalLM
  File ""<frozen importlib._bootstrap>"", line 1039, in _handle_fromlist
  File ""/home/marc/transformers/src/transformers/utils/import_utils.py"", line 1770, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File ""/home/marc/transformers/src/transformers/utils/import_utils.py"", line 1782, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.auto because of the following error (look up to see its traceback):
'type' object is not subscriptable
```","Not sure, but is it intended to be `Any`? Pushed a commit, this should be more accurate now "
34007,2024-10-07T14:26:48Z,2024-10-08T13:04:24Z,muellerzr,1,4,5,5,1,3,2,['DeepSpeed'],1585.0,0,81459.0,0,0,0,0,6851592.197829,,0,5,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34007). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM ! Left a comment  SGTM!,LGTM ! Left a comment  SGTM!,"# What does this PR do?

Fixes a (soon to be failing) DS test with accelerate 1.0. We need to manually override the DS plugin when doing hp sweeps since we now don't allow for arbitrary passing in of a ds plugin to an already initialized accelerator

~~Also notes an actual deprecation point for when we want to force users to use `integrations.deepspeed` rather than `.deepspeed` to 5.0. @ArthurZucker @LysandreJik let me know if that's too soon, but it's something that was irking me :)~~

Seems we already got rid of it! 🔥 

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@SunMarc @LysandreJik 
","I think that the else condition is missing  Prior to 1.0 it didn't matter at all, so the only new blocker is with 1.0.  Post offline discussion, technically we should be able to do `AcceleratorState().reset_state(False)` here instead and it should do what we want without issue. Will try this alternative (then we don't need to limit it to a 1.0 flag either) Works just fine :) "
33991,2024-10-06T18:29:28Z,2024-10-08T12:53:58Z,Cyrilvallez,3,9,11,370,11,1,1,[],1440.0,0,152673.0,0,0,0,0,6852218.881849,,0,11,0,False,"['molbap', 'HuggingFaceDocBuilderDev', 'Cyrilvallez']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33991). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. - added support to automatically remove classes that are wrongly added as part of the dependencies (in addition to functions). This case appear if e.g. inheriting from a `LlamaModel` having dependency `LlamaRMSNorm`, but we redefined the `.norm` attribute as a `nn.LayerNorm`, thus not using the RMSNorm smart use of deque - need that functionality, looking forward to it :D Super nice! Thanks! Could you add some documentation in the supported features see `modular_transformers.md` and maybe an example with these features!",Super nice! Thanks! Could you add some documentation in the supported features see `modular_transformers.md` and maybe an example with these features!,"# What does this PR do?

This improves the modular file converter.  

- support for top-level functions newly defined in the `modular_xxx.py` code and used in a class inheriting from another model class without the function dependency (and recursively adding all other top-level functions that may be needed for that function)
e.g. 
```py
def foo():
    pass
 
class RobertaEmbeddings(BertEmbeddings):
   
    def forward(...)
        foo()
        pass
```
would currently not import `foo()` in the modeling if `foo` is not in the `modeling_bert.py` file.

- support to remove functions that are not needed anywhere but are a dependency of a class somewhere (and thus are wrongly imported at this time)
e.g.
```py
class RobertaPreTrainedModel(PreTrainedModel):
    """"""
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """"""

    config_class = RobertaConfig
    base_model_prefix = ""roberta""
    supports_gradient_checkpointing = True
    _no_split_modules = [""RobertaEmbeddings"", ""RobertaSelfAttention"", ""RobertaSdpaSelfAttention""]
    _supports_sdpa = True

    def _init_weights(self, module):
        """"""Initialize the weights""""""
        if isinstance(module, nn.Linear):
            # Slightly different from the TF version which uses truncated_normal for initialization
            # cf https://github.com/pytorch/pytorch/pull/5617
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)


class RobertaModel(RobertaPreTrainedModel, BertModel):
    def __init__(self, config, add_pooling_layer=True):
        super().__init__(config)
        self.embeddings = RobertaEmbeddings(config)
        self.encoder = RobertaEncoder(config)
        self.pooler = RobertaPooler(config) if add_pooling_layer else None
        # Initialize weights and apply final processing
        self.post_init()
```
would currently import the function `load_tf_weights_in_bert` because `BertModel` has dependency `BertPreTrainedModel` in which the function is used, but here we did not redefine `RobertaPreTrainedModel` based on `BertPreTrainedModel` mostly because this given class attribute (the function) was not needed.

- support to avoid rewriting some top-level variables such as `_CHECKPOINT_FOR_DOC` which should not be changed as part of the graph dependency
- add the correct file-name in the header warning

cc @ArthurZucker ","that's actually simpler than having to use the Parent or Scope provider! smart To update this is more like ""returns wether or not the function node should be added to the updated body. Will be True if the function is called anywhere, false otherwise ```suggestion
        # Check if any new top-level function from the `modular_xxx.py` should be added to the different files (if it is called in a class in the file, then it will be copy pasted from `modular.py` to that file).
``` can be a separate function and call self._check_xxx should be defined above!  cool addition 🎐  Very nice, the more we rely on visiters, the better it is IMO let's add a comment on the right!  Would be nice to have a written example of what this creates!"
33918,2024-10-03T14:48:38Z,2024-10-08T12:43:32Z,Rocketknight1,7,0,9,15,4,1,2,['run-slow'],361.0,0,424497.0,0,0,0,0,6852845.295676,,0,9,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'Rocketknight1', 'ydshieh']","Thank you @Rocketknight1 .

2 questions:

- Is this the only one to change? I see 3 failed tests (in torch pipeline CI reports).
- Do you think it makes sense to have this implemented directly in `BatchFeature` instead of outside it?
 @ydshieh good spot, I fixed it for all three! We could consider rewriting `BatchFeature.to()` like you suggested though, let me ask people The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33918). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. cc @ydshieh I changed the PR to update `BatchFeature.to()` instead, which means the individual pipeline fixes aren't needed anymore cc @ArthurZucker for core maintainer review! This PR is simple: It patches `BatchFeature.to()` to check that its keys are tensors before it calls `.to()` on them. This means that `BatchFeature.to()` works even with string keys now, which used to throw an error. Are there other areas that should be updated? Arthur tells me about Pixtral as an example @LysandreJik I checked the codebase for methods overriding `to()`. Mostly this was in modeling code to create objects like `NestedTensor` - these are safe because their contents should always be `torch.Tensor`.

The exceptions were:
- Pixtral
- `BatchEncoding` (the tokenizer output class)

In both cases, I updated their methods to make sure they only call `to()` on `torch.Tensor`!Thanks @Rocketknight1! perfect, thanks!","Thanks @Rocketknight1! perfect, thanks!","This PR fixes a bug in the preprocessing for several pipelines. The pipelines were calling `.to()` on a `BatchFeature` which had a string feature, which caused an error. This update was also requested internally in Slack!

cc @ydshieh ",
33899,2024-10-02T18:03:48Z,2024-10-08T12:34:29Z,Rocketknight1,7,10,15,197,13,3,1,[],5656.0,0,498644.0,0,0,0,0,6853389.519429,,0,15,0,False,"['Wauplin', 'LysandreJik', 'HuggingFaceDocBuilderDev', 'Rocketknight1']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33899). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Actually thinking more about it and inline with having less and less warnings in `transformers`, I wonder if we want to have a deprecation warning for `images` vs `outputs`, or if we just want to enable both without warning @LysandreJik yes, I'm fine with that! Want me to just drop all the deprecation warnings? I can maybe throw an error if users specify **both** arguments, or something. If no deprecation warning is raised (i.e. accept both `inputs` and `images` on the long run), I would make sure the method signature is in line with the recommendation (i.e. passing `inputs`). In that sense, I don't think that having `inputs: ... = None` on the long run is desirable as it can be misleading to users. If it was only for a few versions before settling for the definitive signature (i.e. no `images` at all), it would have been fine but that's not the case, right? @Wauplin Unfortunately, if we don't set `inputs = None`, then users have to pass `inputs`, and we can't accept `images` as an alternative.

I think it's okay, though! There are a lot of classes in `transformers` that have `None` defaults for mandatory inputs - it just means that we handle the error when the argument is missing somewhere else. I'll let you decide the best course of action in terms of signature/arguments @Rocketknight1, but dropping the deprecation warnings seems to me like the right thing to do here Done! Warnings are removed, but I'll accept both keywords for now - I don't think the main input having a default of `None` should be too confusing, especially since the docstrings are clear.Looks good, but I wonder if we shouldn't mention in the deprecation/in the pipeline docs that the pipelines will now be 1-1 with the HF specs in general, to justify the change from images to inputs","Looks good, but I wonder if we shouldn't mention in the deprecation/in the pipeline docs that the pipelines will now be 1-1 with the HF specs in general, to justify the change from images to inputs","This PR checks for Hub spec compliance for a lot of image pipelines that have very similar inputs. In most cases, no changes are required except deprecating the `timeout` argument, and renaming the `images` input to `inputs`.","I wonder if these warnings (and the docs) shouldn't point to the global definition of specs across the HF ecosystem to justify such a change This file is lacking the copyright! 
Can you add it? Note for myself: this change won't break Inference API since it's using positional argument and not keyword one (see [here](https://github.com/huggingface-internal/api-inference/blob/ef27d885a6a9c0228fa1efd4b4b730bfb007e012/shard/app/main.py#L188) - private link). So all good to change it, same for all other pipelines :) I wonder if this is working as expected. Say you have a user script like this:

```py
pipeline = ImageClassificationPipeline(...)
output = pipeline(images=<my-image>, foo=""bar"")
```

with this PR, the `pipeline` call will fail with something like ""requires 1 positional argument, got 0"" because `inputs` is not provided. what I would suggest is to have a decorator like this:

```py
def _rename_positional_arg(old_name: str) -> Callable:

    def _decorator(fn: Callable) -> Callable:

        def _inner(*args, **kwargs):
            if old_name in kwargs:
                if len(args) > 0 or ""inputs"" in kwargs:
                    raise ValueError(f""Cannot pass 'inputs' and '{old_name}' at the same time."")
                warnings.warn(""Deprecation message 'you should use inputs instead of {old_name}' ..."")
                kwargs[""inputs""] = kwargs.pop(old_name)
        return fn(*args, **kwargs)

    return decorator
```

which you can use like this:

```py
...

@_rename_positional_arg(old_name=""images"")
def __call__(self, inputs: Union[str, List[str], ""Image.Image"", List[""Image.Image""]] = None, **kwargs):
    ...  # here you are guaranteed ""inputs"" is passed
```

This way, the pipeline will both if:
- inputs are passed as positional args (as done in Inference API)
- inputs are passed as `inputs`
- inputs are passed as `images`

(I did not find a satisfying decorator name but you have the idea^^)
 Actually, it should work! I set a new default argument value of `None` for inputs, so the pipeline can be called without that argument. However, it will raise an exception if neither `inputs` nor `images` are passed. What's the right document to link users to as the ""source of truth"" there? Just the folder in the Hub repo [here](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub/inference/_generated/types)? Done! In other words:

Case 1: Users are passing ""images"" as a positional argument. The argument is renamed to `inputs` and they don't notice any change

Case 2: Users are passing ""images"" as a kwarg.  `inputs` is now `None` but `**kwargs` contains an `""images""` key. The code detects this and sets `inputs = kwargs.pop(""images"")`, then prints the deprecation warning Ooh, sorry I did not see the added `= None`. You are completely right, forget what I meant above :)"
32472,2024-08-06T15:21:26Z,2024-10-08T12:26:43Z,yonigozlan,3,9,7,117,11,7,2,[],1216.0,0,5432718.0,0,0,0,0,6853858.416845,,0,7,0,False,"['HuggingFaceDocBuilderDev', 'yonigozlan']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32472). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Now that all the image-text-to-text models have had their processor standardized, I think we can safely merge this PR @ArthurZucker @NielsRogge  I replaced some tests that were using `AutoModelForVision2Seq` with `AutoModelForImageTextToText` and also swapped some instances of `[Model]ForConditionalGeneration` with `AutoModelForImageTextToText` in the documentation.

Of course, there will be more tests and documentation updates including `AutoModelForImageTextToText` once the `ImageTextToText` pipeline is added.

Did you have any other specific tests or doc in mind @ArthurZucker? I didn’t find many tests using AutoClasses except in the pipeline tests.Overall looks great - thanks for adding! 

Do all of these models share common model outputs? If not, what is the largest subset of outputs shared? 

PR in general looks good to merge to me, but let's wait for the PRs for the processors to be unified to be merged first, as we can't group these models yet until that's finalized & merged in  Tried out the models with this mapping - works fine, thanks! LGTM, but this need: 
- tests where you use the class
- documentation: with small example of how to use / with llava for example!  Okay for adding pipeline tests that use it in a follow up pr 😉 LGTM ","Overall looks great - thanks for adding! 

Do all of these models share common model outputs? If not, what is the largest subset of outputs shared? 

PR in general looks good to merge to me, but let's wait for the PRs for the processors to be unified to be merged first, as we can't group these models yet until that's finalized & merged in  Tried out the models with this mapping - works fine, thanks! LGTM, but this need: 
- tests where you use the class
- documentation: with small example of how to use / with llava for example!  Okay for adding pipeline tests that use it in a follow up pr 😉 LGTM ","# What does this PR do?

Add `AutoModelForImageTextToText` object in preparation for the image-text-to-text pipeline
Blocking PR for `image-text-to-text` pipeline.

The following models need to be added to modeling auto:

- [x]  GIT
- [x]  BLIP
- [x]  BLIP-2
- [x]  IDEFICS
- [x]  InstructBLIP
- [x]  LLaVa
- [x]  Fuyu
- [x]  Pix2Struct/DePlot/MatCha
- [x]  UDOP
- [x]  Donut
- [x]  KOSMOS-2
- [x]  Idefics2
- [x]  LLaVA-NeXT
- [x]  PaliGemma
- [x]  VipLlava
- [x] Chameleon
- [x] LlaVa OneVision
- [x] Qwen2-VL
- [x] Pixtral
- [x] Idefics3
- [x] MLlama

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@molbap @amyeroberts 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Chameleon can also do Image-text to text
```suggestion
        (""blip-2"", ""Blip2ForConditionalGeneration""),
        (""chameleon"", ""ChameleonForConditionalGeneration""),
        (""fuyu"", ""FuyuForCausalLM""),
``` I don't think we can do this - vision-encoder-decoder is generic composite model. The user can specify any encoder or decoder they want, and so there is no mapping to any processors +1 - though it can also do image-text to image, do we want it in this mapping still?  +1, was about to say the same Oh yes that makes sense, I think we can guess the processor type from the model name like it is done here for image_processor instead:
https://github.com/huggingface/transformers/blob/e0d82534cc95b582ab072c1bbc060852ba7f9d51/src/transformers/pipelines/__init__.py#L998-L1015 Yep - I think that's the way to do it in the pipeline! I wouldn't worry too much about vision-encoder-decoder support. It's good to have, but because the model is by definition a bit of a frankenstein it's likely to not be fully compatible in all cases anyway. Should the existing classes be deleted from `MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES` or do we keep them there? I just removed the ones that previously were in `IGNORE_NON_AUTO_CONFIGURED`.
Won't deleting them from  `MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES` be a problem for BC? can also add llava-next-video and video-llava? Those two can work with image+text inputs, as well as video+text"
33942,2024-10-04T08:09:26Z,2024-10-08T08:58:49Z,zucchini-nlp,1,5,2,13,4,3,1,[],1419.0,0,348563.0,0,0,0,0,6866333.610644,,0,2,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33942). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Will be cleaner! added a single comment ➕ on Pablo's comment, good to go otherwise!","Will be cleaner! added a single comment ➕ on Pablo's comment, good to go otherwise!","# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/33847. We should not have default values for padding side after the last PR that enabled passing `padding_side` to tokenizer directly. Users are free to set padding side as tokenizer attr or pass it to processors directly

cc @yonigozlan if you had any PRs in progress with default padding-side 😉 
","This was the default kwarg value initially in the processor, wasn't it? I might be mistaken but I thought it was padding_side that was culprit my bad, saw `padding_side` here somehow hehe, will fix back I just realized the issue comes from this line when we use default tokenizer kwargs that were set during init. It a bit breaking if ppl change the attr like `padding_side` and that is not reflected in init_kwargs, so I think we can check first tokenizer attr and then fallback to the init kwargs
@molbap wdyt? Ah I see, yes that makes sense, it should be a ""soft"" check though to make sure we are not requiring anything at this stage, just that if it is present, it should be superseding. 
Handling priorities in this script is :zany_face: good catch though!  yea, i think the check is soft as we first see if attribute if present, and if not fallback to the usual `init_kwargs[key]`"
33859,2024-10-01T10:20:21Z,2024-10-08T08:19:17Z,ArthurZucker,2,9,33,626,15,3,1,['run-benchmark'],1093.0,0,597539.0,0,0,0,0,6868703.885617,,0,33,0,False,['HuggingFaceDocBuilderDev'],"Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33859). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Looks good! Only left a few comments",Looks good! Only left a few comments,"# What does this PR do?
Adds support in modular for:
- deleting super()'s function (with raise Attribute Error)
- a way to call another `super`: if you inherit from both LlamaTokenier and PretrainedTokenizer, you can call `PretrainedTokenizer`'s `__init__` and it will be unravelled as `super().__init__()`
- splitting the __all__ that is created in the `modular_xxx.py`
- camel-cased renaming when dependencies are not found
","Can you add a comment above? or on the same line  unclear, as seen with you in person :eyes:  this should be a super no? clean! this breaks some of the docstring updates That's very nice! It means new models will always have snake casing right? Should be yes!"
33936,2024-10-04T01:27:52Z,2024-10-07T22:59:14Z,yijun-lee,3,0,3,78,2,1,1,[],89077.0,0,336682.0,0,0,0,0,6902310.71701,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'cjfghk5697']",LGTM! LGTM 🚀  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33936). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thank you! 🤗 ,Thank you! 🤗 ,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `gemma.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",
32421,2024-08-05T05:40:26Z,2024-10-08T07:43:40Z,zucchini-nlp,1,16,15,162,37,3,2,[],1077.0,0,5536994.0,0,0,0,0,6870845.430461,,0,15,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32421). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM, thank you for working on this 💛  

One design question about sliding caches LGTM, other than the deprecation cycle and the need to double-check attributes 🤗  Thanks for the in depth changes! Not convinced we have to change the name of the function, and also convinced we should have abstraction on what we call. Checking for is static IMO is not a good way forward. It might be a good internal helper, but in the modeling we would prefer not having to check that wdyt? 

Also fine for this specific case TBH! Just not sure it's gonna scale Thanks!","LGTM, thank you for working on this 💛  

One design question about sliding caches LGTM, other than the deprecation cycle and the need to double-check attributes 🤗  Thanks for the in depth changes! Not convinced we have to change the name of the function, and also convinced we should have abstraction on what we call. Checking for is static IMO is not a good way forward. It might be a good internal helper, but in the modeling we would prefer not having to check that wdyt? 

Also fine for this specific case TBH! Just not sure it's gonna scale Thanks!","# What does this PR do?

Following https://github.com/huggingface/transformers/pull/32315#discussion_r1701676560, this PR adds two attributes on all cache classes, `is_sliding` and `is_static`. We can now rely on these attr to crop/prepare attention mask.
","Because of the changes on the sliding window cache, `target_length` can now be `None`, which means that `causal_mask = torch.full((sequence_length, target_length), ...)` will fail, a few lines below -- right?

I agree that the sliding window has infinite length. To untangle this, I have the following suggestion:
1. Let's rename `get_max_length` to `get_cache_shape` or a similar name. The idea is to have a function to retrieve the shape of the cache object, which is used in other places like here. (`get_max_length`  would have to go through a deprecation cycle, calling `get_cache_shape` under the hood until it is deleted )
2. Explore the current uses of `get_max_length()`. When we want to obtain the shape of the object, we would call `get_cache_shape`. When we want to obtain the maximum cache length, we would do something like `max_length = None if cache.is_sliding else cache.get_cache_shape()`

WDYT? Oh missed the moment we got rid of `max-length` in sliding window. Sounds good but I am not sure if there are cases when we want to know the `max_length` and not `cache_length`. Oke, let me go through the repo and check all possible cases :) > but I am not sure if there are cases when we want to know the max_length and not cache_length

I think the same :D but let's double-check 🙏  ```suggestion
        <add the warning here>
        return self.get_max_cache_shape()
```

Suggestion: this as opposed to updating all cache classes :) on the cache classes, `def get_max_length(self) ...` would be removed
 We will need a deprecation cycle for `get_max_length` before we can delete it :P

(in the raised warning, make sure you mention that users should use `get_max_cache_shape` instead) (double-check whether this new attribute is correct, after rebasing. I think we've added new cache classes since your original commit. In the encoder-decoder cache, these attributes should be loaded from the decoder cache) I am not entirely sure we want to add these kind of checks directly in the modeling code:
`past_key_values.is_static` does not really look specific, but if you allow checking `is_sliding` then we are gonna have `is_encoder_decoder` and etc. 

 yes, I ran tests for generation in llama and all compile-static tests in whisper. Rebased main and `fix-copies` propagated changes to some new models is there a specific reason for the name change ?  Aligned with my previous comment where we need a general `past_key_value.skip_sdpa_correction()` for example  same here The only reason is to be more specific what we mean by `max_length` since I wanted to add `max_length` in sliding cache, which currently doesn't have any (returns `None`). And as comment in code explains it was done with idea that the cache object technically handles infinite amount of tokens, so no max length

But we never want to know how many tokens can cache technically handle, our checks are all about max capacity of particular cache instance  Hmm, maybe we can make a better check to not use `isinstance()` on cache objects. We are having more varieties of cache now and it is mostly either dynamic or static type of some caching method, so it would be nice to not rely on `isinstance()` checks in general

We can remove modeling changes in this PR and use the new attributes in specific cases (linked comment from PR description). What I need from this PR currently is the `get_max_length` that doesn;t return `None` anymore  Let's add this in the comment for example! I was not sure 🤗  Yeah Also want to avoid .is_static and isinstance etc. Not sure how to abstract best tho! 

Okay, let's remove then for now okey, done and rebased main"
33884,2024-10-02T10:41:27Z,2024-10-07T22:35:11Z,mreraser,1,3,7,176,2,3,1,[],474751.0,0,474824.0,1,0,0,0,6903755.825855,,0,7,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33884). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.나머지는 좋은 번역인 것 같습니다! 수고하셨습니다 :) Awesome, thank you!","나머지는 좋은 번역인 것 같습니다! 수고하셨습니다 :) Awesome, thank you!","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `vit.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [x] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@yijun-lee, @mreraser, @jungnerd, @cjfghk5697
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)
<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
Hello @stevhliu !  May you please review this PR? Thank you! 

Have a nice day :)","```suggestion
*Transformer 아키텍처는 자연어 처리 작업에서 사실상 표준으로 자리 잡았으나, 컴퓨터 비전 분야에서의 적용은 여전히 제한적입니다. 비전에서 어텐션 메커니즘은 종종 합성곱 신경망(CNN)과 결합하여 사용되거나, 전체 구조를 유지하면서 합성곱 신경망의 특정 구성 요소를 대체하는 데 사용됩니다. 우리는 이러한 CNN 의존성이 필요하지 않으며, 이미지 패치를 순차적으로 입력받는 순수한 Transformer가 이미지 분류 작업에서 매우 우수한 성능을 발휘할 수 있음을 보여줍니다. 대규모 데이터로 사전 학습된 후, ImageNet, CIFAR-100, VTAB 등 다양한 중소형 이미지 인식 벤치마크에 적용하면 Vision Transformer(ViT)는 최신 합성곱 신경망과 비교해 매우 우수한 성능을 발휘하면서도 훈련에 필요한 계산 자원을 상당히 줄일 수 있습니다.*
``` glossary를 참고해 수정해보았습니다. ```suggestion
- DINO (Vision Transformers의 self-supervised 훈련을 위한 방법) (Facebook AI 개발). DINO 방법으로 훈련된 Vision Transformer는 학습되지 않은 상태에서도 객체를 분할할 수 있는 합성곱 신경망에서는 볼 수 없는 매우 흥미로운 능력을 보여줍니다. DINO 체크포인트는 [hub](https://huggingface.co/models?other=dino)에서 찾을 수 있습니다.
```

한번 개인적으로 자연스럽게 수정해봤습니다!"
33795,2024-09-29T15:42:42Z,2024-10-07T22:34:56Z,mreraser,2,2,3,63,2,2,1,[],470449.0,0,715934.0,1,0,0,0,6903770.936546,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'cjfghk5697']",LGTM! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33795). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.👍 ,👍 ,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `swin2sr.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [x] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@yijun-lee, @mreraser, @jungnerd, @cjfghk5697
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
Hello @stevhliu !  May you please review this PR? Thank you! 

Have a nice day :)","```suggestion
*압축은 스트리밍 서비스, 가상 현실, 비디오 게임과 같은 대역폭이 제한된 시스템을 통해 이미지와 영상을 효율적으로 전송하고 저장하는 데 중요한 역할을 합니다. 하지만 압축은 필연적으로 원본 정보의 손실과 아티팩트를 초래하며, 이는 시각적 품질을 심각하게 저하시킬 수 있습니다. 이러한 이유로, 압축된 이미지의 품질 향상은 활발한 연구 주제가 되고 있습니다. 현재 대부분의 최첨단 이미지 복원 방법은 합성곱 신경망을 기반으로 하지만, SwinIR과 같은 트랜스포머 기반 방법들도 이 작업에서 인상적인 성능을 보여주고 있습니다. 이번 논문에서는 Swin Transformer V2를 사용해 SwinIR을 개선하여 이미지 초해상도 작업, 특히 압축된 입력 시나리오에서 성능을 향상시키고자 합니다. 이 방법을 통해 트랜스포머 비전 모델을 훈련할 때 발생하는 주요 문제들, 예를 들어 훈련 불안정성, 사전 훈련과 미세 조정 간 해상도 차이, 그리고 데이터 의존성을 해결할 수 있습니다. 우리는 JPEG 압축 아티팩트 제거, 이미지 초해상도(클래식 및 경량), 그리고 압축된 이미지 초해상도라는 세 가지 대표적인 작업에서 실험을 수행했습니다. 실험 결과, 우리의 방법인 Swin2SR은 SwinIR의 훈련 수렴성과 성능을 향상시킬 수 있으며, ""AIM 2022 Challenge on Super-Resolution of Compressed Image and Video""에서 상위 5위 솔루션으로 선정되었습니다.*
``` glossary를 참고하여 수정하였습니다! "
33590,2024-09-19T11:48:17Z,2024-10-07T22:34:45Z,boyunJang,1,6,5,379,2,5,2,[],1593957.0,0,1593988.0,0,0,0,0,6903782.665364,,0,5,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33590). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM! 꼼꼼한 번역 감사드려요! Thank you!,LGTM! 꼼꼼한 번역 감사드려요! Thank you!,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `auto.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
<!-- @harheem, @jeongiin, @1kmmk1, @Seungahson, @win2dvp21 --> 
@junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang
<!-- @jungnerd, @cjfghk5697, @yijun-lee, @mreraser --> 
<!-- @heuristicwave, @nuatmochoi, @chhaewxn, @jun048098, @ahnjj, @fabxoe -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
많은 경우, 사용하려는 아키텍처는 `from_pretrained()` 메소드에서 제공하는 사전훈련된 모델의 이름이나 경로로부터 유추할 수 있습니다. AutoClasses는 이 작업을 위해 존재하며, 사전 학습된 weight/config/vocabulary에 대한 이름/경로를 제공하면 자동으로 관련 모델을 가져오도록 도와줍니다.
```
glossary 반영해보았습니다! ```suggestion
# Auto 클래스[[auto-classes]]
```
auto class가 사람이름 같이 고유명사인 것 같아서 제안해보아요! 
(혹은 오토 클래스도 후보가 될 수 있을 것 같습니다!) ```suggestion
이후에는 일반적으로 자동 클래스를 사용하는 것처럼 사용할 수 있습니다!
```
자연스럽게 바꾸어보았습니다! ```suggestion
다음 자동 클래스들은 사전 훈련 헤드가 포함된 모델을 인스턴스화하는 데 사용할 수 있습니다.
```
pretrained model이 glossary에서 사전훈련으로 번역하고 있어서, 통일해보았습니다. ```suggestion
많은 경우, 사용하려는 아키텍처는 `from_pretrained()` 메소드에서 제공하는 사전 학습된 모델의 이름이나 경로로부터 유추할 수 있습니다. AutoClasses는 이 작업을 위해 존재하며, 사전 학습된 모델 가중치/구성/단어사전에 대한 이름/경로를 제공하면 자동으로 관련 모델을 가져오도록 도와줍니다.
```

한국어로 대체할 수 있는 부분인 것 같아 번역해보았습니다! ```suggestion
많은 경우, 사용하려는 아키텍처는 `from_pretrained()` 메소드에서 제공하는 사전 훈련된 모델의 이름이나 경로로부터 유추할 수 있습니다. AutoClasses는 이 작업을 위해 존재하며, 사전 학습된 모델 가중치/구성/단어사전에 대한 이름/경로를 제공하면 자동으로 관련 모델을 가져오도록 도와줍니다.
```"
33799,2024-09-30T01:35:13Z,2024-10-07T22:06:13Z,yijun-lee,2,3,4,188,2,3,1,[],40094.0,0,678660.0,0,0,0,0,6905496.032448,,0,4,0,False,"['mreraser', 'HuggingFaceDocBuilderDev']",review complete 🚀  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33799). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for translating!,Thanks for translating!,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `chameleon.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
Chameleon은 게이티드(gated) 모델이므로 Hugging Face Hub에 대한 액세스 권한이 있고 토큰으로 로그인했는지 확인하세요. 다음은 모델을 로드하고 반정밀도(`torch.bfloat16`)로 추론하는 방법입니다:
```
이해를 위해 원문 내용을 추가하였습니다, ```suggestion
이 모델은 최적화를 위해 Flash-Attention 2와 PyTorch의 [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)를 모두 지원합니다. SDPA는 모델을 로드할 때 기본 옵션입니다. Flash Attention 2로 전환하려면 먼저 flash-attn을 설치해야 합니다. 해당 패키지 설치에 대해서는 [원본 리포지토리](https://github.com/Dao-AILab/flash-attention)를 참고하십시오. 위의 코드 스니펫을 다음과 같이 변경하면 됩니다:
```
glossary를 참고하여 수정하였습니다. ```suggestion
전체 공개 전에 버그를 식별하는 데 도움이 되는 피드백을 환영합니다! 자세한 내용과 피드백은 [이 문서](https://huggingface.co/docs/bitsandbytes/main/en/non_cuda_backends)를 확인하세요.
```"
33543,2024-09-17T15:31:08Z,2024-10-07T22:34:34Z,chhaewxn,1,16,8,112,2,4,1,[],1753257.0,0,1753407.0,2,0,0,0,6903794.171319,,0,8,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33543). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.fix: suggestions fix: suggestions Thanks! 🤗 ,fix: suggestions fix: suggestions Thanks! 🤗 ,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `logging.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe, @nuatmochoi, @heuristicwave

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?
","```suggestion
## `logging` vs `warnings`[[transformers.utils.logging.captureWarnings]]
```
<img width=""1287"" alt=""image"" src=""https://github.com/user-attachments/assets/a7a9a0f4-e36a-4cff-a8f5-ef923f6619ef"">

앵커 클릭후 주소창의 #의 우측을 모두 복사해서 붙여넣어야 해요. ```suggestion
## 기본 설정자 [[transformers.utils.logging.set_verbosity_error]]
``` ```suggestion
## 기타 함수 [[transformers.utils.logging.get_verbosity]]
``` ```suggestion
이 로깅 모듈의 모든 메서드는 아래에 문서화되어 있으며, 주요 메서드는 현재 로거의 출력 수준을 가져오는 [`logging.get_verbosity`]와 원하는 출력 수준으로 설정하는 [`logging.set_verbosity`] 입니다. 출력 수준은 (가장 적은 출력에서 가장 많은 출력 순으로) 다음과 같으며, 해당 수준에 대응하는 정수 값은 괄호 안에 표시됩니다.
``` ```suggestion
- `warnings`는 라이브러리 개발자와 `transformers`에 의존하는 라이브러리 개발자들에게 유리할 것입니다.
``` ```suggestion
- `logging`은 평범한 프로젝트에서 라이브러리를 개발하기보다는 라이브러리를 사용하는 최종 사용자들에게 유리할 것입니다.
```
`should be favored for`를 검색해보니 `유리할 것이다`, `선호될 것이다`, `선호되어야 한다` 정도로 많이 번역 되더군요. 이런 것들 중에서 반영해보면 아마도 더 자연스러울 것 같습니다.
<img width=""943"" alt=""image"" src=""https://github.com/user-attachments/assets/ff6d37b5-1603-4f69-88e7-b6f5e30ef9b7"">
<img width=""955"" alt=""image"" src=""https://github.com/user-attachments/assets/82adef92-2eb7-4483-9c0f-41bf27b328f1"">

 ```suggestion
아래에서 `captureWarnings` 메소드에 대한 참고 사항을 확인할 수 있습니다.
```
이 부분은 glossary를 따라서 `메소드`로 바꿨어요. 개인적으로 오히려 `메서드`가 다른 책이나 튜토리얼에서 충분히 많이 쓰이고 있어서 사람들에게 익숙하고 혼용해도 문제가 없다고 생각을 하는데, 팍팍 하지만 일단 통일감을 위해서 바꿨어요. :)
 ```suggestion
이 로깅 모듈의 모든 메서드는 아래에 문서화되어 있으며, 주요 메서드는 현재 로거의 출력 수준을 가져오는 [logging.get_verbosity]와 원하는 출력 수준으로 설정하는 [logging.set_verbosity]입니다. 출력 수준은 (가장 적은 출력에서 가장 많은 출력 순으로) 다음과 같으며, 해당 수준에 대응하는 정숫값은 괄호 안에 표시됩니다.
``` ```suggestion
- `transformers.logging.CRITICAL` 또는 `transformers.logging.FATAL` (정숫값, 50): 가장 심각한 오류만 보고합니다.
``` ```suggestion
- `transformers.logging.ERROR` (정숫값, 40): 오류만 보고합니다.
``` ```suggestion
- `transformers.logging.WARNING` 또는 `transformers.logging.WARN` (정숫값, 30): 오류와 경고만 보고합니다. 이는 라이브러리에서 기본으로 사용되는 수준입니다.
``` ```suggestion
- `transformers.logging.INFO` (정숫값, 20): 오류, 경고, 그리고 기본적인 정보를 보고합니다.
``` ```suggestion
- `transformers.logging.DEBUG` (정숫값, 10): 모든 정보를 보고합니다.
``` ```suggestion
라이브러리 개발자는 다음과 같은 지침을 따르는 것이 좋습니다.
``` ```suggestion
- `warnings`는 라이브러리 개발자와 `transformers`에 의존하는 라이브러리 개발자들에게 유리합니다.
```
더 자연스러운 것 같아 표현을 바꿔봤어요! :) ```suggestion
- `logging`은 일반적인 프로젝트 라이브러리 개발자보다는, 라이브러리를 사용하는 최종 사용자들에게 유리할 것입니다.
```
더 이해가 될것 같은 표현으로 개발자를 붙여봤어요!"
33797,2024-09-29T16:48:58Z,2024-10-07T22:05:57Z,yijun-lee,2,3,4,56,2,3,1,[],72047.0,0,710219.0,0,0,0,0,6905512.892636,,0,4,0,False,"['mreraser', 'HuggingFaceDocBuilderDev']",review complete 💯  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33797). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks!,Thanks!,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `trainer.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
[`Trainer`] 클래스는 PyTorch에서 완전한 기능(feature-complete)의 훈련을 위한 API를 제공하며, 다중 GPU/TPU에서의 분산 훈련, [NVIDIA GPU](https://nvidia.github.io/apex/), [AMD GPU](https://rocm.docs.amd.com/en/latest/rocm.html)를 위한 혼합 정밀도, 그리고 PyTorch의 [`torch.amp`](https://pytorch.org/docs/stable/amp.html)를 지원합니다. [`Trainer`]는 모델의 훈련 방식을 커스터마이즈할 수 있는 다양한 옵션을 제공하는 [`TrainingArguments`] 클래스와 함께 사용됩니다. 이 두 클래스는 함께 완전한 훈련 API를 제공합니다.
```
이해를 위해 원문을 추가하였습니다. ```suggestion
- 모델은 항상 튜플이나 [`~utils.ModelOutput`]의 서브클래스를 반환해야 합니다.
- 모델은 `labels` 인자가 제공되면 손실을 계산할 수 있고, 모델이 튜플을 반환하는 경우 그 손실이 튜플의 첫 번째 요소로 반환되어야 합니다.
- 모델은 여러 개의 레이블 인자를 수용할 수 있어야 하며, [`Trainer`]에게 이름을 알리기 위해 [`TrainingArguments`]에서 `label_names`를 사용하지만, 그 중 어느 것도 `""label""`로 명명되어서는 안 됩니다.
```
 지우는 게 개인적으로 더 자연스러워서 제안드립니다!"
33809,2024-09-30T08:40:56Z,2024-10-07T22:05:17Z,yijun-lee,3,0,3,47,2,1,1,[],9324.0,0,653061.0,0,0,0,0,6905553.938909,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'cjfghk5697']",LGTM 💯  LGTM😏 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33809). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks!,Thanks!,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `pipelines_utils.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `pipelines_utils.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",
33806,2024-09-30T08:22:36Z,2024-10-07T22:05:00Z,yijun-lee,3,0,3,33,2,1,1,[],10641.0,0,654144.0,0,0,0,0,6905571.256282,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'cjfghk5697']",LGTM 💯  LGTM😺 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33806). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks!,Thanks!,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `time_series_utils.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",
33796,2024-09-29T16:47:26Z,2024-10-07T20:39:22Z,yijun-lee,3,0,3,119,2,1,1,[],73289.0,0,705116.0,0,0,0,0,6910710.08831,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'cjfghk5697']",LGTM 🚀  LGTM🤗 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33796). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thank you! 🤗 ,Thank you! 🤗 ,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `esm.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",
33802,2024-09-30T04:18:27Z,2024-10-07T20:39:10Z,yijun-lee,3,0,3,43,2,1,1,[],25568.0,0,663644.0,0,0,0,0,6910722.006552,,0,3,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'cjfghk5697']","LGTM 💯  LGTM:🎉 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33802). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Awesome, thanks!","Awesome, thanks!","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `audio_utils.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `audio_utils.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",
33566,2024-09-18T15:07:20Z,2024-10-07T19:50:44Z,mreraser,4,0,5,67,2,2,2,[],401386.0,0,1658630.0,0,0,0,0,6913603.399223,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'mreraser', 'cjfghk5697', 'jungnerd']","@mreraser LGTM! LGTM! Thank you for your review!  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33566). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM 👍  LGTM, thank you!","LGTM 👍  LGTM, thank you!","<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `swinv2.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [x] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@yijun-lee, @mreraser, @jungnerd, @cjfghk5697
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)
Hello @stevhliu !  May you please review this PR? Thank you! 

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
<!--
Hello @stevhliu !  May you please review this PR? Thank you! 

Have a nice day :)
-->",
33764,2024-09-27T14:58:12Z,2024-10-07T19:49:08Z,yijun-lee,1,5,4,104,2,3,1,[],881397.0,0,881456.0,0,0,0,0,6913726.329621,,0,4,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33764). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM, thanks for your translation! 🤗 ","LGTM, thanks for your translation! 🤗 ","# What does this PR do?

Translated the `gguf.md` file of the documentation to Korean.
Thank you in advance for your review.

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?","```suggestion
    title: GGUF 파일들과의 상호 운용성
```
`(번역중)`은 번역 완료 이후에는 삭제해도 될 것 같습니다! ```suggestion
GGUF 파일 형식은 [GGML](https://github.com/ggerganov/ggml)과 그에 의존하는 다른 라이브러리, 예를 들어 매우 인기 있는 [llama.cpp](https://github.com/ggerganov/llama.cpp)이나 [whisper.cpp](https://github.com/ggerganov/whisper.cpp)에서 추론을 위한 모델을 저장하는데 사용됩니다.
```
원문을 참고하여 조금더 자연스럽게 번역했습니다. ```suggestion
이제 PyTorch 생태계에서 모델의 양자화되지 않은 전체 버전에 접근할 수 있으며, 다른 여러 도구들과 결합하여 사용할 수 있습니다.
```
unquantized는 '양자화 되지 않은'으로 보는 것이 더 적절한 것 같아 위와 같이 수정하였습니다. Review complete! 👍  ```suggestion
`transformers`에서 `gguf` 파일을 로드하려면 `from_pretrained` 메소드에 `gguf_file` 인수를 지정해야 합니다. 동일한 파일에서 토크나이저와 모델을 로드하는 방법은 다음과 같습니다: 
```
glossary에 맞게 수정했습니다"
33934,2024-10-03T23:48:51Z,2024-10-07T16:32:20Z,mgoin,2,3,1,1,1,3,3,[],30662.0,0,319409.0,0,0,0,0,6925535.143748,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'zucchini-nlp']",cc @gante  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33934). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Good catch! Thank you for fixing 💛  Make sense ! I left a comment ,Good catch! Thank you for fixing 💛  Make sense ! I left a comment ,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes https://github.com/huggingface/transformers/issues/33927


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","```suggestion
                default_config = None
``` it was a typo -- we don't need to set `decoder_config` 👀  I think that there was a typo in this [PR](https://github.com/huggingface/transformers/pull/33219/files) and we should only set `default_config = None` and remove `decoder_config = None` 
EDIT: It was indeed the case ! resolved from gante review"
34004,2024-10-07T12:47:42Z,2024-10-07T16:11:13Z,ArthurZucker,2,0,1,13,13,1,1,[],1406.0,0,12213.0,0,0,0,0,6926800.524947,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34004). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I have absolutely no ideaWhy is this necessary while it was not before?,Why is this necessary while it was not before?,"# What does this PR do?
` RUN_SLOW=1 pytest  -n 4 tests/models/*/test_modeling_* -k test_inference_interpolate_pos_encoding -vvv -s` would not work without this ",
32049,2024-07-18T10:03:22Z,2024-10-07T13:36:54Z,dataKim1201,7,0,2,4,1,2,1,"['trainer', 'DeepSpeed', 'optimization']",868429.0,0,7011213.0,0,0,0,0,6936061.275595,,1,2,0,False,"['dataKim1201', 'maengsanha', 'LysandreJik', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'Joj67']","@amyeroberts @muellerzr Quite simple fix, but seems to save many people hassle - hope this to be merged 🙏 cc @SunMarc as well +1 would be great if this gets added @dataKim1201 Please rebase the commit and release the draft status. Hi @ArthurZucker ,

Thank you for your feedback! I appreciate your patience. I’ll clean up the commit history and update the PR.

Let me know if there’s anything specific you’d like me to focus on.

Thanks again for your understanding!

Best regards The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32049). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks for your patience 🤗 sorry about your wait both merging! Thanks for digging into this and adding this fix @dataKim1201! 

The change looks OK to me, but I'm not very familiar with the deepspeed logic in trainer. Happy to merge once @muellerzr or @SunMarc have confirmed this is OK If you rebase and set the pr as ready for review we can merge!  oops the history is a bit messed up 😓 ","Thanks for digging into this and adding this fix @dataKim1201! 

The change looks OK to me, but I'm not very familiar with the deepspeed logic in trainer. Happy to merge once @muellerzr or @SunMarc have confirmed this is OK If you rebase and set the pr as ready for review we can merge!  oops the history is a bit messed up 😓 ","# What does this PR do?

Dear maintainers,

I am using Trainer for continual pre-training thanks to your dedicated efforts.

Recently, I discovered a bug when using DeepSpeed thorugh HuggingFace Trainer.

It is common to implement custom optimizers while using DeepSpeed with Trainer.
However, as pointed out in issue #15784, even though DeepSpeed can be used in conjunction with custom optimizers, it is implemented to raise a RuntimeError as in PR #16786.

Because of this, the workaround has been to override ``create_optimizer_and_scheduler`` to bypass the aforementioned bug and make things work, but I think there can be a more fundamental solution.

Upon analyzing the current Trainer implementation, I have found that the bug occurs due to the unnecessary branch during the Trainer instantiation at the time of writing, and I have fixed this issue.

Therefore, after bug fix, I have confirmed that using custom optimizers with DeepSpeed through Trainer works as expected, as shown in the below comparison.

- Before:

```sh
Using Default config for accelerate: cfg/accelerate/default_config.yaml                                                                                                                           
...
INFO:__main__:Model is loaded on 0
Counting items of /data/sample_text: 0it [00:00, ?it/s]INFO:__main__:Model is loaded on 1
Counting items of /data/sample_text: 100it [00:00, 1960.49it/s]
Counting items of /data/sample_text: 0it [00:00, ?it/s]INFO:__main__:Dataset is loaded on 0
WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Counting items of /data/sample_text: 100it [00:00, 2147.84it/s]                                                                                                           
INFO:__main__:Dataset is loaded on 1                                                                                                                                                              
[rank1]: Traceback (most recent call last):                                                                                                                                                       
[rank1]:   File ""/train_gpt.py"", line 244, in <module>                                                                                                                    
[rank1]:     main()                                                                                                                                                                               
[rank1]:   File ""/train_gpt.py"", line 95, in main                                                                                                                         
[rank1]:     trainer = Seq2SeqTrainer(                                                                                                                                                            
[rank1]:               ^^^^^^^^^^^^^^^                                                                                                                                                            
[rank1]:   File ""/opt/conda/envs/lib/python3.12/site-packages/transformers/trainer_seq2seq.py"", line 57, in __init__                                                                          
[rank1]:     super().__init__(                                                                                                                                                                    
[rank1]:   File ""/opt/conda/envs/lib/python3.12/site-packages/transformers/trainer.py"", line 568, in __init__                                                                                 
[rank1]:     raise RuntimeError(                                                                                                                                                                  
[rank1]: RuntimeError: Passing `optimizers` is not allowed if Deepspeed or PyTorch FSDP is enabled. You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.       
...
```

- After:

```sh
Using Default config for accelerate: cfg/accelerate/default_config.yaml                                                                                                                           
[2024-07-18 08:51:38,334] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)                                                                           
[2024-07-18 08:51:38,355] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)                                                                           
INFO:__main__:Enabling full determinism with train_cfg.seed=777. It may slow down the training especially depending on multi-node settings. Set seed=null if reproducibility can be ignored in that case.                                                                                                                                                                                           
INFO:__main__:Enabling full determinism with train_cfg.seed=777. It may slow down the training especially depending on multi-node settings. Set seed=null if reproducibility can be ignored in that case.                                                                                                                                                                                           
```

Thank you for your attention to this matter.

Regards,

---

## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).

---

Dear @muellerzr, I kindly ask you to review this PR.",
33979,2024-10-05T14:35:29Z,2024-10-07T08:08:20Z,MagnusS0,6,3,5,185,3,1,1,[],955.0,0,157991.0,0,0,0,0,6947357.193737,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'MagnusS0', 'ArthurZucker']","Ahh, yeah makes sense, then I can extend the example and show how it also can be used e.g.  with PEFT! WDYT? Of course! And if people have example of adding SDPA for example (not here for SAM) or good hacking, will go there! let's call for contribution probably! 🤗  **Updated the PR:** It now adds a new developer guide titled ""How to Hack Any Transformers Model"" 🚀
I've also updated the PR description and title accordingly.

Let me know your thoughts! Do you think we should make the guide more general regarding model hacking? Or do you (or anyone else) have any extra examples to add?
 The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33979). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks @MagnusS0 🤗  Thanks to you as well @ArthurZucker ! Didn't know about pre hooks so super helpful 🤗Nice! I think we can have a separate doc, with sam being the example! 
This could be in the same section as https://huggingface.co/docs/transformers/fast_tokenizers (developer guides!) Yeah looks great 🤗 ","Nice! I think we can have a separate doc, with sam being the example! 
This could be in the same section as https://huggingface.co/docs/transformers/fast_tokenizers (developer guides!) Yeah looks great 🤗 ","# What does this PR do?

This PR adds a new developer guide titled ""How to Hack Any Transformers Model"" to the docs. The guide shows how to modify existing models, using the Segment Anything Model (SAM) as an example. It also encourages community contributions by inviting others to share their own hacks.

## Changes

- Added a new developer guide at `docs/source/en/how_to_hack_models.md`
- Updated `docs/source/en/_toctree.yml` to include the new guide in the Developer Guides section.


Fixes #33928

## Before submitting

- [x] This PR improves the docs.
- [x] I have read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request).
- [x] This was discussed  in the GitHub issue #33928.

## Who can review?

Tagging @ArthurZucker as we discussed this in issue #33928 and the code example was provided by him. Let me know if this is what you had in mind 🤗 ","```suggestion
    title: Model Hacking (overwriting a class to your usage)
``` ```suggestion
<!--Copyright 2024 The HuggingFace Team. All rights reserved.
``` ```suggestion
- **Class Replacement:** By assigning your custom class to `modeling_sam.SamVisionAttention`, any instances of `SamVisionAttention` in the model will use the modified version. Thus when you call `SamModel`, it will use the newly defined `SamVisionAttentionSplit`. 
```"
33861,2024-10-01T10:24:03Z,2024-10-04T20:47:08Z,gante,3,12,13,169,5,3,2,[],656.0,0,514667.0,0,0,0,0,6951368.016487,,0,13,0,False,"['HuggingFaceDocBuilderDev', 'gante']","Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33861). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. (rebased to include a flaky test fix)Great, thanks for finding a less breaking way to support mllama cache :)

Left one comment in quantized cache, I guess the tests are not catching it because it requires `quanto` or `hqq` to be installed in the env Let's be extra extra careful here as people changed their code (cf TGI) for us, we can't break again Let's run slow tests! ","Great, thanks for finding a less breaking way to support mllama cache :)

Left one comment in quantized cache, I guess the tests are not catching it because it requires `quanto` or `hqq` to be installed in the env Let's be extra extra careful here as people changed their code (cf TGI) for us, we can't break again Let's run slow tests! ","# What does this PR do?

Reverts the optional argument in `DynamicCache.__init__`, which broke BC in a few edge cases.

The optional argument was needed in `mllama` because the model skips layers. The original solution was to initialize empty lists on all layers, with the number of layers being an input argument.

This PR removes the optional argument, but adds the ability to handle skipped layers on `DynamicCache`.

EDIT: fixes #33794 

_________________________________________________

✅ same tests are passing as in `main` (there are a couple of slow tests failing!)","Classes that expand dynamic cache don't support skipping layers, at least for now. 

This PR did not add this limitation, but adds the exception :) on `main`, `test_new_cache_format_2` isn't failing all times, but rather is flaky.

In fact, all variants of `test_new_cache_format` are flaky. I suspect that it is because the random model may be generating image tokens (we had a similar issue in other models) new quick test to ensure we don't regress :)

(there is a slow test, but we don't check the slow tests often enough) Quantized cache can support anything that dynamic cache supports, and this check should break any generation with quantized cache because we are no longer able to fill in the cache from zero

If mllama fails with quantized cache, maybe we can do `_supports_quantized_cache=False`  and add a TODO for us? Skipping layers here should be also straightforward I hope oh i didn't know almost all mllama generation tests are being skipped currently, I'll come back to mllama soon and will try to find why they are failing. No good to have them skipped Good catch!

- in the commit you reviewed this line, tests should be failing
- pushed a commit that fixed it (`py.test tests/models/llama/ -k test_generate_with_quant_cache -vv` now passes)
- this probably means, as you wrote, that quanto tests are not being run. will investigate we ought to keep this for BC I am wondering if we are not gonna break other stuff, do you have links to issues were we broke something ?!  broken things as a result of these changes:
- https://github.com/huggingface/peft/pull/2106 -- PEFT was relying on `__bool__` (which maps to `__iter__`, which in turn maps to `__len__`, i.e. `len(self.key_cache)`), which got changed when `num_hidden_layers ` was being passed
- #33794 -- retrieving `num_hidden_layers` to initialize the cache needed to call `config.get_text_config()`, which uses `getattr` under the hood. `getattr` is not compileable by torch.compile

 this fixes compilation as well  Let's warn once. We can also use the wrapper introduce by @qubvel : https://github.com/huggingface/transformers/blob/ce81eb8d14ae39e183a8956cce3f7e2e46ecaa8f/src/transformers/utils/deprecation.py#L32  ```suggestion
    @deprecate_kwarg(old_name=""num_hidden_layers"", version=""4.47"")
    def from_batch_splits(cls, splits: List[""DynamicCache""], num_hidden_layers: int = None) -> ""DynamicCache"":
```"
33994,2024-10-06T19:27:45Z,2024-10-07T09:17:34Z,kaitolucifer,1,0,1,2,1,1,1,[],49795.0,0,49796.0,0,0,0,0,6951617.254746,,0,1,0,False,['ArthurZucker'],🤗 Thanks,Thanks,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #33993

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
34001,2024-10-07T08:25:41Z,2024-10-07T08:56:24Z,ArthurZucker,1,0,1,3,1,0,0,[],1514.0,0,1845.0,0,0,0,0,6952893.103057,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_34001). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33570,2024-09-18T16:32:23Z,2024-10-07T07:52:19Z,Cyrilvallez,2,0,1,3,1,1,1,[],1450.0,0,1610396.0,0,0,0,0,6956741.126251,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'Cyrilvallez']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33570). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. cc @ArthurZucker, forgot to ping you👀 nice hack, awesome that you found about it! ","👀 nice hack, awesome that you found about it! ","# What does this PR do?

https://github.com/huggingface/transformers/pull/31629 & https://github.com/huggingface/transformers/pull/32241 introduced a functionality in FA2 intended for training efficiency. However, it adds unnecessary cuda synchronization at inference time in every forward pass due to always checking `(torch.diff(position_ids, dim=-1) >= 0).all()`  in the `elif` condition. This PR fixes the performance issue by simply switching the order of the different checks in the `elif` condition, to make good use of Python's default short-circuit evaluation. Indeed, at inference time, `query_length` will always be 1 except during prefill, thus we will short-circuit torch synchronization all the time.

Performance degradation was not so significant, but this PR allows to win back around 5-10% speed at inference time from the quick tests I ran.
",
33393,2024-09-09T19:50:58Z,2024-10-07T07:54:07Z,NielsRogge,1,8,7,184,5,2,1,[],1223.0,0,2376189.0,0,0,0,0,6956633.336988,,0,7,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33393). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for adding these updates! 

Just some small nits

cc @zucchini-nlp as this involves VLMs  Thanks for adding these improvements! 

","Thanks for adding these updates! 

Just some small nits

cc @zucchini-nlp as this involves VLMs  Thanks for adding these improvements! 

","# What does this PR do?

This PR improve the docs of various VLMs:

* llava: batched inference
* llava-onevision: minor nit on model naming
* qwen2-vl: add to appropriate multimodal section in toctree, add figure
* pixtral: add `apply_chat_template`, add figure","The previous comment was better. Perhaps we can introduce the example below after the initial introduction to something like `""In the case of limited GPU RAM, one can reduce the resolution, as follows:""` It would be good to explain where the 28 comes in here  ```suggestion
        ],
``` ```suggestion
        ],
``` Done 🤗  Could we link to a page about chat templates here instead? We probably don't want to have to repeat info about the structure in all the model pages Why remove the expected generation? "
33121,2024-08-26T12:30:46Z,2024-10-07T07:20:49Z,avishaiElmakies,7,30,13,66,2,3,1,[],91490.0,0,3611619.0,0,0,0,0,6957217.221515,,0,13,0,False,"['avishaiElmakies', 'HuggingFaceDocBuilderDev']","I tried to keep the API as similar as possible to ```llama``` and ```gamma```. they both put it third. isn't it kinda of a problem with AutoModel to have them at different placements?  it means that you can't load using AutoModel and use the model without keyword arguments?

Thanks for the feedback! about the forward call for the embedding layer. I think it has to take position ids as an argument. otherwise it will not work with packed sentences.  @ArthurZucker I am thinking that maybe the best solution for the embedding layer is to add position_ids as an arg to the forward pass with default None. this is probably backward compatible, but will still help with packed sentences. the problem is probably that the code will not be very nice @gante, thanks! Happy to contribute.

I would love some guidance on the last two comments left, what should I do with the position_ids in the embedding module. In my opion it should be able to get position_ids to work with packed sentences. Maybe a last argument with default None and a check?

And about the one liners, would love some guidance on that @ArthurZucker would love some guidance here so i can finish and move on to other models. @ArthurZucker changed what you said and refactored the embedding class to be backward compatible. would love some feedback The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33121). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Hey! in general the trick is that adding a new argument needs to be done at the end of the forward pass otherwise you are breaking the model for people who directly call the model  cc @gante WDYT about this? In general IMO we should just run basic position ids init. Tho taking padding into account should be ""alright"", it's already done in generate, this would help for forward and training. 

Just need to be careful as we also want to support packing  @ArthurZucker I'm pro `position_ids` as it standardizes OPT wrt other models 🙌 

@avishaiElmakies Thank you for adding the fix 🤗 Have a look at unresolved comments (you'd be surprised with how easy it is to break code for other external libraries, [hyrum's law](https://www.hyrumslaw.com/) definitely applies to `transformers`)   Feel free to merge @gante if it's alright with you! 🤗 and thanks for your contribution!","Hey! in general the trick is that adding a new argument needs to be done at the end of the forward pass otherwise you are breaking the model for people who directly call the model  cc @gante WDYT about this? In general IMO we should just run basic position ids init. Tho taking padding into account should be ""alright"", it's already done in generate, this would help for forward and training. 

Just need to be careful as we also want to support packing  @ArthurZucker I'm pro `position_ids` as it standardizes OPT wrt other models 🙌 

@avishaiElmakies Thank you for adding the fix 🤗 Have a look at unresolved comments (you'd be surprised with how easy it is to break code for other external libraries, [hyrum's law](https://www.hyrumslaw.com/) definitely applies to `transformers`)   Feel free to merge @gante if it's alright with you! 🤗 and thanks for your contribution!","# What does this PR do?

This pull request adds position_ids to the forward of ```OPT``` in a similar fashion to ```gemma``` and ```llama```. #32937 


Some models didn't have an argument for position_ids in their forward pass. 

There are two main reasons we would like for all LM models to get positions ids.

1. to have the API be consistent with all models.
2. position_ids are very important if you want to use flash-attention without padding, during training. if i want to be able to pack two or more sentences in the same sequence. I would like to know that the model handles the sentences accordingly and treats each sentence as it's own different sentence. flash-attention code uses position_ids to check if some sequences are packed and runs an appropriate function to make sure there is no cross-example contamination. but without this, the model can't use this feature. the code always checks if position_ids is not None:

https://github.com/huggingface/transformers/blob/v4.44.1/src/transformers/modeling_flash_attention_utils.py#L270

This handles only OPT, so i can start small and get some feedback.

changes:

- changed [OPTLearnedPositionalEmbedding](https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/models/opt/modeling_opt.py#L63) forward method to get position_ids instead of attention_mask.
- changed all forward passes in the file to get position_ids and pass them forward. 
- update prepare_inputs_for_generation to get position_ids. 
- if position_ids are ```None``` create position_ids based on attention (similar to original version, so it should work the same if position_ids are not given)
- update relevent docs

a few notes:

- this model because of the use of offset in [OPTLearnedPositionalEmbedding](https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/models/opt/modeling_opt.py#L63) needs to represent it's padding token position as -1. I think this is fine to keep compatibility with the weights and everything. 
- [BioGPT](https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/models/biogpt/modeling_biogpt.py#L50) seems to copy OPT's positional embedding class. I needed to remove the comment to disable that (since it no longer copies). this is needed for ```make fixup```.

feature-request #32937 

## Before submitting
- [] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [] Did you write any new necessary tests?

@ArthurZucker would love some feedback.
","let's avoid such one liners please let's just add a TODO here or also update that model that is kind of a breaking change for this module 😓  comment is way to long let's split it on more lines! OK, I will change that. why is this a problem? the weights are the same so loading should work. and this module should not be used by outside code so it is not supposed to break anything. should I return the comment as well? it causes a fail when using make fixup.
I tried to look to update BioGPT, but it seems to be a lot of code from different models, so i didn't know if i should have touched it. I can work on it next.  K ```suggestion
# copied from transformers.models.opt.modeling_opt.OPTLearnedPositionalEmbedding with OPT->BioGpt
# TODO @ArthurZucker bring copied from back Humm let's avoid such one liners Yeah but it has caused issue in the past 😉  should be added at the end of the list of args same hjere it is like two actions: cumsum and multiplying with attention masks. and i did it with two lines. could you be more specific? or give me an example of what you would write so i can learn for the future? would you want each operation in a single line? OK, will change this.  k OK, how do you think I should do it? The module should get position_ids to work with packed sentences. Should I add position_ids as the last argument with None as default?  ah sorry what I mean is we avoid having such a simple piece of code as a separate function! 🤗 We just directly write the code in place where you use this OK, now i understand. I will change that! thanks! yes ! only thing against this is that it's not compile compatible (dynamic slicing on input dependent) but okay!  @gante can confirm but `generate` prepares them no? SHould not be needed AFAIK! is there a way to make it compile compatible? creating a tensor with `torch.arange(past_key_values_length, full_length)` but we would need to test! Ok, do you want me to try? and how do you test if it is compile compatiable? We have a slow test for this: https://github.com/huggingface/transformers/blob/fe496262c119e4366b2f319567bb561cab445dc1/tests/test_modeling_common.py#L4986 to make sure it runs!  ok, I will try to test some options  @ArthurZucker after looking at the test, I think it will not work for opt. the test uses static cache and opt doesn't support static cache. So making compile work for OTP will need further work Ahhh right, I thought #32617 had touched OPT, but not yet! 
Then no worries IMO let's remove it from prepare input for generation at least to de-duplicate"
33903,2024-10-03T00:41:08Z,2024-10-05T14:20:50Z,vasqu,5,1,3,27,1,3,2,['run-slow'],48585.0,0,223341.0,0,0,0,0,7104873.568621,,0,3,0,False,"['vasqu', 'HuggingFaceDocBuilderDev', 'Rocketknight1']","@vasqu I suspect the reason that these errors are silent on GPU is that `tf.gather()` [does not do bounds checking on GPU](https://www.tensorflow.org/api_docs/python/tf/gather):

> Caution: On CPU, if an out of bound index is found, an error is raised. On GPU, if an out of bound index is found, a 0 is stored in the corresponding output value. 

`tf.gather()` is the underlying mechanism used for a lot of things like embedding layers and other index lookups, so this affects a lot of layers.

Either way, reviewing the rest of this now!
 @vasqu can you push another empty commit with the `[run-slow] opt` message to trigger the slow CI there? Oh wow, thanks for the insights into this. This is rather counter-intuitive to me tho :D Even some archaic cuda / gpu errors would suffice for me but it is what it is. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33903). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. tests pass :) gentle ping @Rocketknight1 LGTM!

For core maintainers: This generation test fails on some models because it generates past the limit of the model's `max_position_embeddings`. Because the model is initialized from a config, not a pretrained checkpoint, this PR simply increases `config.max_position_embeddings` or `config.max_target_positions`, which seems like the right fix to me.

cc @lysandrejik for core maintainer review! Thanks 🤗 ","LGTM!

For core maintainers: This generation test fails on some models because it generates past the limit of the model's `max_position_embeddings`. Because the model is initialized from a config, not a pretrained checkpoint, this PR simply increases `config.max_position_embeddings` or `config.max_target_positions`, which seems like the right fix to me.

cc @lysandrejik for core maintainer review! Thanks 🤗 ","# What does this PR do?
Fixes xla generation with models with a limited seq_len that they can generate. It has been recently (re)discovered in #33298, which results in slow runs to fail as we have indexing issues based on absolute positional embeddings (i.e., an index out-of-bounds error). The issue can only be observed on CPUs and not on GPUs (idk why but the error is just silently swallowed). 

The solution is to extend the embedding not only by `max_new_tokens` but also by the current `seq_len` at hand. Atm, `seq_len` hasn't been considered yet.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

No fixes but a ref to #33298
(possibly adding an issue if wanted)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@Rocketknight1 @amyeroberts ",Not sure if input_features also has its sequence length at 1. Just submitting this quickly before I don't have time.
31286,2024-06-06T12:38:09Z,2024-10-06T08:33:17Z,tomlimi,21,20,25,706,11,3,1,[],4392.0,0,10526118.0,0,0,0,0,7040676.287826,,0,25,0,False,"['amyeroberts', 'itazap', 'HuggingFaceDocBuilderDev', 'tomlimi', 'ArthurZucker', 'github-actions[bot]']","cc @ArthurZucker @itazap  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_31286). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. The implementation is ready. I wrote a simple unittest to check the functionality of the tokenizer, but it makes two ci tests fail with the error, saying that tested `Tomlim/myt5-base` is not a valid model handle, even though it is public at: 
https://huggingface.co/Tomlim/myt5-base

```
E   OSError: Tomlim/myt5-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
E   If this is a private repository, make sure to pass a token having permission to this repo either by logging in with huggingface-cli login or by passing token=<your_token> 
```

 Alright! cc @itazap if you can do a first pass!  This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.

Please note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored. Sorry for late reply, I pushed the changes addressing the comments in the review. Can this PR be still re-opened?
Thank you! Thanks for applying the feedback! Looks good, please update with changes from main and address the minor CI failures (+ ruff reformat)! Otherwise LGTM 🤗  I fixed most of CLI errors, unfortunately the issue with run_tests still persitsts. It raises the error, saying that tested Tomlim/myt5-base is not a valid model handle, even though it is public at https://huggingface.co/Tomlim/myt5-base (I also had this problem with earlier version from June 6). Could it be caused by the way unittest class is declared?

```
E   OSError: Tomlim/myt5-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
``` I fixed the issue with loading the model by changing the `MyT5TokenizationTest` class, and all the added tests remained unchanged. I also added a sample ""vocabulary"" file to `fixtures` so the tokenizer can be loaded from it. 

I see that one test fails in CI, but it is in `DPTModelTest` and is unrelated to the changed code. Thank you from prompt reply! Please let me know if there are any other changes or tests that should be added before merging! There might be a couple of minor conflicts to resolve! The conflict was caused by another model being added in the meantime. I resolved it and merged the new changes from the main 🙂. @itazap @ArthurZucker can this PR be merged? I recently presented MyT5 at ACL and expect increased interest in the model (tokenizer) in the upcoming time. Thanks! Thank you for the review, @ArthurZucker. I added a short description of the method in the model doc and added the missing ""Copied from statements"". To address the other points:

1. Added tokens conversion to bytes. Actually, in `myt5_tokenizer` we do not use `added_tokens` . Morphemes are rewritten using the original vocabulary of 256 bytes,  MYTE is a rewriting method that does not require additional tokens. I kept the support of `added_tokens` in case someone would like to extend the vocabulary in their instance of the tokenizer.

2. `byte_map.json` in the hub. It is already there ([Tomlim/myt5-base](https://huggingface.co/Tomlim/myt5-base)). In the testing class, I used the vocab file from `fixtures` as it is done, e.g., in T5. I had tried to load it from the hub, but it did not work for me (I described an error in a comment a month ago).  I think this looks good, let's rebase and merge WDYT? 🤗  Thank you! I merged the updates from the main to the branch and pushed an empty commit with `run-slow` as instructed by the bot 🤖. I see some tests are failing but they are not related to the added code. Oups, `Tomlim/myt5-base` seems private! Could you make it public? CIs are failing because of this! The model is already public. I had this issue with test suite claiming that it was private before, see this [comment](https://github.com/huggingface/transformers/pull/31286#issuecomment-2301458035) from August. 

That was way I tried solving it bt adding tokenization files to `fixtures`, which I have already removed.  Okay I'll have a look!  <img width=""684"" alt=""image"" src=""https://github.com/user-attachments/assets/fb48aed8-1d09-4fc3-8783-36a7b2867989"">
Works for me, merging! Thanks for the contribution! 🤗 @tomlimi Nice, looks clean to me!! :hugs: :hugs: I added a few comments regarding docs, Copied from, and perhaps a couple of edge cases to test to flex the ByteWriter class  Thanks for applying the feedback anad adding ByteRewriter tests, looks great! 🚀 I reran the CI tests & all pass 🟢 
@ArthurZucker any further comments / test suggestions?  In general LGTM! The code is clean I have not read the paper, but sounds interesting! 
Would just like to have a short description of how this work in the myt5.md! 
Left kind of an open question about added tokens and potential slowdowns Let's remove this and merge","@tomlimi Nice, looks clean to me!! :hugs: :hugs: I added a few comments regarding docs, Copied from, and perhaps a couple of edge cases to test to flex the ByteWriter class  Thanks for applying the feedback anad adding ByteRewriter tests, looks great! 🚀 I reran the CI tests & all pass 🟢 
@ArthurZucker any further comments / test suggestions?  In general LGTM! The code is clean I have not read the paper, but sounds interesting! 
Would just like to have a short description of how this work in the myt5.md! 
Left kind of an open question about added tokens and potential slowdowns Let's remove this and merge","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #31260 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","add inserts! could add a 'Copied from' remove # MyT5Tokenizer has no vocab file add docs & docstring, and maybe a test of this functionality? test any edges cases, special tokens, and bytewriter cases Nice! 💯  Having a short description made easy to understand here would help! 
 we are probably missing some copied from here no ?  sama here here you are going to convert added tokens like `""HELLO""` for example with morphological decode. Then you have to convert the content back to utf8? 
This should be avoided, unless when you add a token you morphologically encode it's content. 
 Can you rather put this on the hub ? 🤗  let's use the base instead!   I had tried to load it from the hub, but it did not work for me (I described an error in a comment a month ago).  It is already there ([Tomlim/myt5-base](https://huggingface.co/Tomlim/myt5-base)). In the testing class, I used the vocab file from fixtures as it is done, e.g., in T5. We do not use `added_tokens` . Morphemes are rewritten using the original vocabulary of 256 bytes, MYTE is a rewriting method that does not require additional tokens. I kept the support of `added_tokens` in case someone would like to extend the vocabulary in their instance of the tokenizer. Yep got it, but you are re-computing the set of added tokens decoder and added tokens encoder for token in out tokens. Can be pretty slow IMO the best to keep supporting added tokens is to overwrite `_add_tokens` to apply morpheme on the content but otherwise let's just update the computation  let's remove this, again we should be able to load the tokenizer from pretrained  Why not just use ` MyT5Tokenizer.from_pretrained(""Tomlim/myt5-base"")` ```suggestion
```"
33863,2024-10-01T12:59:33Z,2024-10-05T14:03:13Z,yecohn,4,0,2,38,1,1,1,[],13.0,0,350756.0,0,0,0,0,7105955.277584,,0,2,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'yecohn']","Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). Hey @yecohn, was this discussed in an issue or elsewhere? I'm wondering why the model change :) Hey @LysandreJik  at ai21labs we trained  a tiny model for testing. It  offers a better tractability than the random-tiny model we uploaded. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33863). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks, that's actually really welcome 🤗 ","Thanks, that's actually really welcome 🤗 ","This PR uses the jamba-tiny-dev instead of jamba-tiny-random. It gives a better way to unittest Jamba. 

run-slow
",
33940,2024-10-04T06:21:59Z,2024-10-05T14:19:02Z,VladOS95-cyber,2,2,4,86,3,3,2,[],296.0,0,115023.0,0,0,0,0,7106342.750361,,1,4,0,False,"['HuggingFaceDocBuilderDev', 'VladOS95-cyber']","@SunMarc hi! I fixed a bug with Qwen2Moe. It was wrong expert tensor mapping. Please, take a look on this PR, everything should be fine now. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33940). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for fixing @VladOS95-cyber ! Really appreciate it ! Left a comment thanks both, trusting you 💯 🙁 ","Thanks for fixing @VladOS95-cyber ! Really appreciate it ! Left a comment thanks both, trusting you 💯 🙁 ","# What does this PR do?
Bug fix for expert tensor mapping for Qwen2Moe
Small fix for Bloom gguf test
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Link: https://github.com/huggingface/transformers/pull/33264
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@SunMarc @LysandreJik 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Can you add comments to explain why we need to do these ? I understand the first case but not the second one.  Hi @SunMarc! Yes, sure, it is because of tensor shape, it must be (1, 2048) but we got (2048) from quantized one. And then execution is failed on the moment we call `set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)` in modeling_utils.py (L989-L991) as it compares new value with old one from module, which is one dim. I added a comment that we do it for compatibility here"
33919,2024-10-03T15:02:07Z,2024-10-05T12:13:26Z,Craigacp,2,0,1,3,1,2,2,[],157680.0,0,162679.0,0,0,0,0,7113879.193908,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'itazap']","@Craigacp Nice, thank you!! 🚀  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33919). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks 🤗 ",Thanks 🤗 ,"# What does this PR do?

Updates the documentation for `BatchEncoding.char_to_token` to note that the return value can be `None` if `trim_offsets` is true and the char index points to a whitespace character.

Fixes #33237.


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. Discussed in #33237.


## Who can review?

@ArthurZucker @itazap 
",
33941,2024-10-04T08:06:18Z,2024-10-05T07:47:37Z,zucchini-nlp,1,7,3,23,5,4,3,[],1475.0,0,85280.0,0,0,0,0,7129828.988874,,1,3,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33941). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM, added comment on training case for generation :)  LGTM, thank you for fixing 🤗  Thanks 🤗 ","LGTM, added comment on training case for generation :)  LGTM, thank you for fixing 🤗  Thanks 🤗 ","# What does this PR do?

Fixes the flaky test on paligemma from #33630","this was the cause as it was not masking dummy tokens from static cache, and thus we always ended up with no mask on those token positions if we come to prepare static cache from here, then we cannot be in training mode. I don't think it is common to pass `labels` through generation, right? I'm not seeing many use-cases indeed, except for maybe constrained generation and RL?  aah gotcha. good catch guess so, let's see what generation master (gante) thinks 😄  If labels in `paligemma` has the usual meaning (=tensor with which we compute the loss, with no further uses), then `generate` will never use `labels` :D  nice, yes those are normal labels :)"
33879,2024-10-02T00:29:24Z,2024-10-04T09:20:56Z,saldanhad,1,0,3,21,2,1,1,[],29229.0,0,266627.0,0,0,0,0,7148696.344901,,0,3,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33879). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thank you! cc @stevhliu ,Thank you! cc @stevhliu ,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33325,2024-09-05T14:33:56Z,2024-10-04T14:29:55Z,abuelnasr0,15,20,32,338,2,3,4,[],1874.0,0,2573220.0,0,0,0,0,7124231.484823,,1,32,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'Rocketknight1', 'ArthurZucker', 'abuelnasr0']","This gist shows the before and after GPT2 model generation after adding new tokens: https://colab.research.google.com/gist/abuelnasr0/7cc8decff72ccdbcf5073ad8259ee360/embedding_resize.ipynb Nice! Thanks @abuelnasr0. cc @Rocketknight1 in case you have some bandwidth to do a first review? Yes, I'm happy to take it, it seems like a great PR! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33325). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks @LysandreJik 
And thanks @Rocketknight1 for the review and the comments. I have added a test case to check if the new embeddings' mean is close to the original embeddings' mean, I think that will reduce the effect of any outlier values.
Feel free to let me know if there is anything else I should add. @ArthurZucker Sorry for the delay. last week was very busy for me.
I have added the flag to `resize_token_embeddings`,  `_get_resized_embeddings`, and `_get_resized_lm_head`. 
`_get_resized_lm_head` is very important for models that have untied weights. This should have been added from the beginning but I don't know how I didn't notice that. Bias for linear lm_heads is not included in the article. but I have came to the conclusion that initializing with zero is the best solution.
here is the proof given the article:

$p_{\theta'}(w_i \mid w_{1:i-1}) = \frac{\exp(h_{i-1}^{\top} e_{w_i} + b_{w_i})}{Z + \exp(h_{i-1}^\top e_{n+1} +  b_{n+1} ) }$
$p_{\theta'}(w_i \mid w_{1:i-1}) = \frac{\exp(h_{i-1}^{\top} e_{w_i} + b_{w_i})}{Z + \exp(h_{i-1}^\top e_{n+1} ) exp( b_{n+1})}$

If the new bias equals zero, then exp(0) = 1, and then the proof in the article will remain the same.
 @abuelnasr0 I haven't dived into the math, but that seems counter-intuitive to me. Most models do not have a bias on the output head, but if they do, I would guess that the bias is usually large and negative for most tokens (I can't test this, though, because I can't find any examples of models with bias on the output head!)

If the average bias is large and negative, then initializing new tokens with the mean embedding but a zero bias will mean that logits for the new token will be very large, right? 

UPDATE: I finally found an example of an old model with a bias on the lm_head (`Salesforce/ctrl`). The mean bias was `-0.05`, which is a much smaller magnitude than I expected. This probably isn't a big problem - a zero embedding is fine! @Rocketknight1 That's right. 
I didn't consider that the new value of `Z` has also bias in it.

After considering the new value of `Z`, the new bias should be initialized with the old bias mean. I have tried to be more focused this time. I hope I am not wrong again :sweat_smile:

this link contains a proof that should be replaced with ""Averaging bounds the KL-divergence"" part in the article: https://imgur.com/a/ZZQ3Mwk I have initialized the new bias like this:
```python
new_lm_head.bias.data.normal_(mean=bias_mean, std=bias_std * 1e-5)
```
I have multiplied the std by 1e-5 just because the article initializes the embeddings by multiplying the covariance by 1e-5. I don't really know why, maybe to make the new embeddings alot closer to the mean. The only thing left is to fix the failing test which are related (example torch for example) 🤗  @ArthurZucker Thanks for your review.
Regarding tests, all tests have passed after rebasing.  All good for me! 🤗  Congrats on the PR @abuelnasr0, and thank you! @Rocketknight1 Thank you for the reviews and the help.My overall impression is that this is a great addition that we should definitely merge, because the existing behaviour is highly undesirable, as mentioned in the [article](https://nlp.stanford.edu/~johnhew/vocab-expansion.html). My issues are basically nits:

- We have a mild preference for avoiding ""math variable"" naming, though I don't want to bloat the code either. Maybe replace `mu` and `cov` with `mean_embedding` and `covariance` or `covariance_matrix`?
- It'd be great if we could add a small test for this. There is a `test_resize_tokens_embedding` in `tests/test_modeling_common.py`, and the test could either be appended to that, or as a new test just below it. The test could check that new tokens are relatively close to the mean of the old tokens, though please set the tolerances quite loose - it'll be a real pain if the test becomes flaky because an outlier value is sampled! LGTM! One final extra-minor nit, but I'm happy with it now. cc @lysandrejik for final review Thanks! I think we need a test for `test_resize_tokens_embeddings` with deepspeed and multi GPU as we had a lot of breaking changes in the past year around it! #26102 #32192 #21065 etc etc Looks great to me, it's abreaking change so let's make sure we warn users about it!  Okay looks great! Given how big of a change this is, let's add a flag to `_get_resized_embeddings`, something like `multivariate_resizing` WDYT?  Thanks, getting in a good shape! Let's help our users a little bit more, and make the code more readable !  Cleaner! Thanks a lot for this update","My overall impression is that this is a great addition that we should definitely merge, because the existing behaviour is highly undesirable, as mentioned in the [article](https://nlp.stanford.edu/~johnhew/vocab-expansion.html). My issues are basically nits:

- We have a mild preference for avoiding ""math variable"" naming, though I don't want to bloat the code either. Maybe replace `mu` and `cov` with `mean_embedding` and `covariance` or `covariance_matrix`?
- It'd be great if we could add a small test for this. There is a `test_resize_tokens_embedding` in `tests/test_modeling_common.py`, and the test could either be appended to that, or as a new test just below it. The test could check that new tokens are relatively close to the mean of the old tokens, though please set the tolerances quite loose - it'll be a real pain if the test becomes flaky because an outlier value is sampled! LGTM! One final extra-minor nit, but I'm happy with it now. cc @lysandrejik for final review Thanks! I think we need a test for `test_resize_tokens_embeddings` with deepspeed and multi GPU as we had a lot of breaking changes in the past year around it! #26102 #32192 #21065 etc etc Looks great to me, it's abreaking change so let's make sure we warn users about it!  Okay looks great! Given how big of a change this is, let's add a flag to `_get_resized_embeddings`, something like `multivariate_resizing` WDYT?  Thanks, getting in a good shape! Let's help our users a little bit more, and make the code more readable !  Cleaner! Thanks a lot for this update","# What does this PR do?

This PR initializes new tokens embeddings from a normal distribution with the old embeddings' mean and covariance. as described in this article https://nlp.stanford.edu/~johnhew/vocab-expansion.html. Thanks to this article, you can now add new tokens to your model without affecting its generation accuracy. 

Fixes #32948 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

cc: @ArthurZucker @LysandreJik 

","```suggestion
            old_embeddings_mean = torch.mean(model_embed.weight.data[:-10, :], axis=0)
            new_embeddings_mean = torch.mean(model_embed.weight.data[-10:, :], axis=0)
```
For consistency, let's use -10 in both places! one thing we need to make sure of is that this does not have issue with deepspeed and multinodes! (as for example the mean would require an all gather I think. This computation has to be done on gpu 0 is what my intuition tells me)  That is absolutely right, thanks for mentioning that. I have added support for Deepspeed and tested it manually. I will add test cases for it tomorrow! @ArthurZucker I have added tests for deepspeed but they fail because the test env doesn't have deepspeed installed. what could be a solution for this? should I add the tests to `transformers/tests/deepspeed/test_deepspeed.py` for example? @abuelnasr0 when tests depend on a library like deepspeed, tag them with the `@require_deepspeed` decorator so the test runner knows how to handle them! You can search the codebase for other examples where it's used and just copy the imports/decorator from there. @Rocketknight1 Thank you for your response. It turns out that I was importing Deepspeed without checking if it was available. I fixed it.
@Rocketknight1 @ArthurZucker The code is ready for review now. Could you check it and see if the deepspeed tests are all right? Sorry for the delay! This is a breaking change so can you add 🔴 🔴 🔴 🔴  @ArthurZucker  I have added 🔴 to the PR title. Is that what you meant? or should I add a `logger.warning()` to the code describing the new change in initializing embedding weights? >let's make sure we warn users about it!

I didn't see this comment the first time I opened the PR! I will add a warning to the user describing new changes. ```suggestion
            logger.warning_once(
``` ```suggestion
                old_centered_embedding = old_embeddings_weight - mean_embedding
                covariance = old_centered_embedding.T @ old_centered_embedding / old_num_tokens
```
let's aim for code readability same as below!  same as above! ```suggestion
                ""The new embeddings will be sampled from a multivariate normal distribution that has old embeddings' mean and covariance. ""
                ""As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `multivariate_resizing=False`""
``` all of this can be re-used no? As a `""self.init_tensor""` which checks if deepspeed is available, computes the covariance if not given, uses None otherwise here you could re-use and here as well!  same comment about the covriance computation!  I have introduced three functions:
- `self._init_added_embeddings_weights_with_mean()`
- `self._init_added_lm_head_weights_with_mean()` and it uses `self._init_added_embeddings_weights_with_mean()`
- `self._init_added_lm_head_bias_with_mean()`
This will improve code usability for our case. what do you think? I am open to any other change.

Also, I think that `mean_resizing` is more user-friendly and explains the whole point of the new resizing technique."
33964,2024-10-04T20:41:56Z,2024-10-04T20:45:37Z,ArthurZucker,2,0,1,60,4,0,0,[],230.0,0,1435.0,0,0,0,0,7168337.892996,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'ArthurZucker']",fyi @LysandreJik function was just unused The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33964). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33958,2024-10-04T17:49:49Z,2024-10-04T19:35:31Z,ArthurZucker,4,0,1,6,1,0,0,[],1403.0,0,11009.0,0,0,0,0,7169091.282013,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'mranzinger', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33958). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I think this is still broken. `self.position_embedding` is of type `nn.Embedding`, which means it doesn't have a `.shape` variable.

I think you want to bring back
```
position_embeddings = self.position_embedding.weight.unsqueeze(0)
``` 🫠 yep Opening a new PR ",,"# What does this PR do?
Fixes #33929",
33363,2024-09-06T21:15:46Z,2024-10-04T19:39:45Z,garg-amit,9,30,31,2684,16,2,1,[],862730.0,0,2413459.0,0,0,0,0,7173485.518717,,0,31,0,False,"['merryHunter', 'HuggingFaceDocBuilderDev', 'garg-amit', 'ArthurZucker']","@ArthurZucker @gante can I please get a review? Hi, it seems to be a very important and awaited PR!:) Other frameworks are willing to integrate MoE too, like in litgpt [https://github.com/Lightning-AI/litgpt/issues/1686](https://github.com/Lightning-AI/litgpt/issues/1686).  We are very much willing to integrate it as well 🤗 just came back from the torch conf, was a bit OO because of it 😢  TLDR, overall the mixer needs to be properly documented and written to be more understandable!  @ArthurZucker Thanks for reviewing the PR. I’ve refactored the code according to your suggestions, and it’s ready for another look. Also, the failing test case appears to be unrelated to this PR. Please let me know if it needs to be addressed. Reviewing! 🤗  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33363). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker Thanks for reviewing! I've addressed the comments and moved `PhimoeRotaryEmbedding` out of the `PhimoeAttention` class. Thanks everyone and @garg-amit for bearing with me! Congrats on the model release again 🤗 Thanks! 
Let's go with camel cased classes, If we want to be compile compatible we need to have a script conversion and use the formulation from gpt fast moe with a version implemented here: https://github.com/huggingface/transformers/pull/30793/files#diff-733ab0a772c69f78b1d8ed361e6ae1fda7243652887aed0bab5d3ecf07794c01R789

Lot's of stuff seems similar to phi3 so we can probably copy from it!  LGTM, the only thing needed to merge: 
1. The `Copied` from need a capital letter
2. The core part needs a tad bit more doc as I said, why do we need a specific gradient computation (had to go through the paper to see that indeed you need a special gradient approx)
3. That part of the code is IMO less readable than the rest, but fine for now! 
THanks and sorry for the late revies!  Great work! Thanks for integrating this new model 🔥 ","Thanks! 
Let's go with camel cased classes, If we want to be compile compatible we need to have a script conversion and use the formulation from gpt fast moe with a version implemented here: https://github.com/huggingface/transformers/pull/30793/files#diff-733ab0a772c69f78b1d8ed361e6ae1fda7243652887aed0bab5d3ecf07794c01R789

Lot's of stuff seems similar to phi3 so we can probably copy from it!  LGTM, the only thing needed to merge: 
1. The `Copied` from need a capital letter
2. The core part needs a tad bit more doc as I said, why do we need a specific gradient computation (had to go through the paper to see that indeed you need a special gradient approx)
3. That part of the code is IMO less readable than the rest, but fine for now! 
THanks and sorry for the late revies!  Great work! Thanks for integrating this new model 🔥 ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Integrates `PhiMoE` into transformers. https://huggingface.co/microsoft/Phi-3.5-MoE-instruct


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker @gante 
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","unrelated change it seems no?  ```suggestion
    from .models.phimoe import PhimoeConfig
```
let's follow `OlmoeForCausalLM` convention of camel cased ```suggestion
``` ```suggestion

    Example:

    ```python
``` Let's use this one: https://github.com/huggingface/transformers/blob/7e638ef2b8650aaa3e3a8e575bb63af262a43d95/src/transformers/models/olmoe/configuration_olmoe.py#L167-L171
```suggestion
``` This was updated to https://github.com/huggingface/transformers/blob/7e638ef2b8650aaa3e3a8e575bb63af262a43d95/src/transformers/modeling_flash_attention_utils.py#L183  ```suggestion
# Copied from transformers.models.jetmoe.modeling_jetmoe.load_balancing_loss_func
``` ```suggestion
```
new rope scaling should be added to this: https://github.com/huggingface/transformers/blob/7e638ef2b8650aaa3e3a8e575bb63af262a43d95/src/transformers/modeling_rope_utils.py#L353-L360
 to remove should be completely copied form gemm a same here ```suggestion
``` Euuu this looks like a simple up gate down mlp, copied form gemma as well no?  not sure I understand why we have this. If required we need to document why, we need better naming as well let's doccument these kind of stuff, why are we doing this, which paper / experiments, what is this gumbel sampling etc  this is very weird, we should either use the mixtral / switch transformers formulation, if they are equivalent, or we build form the latter, but we need to make this clean as it is a core component of the model. 


 I think this was removed form the mixtral modeling so to update as well  most probably copied from mixtral  we should not ignore copy here! copied from phi3 no?  this can also be copied from phi3 no? 
 Removed SigLIP Fixed Removed Updated it to use the `rope_config_validation` fn instead Updated Added the comment It's copied from `MixtralFlashAttention2`. Added the comment Removed yeah, it's copied from `transformers.models.mixtral.modeling_mixtral.MixtralBlockSparseTop2MLP`. Fixed"
32613,2024-08-11T23:30:52Z,2024-10-04T17:00:45Z,zhenglongjiepheonix,6,2,15,60,7,2,2,['run-slow'],1161.0,0,4642193.0,0,0,0,0,7183046.417888,,0,15,0,False,"['HuggingFaceDocBuilderDev', 'zhenglongjiepheonix', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32613). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Added a simple gradient match test in training mode @ArthurZucker . According to my local test, the backward latency remains pretty much the same as eager mode when steady(no matter whether cudagraphs are enabled), but we can still benefit from the forward pass in training, and training-time trace does help in my case where I need to do graph analysis under training mode. Failure is most probably not related to you, but the slow tests is bad / badly designed as it should either not use accelerate, or not use parallelism to make sure we are testing apples to apples. 
 Would be nice if you can update the tests 🙏🏻  Accelerate will kick in if memory is not enough. I think the best solution is just to use the current torch device rather than specifying `device_map='sequential'`, it might cause GPU OOM in this way however. Pytests seem to have problems in avoiding GPU memory fragmentation, see https://discuss.pytorch.org/t/torch-pytest-leads-to-memory-fragmentation-how-to-do-proper-integration-testing-of-a-lot-of-torch-models/201231 , I have run into similar issues where if I run single test alone it passes, and fails because of OOM when I run all the tests all together.

According to the failure information of OOM, the GPU memory allocated but not used is about 7MB, I think it's not significant so fragmentation is not so bad to cause the failure, we simply need GPUs with more memory. > Failure is most probably not related to you, but the slow tests is bad / badly designed as it should either not use accelerate, or not use parallelism to make sure we are testing apples to apples.

Yes, we should never use accelerate, but in order to make tests pass robustly, we might need GPU runners with more memory, T4 will definitely not be enough because simply loading single model like llama-7b without doing anything would require 13GBSounds good! similarly to https://github.com/huggingface/transformers/blob/209fcccbba41575fd0ece8b3ff20930dbac34a80/tests/test_modeling_common.py#L4679 can you add the training compile test? 🤗  Nice, can you try to run the slow tests for this! 🤗  Wow long due!","Sounds good! similarly to https://github.com/huggingface/transformers/blob/209fcccbba41575fd0ece8b3ff20930dbac34a80/tests/test_modeling_common.py#L4679 can you add the training compile test? 🤗  Nice, can you try to run the slow tests for this! 🤗  Wow long due!","This pr fixes a scenario where we want to use dynamo trace in training mode, the current attn mask ignore logic creates a problem where data-dependent branch condition `torch.all(attn_mask==1)` will cause graph breaks and disable full-graph tracing, the current solution is to disable mask ignore logic as long as we are in tracing mode no matter we are in training or inference phase.

This will enable compilation for training(forward+backward) like this:
```
model = LlamaForCausalLM(config).cuda()
model = torch.compile(model, fullgraph=True)
loss = model(**inputs)[0]
loss.backward()
```
","It's actually a paradox branch where `is_tracing` and `torch.all(attention_mask == 1)` can never exist together attn mask being full of one is more prone to skipping some branches, would try with ones and zeros as welll! "
33781,2024-09-27T22:43:11Z,2024-10-04T06:25:32Z,htahboub,7,1,6,192,48,3,2,[],215704.0,0,581499.0,0,0,0,0,7185801.637271,,0,6,0,False,"['htahboub', 'qubvel', 'HuggingFaceDocBuilderDev', 'amyeroberts']","Hi @htahboub, I don't think the proposed change is correct. The current logic condition is XOR i.e. it will fail if both or neither are specified, which is what we want to check for. 

If you think the error message can be improved or clarified feel free to propose an update!  Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). Hey @amyeroberts. Sure, I thought that may be the case, in which case the error message is a little unclear. I have made changes to make that more explicit. Thanks @amyeroberts, just made the change to the rest of the repo! Hi @htahboub, there are some tests failing, can you please run `make fix-copies`? Thanks @qubvel, just ran the command and pushed the changes. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33781). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the update! Thanks for updating! 

Could you roll out this change to all the other places in the library where the original message was used? I think it's in a few of our VLM models Thanks 🤗","Thanks for the update! Thanks for updating! 

Could you roll out this change to all the other places in the library where the original message was used? I think it's in a few of our VLM models Thanks 🤗","This error condition appears to be incorrect. Based on the error message, this should be triggered if both `input_ids` and `inputs_embeds` are provided. However, it is currently triggered if neither is provided.

@simonJJJ, @amyeroberts, @qubvel ","```suggestion
                ""You must specify exactly one of input_ids or inputs_embeds""
```"
33582,2024-09-19T06:47:30Z,2024-10-04T15:39:04Z,karan-uppal3,7,0,1,2,1,1,1,[],147065.0,0,1327913.0,0,0,0,0,7187929.662872,,0,1,0,False,"['LysandreJik', 'HuggingFaceDocBuilderDev', 'ArthurZucker', 'karan-uppal3']","Thanks for the PR @karan-uppal3 !

cc @ArthurZucker when you have a second The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33582). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hey @ArthurZucker! I went through the tests and found unit tests for the `SwitchTransformersTop1Router` module but not for the `SwitchTransformersSparseMLP` module, which is where the unnecessary transpose exists ([link](https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L300)). This is why upon running `pytest` I am getting the correct output. Did you run the tests with `RUN_SLOW=1`? https://github.com/huggingface/transformers/blob/21beb57558f90131439bf03e1da672d117777901/tests/models/switch_transformers/test_modeling_switch_transformers.py#L1033 are the relevant tests!  Hey @ArthurZucker, I just ran the tests with `RUN_SLOW=1`, this is the output I am getting

```
==================================================================================================== test session starts ====================================================================================================
platform linux -- Python 3.8.12, pytest-7.4.4, pluggy-1.0.0
rootdir: /home/aiscuser/transformers
configfile: pyproject.toml
plugins: rich-0.1.1, timeout-2.3.1, dash-2.18.1, hypothesis-6.112.2, xdist-3.6.1, pythonpath-0.7.4, cov-3.0.0
collected 299 items                                                                                                                                                                                                         

tests/models/switch_transformers/test_modeling_switch_transformers.py ....s.........s.........s.......sssss.sss.ssssssssssss.ss.ss..ss........ssss......sss...........s.s...ssss.ssssssssssssssssssssssssss..ss..ssss [ 47%]
ssss..ssssssssssss...s.s...........sss.sss.sss.sss.........s...s...ssssss.sssssssssss....ssss......ss.........s.s....ssss........ssssss.sss.sssssss......sss                                                          [100%]

===================================================================================================== warnings summary ======================================================================================================
../../../opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14
  /opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.
    from scipy.sparse.base import spmatrix

src/transformers/deepspeed.py:24
  /home/aiscuser/transformers/src/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
    warnings.warn(

tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_attention_outputs
  /home/aiscuser/transformers/src/transformers/generation/configuration_utils.py:775: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.
    warnings.warn(

tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_batching_equivalence
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_headmasking
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_hidden_states_output
  /home/aiscuser/transformers/src/transformers/generation/configuration_utils.py:775: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.
    warnings.warn(

tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_disk_offload_bin
  /home/aiscuser/.local/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
    return self.fget.__get__(instance, owner)()

tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_feature_extraction
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_feature_extraction_fp16
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_summarization
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_summarization_fp16
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_text2text_generation
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_text2text_generation_fp16
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_translation
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_translation_fp16
  /home/aiscuser/transformers/src/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
    warnings.warn(

tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_summarization
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_summarization_fp16
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_text2text_generation
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_text2text_generation_fp16
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_translation
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_pipeline_translation_fp16
  /home/aiscuser/transformers/src/transformers/generation/utils.py:1230: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================================================================= 145 passed, 154 skipped, 21 warnings in 138.15s (0:02:18) =================================================================================
``` Cool! Tested locally as well the skipped tests (generation ones) not affected thanks!Hey! We have run heavy integration tests here: https://github.com/huggingface/transformers/blob/f83917ed89292072e57c4ba013d1cc6477538b11/tests/models/switch_transformers/test_modeling_switch_transformers.py#L902 

Could you make sure they are either:
- wrong (meaning the output we have today are just wrong)
- right (meaning this does not influence the output)

Otherwise, makes sense and quite supprised that we did not catch this before 😢 ","Hey! We have run heavy integration tests here: https://github.com/huggingface/transformers/blob/f83917ed89292072e57c4ba013d1cc6477538b11/tests/models/switch_transformers/test_modeling_switch_transformers.py#L902 

Could you make sure they are either:
- wrong (meaning the output we have today are just wrong)
- right (meaning this does not influence the output)

Otherwise, makes sense and quite supprised that we did not catch this before 😢 ","# What does this PR do?

There is an additional transpose operation at [link](https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L300) in the Switch Transformer Implementation. The shape of the tensor is [batch_size, seq_len, num_experts] which undergoes `.transpose(1,2)` to get the shape [batch_size, num_experts, seq_len], which is later `.reshape(batch_size * seq_len, num_experts)`. Due to the `transpose` the routing gives incorrect results. Simply removing the `.transpose(1,2)` gives the correct output. A simple reproducible example can be found at #33463.

Fixes #33463 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@LysandreJik @ArthurZucker @younesbelkada

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33924,2024-10-03T16:28:46Z,2024-10-04T13:40:44Z,Rocketknight1,2,2,2,127,1,2,1,[],1458.0,0,76321.0,0,0,0,0,7195046.582046,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33924). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. I'm happy with this now - merging without core maintainer review since it's a pure documentation changeSuper clear, I really like it! I hope model authors read it 😅 ","Super clear, I really like it! I hope model authors read it 😅 ","The chat template docs have an extensive section on using tool templates, but they're missing good guidance to template authors. I added a section to help them, and also so there's a point of reference for the API we expect tool templates to follow.","It is clear that these are examples and there's also a mention at the end, but maybe we can reinforce even more and explicitly state it here. Done!"
33939,2024-10-04T05:57:41Z,2024-10-04T12:46:15Z,ArthurZucker,1,2,2,2,1,1,1,[],1390.0,0,24515.0,0,0,0,0,7198317.928911,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33939). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.OK for me - it might take some extra time in this job.,OK for me - it might take some extra time in this job.,"# What does this PR do?
Update workflow to run on latest torch, not torch docker installed ","```suggestion
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e . && python3 -m pip install --upgrade torch torchaudio torchvision
``` please give me some space!"
33753,2024-09-27T08:58:08Z,2024-10-04T10:39:37Z,zucchini-nlp,2,0,4,59,3,2,3,[],1409.0,0,610889.0,0,0,0,0,7205917.770077,,0,4,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33753). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804).Nice, thanks! Could you add tests that ensures that:

1. A warning is printed when an unneeded key is sent
2. The warning isn't printed when `ignore_keys` is correctly set

Thanks! Awesome, thanks a lot :raised_hands:  LGTM, thank you for fixing it 🙏 ","Nice, thanks! Could you add tests that ensures that:

1. A warning is printed when an unneeded key is sent
2. The warning isn't printed when `ignore_keys` is correctly set

Thanks! Awesome, thanks a lot :raised_hands:  LGTM, thank you for fixing it 🙏 ","# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/33401. Adds ignore_keys argument when checking received keys and adds `mrope_section` in qwen2-vl config file as a key we should ignore

I'll open PRs on the hub to change rope type from mrope to default, but until ten decided to leave the comment. For curious users who might open up more issues trying to find why we do manual modifications on rope type :)",
33882,2024-10-02T07:41:22Z,2024-10-04T09:21:38Z,Artanias,0,0,1,2,1,1,1,[],,0,180529.0,0,0,0,0,7208884.687616,,0,1,0,False,[],Thanks!,Thanks!,"# What does this PR do?

This PR fixes a typo in the `README_ru.md`.

## Who can review?

I would appreciate it if you could spend some time reviewing the changes in documentation: @stevhliu.
",
33920,2024-10-03T15:30:20Z,2024-10-04T09:18:01Z,ylacombe,2,0,2,20,2,1,1,['run-slow'],1530.0,0,64428.0,0,0,0,0,7210447.880369,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'ylacombe']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33920). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. They also tried with the latest transformers version,and used the ASR pipelineOkay, this is in  the branch when the cache is not `EncoderDecoderCache`, which means they are probably not using latest transformers no? Or do we not properly default to use it?","Okay, this is in  the branch when the cache is not `EncoderDecoderCache`, which means they are probably not using latest transformers no? Or do we not properly default to use it?","# What does this PR do?

Distilled versions of Whisper couldn't return segments because of how they computed past key values. This introduces a test that failed before introducing the fix.

cc @eustlb @LysandreJik 

",
33906,2024-10-03T09:58:43Z,2024-10-03T15:45:14Z,gante,2,0,4,14,2,1,1,[],1637.0,0,20794.0,0,0,0,0,7273980.570313,,0,4,0,False,"['HuggingFaceDocBuilderDev', 'regisss']",The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33906). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. Thanks for the fix Joao!Ok!,Ok!,"# What does this PR do?

Round 2, after #33886 -- TL;DR stops raising exceptions when saving a model config with generative parameters, if the user is not responsible for those parameters being there.

(copied from 33886)
```
In a recent PR (https://github.com/huggingface/transformers/pull/32659), we started raising exceptions if we were to store a model config with generative parameters. If we were saving a model, which saves the config, the generative parameters were moved to GenerationConfig before saving the PreTrainedConfig, working around the issue when saving a model.

However, a common use case was uncovered. If a user loads a PreTrainedConfig with generative parameters and tries to save it, the user has done nothing wrong and yet they see an exception. This PR lowers the exception to a warning in that situation, preventing the code from crashing :)
```

33886 would be my intended solution to the problem, but I ran into multiple implementation issues which I'll iron out in separate PRs 👀 This PR is a poor man's version of the solution, to unblock others: simply lower the exception to a warning. 

(cc @regisss )

",
32568,2024-08-09T16:10:30Z,2024-10-03T16:08:24Z,yonigozlan,4,26,9,569,6,6,1,[],1164.0,0,4751874.0,0,0,0,0,7272593.945693,,1,9,0,False,"['HuggingFaceDocBuilderDev', 'yonigozlan']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32568). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. This should now be ready for review! Cc @molbap @amyeroberts .
It also brings some changes to test_processing_common that could benefit other models.
I think with @molbap PR https://github.com/huggingface/transformers/pull/31368 and the following pending PRs https://github.com/huggingface/transformers/pull/32544 https://github.com/huggingface/transformers/pull/33668 https://github.com/huggingface/transformers/pull/32181 merged, that would be all the image-text-to-text processors uniformized! Hey! 🤗 Thanks for your contribution to the `transformers` library!

Before merging this pull request, slow tests CI should be triggered. To enable this:
- Add the `run-slow` label to the PR
- When your PR is ready for merge and all reviewers' comments have been addressed, push an empty commit with the command `[run-slow]` followed by a comma separated list of all the models to be tested, i.e. `[run_slow] model_to_test_1, model_to_test_2`
  - If the pull request affects a lot of models, put at most 10 models in the commit message
- A `transformers` maintainer will then approve the workflow to start the tests

(For maintainers) The documentation for slow tests CI on PRs is [here](https://www.notion.so/huggingface2/CI-for-pull-requests-8335bd9217d24d1e8ed1d3a016f39804). Hey @ArthurZucker !
This should be ready for a final review.
A little overview of the changes:
- Idefics processor needed quite a lot of added logic to work with the new standardized processor signature, as it used to take prompts inputs only and not separate between images and text, but full BC should be supported still.
- Idefics2 processor uniformization is quite straightforward, and is now very close to Idefics3 processor.
- Idefics processor tests needed a bit of overriding as it is very different from other vlms processor in Transformers, and notably doesn't take in `do_rescale` or `scale_factor` as args for its call function, which we use in common tests.
- Idefics2 processor tests needed only overriding the mock inputs, as the text prompts need to include the <image> token for each corresponding input images, and the processor needs nested images when working with batched inputs.
Thanks you!Also looks good to me. Just want to clarify what will be the new format for Idefics to make the pipeline happy. Maybe we can add a test for that new format :) Thanks for working on this processor! 

Let's split up the changes in the processor tests from the changing of the processor. LGTM appart from mentioned breaking change for idefics","Also looks good to me. Just want to clarify what will be the new format for Idefics to make the pipeline happy. Maybe we can add a test for that new format :) Thanks for working on this processor! 

Let's split up the changes in the processor tests from the changing of the processor. LGTM appart from mentioned breaking change for idefics","# What does this PR do?
Adds uniformized processors kwargs following https://github.com/huggingface/transformers/issues/31911 for the following models:
- [x] Idefics
- [x] Idefics2
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
@molbap @zucchini-nlp @amyeroberts 
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Lots of logic is needed for backward compatibility, as idefics used to take only `prompts` where text and images inputs would be interleaved. This added logic preserve supports for these kind of inputs (where `prompts` is replaced by `text` arg), while adding support for usual text and images inputs as in other image-text-to-text models. This will also be useful to support idefics in the image-text-to-text pipeline. I guess this code block is for new processing behavior when users pass images and text.

Not very sure this is a good idea to repeat text several times. Suppose user has one prompt with interleaved images-text, then we would replicate the prompt several times and cause error in downstream modeling code. For ex:

`processor(text=[""User: What do you see here? Assistant: a cat. User: what about this image?""], images=[image1, image2])` we can also put `return_tensors` as `pt` in defaults nit: if this is to be skipped always, we can use `@unittest.skip` decorator and don't have to write the whole body. Also nice to write idefics-specific tests in that case to verify we handle images correctly imo we don't have to overwrite all tests for idefics, seems quite similar to the general test from mixin except for ` padding=""max_length""`. We can and maybe should indicate padding=""max_length"" in mixin tests, cause we can't assume all tokenizer will default to ""max_length"" padding Yes that's a good point. Although interleaved images-text is not really supported when providing both images and text for Idefics, as there is no way to indicate where to put the images in the prompt. Maybe I should add a warning here instead of automatically duplicating the prompts? Ah I see now, indeed Idefics is a bit peculiar.

Yes interleaving like that is not, but providing more than 1 image per prompt like in multi-turn conversation is okey, as in the dosctring of call method. Then we should expect users to pass as many images as prompts, and they would have to wrap images as a batched list if there's more than one per prompt. 

I think we can even raise an error, as we cannot know for sure what is the user expecting with these inputs. An error explaining what kind of input we want and let the user fix it, otherwise users who never read warnings might start complaining in the issues :) ```suggestion
        output_kwargs = self._merge_kwargs(
            IdeficsProcessorKwargs,
            tokenizer_init_kwargs=self.tokenizer.init_kwargs,
            **kwargs,
        )
        # Temporary fix for ""paddding_side"" in init_kwargs
        _ = output_kwargs['text_kwargs'].pop(""padding_side"", None)

```

Removing the argument from the tokenizer init_kwargs might lead to other unintended effects. It is always preferable to modify local variables when doing something a bit hacky :)  Good point! will modify this. Added support for multiple images per prompt, and this warning to make it clearer what input format we expect when using image-text-to-text format:
https://github.com/huggingface/transformers/blob/8b171a777bac10bbb9c9a13bd36d6ffd10be9b9d/src/transformers/models/idefics/processing_idefics.py#L353-L358 +1 - let's do this instead of overriding in all these tests  Same thing here - do we need to override all of these? We should still keep return_tensors in the docstring as it's a necessary argument to properly set e.g. pytorch tensors output, is common across all processors and is independent of the processing class (tokenizer / image processor etc.)  Let's wait to merge until we have the work for adding this logic to processing_utils Recently `prepare_image_inputs` was moved out to the ProcessorTesterMixin to remove this repeated code. I'd instead remove this method and add batching logic to the mixin's method Rather than `batched`, I'd have this as an optional int, which returns a single image if not set and a list of `n` images if set  Lets keep this in the docstring Note on that matter @yonigozlan there are some issues when using a rescale factor of -1 on a `uint8` 0-255 ranged image: it causes an overflow - easy to fix either in `rescale` or in this `prepare_image_inputs` (I'd go for the latter). Noticed that while trying to fix other broken tests!  @amyeroberts I see, I'll add the batching logic to the mixin method by giving an int as you suggested above. However I think I will still have to override the prepare_image_inputs here, as Idefics2 expects batched images to be formatted as `[[images_for_text1], [images_for_text2]]`. 
@molbap That's weird I thought it would auto cast them to int16, do you have an example I can reproduce? OK, makes sense. Regarding the batching behaviour - is this not how all the processors expect the batching to occur given a variable number of images per sample? If not - then we probably have another round of work to standardize this across the processing classes  It seems like not all models support batching in that sense... For example, Paligemma or Udop don't support nested images, so I'm not sure how/if they can deal with a variable number of images per sample. I will look into that more once I start working on the image-text-to-text pipeline again @yonigozlan OK, sounds good!  The batching is indeed different for models with cross attention, as those require to pack and pad images with exactly `batch_size, num_images, ...` shape

For paligemma we recently did a fix to follow llava-like style, given the general architecture is +- same. Even though we can make idefics accept flat list of images (by inferring how many image tokens a each text has), I'm not sure it is intuitive. I remember we had similar discussion with @amyeroberts months ago, before accepting the `flat list` as possible input format. But now I am starting to think, it was a bad idea to push that into `main` hehe

But yeah, removing smth that users got used to is too much pain, so I guess we'll stick to accepting both formats... we have a breaking change here no? `prompts` will be passed by previous workflows (ex: `prompts=xxx`), and we don't check if `prompt in kwargs` I think this should be handled by https://github.com/huggingface/transformers/blob/7a63c6f7c11ecdb423bedb5558b2cbe32c43ed37/src/transformers/models/idefics/processing_idefics.py#L236
with `prompts` being replaced by `text` automatically Ah right! missed this indeed, good to go then! "
32481,2024-08-07T01:50:23Z,2024-10-03T15:03:42Z,jerryzh168,9,4,2,22,1,3,3,[],76541.0,0,4972416.0,0,0,0,0,7276459.168547,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'amyeroberts', 'ArthurZucker', 'jerryzh168']","I learned from @mikaylagawarecki that `weights_only` is going to be set to True by default in the future. but this flag could still be helpful if people are using older versions of pytorch and want to use torchao I think Hi @ArthurZucker can you take a look at this PR? please let me know if we want to add a test, but load test is a bit harder to write because it needs to upload some models like: https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/tests/test_modeling_utils.py#L930 Can you rebase on main and resolve the conflicts?  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32481). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. thanks for the review @ArthurZucker and @amyeroberts I just updated the PR, please feel free to merge when the CI is green @jerryzh168 Could you run `make fix-copies` and push the changes? This should resolve the failing quality checks thanks, updated, please take a look again @amyeroberts  Thanks once more @jerryzh168 Thanks for adding this support! 

Overall, I think this looks good. Let's get a second 👍 from @ArthurZucker too as it's touching core code  LGTM sorry for being late on the review here!   let's revert unrelated changes and let's go  Sorry for the delay would kindly ask to rebase to make sure we are up to date as other things were merged ! ","Thanks for adding this support! 

Overall, I think this looks good. Let's get a second 👍 from @ArthurZucker too as it's touching core code  LGTM sorry for being late on the review here!   let's revert unrelated changes and let's go  Sorry for the delay would kindly ask to rebase to make sure we are up to date as other things were merged ! ","Summary:
This is to enable loading a state_dict with wrapper tensor subclasses (used in torchao to for quantized weights)

Test Plan:
tested locally with torchao weights, also need https://github.com/huggingface/transformers/pull/32306:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import TorchAoConfig
from torchao.utils import benchmark_model
import torchao

DEVICE_TYPE = ""cuda""

def init_model_and_benchmark(model_id, torch_dtype=torch.bfloat16, quantization_config=None):
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    if quantization_config is not None:
        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=DEVICE_TYPE, torch_dtype=torch.bfloat16, quantization_config=quantization_config)
    else:
        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=DEVICE_TYPE, torch_dtype=torch.bfloat16, weights_only=False)

    # sanity check: run the model
    input_text = ""What are we having for dinner?""
    input_ids = tokenizer(input_text, return_tensors=""pt"").to(DEVICE_TYPE)
    output = model.generate(**input_ids, max_new_tokens=1000)
    print(tokenizer.decode(output[0], skip_special_tokens=True))

    NUM_WARMUP = 1
    NUM_RUNS = 5

    if quantization_config is not None:
        torchao.quantization.utils.recommended_inductor_config_setter()

    model = torch.compile(model, mode=""max-autotune"")

    benchmark_model(model.generate, NUM_WARMUP, kwargs=input_ids, device_type=DEVICE_TYPE)
    print(""running benchmark"")
    results = benchmark_model(model.generate, NUM_RUNS, kwargs=input_ids, device_type=DEVICE_TYPE)
    return model, results

model_id = ""jerryzh168/test-model""
torchao.quantization.utils.recommended_inductor_config_setter()
bf16_model, bf16_time = init_model_and_benchmark(model_id)
print(f""bf16: {bf16_time}"")
```

Reviewers:

Subscribers:

Tasks:

Tags:
","If we're emitting to the terminal, it should be using `logging`. I don't think this would be very informative for the user however

```suggestion
``` this unrelated change should not be included here ! 🤗  @ArthurZucker this is fix from running `make fix-copies` 😅 ok no worries then let's merge! "
33911,2024-10-03T12:11:28Z,2024-10-03T14:34:10Z,ArthurZucker,1,1,2,7,1,1,0,[],1441.0,0,8564.0,0,0,0,0,7278247.829601,,0,2,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33911). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.,,"# What does this PR do?
Following #32385, adding a setter to not break TRL!","```suggestion
    def tokenizer(self, processing_class) -> None:
```"
33372,2024-09-08T04:40:06Z,2024-10-03T13:30:03Z,KanTakahiro,5,0,2,2,1,1,0,[],75295.0,0,2194443.0,0,0,0,0,7279451.064606,,0,2,0,False,"['HuggingFaceDocBuilderDev', 'KanTakahiro', 'ArthurZucker']","It seems that the CI test environment doesn't have unidic-lite installed so the test failed. Would you like me to push directly?  Sure! Please push it directly! I am not very familiar with how to do this. The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33372). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @KanTakahiro it was just the default value that needed an update!Hey! The docker is created here: https://github.com/huggingface/transformers/blob/main/docker/custom-tokenizers.dockerfile#L20 but what you can do is change this line: 


adding:
```
    install_steps=[""uv venv && uv pip install .[ja]""],
```
will make sure it's installed!

cc @ydshieh I'll open a PR for all ci jobs afterwards to properly rely on extras mapped to names!","Hey! The docker is created here: https://github.com/huggingface/transformers/blob/main/docker/custom-tokenizers.dockerfile#L20 but what you can do is change this line: 


adding:
```
    install_steps=[""uv venv && uv pip install .[ja]""],
```
will make sure it's installed!

cc @ydshieh I'll open a PR for all ci jobs afterwards to properly rely on extras mapped to names!","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes the [issue](https://github.com/huggingface/transformers/issues/32482) that transformers default request `ipadic` as the tokenization dictionary of Japanese language. As the `ipadic` is old and `ipadic` developers suggest to use `unidic-lite`.(see their [GitHub](https://github.com/polm/ipadic-py))


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. -> **[this issue](https://github.com/huggingface/transformers/issues/32482)**
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests? **No but I modified the source code and ran my project without any issue.**


## Who can review?

Library:

- tokenizers: @ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker

Library:

- tokenizers: @ArthurZucker 

 -->

## First-time Contributor Note

This is my first time contributing to an open source project. I've done my best to follow the project's guidelines and coding standards, but please let me know if there's anything I need to adjust or improve. Thank you for considering my contribution!",
33685,2024-09-24T18:13:35Z,2024-10-03T13:01:24Z,gante,2,14,16,1147,34,4,3,[],1601.0,0,758873.0,0,0,0,0,7283812.748994,,0,16,0,False,"['HuggingFaceDocBuilderDev', 'gante']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33685). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker that is a cool idea, model-agnostic inputs, potentially dependent on modality (or perhaps simply looking at the signature of the fwd pass?)

I took note of it to explore after the current round of reactors, to avoid adding more parallel threads :D Awesome! So much code cleaned up, thanks! 💓 

Overall looks good to me, just a few question for my general understanding. I see some VLMs are failing the CI. I remember skipping one of the beam search tests for VLMs earlier so it's prob that. But lmk if you want me to look at it :) Very welcome and clean changes! Nice to see less tests being overwritten :ok_hand:  Very welcome.

IMO we should have:
- a dict of common forward input names, with default values, that range from 0 to the small model vocab size. 
- a variant for vLMs
- a variant for image only 
- a variant for audio 
And then you just take them using inspect to inspect the forward pass. 

Now for testing the padding and else, of course you need somethibng else, but this way people don't have to do this ever again: https://github.com/huggingface/transformers/blob/1dba608df93ffb10a9c268ef35191adf2424c5ca/tests/models/sam/test_modeling_sam.py#L448-L458
","Awesome! So much code cleaned up, thanks! 💓 

Overall looks good to me, just a few question for my general understanding. I see some VLMs are failing the CI. I remember skipping one of the beam search tests for VLMs earlier so it's prob that. But lmk if you want me to look at it :) Very welcome and clean changes! Nice to see less tests being overwritten :ok_hand:  Very welcome.

IMO we should have:
- a dict of common forward input names, with default values, that range from 0 to the small model vocab size. 
- a variant for vLMs
- a variant for image only 
- a variant for audio 
And then you just take them using inspect to inspect the forward pass. 

Now for testing the padding and else, of course you need somethibng else, but this way people don't have to do this ever again: https://github.com/huggingface/transformers/blob/1dba608df93ffb10a9c268ef35191adf2424c5ca/tests/models/sam/test_modeling_sam.py#L448-L458
","# What does this PR do?

Requirement for #33212
Follow-up to #33663  

This PR rewrites the LLM-centric `_get_input_ids_and_config()`, the function that creates random model inputs for tests, into a modality-agnostic `prepare_config_and_inputs_for_generate()`

The rest of the diff consists in propagating the change. Highlights:
1. most `_get_input_ids_and_config()` overwrites were deleted as a result of the changes 🔪 
2. most test generate calls receive a dictionary of input, as opposed to `input_ids`
3. `_check_outputs` now receives the model's main input, as opposed to `input_ids`
4. because of the changes above, a few test overwrites that needed to be updated could be deleted instead 🙏 

___________________________

In a follow-up PR: hunt tests that no longer need to be skipped/overwritten as a result of these changes 🎯 ","some models were overwritting `_check_outputs` to apply subsampling on the sequence length. Since it was a single pattern in common with the overwrites, and overloading couldn't be applied here (it changes the internals), I've decided to move the pattern here common pattern: `input_mask`, which was then passed around as `attention_mask`, was a `torch.float32` instead of a `torch.long` 👀  `audio_channels=1` is the default in the config. In other words, this doesn't change the tests, but allows us to quickly override the config when needed (see overloaded test below) (all this deleted code corresponds to overwritten functions that no longer need to be overwritten) i just had realized, does this work for composite models or we also need to `config.get_text_config().eos_token_id`? Just a thought, since I started noticing some patterns where modeling code expects the text config to be modified

At least it was the case for `config.output_attentions` wondering what happens if we use `model_cls.main_input_name`? AFAIR from few months ago, there were some inconsistencies in how model main input is defined, and we could make another round of cleaning up on that because `main_input_name` is also used in `generate()`. Maybe we can have a more generalized interface and testing suite?
 totally unrelated to this PR: why is it `hasattr(config, ""use_cache"")` and not `getattr`? love it! nice to see a cleaner way That's very nice Good point! I agree it should be set in the text config, will update I agree we should use the model's `main_input_name`. I will make the change and see what breaks 🤞 

If all tests pass, I'll update it. Otherwise I'll add a TODO for us :) 👀 good catch, will change Not only it worked, but also it allowed us to remove the `input_name` attribute from all testers 💛 "
32652,2024-08-13T11:16:03Z,2024-10-03T12:49:56Z,ariepratama,3,7,5,175,2,2,0,['Core: Tokenization'],3372240.0,0,4412033.0,0,0,0,0,7284506.080148,,0,5,0,False,"['HuggingFaceDocBuilderDev', 'ariepratama', 'ArthurZucker']","> Looks alright, but in general we try to copy a lot from other tests, this is missing `Copied from` in a few places

thanks! I believe I've applied the comments  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_32652). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. THanks for your contribution! Looks alright, but in general we try to copy a lot from other tests, this is missing `Copied from` in a few places Update","Looks alright, but in general we try to copy a lot from other tests, this is missing `Copied from` in a few places Update","# What does this PR do?
* Add `SplinterTokenizer` unit test
* Pass `question_token` in `SplinterTokenizer.save_pretrained`, to ensure the saved tokenizer_config will have the same parameter.

Fixes #16627


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@ArthurZucker 





","let's rather load these from the hub please you should be able to add a list of tokens use # fmt:skip 
to avoid this super long line  oh right! thanks will change this! I think I can only use `fmt:on` and `fmt:off` so I have used that removed this new file and use the existing file! ```suggestion
            [101, 1109, 5855, 2068, 1113, 1103, 3751, 7956, 117, 2903, 4939, 1107, 1103, 4605, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1153, 19983, 170, 13108, 10851, 1111, 1123, 2104, 112, 188, 5913, 1710, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1109, 3336, 1383, 1166, 1103, 11385, 117, 3504, 1103, 3901, 1114, 18652, 5769, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```
let's remove all the space "
33632,2024-09-20T17:18:04Z,2024-10-03T12:41:50Z,Ben-Schneider-code,3,0,4,3,1,1,1,[],594542.0,0,1106632.0,0,0,0,0,7284986.157672,,0,4,0,False,"['HuggingFaceDocBuilderDev', 'Ben-Schneider-code', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33632). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. > This LGTM but can you please add a comment about why we are doing this? 🤗

Done! :+1: Thanks 🫡  This LGTM but can you please add a comment about why we are doing this? 🤗 ",This LGTM but can you please add a comment about why we are doing this? 🤗 ,"# What does this PR do?
Fixes #28808

Behaviour:  
```loaded_keys = {k.replace(f""{module_name}."", """") for k in state_dict_keys if k.startswith(f""{module_name}."")}``` in ```Trainer.py``` misses the case where the module_name is the empty string (the root module), as its looking for state_dict_keys that start with ```"".""```. This causes the root module (CLIPModel, AlignModel, etc.) to always have ```model.apply(model._initialize_weights)``` called on it. Which leads to initialization of an empty parameter tensor when running zero3 (that doesn't need to be initialized as it will be replaced later) in ```modeling_align.py```. The reason this causes a crash in ```modeling_align.py``` and not other models like CLIP is because ALIGN uses ```xavier_uniform_``` which asserts the tensor must be 2d whereas CLIP uses ```nn.init.normal_``` which just returns another empty tensor.

Fix:  
Use the whole state_dict as the root module's set of load_keys.

Please let me know if there is anything I should revise / this should be fixed in another way.😄 
@ArthurZucker @muellerzr

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?",
33849,2024-10-01T08:10:31Z,2024-10-03T12:39:49Z,glegendre01,1,5,11,184,7,2,2,[],1460.0,0,188959.0,0,0,0,0,7285113.237951,,0,11,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33849). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks a lot of this update! LGTM except a few nits. Thank you!,Thanks a lot of this update! LGTM except a few nits. Thank you!,"# What does this PR do?

Migrate the CI runners to the new clusters.
","```suggestion
          mkdir -p /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports
``` ```suggestion
        working-directory: /workspace/transformers
``` ```suggestion
        working-directory: /workspace/transformers
``` ```suggestion
    os.makedirs(out_path, exist_ok=True)
``` this image (deepspeed) is somehow a bit different"
33630,2024-09-20T16:33:53Z,2024-10-03T11:27:02Z,gante,2,1,1,3,1,3,1,[],1434.0,0,1104793.0,0,0,0,0,7289478.359648,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'gante']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33630). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. (cc @LysandreJik -- can we fix this band-aid for the time being? It is a common failure in our CI :) )Thanks for flagging it, will definitely check yes sure!","Thanks for flagging it, will definitely check yes sure!","# What does this PR do?

`test_static_cache_matches_dynamic` is VERY flaky on some VLMs. Tags it as flaky to avoid breaking CI while it is being investigated (cc @zucchini-nlp)

This test was enabled recently on VLMs on #33533 ",restores the original precision
33832,2024-09-30T22:46:36Z,2024-10-03T08:27:49Z,fadingNA,3,2,4,14,1,3,2,[],87328.0,0,207673.0,0,0,0,0,7300235.486373,,0,4,0,False,"['HuggingFaceDocBuilderDev', 'fadingNA']","- issue #33193  The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33832). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @SunMarc  I have updated into the _evaluate function and commit your suggestion case error.Thanks for the PR ! This won't solve the issue you linked but we can definitely merge the PR. Left a comment LGTM !  Awesome, thank you!","Thanks for the PR ! This won't solve the issue you linked but we can definitely merge the PR. Left a comment LGTM !  Awesome, thank you!","# What does this PR do?

### This PR improves the error handling in the `_save_checkpoint` 
- function by updating the `KeyError` message when the `metric_for_best_model` argument is not found in the evaluation metrics. The new error message provides clearer guidance, including suggestions on checking the `compute_metrics` function and updating `TrainingArguments` if necessary.


Fixes issues #33193

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@muellerzr and @SunMarc

","Need also to be changed in the _evaluate function
```suggestion
                    f""The `metric_for_best_model` training argument is set to '{metric_to_check}', ""
                    f""which is not found in the evaluation metrics. ""
                    f""The available evaluation metrics are: {list(metrics.keys())}. ""
                    f""Please ensure that the `compute_metrics` function returns a dictionary that includes '{metric_to_check}' or ""
                    f""consider changing the `metric_for_best_model` via the TrainingArguments.""
``` @SunMarc  Thanks Marc let me change in the _eval function"
33314,2024-09-05T06:22:12Z,2024-10-03T07:51:03Z,HofitBata,16,2,5,11,1,3,1,[],33700.0,0,2425674.0,0,0,0,0,7301299.629482,,0,5,0,False,"['vasqu', 'HofitBata', 'HuggingFaceDocBuilderDev', 'ArthurZucker']","Could you also share your environment, i.e. `transformers-cli env`, and a stack trace of your error? Possibly in a separate issue (for bugs) to keep this PR clean.  > Could you also share your environment, i.e. `transformers-cli env`, and a stack trace of your error? Possibly in a separate issue (for bugs) to keep this PR clean.

I can do it after this PR will be merged and so I can add the relevant code to the model card with the updated transformers version. I created an environment to run a full finetune of [Jamba-Mini](https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini) which requires both multi nodes setting and fsdp. Seems like it's working ok, you can see one of the temporary finetuned model here - https://huggingface.co/hofitB/Jamba-1.5-Mini-Full-FT

This is the error I got with the current transformers version:

```
self.dt_proj.bias = time_proj_bias
TypeError: cannot assign 'torch.cuda.BFloat16Tensor' as parameter 'bias' (torch.nn.Parameter or None expected)
``` `transformers-cli env` would help to reproduce issues like this as the versions of the packages might be relevant, e.g. I wasn't aware that it's an FSDP multi-node setting. Some small reproducer script along with it would be ideal.

  > `transformers-cli env` would help to reproduce issues like this as the versions of the packages might be relevant, e.g. I wasn't aware that it's an FSDP multi-node setting. Some small reproducer script along with it would be ideal.

There wasn't a code that wrapped the both using AWS machines so I built one. [AI21-Jamba-1.5-Mini](https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini) is a pretty big model and therefore requires multi nodes and FSDP setting. I uploaded the code to a git repo [hf-finetune-sagemaker](https://github.com/AI21Labs/hf-finetune-sagemaker). You can follow the instructions there in order to run it from you local computer. If you want to reproduce the bug, just change the transformers version (to 4.44.2 for example) in sagemaker_training/requirements.txt I see, thank you! I was mostly interested in the package version ;) LGTM

I guess the `.data` is necessary (small nit) since some convert to tensors (fsdp) instead of nn.parameters? 

Jamba cc @ArthurZucker  > I see, thank you! I was mostly interested in the package version ;) LGTM
> 
> I guess the `.data` is necessary (small nit) since some convert to tensors (fsdp) instead of nn.parameters?
> 
> Jamba cc @ArthurZucker

it's not necessarily relates only to FSDP and as I wrote in the PR description the problem was with the previous None assignment - when we set self.dt_proj.bias = None, it **removed** the bias parameter from the model. When we later tried to assign a tensor to self.dt_proj.bias, it caused a TypeError because PyTorch expects a Parameter object (as it had before the None). In the new change we keep the bias as part of the model network and just temporarily changing its value. I understand the way you resolve this issue and I understand the issue at hand at least according to the description.

The problem is how torch is tracking modules and respective belonging parts of the model: My issue has been that I tried a minimal script to reproduce this scenario which didn't cause this issue; I'll send it later when I have time - in short it imitates the removal of the linear bias the way it has been atm. My assumption is that this only happens on bigger models, FSDP, potentially other different distributed strategies (e.g. deepspeed), or even the version of pytorch (e.g. I used 2.3.0 vs 2.1 in your env). The problem isn't as easy imo, at least the root cause of it. If you know more about the inner mechanism of torch and when it removes these part of the models (as happening here), then I gladly here you out. 

Also, according to your error log, it seems like torch previously saved the bias as tensor (not wrapped around with nn.Parameter), removed the bias from the model, and now expects an nn.Parameter on new initialization. This seems very FSDP specific to me (as they exchange original parameters with tensor views iirc) but I could be wrong. See reference in PyTorch RFC: https://github.com/pytorch/pytorch/issues/76501 (TL;DR / Problem & Motivation)
- FSDP replaces original parameters with a view of tensors.

Regarding my nit: ""I guess the .data is necessary (small nit) since some convert to tensors (fsdp) instead of nn.parameters?""
- Using `.data` is kinda dirty. It's accessing the underlying tensor either way (even if it's nn.Parameter or a pure tensor) so it's no biggie. FSDP could potentially introduce a tensor object where in a normal single GPU case we would most likely encounter nn.Parameter.

Attempt at reproducing the error with torch==2.3.1:
```python
import torch
import torch.nn as nn


class Model(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # We only care about the linear layer
        self.dt_proj = nn.Linear(16, 32, bias=True)

    def forward(self, x, apply_fix_route=False):
        if apply_fix_route:
            # Suggested fix
            time_proj_bias = self.dt_proj.bias.data

            with torch.no_grad():
                self.dt_proj.bias.data = torch.zeros_like(self.dt_proj.bias.data)

            x = self.dt_proj(x)

            with torch.no_grad():
                self.dt_proj.bias.data = time_proj_bias

            # Used to track if any gradients were potentially applied, not used in this script tho
            return self.dt_proj(x[:, :, :16])
        else:
            # Old ""fix"" version

            # type(time_proj_bias) == nn.Parameter
            time_proj_bias = self.dt_proj.bias

            self.dt_proj.bias = None

            x = self.dt_proj(x)

            self.dt_proj.bias = time_proj_bias

            return self.dt_proj(x[:, :, :16])


model = Model().to(""cuda"", dtype=torch.float16)
optimizer = torch.optim.SGD(model.parameters())

for _ in range(3):
    # Arbitrary loss and input, we only care about the training flow and the effect of nn.Linear
    y = model(torch.ones(size=(2,2,16), dtype=torch.float16).to(""cuda""), apply_fix_route=False)
    loss = y.sum() + 5

    loss.backward()
    optimizer.step()

    model.zero_grad()
    optimizer.zero_grad()
```
- According to your description, this should trigger an Error or something similar imo. This is not the case and that's why I digged deeper after the env and general setup to see why and where this issue may arise. > I understand the way you resolve this issue and I understand the issue at hand at least according to the description.
> 
> The problem is how torch is tracking modules and respective belonging parts of the model: My issue has been that I tried a minimal script to reproduce this scenario which didn't cause this issue; I'll send it later when I have time - in short it imitates the removal of the linear bias the way it has been atm. My assumption is that this only happens on bigger models, FSDP, potentially other different distributed strategies (e.g. deepspeed), or even the version of pytorch (e.g. I used 2.3.0 vs 2.1 in your env). The problem isn't as easy imo, at least the root cause of it. If you know more about the inner mechanism of torch and when it removes these part of the models (as happening here), then I gladly here you out.
> 
> Also, according to your error log, it seems like torch previously saved the bias as tensor (not wrapped around with nn.Parameter), removed the bias from the model, and now expects an nn.Parameter on new initialization. This seems very FSDP specific to me (as they exchange original parameters with tensor views iirc) but I could be wrong.

Sorry, I think I misunderstood you. The traceback is in the FSDP wrapper flow of course, what I meant is that it might happens with other distributed methods, I didn't check it. with non-distributed framework it doesn't happen (this is why you didn't see it in your minimal script I guess) but this case is for small models and so not relevant for jamba-mini which is 50B params for example.

As for newer torch version, using the sagemaker package for multi node setting (as in the [hf-finetune-sagemaker](https://github.com/AI21Labs/hf-finetune-sagemaker) repo) I can't use a version which is newer than 2.1.0, it didn't seem to be supported when I tried it.

When the model is loaded using the wrapper all the parameters have type torch.nn.parameter.Parameter or I didn't understand you correctly. No worries, I think we straight up talked past each other :sweat: 

Yea, it could also possibly happen with DeepSpeed ig, I doubt DDP introduces special wrapping as it's merely duplicating across the GPUs but who knows. It would be interesting if this issue also arose on smaller toy models with similar code just to see if it's dependent on the model size. 

As for the torch version, that's alright but it can matter in certain cases.


> When the model is loaded using the wrapper all the parameters have type torch.nn.parameter.Parameter or I didn't understand you correctly. 

I think this is the rather peculiar part but when we look at the code: 
https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src/transformers/models/jamba/modeling_jamba.py#L713-L716
1. `time_proj_bias = self.dt_proj.bias` should assign the nn.Parameter of the bias. 
2. `self.dt_proj.bias = None` removes the bias. 
3. `self.dt_proj.bias = time_proj_bias` reassigns the bias.
But we get the error `TypeError: cannot assign 'torch.cuda.BFloat16Tensor' as parameter 'bias' (torch.nn.Parameter or None expected)`. This means that the bias has been a tensor at the first step. Idk why and where this happens, but it indicates that FSDP might have done something there as it's wrapping the model parameters. Does that clear up what I mean regarding FSDP / nn.Parameter? > No worries, I think we straight up talked past each other 😓
> 
> Yea, it could also possibly happen with DeepSpeed ig, I doubt DDP introduces special wrapping as it's merely duplicating across the GPUs but who knows. It would be interesting if this issue also arose on smaller toy models with similar code just to see if it's dependent on the model size.
> 
> As for the torch version, that's alright but it can matter in certain cases.
> 
> > When the model is loaded using the wrapper all the parameters have type torch.nn.parameter.Parameter or I didn't understand you correctly.
> 
> I think this is the rather peculiar part but when we look at the code:
> 
> https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src/transformers/models/jamba/modeling_jamba.py#L713-L716
> 
> 1. `time_proj_bias = self.dt_proj.bias` should assign the nn.Parameter of the bias.
> 2. `self.dt_proj.bias = None` removes the bias.
> 3. `self.dt_proj.bias = time_proj_bias` reassigns the bias.
>    But we get the error `TypeError: cannot assign 'torch.cuda.BFloat16Tensor' as parameter 'bias' (torch.nn.Parameter or None expected)`. This means that the bias has been a tensor at the first step. Idk why and where this happens, but it indicates that FSDP might have done something there as it's wrapping the model parameters. Does that clear up what I mean regarding FSDP / nn.Parameter?

It's happening with very small models as well as long as you are using fsdp. For example I played also with ai21labs/Jamba-tiny-dev which is a small debug model (for system tests in HF). Moreover, I get this error even when setting fsdp group size to 1, which mean the tensors aren't really splitted between gpus but we do use the fsdp wrapper. 

You wrote the following in an earlier message 
""Also, according to your error log, it seems like torch previously saved the bias as tensor (not wrapped around with nn.Parameter)""
Note that according to what I saw at least, the weights are wrapped with nn.Parameter already after the model is loaded (before training starts) for both fsdp and non-fsdp case.  Thx for trying stuff further out! I'm out of my wits tbh, but something does happen there (maybe just during training which is why the initialization at least keeps it in nn.Parameter). 

Maybe someone on the accelerate team has more insights on this (fsdp) @SunMarc @muellerzr  cc @ArthurZucker  OUps sorry will have a look asap!  IMO it's a hot fix and not a big issue for me to have this in the mean time! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33314). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.LGTM thanks for explaining 🤗 ",LGTM thanks for explaining 🤗 ,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fix for training full finetune of Jamba model (hybrid mamba-transformer model).
The fix was required in the mamba block - when we set self.dt_proj.bias = None, it removed the bias parameter from the model. When we later tried to assign a tensor to self.dt_proj.bias, it caused a TypeError because PyTorch expects a Parameter object.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","Not sure why you would disable the gradients over here. The original code only seems to ignore the bias for the discrete time step.  In general, I rather disable gradient computation wherever I can in order to save memory. Here we remove temporarily the bias, so the grads aren't needed.
This blocks wrapping aren't the main issue but the old None assignment that can't work in **full** training (comparing to lora/inference), I just added it as nice to have/save."
33798,2024-09-29T19:30:30Z,2024-10-02T12:12:03Z,kylesayrs,1,0,1,2,1,5,4,[],146557.0,0,243125.0,0,0,0,0,7362951.448753,,0,1,0,False,['HuggingFaceDocBuilderDev'],The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_33798). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Thanks for the fix !  Thank you 🙏,Thanks for the fix !  Thank you 🙏,"# What does this PR do?
Fixes wrongly passed kwargs in `AutoQuantizationConfig.from_pretrained`
```python3
from transformers.quantizers import AutoQuantizationConfig
q_config = AutoQuantizationConfig.from_pretrained(""nm-testing/tinyllama-w4a16-compressed-hf-quantizer"")
```
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ksayers/kylesayrs_transformers/src/transformers/quantizers/auto.py"", line 111, in from_pretrained
    quantization_config.update(kwargs)
TypeError: QuantizationConfigMixin.update() takes 1 positional argument but 2 were given
```

## Who can review?
@SunMarc @ArthurZucker
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
33828,2024-09-30T19:41:55Z,2024-10-02T12:11:45Z,kylesayrs,0,1,2,3,1,4,3,[],,0,155997.0,0,0,0,0,7362994.475417,,0,2,0,False,[],"These changes look correct to me. It brings up a separate consideration that we'll need at some point in the future, which is to gather the compressed tensors version so we can compare if there are breaking changes between the config format that is stored with the model and the compressed tensors that is loaded, especially with it loading nightly dev builds LGTM ! ","These changes look correct to me. It brings up a separate consideration that we'll need at some point in the future, which is to gather the compressed tensors version so we can compare if there are breaking changes between the config format that is stored with the model and the compressed tensors that is loaded, especially with it loading nightly dev builds LGTM ! ","# What does this PR do?
This PR follows up on #31704 by allowing the `compressed-tensors-nightly` package to be used to load compressed tensors modules. This is done by relaxing installation requirements to only require the `compressed-tensors` module spec, but not the `compressed-tensors` package.

Compressed Tensors models cannot be loaded with nightly package
```python3
from transformers import AutoModel
model = AutoModel.from_pretrained(""nm-testing/TinyLlama-1.1B-Chat-v1.0-W8A8_tensor_weight_static_per_tensor_act-e2e"")
# ImportError: Using `compressed_tensors` quantized models requires the compressed-tensors library: `pip install compressed-tensors`
```

Compressed Tensors is reported as unavailable, despite the module being available through the nightly package
```python3
from transformers.utils.import_utils import is_compressed_tensors_available
is_compressed_tensors_available()
# False
```

After the changes, models can be imported with the nightly package and compressed tensors is reported as being available

## Who can review?
@SunMarc @ArthurZucker
@dsikka @dhuangnm @andy-neuma
Anyone in the community is free to review the PR once the tests have passed.
","Typo, `compressed-tensors-nightly`"
35257,2024-12-13T11:51:52Z,2024-12-27T16:33:44Z,qubvel,3,0,3,13,2,1,1,[],1674.0,0,1226513.0,0,0,0,0,90035.803095,,0,3,0,False,"['qubvel', 'HuggingFaceDocBuilderDev', 'ArthurZucker']","The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35257). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update. @ArthurZucker Do you mean to move it to the `PretrainedModel` level?

```python

class PreTrainedModel:

    _accept_loss_kwargs = None
    
    @property
    def accepts_loss_kwargs(self):
        if self._accept_loss_kwargs is not None:
            return self._accept_loss_kwargs
        # otherwise check for signature here
        ...
        return accept_loss_kwargs
```

And then in modeling:

```python
class MyModel(PreTrainedModel):
    _accept_loss_kwargs = False
``` No for now let's merge as is! Not sure we'll need the abstraction for more models! Okay, I think we might want to document  `accepts_loss_kwargs` as it's scope right now is super small. 
This can't be set to `True` by default, we can just add a property that checks the forward signature, unless `_accept_loss_kwargs` is set to `false`. 
","Okay, I think we might want to document  `accepts_loss_kwargs` as it's scope right now is super small. 
This can't be set to `True` by default, we can just add a property that checks the forward signature, unless `_accept_loss_kwargs` is set to `false`. 
","# What does this PR do?

`TimmWrapper` model training is broken due to the `num_items_in_batch` param passed to the model forward. `TimmWrapper` model has `**kwargs` in the `transformers` model forward, but it goes straight to the `timm` model and not to the loss. This PR introduces a fix to avoid passing loss_kwargs even if `**kwargs` are in model forward.

Relevant to:
 - https://github.com/huggingface/transformers/pull/35121
 - https://github.com/huggingface/transformers/pull/34915

cc @ArthurZucker ",
35189,2024-12-10T20:37:56Z,2024-12-27T12:21:44Z,KSafran,1,0,4,2,1,3,3,[],23160.0,0,1439028.0,0,0,0,0,105157.277118,,0,4,0,False,['HuggingFaceDocBuilderDev'],"The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35189). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.Nice find, thanks!

cc @Rocketknight1  Thanks !  Thanks!","Nice find, thanks!

cc @Rocketknight1  Thanks !  Thanks!","# Fix f-string to show `ACCELERATE_MIN_VERSION` on error

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

If `accelerate` is not installed a helpful warning message shows the `ACCELERATE_MIN_VERSION` required - but the f-string was missing so the package version wasn't displayed. This minor PR fixes that.


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

Minor typo, no new tests.

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
 @muellerzr and @SunMarc for simple review",
35384,2024-12-21T08:21:10Z,2024-12-27T12:12:32Z,gau-nernst,0,0,3,2,1,1,1,[],,0,532792.0,0,0,0,0,105200.845682,,0,3,0,False,[],Thank you!,Thank you!,"# What does this PR do?

Change fairseq to OpenAI in the CLIP conversion script

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->

@ArthurZucker",
35289,2024-12-16T07:51:44Z,2024-12-27T12:07:31Z,ningyuv,2,0,1,6,3,3,2,[],21459.0,0,965747.0,0,0,0,0,106012.408719,,0,1,0,False,"['HuggingFaceDocBuilderDev', 'Rocketknight1']",cc @nielsrogge since (I think?) you worked on those models! The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers/pr_35289). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.keyword argument name in [SwinConfig](https://github.com/huggingface/transformers/blob/75be5a0a5b1898ee86e5e0c1f7b58b77bb105101/src/transformers/models/swin/configuration_swin.py#L115) is `num_channels` Thanks for fixing! Thank you!,keyword argument name in [SwinConfig](https://github.com/huggingface/transformers/blob/75be5a0a5b1898ee86e5e0c1f7b58b77bb105101/src/transformers/models/swin/configuration_swin.py#L115) is `num_channels` Thanks for fixing! Thank you!,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes keyword argument in_channels to num_channels


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",
